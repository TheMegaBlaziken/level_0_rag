# LAC: GRAPH CONTRASTIVE LEARNING WITH LEARNABLE AUGMENTATION IN CONTINUOUS SPACE <sup>∗</sup>

Zhenyu Lin, Hongzheng Li, Yingxia Shao, Guanhua Ye, Yawen Li, and Quanqing Xu School of Computer Science Beijing University of Posts and Telecommunications Haidian, Beijing, 100088.

{linzy, Ethan Lee, shaoyx, g.ye}@bupt.edu.cn, warmly0716@126.com, xuquanqing.xqq@oceanbase.com

#### Abstract

Graph Contrastive Learning frameworks have demonstrated success in generating high-quality node representations. The existing research on efficient data augmentation methods and ideal pretext tasks for graph contrastive learning remains limited, resulting in suboptimal node representation in the unsupervised setting. In this paper, we introduce LAC, a graph contrastive learning framework with learnable data augmentation in an orthogonal continuous space. To capture the representative information in the graph data during augmentation, we introduce a continuous view augmenter, that applies both a masked topology augmentation module and a cross-channel feature augmentation module to adaptively augment the topological information and the feature information within an orthogonal continuous space, respectively. The orthogonal nature of continuous space ensures that the augmentation process avoids dimension collapse. To enhance the effectiveness of pretext tasks, we propose an information-theoretic principle named InfoBal and introduce corresponding pretext tasks. These tasks enable the continuous view augmenter to maintain consistency in the representative information across views while maximizing diversity between views, and allow the encoder to fully utilize the representative information in the unsupervised setting. Our experimental results show that LAC significantly outperforms the state-of-the-art frameworks.

Keywords Graph Contrastive Learning · Data Augmentation · Information Theory.

# 1 Introduction

Graph Contrastive Learning (GCL) [\[1,](#page-13-0) [2\]](#page-13-1) enhances generalization performance by leveraging multiple augmented views to learn latent information in graph data. GCL serves as a powerful tool to address challenges associated with label sparsity and unlabeled data. It alleviates the expensive data labeling cost while significantly enhancing the generalization capabilities of graph neural network models across diverse downstream tasks.

Typically, GCL frameworks typically consist of pretext tasks, view augmenters, and encoders. The pretext tasks guide the view augmenter and encoder to utilize representative information related to downstream tasks. The view augmenters transform input graphs into multiple correlated augmented views [\[3\]](#page-13-2). The encoders get representations from different augmented views. The augmenters and encoders mutually influence each other [\[4\]](#page-13-3). Specifically, high-quality augmented views contribute to the training of more powerful and generalized encoders [\[5,](#page-13-4) [6,](#page-13-5) [7\]](#page-13-6), and the quality of encoders assists in learning effective augmenters as well.

GCL plays a pivotal role in various domains, including emotion recognition [\[8,](#page-13-7) [9\]](#page-13-8), multi-modal recommendation systems [\[10,](#page-13-9) [11,](#page-13-10) [12\]](#page-13-11), and anomaly detection [\[13,](#page-13-12) [14,](#page-13-13) [15\]](#page-13-14). Despite these capabilities, existing GCL frameworks still encounter two significant challenges in the unsupervised setting:

Existing augmentation methods are not sufficient. View augmenters in GCL frameworks fall into two categories: manual augmentation and learnable augmentation. The manual augmentation [\[5,](#page-13-4) [7,](#page-13-6) [16,](#page-13-15) [17,](#page-13-16) [18,](#page-13-17) [19,](#page-13-18) [20,](#page-13-19) [21\]](#page-13-20) involves selecting augmenters and their hyperparameters empirically from predefined options through numerous trial-and-error experiments per dataset [\[6\]](#page-13-5). The learnable augmentation [\[4,](#page-13-3) [22,](#page-14-0) [3,](#page-13-2) [23,](#page-14-1) [24,](#page-14-2) [25,](#page-14-3) [26,](#page-14-4) [27,](#page-14-5) [28\]](#page-14-6) automatically learns from data and generates augmented views. Although this approach avoids the laborious [\[23\]](#page-14-1) and timeconsuming [\[25\]](#page-14-3) process of numerous trial-and-error experiments, it augments the original graph data discretely, yielding non-ideal augmented views. This non-ideal situation lies in both topology augmentation and feature augmentation. Firstly, the discrete perturbation of topological information leads to non-ideal views. Even the slightest perturbation to the topology such as removing an edge or dropping a node can destroy the representative information

[\[3,](#page-13-2) [25\]](#page-14-3) in the graph. For example, in chemical molecular or atomic classification tasks, where key chemical bonds are representative information [\[28\]](#page-14-6), random removal of these edges produces a low-quality view. Secondly, the discrete perturbation of continuous feature information also leads to non-ideal views. Initially, node features are represented in a continuous high-dimensional space, with each dimension containing rich information. However, existing works adopt a discrete approach to feature augmentation. For example, some work [\[6,](#page-13-5) [3,](#page-13-2) [26\]](#page-14-4) randomly mask the specific dimension of all node features. The discrete augmentation for features also destroys the representative information in the graph data and may cause dimensional collapse [\[29,](#page-14-7) [30\]](#page-14-8).

Pretext tasks are not effective. Existing studies [\[30,](#page-14-8) [3,](#page-13-2) [22\]](#page-14-0) emphasize that an effective pretext task should maintain consistency in the representative information across multiple augmented views, while ensuring the diversity of these views to prevent the model from generating overly similar views and embeddings [\[30\]](#page-14-8). Suppose there is a graph-based multi-modal recommendation task [\[10,](#page-13-9) [11,](#page-13-10) [12\]](#page-13-11). The multimodal features of items are extracted from their contents, which include visual and textual information. For example, the visual representative information that effectively distinguishes items may be about the shape and color of the item, rather than other attributes such as the visual background. The textual representative information of items may be brand and title instead of size information. When applying GCL algorithms, an effective pretext task must keep the different augmented views consistent with representative information about the item's shape, color, brand and title, while maximizing diversity by including as many different backgrounds and sizes as possible to improve the generalization ability of the model. If the textual and visual features contain the same semantic information (i.e. text description about the item's visual color), the consistency of crossmodal representative information should be maintained as well. However, several studies based on the InfoMin principle [\[22,](#page-14-0) [3,](#page-13-2) [23,](#page-14-1) [25\]](#page-14-3) neglect consistency constraints, leading to a loss of representative information during graph data augmentation [\[22\]](#page-14-0). Other research [\[24\]](#page-14-2) ignores diversity constraints, producing overly similar augmented views that result in shortcut solutions. This ultimately impairs the performance of GCL frameworks [\[22,](#page-14-0) [31\]](#page-14-9).

To overcome the above two challenges, we propose LAC, a novel Graph Contrastive Learning framework featuring Learnable Augmentation in Continuous space. To address the first challenge, we propose a learnable Continuous View augmenter (CVA) in LAC, which identifies an orthogonal continuous Space by spectral theorem [\[32\]](#page-14-10) and augments topological and feature information in this space. We propose a Masked Topology Augmentation (MTA) module and a Cross-channel Feature Augmentation (CFA) module in CVA to search for appropriate augmented information. To address the second challenge, we introduce a principle named Information Balance (InfoBal) for both the augmenter and the encoder in LAC. InfoBal's two

sub-principles ensure consistency in the representative information among augmented views and while maximizing diversity. Furthermore, we introduce two pretext tasks derived from the two sub-principles for the training of augmenter and encoder. Experiments are carried out on seven public datasets to validate the effectiveness of LAC compared to state-of-the-art (SoTA) methods. In summary, our contributions are as follows:

- We propose a graph contrastive learning framework called LAC, which includes a learnable continuous augmentation method and effective pretext tasks.
- We design CVA, a learnable augmentation module that augments graph data in orthogonal continuous space to generate more ideal augmented views and avoid dimensional collapse.
- We introduce the InfoBal principle and design two pretext tasks based on it to guide view augmenters in maximizing diversity and ensuring consistency of representative information across views, while helping encoders fully utilize representative information.
- Experimental results on seven sparse datasets show that LAC outperforms SoTA GCL frameworks in an unsupervised setting.

# 2 Preliminaries

## 2.1 Notions

An undirected and connected graph is denoted as G = (V, E), where V represents the node set and E ⊂ V × V represents the edge set. The topological adjacency matrix is A ∈ R <sup>N</sup>×<sup>N</sup> , the node feature matrix is X ∈ R N×d , and x<sup>i</sup> denotes the feature of node i, ∀i ∈ V. The normalized Laplacian matrix of the graph G is given as L˜ = I − D˜ <sup>−</sup>1/2A˜ D˜ <sup>−</sup>1/<sup>2</sup> , where D˜ denotes the degree matrix of the normalized adjacency matrix A˜ . In contrastive learning, each graph can also be denoted as view V.

# 2.2 Graph Contrastive Learning

The objective of graph contrastive learning is to distinguish between latent representative information in the data [\[33\]](#page-14-11) and to learn embeddings for downstream tasks. View augmenters can either be manually selected from predefined pools [\[5,](#page-13-4) [7,](#page-13-6) [18,](#page-13-17) [17,](#page-13-16) [19,](#page-13-18) [20,](#page-13-19) [21\]](#page-13-20) or designed as learnable and generative [\[4,](#page-13-3) [6,](#page-13-5) [22,](#page-14-0) [3,](#page-13-2) [23,](#page-14-1) [24,](#page-14-2) [25,](#page-14-3) [26\]](#page-14-4). The augmenters generate one or multiple augmented views. For example, given original data, data augmenters g1, g2, original data view V , the augmented view V ′ can be denoted as:

$$
V' = (A', X') = g(V) = g((A, X)).
$$
 (1)

Subsequently, based on the positive and negative sample pairs defined across multiple views, GCL frameworks train the encoders to obtain embeddings for downstream tasks. We denote encoders as f1, f2, InfoNCE loss [\[34\]](#page-14-12) as I. The unsupervised GCL framework often uses pretext tasks based on the InfoMax [\[35\]](#page-14-13) and InfoMin [\[36\]](#page-14-14) principle to train encoders and augmenters respectively, which can be expressed as:

$$
\min I(f_1(g_1(V)), f_2(g_2(V))), fix f_1, f_2,
$$
  
s.t. 
$$
\max I(f_1(g_1(V)), f_2(g_2(V))), fix g_1, g_2.
$$
 (2)

#### 2.3 Theorems

In linear algebra, the spectral theorem [\[32\]](#page-14-10) establishes a standard framework for decomposition vector space and gives the condition that a matrix can be diagonalized.

<span id="page-2-1"></span>Theorem 1. *(Spectral Theorem). Let P be a symmetric matrix on* R <sup>n</sup>×n*, then P can be decomposed into:*

$$
\mathbf{P} = \mathbf{U}\mathbf{\Lambda}\mathbf{U}^T,\tag{3}
$$

*where* U ∈ R <sup>n</sup>×<sup>n</sup> *is a set of orthogonal eigenvectors of* P*,* Λ ∈ R <sup>n</sup>×<sup>n</sup> *is the corresponding diagonal matrix of eigenvalues, and* U<sup>T</sup> *is the transposed matrix of* U*, satisfying:*

$$
UU^T = I_n,\t\t(4)
$$

*where* I<sup>n</sup> ∈ R <sup>n</sup>×<sup>n</sup> *is an identity matrix.*

In approximation theory [\[32\]](#page-14-10), an arbitrary complicated multivariate function can be approximated by a series of simple univariate functions.

<span id="page-2-2"></span>Theorem 2. *(Kolmogorov–Arnold Theorem) [\[37\]](#page-14-15). An arbitrary multivariate function* f : [0, 1]<sup>N</sup> → R(N > 0) *can be modeled using the following expression :*

$$
f(x_1, ..., x_N) = \rho(\sum_{p=1}^N \alpha_{i,p} \phi(x_p)),
$$
 (5)

*where* ρ : R <sup>d</sup> → R*,* ϕ : R → R d , d > 0 *and* αi,p *is the weight.*

<span id="page-2-0"></span>Theorem 3. *(Information Inequality). The upper bound of mutual information between the representations of the augmented view* V ′ *and original view* V *is given by:*

$$
I(f(V); f(V')) \le \min\{I(V; V'), I(V'; f(V')), I(V; f(V'))\}
$$
(6)

*where* f *is an information encoder.*

*Proof.* We have a Markov chain: V → V ′ = g(V ) → f(g(V )), the first arrow means augmentation, and the second arrow denotes information encoding. According to the data process inequality [\[38\]](#page-14-16), we infer that:

I(V ; f(V ′ )) ≤ min{I(V ; V ′ ), I(V ′ ; f(V ′ )}. (7) Meanwhile, there is another Markov chain: f(V ) ← V → f(V ′ ), which is Markov equivalent to f(V ) → V → f(V ′ ) since f(V ) and f(V ′ ) are conditionally independent after observing V . Thus, the following equation holds:

$$
I(f(V); f(V')) \le \min\{I(V; f(V')), I(V; f(V))\} \le \lim_{\substack{\longleftarrow \\ \longleftarrow}} \{I(V; V'), I(V'; f(V')), I(V; f(V))\}.\tag{8}
$$

Above all, Theorem [3](#page-2-0) is proven.

# 3 LAC Framework Overview

Figure [1](#page-3-0) provides an overview of LAC, which includes the Continuous View Augmenter, the Shared Information Encoder, and the pretext tasks built on the InfoBal.

Continuous View Augmenter (CVA). The CVA augments both topological and feature information in an orthogonal continuous space, generating augmented views accordingly. In CVA, we propose the Masked Topology Augmentation module for topology augmentation and the Cross-channel Feature Augmentation module for feature augmentation, respectively. The continuous augmentation method employed by the CVA minimizes information loss typically associated with discrete augmentation methods, thereby enhancing suitability for graph data. The detailed design of CVA is presented in Section [4.](#page-3-1)

Shared Information Encoder. The shared information encoder extracts information from the original view and the augmented view to learn the node embeddings. The encoder f comprises a K-layer Graph Neural Network (GNN). The shared information encoder is a common and widely adopted technique in existing works [\[19,](#page-13-18) [18\]](#page-13-17). The embeddings Z for nodes in view V and the embeddings of nodes in augmented views Z ′ are yielded by shared information encoder f:

$$
Z = f(V) = f(A, X), Z' = f(V') = f(A', X'). (9)
$$

InfoBal Pretext Task. To achieve high-quality node representations in unsupervised scenarios, the InfoBal pretext task is proposed. The InfoBal framework comprises two components. Firstly, it introduces diversity, and consistency constraints for the CVA, and guides the CVA to generate appropriate augmented views with adequate augmentation variances while preserving representative information. Secondly, InfoBal imposes a sufficiency constraint on the training of the shared encoder. This helps the shared encoder to extract more representative information from views. In contrast to InfoMin [\[36\]](#page-14-14), which is typically employed for training augmenters, InfoBal emphasizes the balance of consistency and diversity among augmented views. As opposed to InfoMax [\[35\]](#page-14-13), traditionally used for encoder training, InfoBal boosts the sufficiency of utilizing representative information in augmented views. Thus, InfoBal allows the model to generate high-quality node representations in the unsupervised scenario. The detailed design of InfoBal is presented in Section [4.](#page-3-1)

In the InfoBal pretext tasks, we use InfoNCE [\[34\]](#page-14-12) to measure the mutual information between two views. We regard the embeddings of same node in the original view V and augmented view V ′ as positive pairs (e.g., (z<sup>i</sup> , z ′ i | i ∈ V)). On the other hand, we consider the embeddings of different nodes in the two views as negative pairs (e.g., (z<sup>i</sup> , z ′ k | i, k ∈ V, i ̸= k)). We define the mutual information extracted from two views by encoder as follows:

$$
I(f(V); f(V')) = \frac{1}{2N} \sum_{i=1}^{N} \log \frac{e^{s(\mathbf{z}_i, \mathbf{z}'_i)/\tau}}{\sum_{k \in \mathcal{V}} e^{s(\mathbf{z}_i, \mathbf{z}'_k)/\tau}}. (10)
$$

<span id="page-3-0"></span>![](_page_3_Figure_1.jpeg)

**Caption:** Figure 1 illustrates the LAC framework, highlighting the Continuous View Augmenter (CVA) and Shared Information Encoder. The CVA employs Masked Topology and Cross-channel Feature Augmentation to generate diverse augmented views while preserving representative information, enhancing the encoder's performance in unsupervised settings.

Figure 1: The overview of LAC.

The similarity between two embeddings of nodes is measured by the cosine function s(·). The temperature parameter τ controls the sharpness of the similarity scores.

Finally, the augmenter and encoder are trained against each other, and the loss function is expressed as follows:

$$
\min_{g} I_{diversity} + \alpha I_{consistency}.
$$
  
s.t. 
$$
\max_{f} I_{sufficiency}.
$$
 (11)

# <span id="page-3-1"></span>4 Continuous View Augmenter

In this section, we introduce the Continous View Augmenter (CVA). It first transforms the original graph data into the representations in an orthogonal continuous space represented by the bases U. On basis of the continous representation, CVA applies Masked Topology Augmentation (MTA) and Cross-channel Feature Augmentation (CFA) to generate augmented topology and feature of the graph data, respectively. Finally, CVA transforms the augmented results back to the original discrete space to get the final augmented views.

#### 4.1 Representation in Orthogonal Continuous Space

To represent information in a continuous space, establishing a coordinate system with orthogonal vector bases is critical. This approach allows topological data A and feature data X to be expressed as linear combinations of these bases, preserving data integrity and preventing dimensional collapse during perturbations.

Topology representation. The undirected graph topological information is denoted as a symmetric and real matrix

A. According to the Theorem [1,](#page-2-1) A can be decomposed into a set of orthogonal vector bases U ∈ R <sup>N</sup>×<sup>N</sup> and a diagonal eigenvalue matrix Λ as follows:

$$
\mathbf{A} = \mathbf{U}\Lambda\mathbf{U}^{\mathbf{T}}, \mathbf{U}^{\mathbf{T}}\mathbf{U} = I_N, \tag{12}
$$

where the vector bases U are the *orthogonal continuous space*. The representation of topological information A in the continuous space associated with U is U<sup>T</sup> A according to the definition of linear mapping [\[32\]](#page-14-10).

Augmenting topological information in a continuous space presents a significant challenge due to the high computational complexity. The standard approach results in a complexity of O(n 2 ). To address this issue, we utilize a key characteristic of the matrix product U<sup>T</sup> A to significantly reduce the computational overhead during the augmentation process. We express it as follows:

$$
\mathbf{U}^T \mathbf{A} = \mathbf{U}^T \mathbf{U} \mathbf{\Lambda} \mathbf{U}^T = \mathbf{\Lambda} \mathbf{U}^T.
$$
 (13)

Considering that the basis of continuous space U is fixed, this insight enables us to perturb Λ rather than U<sup>T</sup> A. This alteration simplifies the augmentation process, and reduces the computational complexity from O(n 2 ) to O(n), marking a substantial improvement.

Feature representation. The representation of X in the orthogonal continuous space is denoted as U<sup>T</sup> X. However, U<sup>T</sup> X does not have a property similar to U<sup>T</sup> A, and we cannot simplify the perturbation of U<sup>T</sup> A. For convenience, we use C to denote the representation of feature information in continuous space as follows:

$$
\mathbf{C} = \mathbf{U}^T \mathbf{X}.\tag{14}
$$

## 4.2 Masked Topology Augmentation

MTA augments the topological information represented by the Λ matrix in continuous space. It's worth noting

<span id="page-4-0"></span>![](_page_4_Figure_1.jpeg)

**Caption:** Figure 2 depicts the structure of the Masked Topology Augmentation (MTA) module within the CVA. It shows how eigenvalues are perturbed collectively to maintain topological integrity, ensuring effective augmentation while minimizing computational complexity.

Figure 2: The structure of MTA.

that the topological information Λ exhibits two unique properties among the eigenvalues on its diagonal: (1) The eigenvalues λ<sup>i</sup> , ∀i ∈ [1, N] along the diagonal of the Λ matrix are within the range of [-1,1], each containing crucial topological information; (2) The difference between adjacent eigenvalues ∆λ<sup>i</sup> = λi+1 − λ<sup>i</sup> encapsulates key topological insights [\[39\]](#page-14-17).

On the basis of the above two properties, it is crucial to perturb both eigenvalues and their interdependencies collectively when augmenting topological information. The relationship between eigenvalues satisfies a multivariate function. According to the Theorem [2,](#page-2-2) each augmented λ ′ i (i ∈ [1, N]) can be denoted by a multivariate function ψi :

$$
\lambda_i' = \psi_i(\lambda_1, ..., \lambda_N) = \rho(\sum_{p=1}^N \alpha_{i,p} \phi(\lambda_p)), \quad (15)
$$

where ρ : R <sup>d</sup> → R, ϕ : R → R d , αi,p represents the weight of ϕ(λp) for d > 0.

In this paper, we introduce a transformer-based MTA module to implement these functions ψ<sup>i</sup> . Figure [2](#page-4-0) illustrates that MTA comprises an eigen embedder, an encoder, and a decoder. As stated in Theorem [4](#page-5-0) from Section [4.4,](#page-5-1) MTA can be trained to approximate any multivariate function.

Eigen Embedder. The eigen embedder ϕ maps each λ<sup>i</sup> to a d-dimensional embedding ϕ(λi). The j-th value of the embedding ϕ(λi) is:

$$
\phi(\lambda_i)_j = \begin{cases} \lambda_i, & j = 1, \\ \sin(\frac{\epsilon \lambda_i}{N^{2k/d}}), & j = 2k \\ \cos(\frac{\epsilon \lambda_i}{N^{2k/d}}), & j = 2k + 1 \end{cases}
$$
 (16)

where ϵ is a hyperparameter, and k is the floor function applied to j/2. This type of embedding represents the differences between eigenvalues in a way similar to the positional embedding [\[39\]](#page-14-17) which is usually used in transformer architectures. Consequently, we get the initial high dimensional embeddings of Λ:

$$
\mathbf{Z} = [\phi(\lambda_1), ..., \phi(\lambda_N)]^T \in \mathbb{R}^{N \times d}.
$$
 (17)

Encoder. The encoder is designed for capturing the underlying relationship among the ϕ(λi), i ∈ [1, N]. Each encoder layer consists of two main components: a multi-head self-attention (MHA) module with residual connections and a feedforward network (FFN) with residual connections. Layer normalization (LN) is applied before feeding the representation into each module [\[40\]](#page-14-18). Formally, each embedded vector ϕ(λi) treated as a concatenation of H vectors across different heads:

$$
\mathbf{Z}_{i} = \phi(\lambda_{i}) = ||(\phi_{1}(\lambda_{i}), ..., \phi_{H}(\lambda_{i})), i \in [1, H], \quad (18)
$$

where ∥ means concatenation operation, ϕ<sup>i</sup> : R → R d/H, i ∈ [1, H]. In other words, we divide whole Z into H heads. The calculation of each head is as follows:

$$
\tilde{\mathbf{Z}}_h^l = MHA(LN(\hat{\mathbf{Z}}_h^{l-1})) + \hat{\mathbf{Z}}_h^{l-1}, \hat{\mathbf{Z}}_h^0 = \mathbf{Z}_h, \qquad (19)
$$

$$
\hat{\mathbf{Z}}_h^l = FFN(LN(\tilde{\mathbf{Z}}_h^l)) + \tilde{\mathbf{Z}}_h^l. \tag{20}
$$

<span id="page-4-1"></span>
$$
\hat{\mathbf{Z}}_h^L = [\sum_{p=1}^N \alpha_{1,p}^h \phi_h(\lambda_p), ..., \sum_{p=1}^N \alpha_{N,p}^h \phi_h(\lambda_N)]^T, \quad (21)
$$

where α h i,p means the weight of ϕh(λi) and ϕh(λp). The Eq. [\(21\)](#page-4-1) illustrates that the function of the encoder of MTA is to calculate weighted aggregation of Z.

Masked Multi-head Decoder. The decoder ρ decodes the high dimensional topological information into the augmented diagonal matrix Λ ′ . The output Zˆ<sup>L</sup> h from the encoder is then decoded by the multi-head MLP, resulting in the h-th perturbed eigenvalues matrix Λ ′ h :

$$
\Lambda_h' = diag(\sigma(\hat{\mathbf{Z}}_h^L \mathbf{W}^h)),\tag{22}
$$

where σ represents the activation function and W<sup>h</sup> ∈ R d/H×1 . The augmented matrix Λ ′ is derived by performing mean pooling across the eigenvalues matrices obtained from attention heads:

$$
\Lambda^{'} = Mean(\Lambda^{'}_h | \forall h \in [1, H]). \tag{23}
$$

To enhance the quality of augmentation outcomes, we introduce an adjustable diagonal mask matrix M = diag(m1, ..., m<sup>N</sup> ) ∈ R <sup>N</sup>×<sup>N</sup> used to mask a specific range of eigenvalues on the Λ ′ matrix. The ultimate Λ ′ is de-

<span id="page-5-2"></span>![](_page_5_Figure_1.jpeg)

**Caption:** Figure 3 presents the projection of node features in the orthogonal continuous space. It illustrates how the distribution of projected values across channels can lead to dimensional collapse, emphasizing the need for effective feature augmentation strategies like Cross-channel Feature Augmentation.

Figure 3: The projection of node features.

noted as follows:

$$
\Lambda' = (\mathbf{I} - \mathbf{M}) \odot \Lambda + \mathbf{M} \odot \Lambda'
$$
\n
$$
= diag((1 - m_1)\lambda_1 + m_1 * \sigma(\frac{1}{H} \sum_{h=1}^{H} \sum_{p=1}^{N} \alpha_{1,p}^h \phi_h(\lambda_1) W_h),
$$
\n
$$
\dots, (1 - m_N)\lambda_N + m_N * \sigma(\frac{1}{H} \sum_{h=1}^{H} \sum_{p=1}^{N} \alpha_{N,p}^h \phi_h(\lambda_N) W_h)),
$$
\n
$$
= diag(\rho(\sum_{p=1}^{N} \alpha_{1,p} \phi(\lambda_p)), \dots, \rho(\sum_{p=1}^{N} \alpha_{N,p} \phi(\lambda_p))).
$$
\n
$$
= diag(\sum_{p=1}^{M} \alpha_{1,p} \phi(\lambda_p)), \dots, \rho(\sum_{p=1}^{N} \alpha_{N,p} \phi(\lambda_p))).
$$
\n
$$
(24)
$$

Here, I ∈ R <sup>N</sup>×<sup>N</sup> denotes the identity matrix, and ⊙ indicates element-wise multiplication between matrices.

#### 4.3 Cross Channel Feature Augmentation

The continuous representation of feature information C = U<sup>T</sup> X is modeled as the coordinate coefficient of X in the orthogonal continuous space represented by U, as depicted in Figure [3.](#page-5-2) Specifically, ci,j represents the projected value of the j-th channel graph signal x:,j onto basis u<sup>i</sup> . Each column of C delineates the distribution of projected values for a channel of the graph signal across various bases.

Randomly masking specific node features or feature dimensions can result in *dimensional collapse* [\[41,](#page-14-19) [29\]](#page-14-7). To analyze this phenomenon, we provide empirical results on the Cora. As shown in Figure [5,](#page-12-0) the blue line represents the coefficients C, which exhibits a long-tail distribution. The imbalance of the distribution of projected values causes a few channels to dominate the feature information in the continuous space [\[29\]](#page-14-7). For example, the projected values across certain channels in C are very small, while the projected values across other channels are very large. Consequently, these former channels become negligible in their capacity to describe information compared to the latter channels, leading to their invalidation. This causes a phenomenon that the high-dimensional representations of node features in continuous space collapse [\[41\]](#page-14-19) into low-dimensional representations, which makes it difficult to distinguish between different nodes [\[29\]](#page-14-7).

To avoid the dimensional collapse problem, cross channel feature augmentation (CFA) is introduced. This method adaptively perturbs C using a cross-channel convolution g<sup>F</sup> , which automatically captures the relationships of projected values across different channels and generates properly augmented features with balanced distributions. We begin by reshaping C ∈ R N×d into the N × d × 1 matrix as the input of CFA. Thus the shape of embeddings for each node is d × 1. We apply a convolution kernel K ∈ R d×d×1 for each node embedding through the broadcast mechanism, producing an output of d × d × 1. This output represents a vector with d channels, where each channel is a d × 1 vector. Summing along the second dimension gives a new embedding of the node after merging different channel information, the shape is still d × 1. A single convolution kernel K is shared among all nodes. Finally, the augmented results of each node are concatenated to obtain C′ ∈ R N×d , where the element C ′ i,l is denoted as follows:

$$
\mathbf{C}'_{i,l} = g_F(\mathbf{C})_{i,l} = \sum_{j=1}^{D} C_{(i,j)} K_{(l,j,1)}.
$$
 (25)

The existing random feature masking method is the special case of the proposed CFA (refer to Theorem [4](#page-5-0) in Section [4.4](#page-5-1) for details). In contrast to existing continuous feature perturbation methods [\[42,](#page-14-20) [29\]](#page-14-7), CFA facilitates adaptive augmentation across diverse datasets, thereby reducing dependence on trial-and-error experimentation. Ultimately, it enables CVA to adaptively generate more optimal node features for the augmented views.

#### <span id="page-5-1"></span>4.4 Discussion of CVA

The Λ ′ and C ′ generated by the MTA and CFA cannot be directly applied in the GNN-based information encoder. We convert Λ ′ and C ′ from the continuous space back to the original space to obtain the final augmented view V ′ as follows:

$$
V' = (\mathbf{A}', \mathbf{X}') = (\mathbf{U}\mathbf{\Lambda}'\mathbf{U}^T, \mathbf{U}\mathbf{C}').
$$
 (26)

Such augmented view can be utilized across various types of encoders.

## 4.4.1 Relationship with the Augmentation in the Original Space

We next analyze the connection between CVA and view augmentation in the original space. In the original space, topological perturbations generate edges in A with integer weights of 0 or 1, while discrete feature perturbations mask random dimensions in X to zeros. Every discrete augmentation in the original space has a corresponding augmentation in our proposed continuous space, as proven by Theorem [4.](#page-5-0)

<span id="page-5-0"></span>Theorem 4. *Any topological augmentation in discrete space and augmentation of discretized features has a corresponding augmentation on the topological and feature information in continuous space of LAC.*

*Proof.* According to theoretical justification in [\[28\]](#page-14-6), the perturbation of Λ by flipping the edge between nodes p and q is given by the sum of all ∆λ<sup>i</sup> = λ ′ <sup>i</sup> − λ<sup>i</sup> :

$$
\sum_{i=1}^{N} |\Delta \lambda_i| = \sum_{i=1}^{N} |2u_{i,p}u_{i,q} - \lambda_i (u_{i,p}^2 + u_{i,q}^2)|. \tag{27}
$$

Consequently, perturbing any edge in the discrete space corresponds to a specific perturbation of the Λ matrix. Moreover, removing a node is equivalent to eliminating all edges associated with that node.

The overall perturbation on Λ is obtained by directly summing up all the perturbations of single edges on Λ. For example, when removing node s, the perturbation to Λ is denoted as follows:

$$
\sum_{t}^{N_s} \sum_{i=1}^{N} |\Delta \lambda_i| = \sum_{t}^{N_s} \sum_{i=1}^{N} |2u_{i,s}u_{i,t} - \lambda_i(u_{i,s}^2 + u_{i,t}^2)|,
$$
\n(28)

where N<sup>s</sup> is the neighborhood of s. Thus, any perturbation of the topological structure in discrete space corresponds to a specific perturbation of Λ in continuous space of LAC.

On the other hand, the random feature masking of node features X is equivalent to setting all the values in a dimension to zero. Specifically, if GCL frameworks mask the l-th dimension of features X, it results in a matrix X ′ :l where the values in the l-th column are all set to 0. Since X ′ :<sup>l</sup> = (UC′ ):<sup>l</sup> = 0, we conclude that the values in the l-th column of C ′ are zero. CFA can achieve this by learning an appropriate set of convolutional kernel coefficients:

$$
\mathbf{C}'_{i,l} = \sum_{j=1}^{D} C_{(i,j)} K_{(i,l,j)} = 0, \forall i \in [1, N]. \tag{29}
$$

This demonstrates that any discretized feature perturbation is equivalent to perturbing C in the space introduced in LAC.

Therefore, any discrete augmentation, whether of topological structure or node features, has a corresponding representation in continuous space. Theorem [4](#page-5-0) is proven.

#### 4.4.2 Complexity Analysis

Time Complexity. The total time cost consists of preprocessing time and training time. First, the preprocessing time is the time taken to decompose the A and X matrices. As CVA directly augments Λ and C, the matrix decomposition is required only once during the preprocessing stage, rendering the preprocessing time effectively constant. Second, the training time overhead includes two parts. The first part comes from the computation of MTA and CFA in CVA. The encoder and decoder in MTA have O(24ND<sup>2</sup> + 4DN<sup>2</sup> ) and O(6NH) time complexity, respectively. The time complexity of a CFA is O(N d<sup>2</sup> ). D is the hidden dimension, d is the feature dimension, H is the number of heads, and they are much small compared to the datasize N. Therefore, the total time complexity

of the first part is O(N<sup>2</sup> ). The second part of training time overhead comes from GNN encoding and contrastive learning, which are O(N<sup>2</sup> ).

Space Complexity. The space overhead of LAC comes from two aspects. One is the storage overhead of Λ, U, and C, which are O(N<sup>2</sup> ). The other is the parameter storage overhead brought by the CVA and GNN modules, which is independent of the dataset size. The parameters in MTA are O(3LHD<sup>2</sup> + 2LD<sup>2</sup> + HD). The parameters in CFA are O(d 2 ). L, H is the number of layers of the encoder, the number of heads in MTA, which is usually 1 or 2. D is the hidden dimension of MTA, which is 128 or 256 in general. The space complexity of GNN is O(dD + D<sup>2</sup> ). Therefore, the total space complexity is O(N<sup>2</sup> ).

# 5 The Pretext Tasks Based on InfoBal

In this section, we introduce a universal principle known as InfoBal. According to its sub-principles, we design two specific pretext tasks for the continuous view augmenter and the shared information encoder.

### 5.1 InfoBal Principle

The InfoBal principles enable the GCL framework to maintain consistency of representative information across multiple augmented views, while ensuring diversity, thereby facilitating efficient extraction of representative embeddings by the encoder. Specifically, InfoBal adheres to two sub-principles that guide the augmenter and the encoder. The objectives of the two sub-principles are as follows: 1) the view augmenter should create diverse views (*diversity constraint*) while maintaining representative information (*consistency constraint*). 2) the encoder should extract as much representative information as possible from these views (*sufficiency constraint*) to get embeddings while fulfilling the goal of maximizing mutual information.

## 5.2 Pretext Task for CVA

In semi-supervised contrastive learning, the InfoMin principle is usually used to train the augmenter. It optimizes the balance between view consistency and view diversity, extracting representative information from raw graph data with the help of labels [\[36\]](#page-14-14). In unsupervised scenarios, extracting as much representative information as possible is equivalent to finding a set of views with maximal augmented variances (*diversity constraint*) while preserving sufficient representative information (*consistency constraint*). However, in the absence of labels, InfoMin, as utilized in unsupervised GCL frameworks [\[22,](#page-14-0) [3,](#page-13-2) [23,](#page-14-1) [25\]](#page-14-3), cannot guarantee the consistency of label-related information between augmented views when the encoders are trained using the InfoMax principle. It causes the corruption of representative information in the augmented views during augmentation. Theorem [5](#page-7-0) formalizes the above problem:

<span id="page-7-0"></span>Theorem 5. *If the GCL framework adopts Info-Max principle* max I(f(V ); f(V ′ ))*, InfoMin principle* min I(V ; V ′ ) *to train the shared information encoder* f *and the augmenter* g *respectively, and the generated view will converge to a complete noise graph, making the representative information in the original graph data completely lost.*

*Proof.* The optimal case of the min I(V ; V ′ ) is I(V ; V ′ ) = 0, i.e., the augmenter generates the augmented view without utilizing any information from the original data at all so that the augmented view does not contain any representative information. At the same time, according to Theorem [3,](#page-2-0) the InfoMax principle for training encoder has:

$$
I(f(V); f(V')) \leq I(V; f(V'))
$$
  
\n
$$
\leq \min\{I(V; V'), I(V'; f(V'))\} (30)
$$
  
\n
$$
= \min\{0, I(V'; f(V'))\} = 0,
$$

which illustrates that the level of utilization of the augmented information is limited by the level of utilization of the original information. Thus, when the optimal solution of the InfoMin principle is achieved—where the augmented view generated, V ′ = g(V ), satisfies V ′ = min g I(V ; g(V ))—the augmenter is unable to utilize any information from the original data. Above all, Theorem [5](#page-7-0) is proven.

Other works, such as those by [\[4,](#page-13-3) [6,](#page-13-5) [24\]](#page-14-2), also face challenges, despite not employing the InfoMin principle. These studies overlook the need for diversity in augmentations, resulting in non-ideal views that resemble the original graph too closely and hindering the encoder's ability to discern representative information.

To address this issue, we integrate a consistency constraint with the diversity constraint as a pretext task in CVA to generate ideal augmented views. The consistency constraint uses MSE loss to measure the consistency between augmented values and original values. The diversity constraint is the minimization of mutual information between two views.

Assume view V is denoted as (U, Λ, C) in LAC, the perturbation to view V can be regarded as the perturbation to U, Λ, and C. Considering that U remains unchanged as the vector bases during the augmentation process, the diversity constraint compels LAC to maximally perturb Λ and C. Building on the diversity constraint, we introduce a regularized loss (*consistency constraint*) to prevent excessive perturbation of Λ and C. Therefore, the loss function of the pretext task is formulated as follows:

<span id="page-7-3"></span>
$$
\min_{g} \underbrace{I(V; g(V))}_{diversity} + \alpha \underbrace{\left( \left\| \mathbf{\Lambda} - \mathbf{\Lambda}' \right\|_{F}^{2} + \left\| \mathbf{C} - \mathbf{C}' \right\|_{F}^{2}}_{consistency}, \tag{31}
$$

<span id="page-7-2"></span>Algorithm 1 Unsupervised Training Algorithm.

Input: original view V , view augmenter g, shared information encoder f, hyperparameters α, β, MI estimator I

Output: the representation of nodes in views z, z ′

1: while epoch ̸= max\_epochs do

2: F ix g, get V ′ = g(V ) 3: I(f(V ); f(V ′ )) + βI(V ; f(V )) 4: U pdate f to minimize line 3 5: F ix f 6: I(f(V ), f(g(V ))) +α(∥Λ−Λ ′ ∥ 2 <sup>F</sup> +∥C −C ′ ∥ 2 F ) 7: U pdate g to minimize line 6 8: end while 9: z, z ′ = f(V ), f(g(V )) 10: return z, z ′

where α is a hyperparameter for adjusting the weight of consistency constraint.

According to Theorem [6,](#page-7-1) it is proven that the new pretext task does not corrupt representative information in augmented views during augmentation.

<span id="page-7-1"></span>Theorem 6. *When applying the InfoMin principle* min I(V ; V ′ ) *to train the augmenter in LAC, the mutual information between the original view and the augmented view* I(V ; V ′ ) *is always greater than zero.*

*Proof.* In the continuous space of LAC, the view V is represented as (U, Λ, C), and the augmented view as V ′ = (U, Λ′ , C′ ). The mutual information between the two views is denoted as:

$$
I(V;V') = I((\mathbf{U}, \mathbf{\Lambda}, \mathbf{C}); (\mathbf{U}, \mathbf{\Lambda}', \mathbf{C}'))
$$
  
\n
$$
= I(\mathbf{U}; (\mathbf{U}, \mathbf{\Lambda}', \mathbf{C}')) + I(\mathbf{\Lambda}; (\mathbf{U}, \mathbf{\Lambda}', \mathbf{C}'))
$$
  
\n
$$
+ I(\mathbf{C}; (\mathbf{U}, \mathbf{\Lambda}', \mathbf{C}'))
$$
  
\n
$$
\geq I(\mathbf{U}; \mathbf{U}) + I(\mathbf{\Lambda}; \mathbf{\Lambda}') + I(\mathbf{C}; \mathbf{C}')
$$
  
\n
$$
\geq I(\mathbf{U}; \mathbf{U}) > 0.
$$
  
\n(32)

The equation demonstrates that the mutual information between the original and the augmented views, I(V ; V ′ ), is greater than zero. Above all, Theorem [6](#page-7-1) is proven.

#### 5.3 Pretext Task for the Shared Information Encoder

The InfoMax principle is a widely used approach for training encoders. However, training an encoder using only the InfoMax principle in LAC can lead to shortcut problems. According to Theorem [3,](#page-2-0) we have:

$$
I(f(V); f(V')) \le I(V'; f(V')), \tag{33}
$$

$$
I(f(V); f(V')) \le I(V; f(V)).\tag{34}
$$

It shows that the representative information in augmented views utilized by the encoder with the InfoMax is limited by I(V ′ ; f(V ′ )) and I(V ; f(V )). Specifically, the

encoder f may resort to shortcuts, as it encodes only limited information from augmented views, thereby lowering the upper bound of I(f(V ); f(V ′ )). This makes the model easier to train for convergence but ends up with a low-quality encoder. Therefore, we propose a *sufficiency constraint* that adds a bottleneck loss on top of the mutual information maximization term to exploit the representative information as follows sufficiently:

<span id="page-8-0"></span>
$$
\max_{f} \underbrace{I(f(V); f(V')) + \beta(I(V'; f(V')) + I(V; f(V))),}_{sufficiency}
$$
\n(35)

where β is a hyperparameter for controlling the weight of bottleneck loss.

To compute the second term of Eq. [35,](#page-8-0) we follow the setup in [\[3\]](#page-13-2), and employ an encoder to map the information in V ′ into an embedding, ensuring that it shares the same dimensions as f(V ′ ).

Unsupervised Learning. The goal of the unsupervised training, guided by the proposed objective InfoBal is to obtain high-quality embeddings. The training process for LAC consists of two stages. Initially, we fix the weights of the view augmenter g and use the pretext task based on sufficiency constraint to train the encoder f. Subsequently, we train the view augmenter using the pretext task, which integrates both consistency and diversity constraints. These two stages alternate in the training process. The algorithm is illustrated in Alg. [1.](#page-7-2)

# 6 Experiments

We begin this section with the introduction of the experimental setup, which includes the details of datasets, evaluation protocols, and baselines. Then we conduct experiments to evaluate LAC by addressing the following research questions:

- RQ1. (Effectiveness) Does LAC perform better than the SoTA GCL frameworks in the unsupervised setting for node classification task?
- RQ2. (Generalization Ability) How well does LAC generalize on various types of graphs and other tasks?
- RQ3. (The necessity of each component) Are the view augmenter CVA and InfoBal based pretext tasks in LAC both necessary?
- RQ4. (Model Analysis) Does the effect of MTA and CFA meet the design expectations?
- RQ5. (Sensitivity) Is LAC sensitive to hyperparameters like α, γ, τ , and mask ratio in MTA?

## 6.1 Experimental Setup

Datasets. We evaluate our approach using seven sparse and homogeneous datasets, including Cora [\[47\]](#page-14-21), CiteSeer [\[47\]](#page-14-21), PubMed [\[47\]](#page-14-21), CS [\[48\]](#page-14-22), Phy [\[48\]](#page-14-22), Photo [\[48\]](#page-14-22) and Computers [\[48\]](#page-14-22). We also perform experiments on two heterogeneous graph datasets—Chameleon and Squirrel and two dense graph datasets—Texas and Cornell (all cited from [\[49\]](#page-14-23)). The detailed statistics of the datasets are summarized in Table [1.](#page-9-0)

Evaluation Protocols. Following the settings in [\[22\]](#page-14-0), LAC uses a logistic classifier to evaluate the unsupervised trained model for the node classification task. We train LAC using Adam Optimizer and apply Xavier initialization uniformly for the modules in the network. We search for the hyperparameter α in different ranges for different datasets, usually between 0.1 and 1.5 with a minimal interval of 0.05. Similarly, for the β, τ , and mask ratio parameters, we search between 0.1 and 1.0 with a minimal interval of 0.05. The layer of encoder in MTA is in [1,2]. We run the experiments by PyTorch 1.12.0 and Pytorch Geometric 2.1.0. All experiments are conducted on an NVIDIA A6000 GPU (48GB) and an NVIDIA A100 GPU (80GB).

Compared baselines. We mainly compare our LAC[2](#page-8-1) framework with other SoTA frameworks for node classification. The SoTA methods include six categories: (1) Graph representative learning methods, such as RawFeat [\[19\]](#page-13-18), Node2Vec [\[43\]](#page-14-24), and DGI [\[44\]](#page-14-25). (2) Graph generative learning models such as GAE and VGAE [\[45\]](#page-14-26). These models are chosen because LAC contains a generative view augmenter. (3) Manual GCL frameworks include GRACE [\[19\]](#page-13-18), GCA [\[18\]](#page-13-17), MVGRL [\[5\]](#page-13-4), BGRL [\[17\]](#page-13-16) and COSTA [\[42\]](#page-14-20). (4) GCL framework based on statistics include CCA-SSG [\[46\]](#page-14-27) and MC-DCD [\[30\]](#page-14-8). (5) Automated GCL frameworks contain ADGCL [\[22\]](#page-14-0), JOAO [\[6\]](#page-13-5), and AutoGCL [\[3\]](#page-13-2). (6) Spectral GCL framework contains GCL-SPAN [\[28\]](#page-14-6).

## 6.2 Effectiveness of LAC

Table [2](#page-9-1) lists the accuracy of node classification using LAC and baselines to learn node embeddings in the unsupervised setting on seven sparse & homogeneous datasets. We observe that LAC achieves state-of-the-art (SoTA) accuracy on the node classification task. Specifically, we have the following observations:

(1) LAC achieves average improvements of 2.45% and 4.42% compared to the best graph representation learning method and the best graph generative learning method, respectively. This indicates the effectiveness of LAC as a graph-contrastive learning framework.

(2) The LAC framework demonstrates an average accuracy improvement of 0.76% over the best manual GCL framework across all seven sparse and homogeneous datasets. It indicates that the LAC framework can replace manual GCL frameworks as it eliminates the need for trial-anderror experiments to find appropriate augmented views and sufficiently extract representative information from those augmented views.

<span id="page-8-1"></span><sup>2</sup> <https://github.com/linln1/LAC.git>

<span id="page-9-0"></span>

| Category | Dataset   | #Nodes | #Edges  | #Sparsity | #Hetero |
|----------|-----------|--------|---------|-----------|---------|
|          | Cora      | 2,708  | 5,429   | 0.00074   | 0.19    |
| Homo-    | CiteSeer  | 3,327  | 4,552   | 0.00042   | 0.26    |
| geneous  | PubMed    | 19,717 | 44,324  | 0.00011   | 0.20    |
| &        | Photo     | 7,650  | 119,081 | 0.00203   | 8       |
| Sparse   | Computers | 13,752 | 245,861 | 0.00130   | 10      |
|          | CS        | 18,333 | 81,894  | 0.00024   | 15      |
|          | Phy       | 34,493 | 247,962 | 0.00020   | 5       |
| Hetero-  | Chameleon | 2,277  | 36,101  | 0.00696   | 0.77    |
| geneous  | Squirrel  | 5,201  | 217,073 | 0.00802   | 0.78    |
| Dense    | Cornell   | 183    | 295     | 0.00880   | 0.89    |
|          | Texas     | 183    | 309     | 0.00922   | 0.89    |

Table 1: The detailed statistics of the dataset.

<span id="page-9-1"></span>Table 2: The performance of LAC and baselines on seven sparse and homogenous datasets in terms of accuracy in percentage with standard deviations over ten runs. \* means the results are quoted from the corresponding papers [\[19,](#page-13-18) [18,](#page-13-17) [7,](#page-13-6) [5,](#page-13-4) [29\]](#page-14-7). † means the experiment results are reported based on the open public code. OOM means out of memory. The best performance is highlighted in boldface. The second-best performance is underlined. - means no public code and results can be found.

| Category           | Model          | Cora         | CiteSeer     | PubMed       | CS           | Phy          | Photo        | Computers    |
|--------------------|----------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|
| Graph Represent    | RawFeat* [19]  | 56.89 ± 0.08 | 60.70 ± 0.04 | 83.84 ± 0.10 | 73.91 ± 0.20 | 94.53 ± 0.34 | 83.52 ± 0.31 | 78.53 ± 0.00 |
| Learning           | Node2Vec* [43] | 64.50 ± 0.51 | 53.05 ± 1.02 | 74.29 ± 0.39 | 78.03 ± 0.39 | 94.66 ± 0.15 | 89.61 ± 0.24 | 84.39 ± 0.08 |
|                    | DGI* [44]      | 83.25 ± 0.68 | 72.03 ± 0.65 | 84.75 ± 0.19 | 92.75 ± 0.10 | 94.42 ± 0.67 | 90.76 ± 0.29 | 83.95 ± 0.47 |
| Graph Generative   | GAE* [45]      | 76.90 ± 0.25 | 60.60 ± 0.14 | 82.94 ± 0.31 | 90.01 ± 0.71 | 94.92 ± 0.07 | 91.62 ± 0.13 | 85.27 ± 0.19 |
| Learning           | VGAE* [45]     | 78.92 ± 0.46 | 61.22 ± 0.11 | 83.07 ± 0.29 | 92.11 ± 0.09 | 94.52 ± 0.01 | 92.20 ± 0.11 | 86.37 ± 0.21 |
| Manual             | GRACE* [19]    | 83.32 ± 0.41 | 72.10 ± 0.53 | 85.51 ± 0.37 | 91.12 ± 0.20 | 95.41 ± 0.13 | 92.15 ± 0.25 | 87.25 ± 0.25 |
| GCL                | GCA* [18]      | 82.89 ± 0.21 | 72.89 ± 0.13 | 85.12 ± 0.23 | 93.10 ± 0.01 | 95.68 ± 0.05 | 92.49 ± 0.09 | 87.85 ± 0.31 |
| Frameworks         | MVGRL* [5]     | 83.11 ± 0.12 | 73.33 ± 0.03 | 84.27 ± 0.04 | 93.11 ± 0.12 | 95.33 ± 0.03 | 91.74 ± 0.07 | 87.52 ± 0.11 |
|                    | BGRL* [17]     | 82.83 ± 1.61 | 72.32 ± 0.89 | 86.03 ± 0.33 | 93.31 ± 0.13 | 95.73 ± 0.05 | 93.17 ± 0.30 | 88.54 ± 0.03 |
|                    | COSTA †[42]    | 84.32 ± 0.22 | 72.92 ± 0.31 | 86.01 ± 0.19 | 92.56 ± 0.45 | 95.01 ± 0.09 | 92.56 ± 0.42 | 88.32 ± 0.30 |
| Statistical        | CCA-SSG* [46]  | 84.20 ± 0.40 | 73.10 ± 0.30 | 85.39 ± 0.51 | 93.31 ± 0.22 | 95.38 ± 0.06 | 93.14 ± 0.14 | 88.74 ± 0.28 |
| GCL Frameworks     | MC-DCD †[30]   | -            | -            | -            | 93.60 ± 0.08 | 95.50 ± 0.04 | 93.31 ± 0.13 | 88.78 ± 0.25 |
| Automated          | JOAO-v2 † [6]  | 82.47 ± 0.43 | 70.29 ± 0.44 | 83.81 ± 0.05 | 91.50 ± 0.29 | 94.79 ± 0.53 | 91.39 ± 0.25 | 86.73 ± 0.44 |
| GCL                | ADGCL † [22]   | 83.51 ± 0.63 | 72.42 ± 0.36 | 85.45 ± 0.33 | 93.26 ± 0.30 | 95.57 ± 0.09 | 91.45 ± 0.12 | 86.03 ± 0.21 |
| Frameworks         | AutoGCL† [3]   | 83.86 ± 0.25 | 72.62 ± 0.50 | 84.27 ± 0.52 | 92.37 ± 0.24 | 95.15 ± 0.14 | 92.25 ± 0.30 | 87.11 ± 0.84 |
| Spectral Framework | GCL-SPAN† [28] | 85.51 ± 0.61 | 72.46 ± 0.35 | 85.45 ± 0.16 | 93.76 ± 0.38 | OOM          | 92.58 ± 0.17 | 89.96 ± 0.13 |
| Ours               | LAC            | 86.02 ± 0.45 | 73.51 ± 0.62 | 86.05 ± 0.21 | 93.84 ± 0.30 | 96.08 ± 0.15 | 93.75 ± 0.43 | 90.55 ± 0.16 |

Table 3: Node classification accuracy in dense graphs, and heterogeneous graphs

<span id="page-9-2"></span>

| Dataset   | type          | manual<br>BGRL | statistical<br>CCA-SSG | automated<br>ADGCL | spectral<br>GCL-SPAN | ours<br>LAC |
|-----------|---------------|----------------|------------------------|--------------------|----------------------|-------------|
| Squirrel  | dense         | 39.65± 1.88    | 40.07±1.10             | 39.19±0.91         | 38.13±0.17           | 46.83±1.38  |
| Cornell   | dense         | 56.16±0.96     | 50.27±1.65             | 51.57±6.98         | 51.58±6.13           | 72.63±3.93  |
| Chameleon | heterogeneous | 55.02±4.04     | 57.55± 2.69            | 55.89±3.33         | 54.50±1.44           | 63.66±0.98  |
| Texas     | heterogeneous | 57.89±8.80     | 58.90±1.56             | 63.15±9.98         | 69.21±4.31           | 83.15±2.10  |

Table 4: Node Clustering task results

<span id="page-9-3"></span>

|           |           | BGRL      |           | CCA-SSG   |           | ADGCL     |           | GCL-SPAN  |           | LAC       |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| metrics   | NMI       | ARI       | NMI       | ARI       | NMI       | ARI       | NMI       | ARI       | NMI       | ARI       |
| Cora      | 0.27±0.02 | 0.18±0.02 | 0.50±0.02 | 0.40±0.02 | 0.32±0.03 | 0.20±0.03 | 0.52±0.01 | 0.45±0.01 | 0.53±0.01 | 0.46±0.01 |
| Cornell   | 0.16±0.01 | 0.08±0.02 | 0.14±0.01 | 0.06±0.01 | 0.14±0.01 | 0.07±0.02 | 0.24±0.01 | 0.12±0.01 | 0.24±0.02 | 0.12±0.01 |
| Chameleon | 0.10±0.01 | 0.05±0.00 | 0.14±0.02 | 0.04±0.01 | 0.11±0.01 | 0.06±0.00 | 0.12±0.01 | 0.06±0.00 | 0.19±0.01 | 0.11±0.01 |

(3) Compared to state-of-the-art automated GCL frameworks, statistical frameworks, and a spectral framework.

Across the seven sparse and homogeneous datasets, LAC achieves average accuracy improvements of 1.38%, 0.92%,

Table 5: The ablation study results of continuous view augmenter in LAC.

<span id="page-10-0"></span>

| Model       | Cora         | CiteSeer     | PubMed       | CS           | Phy          | Photo        | Computers    |
|-------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|
| LAC-w/o-MTA | 85.60 ± 0.91 | 72.64 ± 0.17 | 85.76 ± 0.14 | 93.15 ± 0.27 | 94.68 ± 0.30 | 93.56 ± 0.51 | 89.53 ± 0.35 |
| LAC-EgMsk   | 85.97 ± 0.81 | 73.33 ± 0.88 | 85.85 ± 0.39 | 93.04 ± 0.14 | 95.49 ± 0.23 | 93.62 ± 0.59 | 89.46 ± 0.14 |
| LAC-w/o-CFA | 83.52 ± 0.85 | 69.70 ± 1.27 | 84.59 ± 0.10 | 93.79 ± 0.31 | 95.87 ± 0.18 | 93.56 ± 0.43 | 89.95 ± 0.44 |
| LAC-FtMsk   | 85.80 ± 0.64 | 71.73 ± 0.95 | 85.73 ± 0.26 | 93.60 ± 0.42 | 95.94 ± 0.09 | 93.25 ± 0.26 | 89.44 ± 0.19 |
| LAC         | 86.02 ± 0.45 | 73.51 ± 0.62 | 86.05 ± 0.21 | 93.84 ± 0.38 | 96.08 ± 0.15 | 93.75 ± 0.43 | 90.55 ± 0.16 |

and 0.67%, respectively. For instance, LAC demonstrates an accuracy enhancement of 2.16% over AutoGCL, 1.82% over CCA-SSG, and 0.51% over GCL-SPAN on the Cora dataset. This is because LAC generates appropriate augmented views and uses representative information in graph data sufficiently.

In summary, LAC consistently surpasses state-of-the-art baselines across all datasets. This demonstrates the effectiveness of the LAC framework.

## 6.3 Generalization of LAC

The generalization ability of LAC on different datasets. Table [3](#page-9-2) reports the performance of LAC on heterogeneous and dense datasets with the node classification task, demonstrating its generalization capability to various types of datasets. We select Chameleon and Texas as the representatives of heterogeneous datasets, which have heterogeneity scores of 0.77 and 0.89, respectively, and choose Cornell and Texas as dense datasets, whose sparsity is around 0.9%. We compare a few of the best models in manual GCL frameworks (BGRL), statistical GCL frameworks (CCA-SSG), automated GCL frameworks (ADGCL), and spectral frameworks (GCL-SPAN) as the baselines. The results clearly demonstrate that LAC consistently outperforms baselines across different types of graphs.

The generalization ability of LAC on different tasks. We conduct unsupervised node clustering experiments on the Cora, Cornell, and Chameleon datasets to demonstrate LAC's advantages on other tasks. The results are reported in Table [4.](#page-9-3) NMI (Normalized Mutual Information score) and ARI (Adjusted Rand Information score) are used to evaluate the performance of node clustering. The results demonstrate that LAC achieves the best performance on different types of graphs.

## 6.4 Ablation Study

This section presents an ablation analysis of the various components within LAC to demonstrate the necessity of the proposed techniques.

The Effectiveness of the Continuous View Augmenter. To evaluate the effectiveness of the augmentation of both topological and feature information, we have developed two new variants of LAC. These variants are LAC-w/o-MTA, which excludes the MTA module, and LAC-w/o-CFA, which removes the CFA module from the framework. To validate the effectiveness of continuous augmentation, we replace MTA in the LAC framework with a random edge masking augmenter, denoted by LAC-EgMsk, and replace CFA with a random feature masking augmenter, denoted by LAC-FtMsk. The results are presented in Table [5.](#page-10-0)

(1) LAC demonstrates superior performance compared to its variants LAC-EgMsk and LAC-FtMsk across all datasets. Specifically, across the seven sparse datasets listed in Table [1,](#page-9-0) LAC improves accuracy by an average of 0.43% and 0.62% over LAC-EgMsk and LAC-FtMsk, respectively. The results indicate that augmenting topology and feature information in continuous space generates more appropriate augmentation views than discrete augmentation.

(2) LAC performs better than LAC-w/o-MTA and LACw/o-CFA in all datasets. On average, LAC is 0.69% and 1.26% more accurate than LAC-w/o-MTA and LAC-w/o-CFA, respectively. This demonstrates that both feature augmentation and topology augmentation are necessary.

The Effectiveness of the Pretext Tasks based on the InfoBal. To validate the effectiveness of the pretext tasks within LAC, we introduce two variants of our pretext tasks. To standardize the naming of variants, we adopt the A-B format to describe these variants. Specifically, pretext task A is used to train the CVA, and pretext task B is used to train the encoder. The first variant, Min-BN, removes regularized loss (consistency constraint) in Eq. [31](#page-7-3) to train the CVA and uses loss in Eq. [35](#page-8-0) to train the encoder. Similarly, the second variant is named Reg-Max, which removes the bottleneck loss in Eq. [31](#page-7-3) for the training of the encoder. Additionally, we introduce two variants based on commonly used pretext tasks. The Max-Max variant utilizes an InfoMax-based loss to train the CVA and the encoder, respectively. The Min-Max variant employs an InfoMin-based loss to train the CVA while using an InfoMax-based loss to train the encoder.

The results, presented in Table [6,](#page-11-0) show that the pretext tasks based on InfoBal significantly outperform the other tasks. InfoBal's superior performance compared to both Reg-Max and Min-BN illustrates the effectiveness of the consistency constraint and the sufficiency constraint, respectively. In addition, the Max-Max task is better than the Min-Max task on most datasets, which indicates that the Min-Max task excessively destroys consistent information when using InfoMin to generate augmented views, and

Table 6: The ablation study results of training pretext task InfoBal in LAC.

<span id="page-11-0"></span>

| Tasks   | Cora         | CiteSeer     | PubMed       | CS           | Phy          | Photo        | Computers    |
|---------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|
| Max-Max | 86.02 ± 0.65 | 72.83 ± 0.29 | 83.27 ± 0.28 | 92.83 ± 0.45 | 94.83 ± 0.26 | 89.80 ± 0.70 | 82.71 ± 3.88 |
| Min-Max | 81.10 ± 1.94 | 72.63 ± 0.44 | 83.19 ± 0.51 | 93.52 ± 0.38 | 90.22 ± 1.97 | 90.35 ± 2.19 | 79.84 ± 1.15 |
| Min-BN  | 83.97 ± 2.04 | 73.13 ± 0.59 | 83.54 ± 0.39 | 93.52 ± 0.32 | 94.58 ± 0.45 | 92.05 ± 0.35 | 83.75 ± 2.14 |
| Reg-Max | 84.85 ± 2.43 | 73.25 ± 0.29 | 84.36 ± 0.60 | 93.04 ± 0.36 | 95.18 ± 0.60 | 93.62 ± 0.05 | 82.19 ± 3.78 |
| InfoBal | 86.02 ± 0.45 | 73.51 ± 0.62 | 86.05 ± 0.21 | 93.84 ± 0.38 | 96.08 ± 0.15 | 93.75 ± 0.43 | 90.55 ± 0.16 |

<span id="page-11-1"></span>![](_page_11_Figure_3.jpeg)

**Caption:** Figure 4 compares the performance of different architectures used in the CVA, demonstrating that the proposed MTA and CFA modules outperform simpler architectures like MLPs. This highlights the effectiveness of the continuous augmentation approach in generating high-quality augmented views.

Figure 4: The results of different architectures used in CVA.

eventually leads to performance degradation. InfoBal's accuracy has a 4.13% improvement over the sparse datasets compared to the Min-Max. This demonstrates that InfoBal can prevent excessive perturbation of representative information caused by the InfoMin and effectively utilize it to improve the encoder's quality.

To evaluate the effectiveness of the consistency constraint on the overall framework, we compare the performance of the Reg-Max and Min-Max. The results show that the Reg-Max improves accuracy by an average of 2.23% over the Min-Max. This suggests that the Min-Max task causes the view augmenter to corrupt excessive representative information during view augmentation, thereby degrading the performance of the GCL framework. On the other hand, adding the consistency constraint on top of InfoMin allows the augmenter to adaptively control the diversity and consistency during augmentation. It significantly improves the quality of augmented views. Therefore, we conclude that the Reg-Max task is more effective than the Min-Max task.

Furthermore, a comparison between the performance of Min-BN and Min-Max reveals that the former achieves an average accuracy improvement of 1.95% over the latter. It indicates that the sufficiency constraint on InfoMax enables the encoder to extract sufficient representative information from the augmented view.

#### 6.5 Model Analysis

The Analysis of the MTA and CFA designed for CVA. We conduct experiments on MTA and CFA to assess whether these carefully designed architectures outperform simpler architectures, such as MLPs. Specifically, we substitute the MTA in the CVA with a two-layer MLP. During the augmentation of views, the MLP does not consider the potential multivariate relationships between different values on the diagonal of the topological information matrix Λ compared to MTA. Similarly, we replaced the CFA module with a two-layer MLP, which cannot capture relationships between features in different channels. The two variants are named MTA-MLP and MLP-CFA, respectively. Furthermore, we replace both modules with two-layer MLP, resulting in the MLP-MLP model. To demonstrate the advantages of augmenting feature information within a continuous space, we also directly perturb node features X instead of perturbing C, and we denote this model as MTA-rawX. Our experimental results are summarized in Figure [4.](#page-11-1)

In our study, we compare the accuracy of the CVA with that of MTA-MLP. Across three datasets, CVA exhibits an average accuracy improvement of 0.95% compared to MTA-MLP. This suggests that the CFA module effectively augments node features by learning and perturbing the distribution of cross-channel node features in continuous space. Additionally, we compare the performance of the CVA with MLP-CFA. We find that CVA achieved an average accuracy improvement of 2.57% across the three datasets, indicating the effectiveness of the MTA module design. Finally, when comparing the original architecture to MLP-MLP, we observe that CVA produces higherquality augmented views across three datasets, achieving an average accuracy improvement of 3.03%. Additionally, on average, CVA outperformed MTA-rawX by 0.15%, indicating the advantage of perturbing C over directly perturbing X. These results validate the effectiveness of the MTA and CFA designs.

Visual Analysis for augmented node features by CFA. Figure [5](#page-12-0) denotes the value distribution of C over multiple channels in the Cora dataset. The horizontal axis represents different channels, namely distinct dimensions of C, while the vertical axis represents the values in corresponding channels. In addition to the C from the original data X represented by the blue line, we also augment X using the random feature masking policy with probabil-

<span id="page-12-0"></span>![](_page_12_Figure_1.jpeg)

**Caption:** Figure 5 shows the value distribution of projected features across multiple channels in the Cora dataset. It compares the original feature distribution with those augmented using random masking and the proposed Cross-channel Feature Augmentation, illustrating the latter's ability to smoothen the distribution.

![](_page_12_Figure_2.jpeg)

**Caption:** Figure 6 illustrates the impact of different hyperparameters on the performance of LAC on the Cora dataset. It highlights the model's stability and low sensitivity to variations in the weights of regularized and bottleneck losses, indicating robust performance across parameter settings.

Figure 5: The changes of C on Cora.

<span id="page-12-3"></span>Figure 6: The Impact of different α and β on Cora.

<span id="page-12-2"></span><span id="page-12-1"></span>![](_page_12_Figure_5.jpeg)

**Caption:** Figure 7 examines the sensitivity of the temperature parameter τ and mask ratio in the MTA. It shows how variations in these parameters affect model accuracy, revealing that LAC maintains consistent performance despite changes, particularly in the mask ratio.

Figure 7: The sensitivity of τ , and mask ratio in LAC.

ity equal to 0.2 and calculate C based on the augmented data, whose distribution is finally represented by the green line. Furthermore, we use CFA to automatically learn the augmented C, whose distribution is represented by the red line. The random feature masking results do not improve the unbalanced distribution of original C. On the other hand, the distribution of C augmented by CFA is much smoother than the original. Thus, CFA can effectively prevent feature collapse caused by uneven distribution [\[41\]](#page-14-19).

#### 6.6 Hyperparameters Sensitivity Analysis

The sensitivity of weights α and β in InfoBal. Through experiments on Cora, we investigate the impact of the weights of the regularized loss α and bottleneck loss β on LAC's performance. The results are reported in Figure [6.](#page-12-0) We observe that the LAC framework exhibits low sensitivity to the two crucial hyperparameters. In other words, when other parameters are fixed, changes in α and β do not lead to significant performance variations. Specifically, LAC's performance remains stable on the Cora dataset when α and β, with other parameters held constant. The highest accuracy performance is only 1.15% higher than the lowest.

The τ in InfoNCE. In Figure [7](#page-12-1)[\(a\),](#page-12-2) we depict the model accuracy with variations in the temperature parameter τ in InfoNCE. The model's accuracy ranges from 79.04% to 85.73% on Cora and from 93.41% to 93.75% on Amazon-Photo as the temperature changes. A marginal accuracy change of 1.52% is observed on CiteSeer. Based on these observations, We conclude that the performance of LAC is not significantly affected by modifications to τ as τ increases beyond 0.5.

0.3 The mask mechanism in MTA. In Figure [7](#page-12-1)[\(b\),](#page-12-3) the impact of the hyperparameter mask ratio on the MTA is illustrated. Mean-variance curves are utilized to visualize the results, with shaded areas indicating variance. On the Cora dataset, setting the mask ratio to 0 results in a 0.96% accuracy loss compared to the accuracy at τ = 0.1, accompanied by a significant variance. This suggests that the mask mechanism significantly influences LAC's accuracy in specific datasets. However, when increasing the mask ratio from 0.1 to 0.9, the framework's average accuracy variation remains below 2.0%. This indicates that LAC is not sensitive to changes in the mask ratio.

# 7 Related Works

### 7.1 Data Augmentation Methods in GCL

Graph data augmentation encompasses two primary aspects: topology and features. For topology, traditional methods involve manually selecting probabilities for node deletion [\[3\]](#page-13-2), edge perturbation [\[19\]](#page-13-18), subgraph selection [\[16\]](#page-13-15), and graph diffusion [\[5\]](#page-13-4) based on trial-and-error experiments. Recent works have shifted towards automated GCL frameworks that optimize augmentation strategies strategies through data-driven techniques. JOAO [\[6\]](#page-13-5) utilizes min-max optimization to determine the optimal weights for various discrete data augmentation methods. AutoGCL [\[3\]](#page-13-2) learns the probability of node masking from the data. ADGCL [\[22\]](#page-14-0) learns the Bernoulli distribution probability for each edge. GPA [\[4\]](#page-13-3) learns the optimal combination weights of discrete data augmentation methods for each graph based on JOAO. Ada-MIP [\[26\]](#page-14-4) uses hybrid augmentation strategies. However, the above methods all use a discrete way to augment the topology information. For feature information, the existing work usually masks certain node features or dimensions [\[19,](#page-13-18) [5,](#page-13-4) [6\]](#page-13-5). These methods do not continuously change the value of the node feature. The random projection of the feature matrix by COSTA is equivalent to adding an orthogonal continuous unbiased noise to the feature matrix. Through SVD decomposition of the feature matrix and rebalancing of the singular values, SFA [\[29\]](#page-14-7) generates a continuous perturbation of the feature information, thereby avoiding dimension collapse. However, they still overlook the continuous augmentation of topological information.

## 7.2 Pretext Tasks in GCL

The pretext tasks within the GCL framework are categorized into two main types [\[30\]](#page-14-8). The first type is based on information theory principles. In this category, mutual information (MI) estimators calculate MI across multiple views or representations. The most commonly used principle is InfoMax [\[35\]](#page-14-13), which aims to maximize consistency in the representation across various views. Conversely, InfoMin [\[36\]](#page-14-14) seeks to enhance the diversity of information between views in unsupervised settings [\[22,](#page-14-0) [25,](#page-14-3) [3\]](#page-13-2). They ignore the consistency of information between multiple views and finally produce completely different augmented

views, which is a shortcut solution. The second category of pretext tasks are designed based on statistics theory, such as CCA-SSG [\[46\]](#page-14-27) and MC-DCD [\[30\]](#page-14-8). These GCL frameworks use statistical metrics instead of MI estimators. CCA-SSG applies canonical correlation analysis [\[50\]](#page-14-28) to graph data, while MC-DCD identifies optimal statistical indicators from many candidates for different datasets.

# 8 Conclusion

GCL is an effective methodology for learning latent information in graph data. However, existing GCL frameworks generate inappropriate augmentations and utilize the representative information in graph data insufficiently. We proposed a novel framework for GCL called LAC. The CVA within LAC simultaneously augments topology and feature information in an orthogonal continuous space, thereby generate appropriate augmented views. The InfoBal pretext task applied in LAC sufficiently utilizes the representative information in the graph data by adding a consistency constraint on InfoMin and a sufficiency constraint on InfoMax. Our experimental results demonstrate that LAC significantly outperforms existing GCL frameworks.

# References

- <span id="page-13-0"></span>[1] Xiao Luo, Wei Ju, Meng Qu, Chong Chen, Minghua Deng, Xian-Sheng Hua, and Ming Zhang. Dualgraph: Improving semi-supervised graph classification via dual contrastive learning. In ICDE, pages 699–712, 2022.
- <span id="page-13-1"></span>[2] Yiming Xu, Bin Shi, Teng Ma, Bo Dong, Haoyi Zhou, and Qinghua Zheng. CLDG: contrastive learning on dynamic graphs. In ICDE, pages 696–707, 2023.
- <span id="page-13-2"></span>[3] Yihang Yin, Qingzhong Wang, Siyu Huang, Haoyi Xiong, and Xiang Zhang. Autogcl: Automated graph contrastive learning via learnable view generators. In AAAI, pages 8892–8900, 2022.
- <span id="page-13-3"></span>[4] Xin Zhang, Qiaoyu Tan, Xiao Huang, and Bo Li. Graph contrastive learning with personalized augmentation. CoRR, abs/2209.06560, 2022.
- <span id="page-13-4"></span>[5] Kaveh Hassani and Amir Hosein Khas Ahmadi. Contrastive multi-view representation learning on graphs. In ICML, volume 119, pages 4116–4126, 2020.
- <span id="page-13-5"></span>[6] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In ICML, volume 139, pages 12121–12132, 2021.
- <span id="page-13-6"></span>[7] Ming Jin, Yizhen Zheng, Yuan-Fang Li, Chen Gong, Chuan Zhou, and Shirui Pan. Multi-scale contrastive siamese networks for self-supervised graph representation learning. In IJCAI, pages 1477–1483, 2021.
- <span id="page-13-7"></span>[8] Yan Li, Liang Zhang, Xiangyuan Lan, and Dongmei Jiang. Towards adaptable graph representation learn-

ing: An adaptive multi-graph contrastive transformer. In MM, pages 6063–6071, 2023.

- <span id="page-13-8"></span>[9] Rui Li, Yiting Wang, Wei-Long Zheng, and Bao-Liang Lu. A multi-view spectral-spatial-temporal masked autoencoder for decoding emotions with selfsupervised learning. In MM, pages 6–14, 2022.
- <span id="page-13-9"></span>[10] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, and Liang Wang. Mining latent structures for multimedia recommendation. In MM, pages 3872–3880, 2021.
- <span id="page-13-10"></span>[11] Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, and Tat-Seng Chua. Graph-refined convolutional network for multimedia recommendation with implicit feedback. In MM, pages 3541–3549, 2020.
- <span id="page-13-11"></span>[12] Xin Zhou and Zhiqi Shen. A tale of two graphs: Freezing and denoising graph structures for multimodal recommendation. In MM, pages 935–943, 2023.
- <span id="page-13-12"></span>[13] Yu Zheng, Ming Jin, Yixin Liu, Lianhua Chi, Khoa Tran Phan, and Yi-Ping Phoebe Chen. Generative and contrastive self-supervised learning for graph anomaly detection. TKDE, 35(12):12220–12233, 2023.
- <span id="page-13-13"></span>[14] Jingcan Duan, Siwei Wang, Pei Zhang, En Zhu, Jingtao Hu, Hu Jin, Yue Liu, and Zhibin Dong. Graph anomaly detection via multi-scale contrastive learning networks with augmented view. In AAAI, pages 7459–7467, 2023.
- <span id="page-13-14"></span>[15] Bo Chen, Jing Zhang, Xiaokang Zhang, Yuxiao Dong, Jian Song, Peng Zhang, Kaibo Xu, Evgeny Kharlamov, and Jie Tang. Gccad: Graph contrastive learning for anomaly detection. TKDE, 2022.
- <span id="page-13-15"></span>[16] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. In NeurIPS, pages 1–12, 2020.
- <span id="page-13-16"></span>[17] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L. Dyer, Rémi Munos, Petar Velickovic, and Michal Valko. Large-scale representation learning on graphs via bootstrapping. In ICLR, pages 1–21, 2022.
- <span id="page-13-17"></span>[18] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In WWW, pages 2069– 2080, 2021.
- <span id="page-13-18"></span>[19] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive representation learning. pages 1–17, 2019.
- <span id="page-13-19"></span>[20] Yujie Mo, Liang Peng, Jie Xu, Xiaoshuang Shi, and Xiaofeng Zhu. Simple unsupervised graph representation learning. In AAAI, pages 7797–7805, 2022.
- <span id="page-13-20"></span>[21] Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. GCC: graph contrastive coding for graph neural

network pre-training. In KDD, pages 1150–1160, 2020.

- <span id="page-14-0"></span>[22] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. In NeurIPS, pages 15920– 15933, 2021.
- <span id="page-14-1"></span>[23] Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. Bringing your own view: Graph contrastive learning without prefabricated data augmentations. In WSDM, pages 1300–1309, 2022.
- <span id="page-14-2"></span>[24] Xiao Shen, Dewang Sun, Shirui Pan, Xi Zhou, and Laurence T. Yang. Neighbor contrastive learning on learnable graph augmentation. In AAAI, pages 9782–9791, 2023.
- <span id="page-14-3"></span>[25] Xinyan Pu, Ke Zhang, Huazhong Shu, Jean-Louis Coatrieux, and Youyong Kong. Graph contrastive learning with learnable graph augmentation. In ICASSP, pages 1–5, 2023.
- <span id="page-14-4"></span>[26] Yuyang Ren, Haonan Zhang, Peng Yu, Luoyi Fu, Xinde Cao, Xinbing Wang, Guihai Chen, Fei Long, and Chenghu Zhou. Ada-mip: Adaptive selfsupervised graph representation learning via mutual information and proximity optimization. TKDE, 17(5):69:1–69:23, 2023.
- <span id="page-14-5"></span>[27] Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian Pei. Revisiting graph contrastive learning from the perspective of graph spectrum. In NeurIPS, pages 1–25, 2022.
- <span id="page-14-6"></span>[28] Lu Lin, Jinghui Chen, and Hongning Wang. Spectral augmentation for self-supervised learning on graphs. In ICLR, pages 1–27, 2023.
- <span id="page-14-7"></span>[29] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, and Irwin King. Spectral feature augmentation for graph contrastive learning and beyond. In AAAI, volume 37, pages 11289–11297, 2023.
- <span id="page-14-8"></span>[30] Jinyong Wen, Shiming Xiang, and Chunhong Pan. Exploring universal principles for graph contrastive learning: A statistical perspective. In MM, pages 3579–3589, 2023.
- <span id="page-14-9"></span>[31] Tailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. Graph information bottleneck. In NeurIPS, pages 1–20, 2020.
- <span id="page-14-10"></span>[32] Ren-Cang Li. Matrix perturbation theory. In Handbook of linear algebra, pages 15–1. 2006.
- <span id="page-14-11"></span>[33] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, volume 119, pages 1597–1607, 2020.
- <span id="page-14-12"></span>[34] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, pages 1– 13, 2018.
- <span id="page-14-13"></span>[35] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam

Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In ICLR, 2019.

- <span id="page-14-14"></span>[36] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? In NeurIPS, pages 1–24, 2020.
- <span id="page-14-15"></span>[37] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. NeurIPS, 30, 2017.
- <span id="page-14-16"></span>[38] Thomas M Cover. Elements of information theory. 1999.
- <span id="page-14-17"></span>[39] Deyu Bo, Chuan Shi, Lele Wang, and Renjie Liao. Specformer: Spectral graph neural networks meet transformers. In ICLR, pages 1–18, 2023.
- <span id="page-14-18"></span>[40] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In ICML, pages 10524–10533, 2020.
- <span id="page-14-19"></span>[41] Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse in contrastive self-supervised learning. In ICLR, pages 1–17, 2022.
- <span id="page-14-20"></span>[42] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, and Irwin King. Costa: covariance-preserving feature augmentation for graph contrastive learning. In SIGKDD, pages 2524–2534, 2022.
- <span id="page-14-24"></span>[43] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In SIGKDD, pages 855–864, 2016.
- <span id="page-14-25"></span>[44] Petar Velickovic, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, and R. Devon Hjelm. Deep graph infomax. In ICLR, pages 1–17, 2019.
- <span id="page-14-26"></span>[45] Thomas N. Kipf and Max Welling. Variational graph auto-encoders. NeurIPS, pages 1–3, 2016.
- <span id="page-14-27"></span>[46] Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonical correlation analysis to self-supervised graph neural networks. NeurIPS, 34:76–89, 2021.
- <span id="page-14-21"></span>[47] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In ICML, volume 48, pages 40– 48, 2016.
- <span id="page-14-22"></span>[48] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. CoRR, abs/1811.05868:1–11, 2018.
- <span id="page-14-23"></span>[49] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. 2020.
- <span id="page-14-28"></span>[50] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation analysis. In ICML, pages 1247–1255, 2013.