# Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI

An Extended Version

LINGXI CUI, HUAN LI, KE CHEN, LIDAN SHOU, and GANG CHEN, The State Key Laboratory of Blockchain and Data Security, Zhejiang University, China

Machine learning (ML) on tabular data is ubiquitous, yet obtaining abundant high-quality tabular data for model training remains a significant obstacle. Numerous works have focused on tabular data augmentation (TDA) to enhance the original table with additional data, thereby improving downstream ML tasks. Recently, there has been a growing interest in leveraging the capabilities of generative AI for TDA. Therefore, we believe it is time to provide a comprehensive review of the progress and future prospects of TDA, with a particular emphasis on the trending generative AI. Specifically, we present an architectural view of the TDA pipeline, comprising three main procedures: pre-augmentation, augmentation, and post-augmentation. Preaugmentation encompasses preparation tasks that facilitate subsequent TDA, including error handling, table annotation, table simplification, table representation, table indexing, table navigation, schema matching, and entity matching. Augmentation systematically analyzes current TDA methods, categorized into retrieval-based methods, which retrieve external data, and generation-based methods, which generate synthetic data. We further subdivide these methods based on the granularity of the augmentation process at the row, column, cell, and table levels. Post-augmentation focuses on the datasets, evaluation and optimization aspects of TDA. We also summarize current trends and future directions for TDA, highlighting promising opportunities in the era of generative AI. In addition, the accompanying papers and related resources are continuously updated and maintained in the GitHub repository at <https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation> to reflect ongoing advancements in the field.

#### 1 INTRODUCTION

Tabular data, such as relational tables, Web tables and CSV files, is among the most primitive and essential forms of data [\[11\]](#page-38-0) in machine learning (ML), characterized by excellent structural properties, readability, and interpretability. A testament to its significance, more than 65% of datasets available on the Google Dataset Search platform are tabular files [\[6\]](#page-38-1). This prevalence underscores its critical role across a myriad of fields, such as finance [\[81\]](#page-42-0), healthcare [\[41\]](#page-40-0), education [\[68\]](#page-41-0). The growing availability of repositories containing structured or semi-structured data offers new opportunities for tabular data research and applications built upon it, particularly in the fields of ML and artificial intelligence (AI).

However, acquiring substantial amounts of high-quality tabular data for ML model training remains a persistent challenge [\[17,](#page-39-0) [64\]](#page-41-1). This is especially demanding because each individual table is modest in size and self-contained, making the overall data collection process resource-intensive and time-consuming. According to the oft-cited [\[79\]](#page-41-2) statistics, data scientists spend over 80% of their time on ML data preparation tasks, including data discovery and augmentation. The complexity and uneven quality of massive tabular datasets from various domains further complicate the acquisition of high-quality tabular data [\[15,](#page-38-2) [33\]](#page-39-1). Furthermore, in the era of large language models (LLMs), tabular data is one of the preferred data formats that LLMs consume, and existing high-quality tabular datasets may soon be exhausted [\[117\]](#page-43-0). Additionally, in the industrial sector where tabular data is most commonly used, the availability of data is often limited due to privacy concerns [\[57\]](#page-40-1). All of these factors have led to significant efforts being devoted to developing techniques that support tabular data augmentation (TDA). Through our extensive investigation, we have collected a

Authors' address: Lingxi Cui, cuilingxi.cs@zju.edu.cn; Huan Li, lihuan.cs@zju.edu.cn; Ke Chen, chenk@zju.edu.cn; Lidan Shou, should@zju.edu.cn; Gang Chen, cg@zju.edu.cn, The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China, 310027.

total of 70 highly relevant studies from 2010 to 2024. In this study, we define TDA as the process of augmenting the original dataset (table) to enhance the performance of downstream ML models. An illustration of TDA and its role in ML scenarios is provided in Fig. [1.](#page-1-0) TDA techniques aim to enrich the ML tasks by incorporating additional data, either from external data sources[1](#page-1-1) or synthesized by generative methods[2](#page-1-2) .

On the business side, the market for tabular data persists in its growth. For example, by year 2022, the US open data market (data.gov) has amassed a total of 335,000 datasets, contributing to a staggering \$3 trillion to the US economy [\[33\]](#page-39-1). Moreover, the Augmented Analytics Market is expected to attain \$35.6 billion expanding at a 22.70% CAGR (Compound Annual Growth Rate) report by Market Research Future[3](#page-1-3) . Overall, TDA has garnered extensive attention from the research community and generated significant demand from the business sector.

<span id="page-1-0"></span>![](_page_1_Figure_3.jpeg)

**Caption:** Figure 1 illustrates the Tabular Data Augmentation (TDA) process for machine learning. It depicts a data scientist enhancing a house price prediction model by augmenting a limited dataset with additional attributes, records, and corrected values. The process includes pre-augmentation steps like table annotation, followed by retrieval-based and generation-based augmentation methods, culminating in an improved dataset for training a more accurate model.

Fig. 1. Example of TDA for ML: ‚ë† A data scientist aims to predict house prices based on factors like location and floor using an original training set ( ) with limited features and records. The initial ML model yields suboptimal results due to insufficient data and numerous missing or incorrect values. To improve performance, the scientist uses TDA to augment the original dataset with additional attributes (columns), records (rows), and corrected values (cells). ‚ë° Before augmentation, preparation steps, such as table annotation (e.g., recovering the missing column type of the 4th column in ), enhance the TDA process's effectiveness. <sup>‚ë¢</sup> Augmentation can be achieved through retrieval-based methods (e.g., integrating the 2nd row in 1 from external data source) or generation-based methods that synthesize new data. <sup>‚ë£</sup> The augmented table ( ) combines the original and new data. ‚ë§ After augmentation, evaluation steps evaluate the effectiveness of TDA process. ‚ë• Finally, the result-TDA table enable the scientist to train a more accurate price prediction model.

However, the TDA task can be particularly challenging due to the unique characteristics of tabular data and the potential scale of table pools. Unlike homogeneous data such as images or text, tabular data is heterogeneous, typically containing both dense numerical and sparse categorical

<span id="page-1-1"></span><sup>1</sup> In this study, we also refer to external data sources as table pools as they can contain various table sources, including databases, Web tables, and so on.

<span id="page-1-2"></span><sup>2</sup> In addition to the recently much-discussed language models [\[90\]](#page-42-1), generative methods also include statistical approaches such as MICE [\[12\]](#page-38-3), deep generative models like diffusion models [\[66\]](#page-41-3), and so on.

<span id="page-1-3"></span><sup>3</sup>[https://www.marketresearchfuture.com/reports/augmented-analytics-market-7464.](https://www.marketresearchfuture.com/reports/augmented-analytics-market-7464)

attributes [\[10\]](#page-38-4). Additionally, it has complicated structure, such as row and column permutation invariance and hierarchical organization, where cells belong to rows and rows belong to tables. Moreover, many TDA tasks involve large-scale table pools, sometimes encompassing millions of tables. These tables often have inconsistent attribute naming and value formatting, and table pools themselves are dynamic, changing over time. Various TDA approaches have been proposed to address these challenges in diverse ways. As mentioned and illustrated in Fig. [1,](#page-1-0) these methods can be broadly categorized into retrieval-based approaches, which involve retrieving data from table pools, and generation-based approaches, which involve generating new content. With the rise of generative AI, many recent TDA works including both retrieval- and generation-based approaches (16 from 2023 to 2024 in our review) have embraced these models from different perspectives, and this trend continues to grow. Generative AI, designed for generating new content based on patterns learned from training data, broadly includes various models such as pre-trained language models (e.g., BERT [\[34\]](#page-39-2) and T5 [\[36\]](#page-39-3)), large language models (e.g., ChatGPT [\[54\]](#page-40-2) and LLaMA [\[88\]](#page-42-2)), variational autoencoders [\[76\]](#page-41-4), generative adversarial networks [\[31\]](#page-39-4), and diffusion models [\[66\]](#page-41-3). Such models have swept across various fields from computer vision (CV) [\[58,](#page-41-5) [96,](#page-42-3) [117\]](#page-43-0) to natural language processing (NLP) [\[20,](#page-39-5) [37,](#page-39-6) [58,](#page-41-5) [96,](#page-42-3) [117\]](#page-43-0), and are now beginning to permeate the field of tabular data analysis. In this context, we observe that there has yet to be a systematic review, synthesis, and categorization of existing TDA methods, let alone a discussion on the integration of current trending generative AI methods. Therefore, we believe it is both timely and essential to conduct such a survey to help readers and practitioners grasp the advancements in this critical field and pinpoint significant research opportunities in the era of generative AI.

<span id="page-2-0"></span>Table 1. Overview of prior related surveys. Some related concepts to TDA: Data Integration is the task of combining information from different relational data sources without an original table, while TDA aims to augment the original table to boost downstream ML tasks; Table Discovery identifies datasets from external data sources that may contain useful information, often serving as an intermediate step in TDA; Table Representation aims to transform tabular data into meaningful vector representation for further processing, which can be considered as one of preparation tasks for TDA.

| Reference              | Field                   | Data Type                                  | Task                                                |
|------------------------|-------------------------|--------------------------------------------|-----------------------------------------------------|
| Zhang and Balog [111]  | Data Management         | Web Tables                                 | Web Table Extraction, Retrieval, and Augmentation   |
| Li et al. [58]         | NLP, CV                 | Relational Tables, Text and Images         | Data Augmentation, Data Preparation and Integration |
| Zhou et al. [117]      | NLP, CV, and Multimedia | Text, Image, Audio Signal                  | Data Augmentation                                   |
| Wang et al. [96]       | NLP, CV, and Multimedia | Image, Text, Graph, Table, and Time-series | Data Augmentation                                   |
| Fonseca and Bacao [38] | Data Management         | Relational and Web Tables<br>‚ô£             | Tabular Data Generation                             |
| Hulsebos et al. [43]   | Data Management         | Relational and Web Tables                  | Table Representation                                |
| Chapman et al. [18]    | Data Management         | Relational and Web Tables                  | Table Discovery                                     |
| Fan et al. [33]        | Data Management         | Relational and Web Tables                  | Table Discovery                                     |
| Paton et al. [79]      | Data Management         | Relational and Web Tables                  | Table Discovery and Exploration                     |
| Ours                   | Data Management         | Relational and Web Tables                  | Data Augmentation                                   |

‚ô£ To the best of our knowledge, the TDA studies we primarily review focus on relational tables and Web tables, whereas other data formats, such as CSV and JSON files, can be handled with minimal conversion.

Previous efforts, as summarized in Table [1,](#page-2-0) have focused on other research fields or only touched on aspects of TDA. For example, Zhang et al. [\[111\]](#page-43-1) mention tabular data augmentation only briefly in their research on Web tables. Another tutorial [\[58\]](#page-41-5) focuses on data augmentation for data preparation and data integration, but does not specifically discuss its support for ML training; moreover, this tutorial does not place a particular emphasis on tabular data. A more recent survey [\[117\]](#page-43-0) on data augmentation in the era of large models mainly focuses on NLP and CV tasks, rather than tabular data. Similarly, Wang et al. [\[96\]](#page-42-3) cover data augmentation techniques across various data modalities (i.e., image, text, graph, table, and time-series), but TDA is only a small part of their broader scope. Their modality-independent taxonomy may not be the most suitable for an

in-depth exploration of TDA specifically. For example, one of the key methods covered in our survey, retrieval-based TDA, is not addressed in their work. Other literature reviews on tabular data focus on table discovery [\[18,](#page-39-7) [33,](#page-39-1) [79\]](#page-41-2), table representation [\[43\]](#page-40-4) or tabular data generation [\[38\]](#page-40-3), rather than augmentation.

Our survey stands out from previous reviews and tutorials by providing a comprehensive examination of TDA methods tailored for ML scenarios, with a special emphasis on the recent advancements in incorporating generative AI techniques. We have meticulously selected 70 significant works from the fields of data management and artificial intelligence, dating back to 2010, to offer a diverse range of perspectives. Based on these works, we thoroughly investigate TDA-related research. Specifically, we introduce taxonomies that categorize these works from both task and table content granularity perspectives. This allows us to clearly compare retrieval-based and generation-based methods, highlighting their respective strengths and weaknesses. Furthermore, we present a complete TDA pipeline that covers the entire process from preparation to augmentation to evaluation. Finally, we summarize future trends and highlight new opportunities in the TDA field, particularly in the context of generative AI.

The survey is structured as follows. Section [2](#page-3-0) provides preliminaries on TDA for ML and outlines an overall pipeline to characterize and classify approaches. Section [3](#page-8-0) offers a comprehensive overview of eight classes of pre-augmentation techniques. Section [4](#page-19-0) delves into the details of specific TDA techniques at different levels, categorized into retrieval-based and generation-based methods. Section [5](#page-31-0) explores post-augmentation techniques mainly for evaluation and optimization after TDA. Section [6](#page-34-0) discusses challenges and future directions, with an emphasize on the trending technologies like generative AI. Section [7](#page-37-0) concludes the survey.

#### <span id="page-3-0"></span>2 PRELIMINARIES

In this section, we will start by introducing the notation related to TDA and outlining the level-based taxonomy that defines the various levels of TDA methods (i.e., row, column, cell, and table) in Section [2.1.](#page-3-1) Subsequently, we will present the TDA pipeline and offer a taxonomy of methods from a task-oriented perspective in Section [2.2.](#page-6-0) In the following sections, tasks will serve as the primary basis for categorization, with levels providing a more granular categorization criterion.

#### <span id="page-3-1"></span>2.1 Notation in TDA and Level-based Taxonomy

First, we provide a formalization of tables, a prevalent data structure essential for the organization and presentation of data as follows.

Definition 1 (Table). <sup>A</sup> table is an arrangement that organizes data into rows and columns, forming a grid of cells for systematic information representation. Each cell, denoted as [, ], is at the intersection of row and column , serving as the basic unit for data storage. The rows ([, :]) run horizontally and group data entries, while columns ([:, ]) extend vertically, with each focusing on a specific data attribute. Additionally, metadata, such as table captions, provides contextual textual information around the table.

An intuitive example of a table and its primary components ‚Äî columns, rows, and cells ‚Äî is depicted in Fig. [1.](#page-1-0) Given a table , we use .<sup>‚Ñõ</sup> and . to denote the set of its rows and columns (attributes), respectively. Notably, our analysis is restricted to tables that solely manage numerical and textual data structured in rows and columns. This explicitly excludes tables that incorporate nested tables, lists, forms, images, or any other non-textual and non-numerical values within their cells. We now provide the formal definition of tabular data augmentation as follows.

Definition 2 (Tabular Data Augmentation, TDA). Given an original table and a specific ML model (Œò) parameterized by <sup>Œò</sup>, the task of Tabular Data Augmentation aims to expand

into an augmented table that includes additional data values in its rows and/or columns. The goal is for the ML model (Œò), trained with , to achieve superior model performance compared to the version trained with . Formally,

$$
T^{A} \leftarrow \text{TDA}(T^{O}, \text{level}, [\mathbb{T}], [\mathbb{G}]),
$$
  
s.t. 
$$
\mathbb{E}(f(\Theta^{A})) < \mathbb{E}(f(\Theta^{O})),
$$
\n
$$
(1)
$$

where level <sup>=</sup> {row, column, cell, table} refers to the granularity at which the TDA operates; <sup>T</sup>, an optional input for retrieval-based TDA, represents the pool of tables (simply called a table pool) for information enrichment use[4](#page-4-0) ; G, an optional input for generation-based TDA, implies the use of a specific generative method. and <sup>E</sup>( (Œò)) and <sup>E</sup>( (Œò)) refer to the empirical errors of the ML models trained on the datasets and , respectively[5](#page-4-1) .

<span id="page-4-2"></span>Example 1. Fig. [2](#page-5-0) depicts a typical TDA scenario: A data scientist aims to build a house price prediction model, but the available training data (the original table containing locations and prices) is limited in features and records, and contains numerous missing or incorrect metadata and cell values (shown in gray). A model trained on this low-quality data is likely to produce subpar results. To address this issue, the data scientist needs to augment the original training set with more comprehensive data. This can be done a) by retrieving additional data from table pools for retrieval-based TDA, or b) by generating new data using existing generative methods for generation-based TDA. The augmented data can include additional attributes, records, and/or cell values, reflected as enriched features and samples in the training set. The primary goal of this TDA process is to improve the overall quality and performance of the downstream house price prediction model.

Referring to Fig. [2,](#page-5-0) these TDA sub-tasks are instantiated based on the level that the TDA procedure is acting on. We formally define these tasks one by one as follows.

Definition 3 (Row-level TDA). One significant challenge in ML is the scarcity and often uneven distribution of samples, such as in long-tail data. To address this, row-level TDA involves adding additional rows to a given table . This procedure aims to obtain more samples for training, potentially increasing the variety of sample categories and altering the sample distribution to some extent. For row-level TDA, only additional rows are considered, and the relationship between and satisfies:

$$
(T^O \mathcal{A} = T^A \mathcal{A}) \wedge (T^O \mathcal{R} \subset T^A \mathcal{R}).
$$

Example 2. Continuing from the scenario in Example [1,](#page-4-2) the data scientist aims to expand the size and diversity of samples in the training set. Fig. [2](#page-5-0) (a) demonstrates a generation-based TDA that only takes the original table as input without external data. Generative methods typically learn the structure and the pattern of the original table and then generate new synthetic record (shown in purple), resulting in the augmented table 1 . These missing values can be addressed during post-processing steps, such as filtering and imputation.

Definition 4 (Column-level TDA). Adequate features are crucial for training high-performing ML models, but good features alone are not always enough. Therefore, column-level TDA involves extending the original table with additional columns, enriching the feature set. Retrieval-based TDA at the column level often involves joining related tables. This procedure may necessitate feature engineering

<span id="page-4-0"></span><sup>4</sup>The pool of tables, T, is often necessary for retrieval-based TDA methods. However, for generation-based TDA methods, the original table as input is usually sufficient. This is because generative models have typically been pre-trained on large amounts of external data, allowing them to retain and leverage a wealth of background information.

<span id="page-4-1"></span><sup>5</sup>Notably, the model training process can involve advanced feature engineering techniques such as coreset [\[93\]](#page-42-4) and feature selection [\[22\]](#page-39-8) to refine the training data from the input table. Here, we assume that these techniques will be identically applied despite the difference in the input tables.

<span id="page-5-0"></span>![](_page_5_Figure_1.jpeg)

**Caption:** Figure 2 showcases the instantiation of TDA tasks at various levels: row-level TDA adds new rows, column-level TDA introduces new columns, cell-level TDA addresses missing values, and table-level TDA enhances the table with both rows and columns. It highlights retrieval-based methods (grey arrows) and generation-based methods (blue arrows) for augmenting the original table, using a housing dataset as an example.

Fig. 2. Instantiation of TDA tasks at different levels: (a) row-level TDA, which adds new rows to the original table, and (b) column-level TDA, which introduces new columns, (c) cell-level TDA addresses missing values, while (d) table-level TDA enhances the table by adding both rows and columns. From another viewpoint, retrieval-based TDA methods (grey arrows) augment the original table with data sourced from the table pool, while generation-based TDA methods (blue arrows) generate new data directly based on the original table. Both retrieval- and generation-based TDA methods can be implemented at various levels, including row, column, cell, and table. The illustration shows only some of these possibilities for clarity. We use the housing dataset [\[24\]](#page-39-9) for the toy example.

to remove less informative records and features, which can result in the augmented table having fewer rows than the original (| .‚Ñõ| <sup>&</sup>gt; <sup>|</sup> .‚Ñõ|). Generation-based column-level TDA, on the other hand, retains the number of rows (| .‚Ñõ| <sup>=</sup> <sup>|</sup> .‚Ñõ|). All in all, it satisfies:

$$
(T^{O}.\mathcal{A} \subset T^{A}.\mathcal{A}) \wedge (|T^{O}.\mathcal{R}| \geq |T^{A}.\mathcal{R}|).
$$

Example 3. Continuing from Example [1,](#page-4-2) the data scientist decides to expand the number of features in . Still, we take the retrieval-based TDA as an example, as shown in Fig. [2](#page-5-0) (b). This procedure involves computing column similarity. For example, if the column "Location" (the 2nd column in ) and the column "City" (the 1st column in table 1 ) have largely similar values, can be augmented with the column "DIS (distance)" (the 2nd column from 1 ), resulting in the augmented table 2 .

Definition 5 (Cell-level TDA). Empty table cells are common, and generating data representations with null values can lead to suboptimal results in downstream tasks. Cell-level TDA involves filling these empty table cells to improve the quality of training data for ML tasks. For cell-level TDA, the relationship between and satisfies:

$$
(T^{O} \mathcal{A} = T^{A} \mathcal{A}) \wedge (|T^{O} \mathcal{R}| = |T^{A} \mathcal{R}|) \wedge (\forall i, j : T^{A}[i, j] \neq \text{null}).
$$

Example 4. Continuing from Example [1,](#page-4-2) the data scientist decides to fill the empty cells using a generation-based TDA, as shown in Fig. [2](#page-5-0) (c). Generative methods leverage the context (e.g., statistical distribution) from the original table to fill in missing metadata and data values (shown in blue), resulting in the augmented table 3 .

Definition 6 (Table-level TDA). Table-level TDA involves enriching the original table with both additional rows and columns. This aims to acquire more features and samples for ML purposes. For table-level TDA, the relationship between and satisfies:

$$
(T^O.\mathcal{A} \subset T^A.\mathcal{A}) \wedge (|T^O.\mathcal{R}| < |T^A.\mathcal{R}|).
$$

Example 5. Continuing from Example [1,](#page-4-2) the data scientist now seeks a more balanced approach that can augment the table along both the row and column dimensions. Using a retrieval-based TDA, as shown in Fig. [2](#page-5-0) (d), the approach first retrieves the top-related tables from the table pool (e.g., 1 and 2 ). These tables are then integrated with , resulting in the augmented table 4 , which includes additional rows (e.g., the 1st row from 2 ) and columns (e.g., the 2nd column from 1 ).

The aforementioned TDA methods of four different levels are summarized in Table [2.](#page-6-1) While the approaches differ, they all aim to enhance the quality of the original training dataset, thereby improving the performance of the resulting trained model.

<span id="page-6-1"></span>

| level  | Description                                              | Relationship between ùëá<br>ùê¥ and ùëá<br>ùëÇ                                                     |
|--------|----------------------------------------------------------|--------------------------------------------------------------------------------------------|
| row    | enrich<br>ùëÇ with additional rows                         | (<br>ùëÇ .ùíú<br>ùê¥.ùíú) ‚àß (<br>ùëÇ .‚Ñõ ‚äÇ<br>ùê¥.‚Ñõ)<br>= ùëá                                             |
| column | ùëá<br>enrich<br>ùëÇ with additional columns                 | ùëá<br>ùëá<br>ùëá<br>(<br>ùëÇ .ùíú ‚äÇ<br>ùê¥.ùíú) ‚àß ( <br>ùëÇ .‚Ñõ  ‚â•  <br>ùê¥.‚Ñõ )                              |
| cell   | ùëá<br>fill in the missing values within the cells of<br>ùëÇ | ùëá<br>ùëá<br>ùëá<br>ùëá<br>(<br>ùëÇ .ùíú<br>ùê¥.ùíú) ‚àß ( <br>ùëÇ .‚Ñõ <br>=  <br>ùê¥.‚Ñõ ) ‚àß<br>= ùëá               |
|        | ùëá                                                        | ùëá<br>ùëá<br>ùëá<br>(‚àÄ<br>:<br>ùê¥[<br>]<br>‚â† null)                                               |
| table  | enrich<br>ùëÇ with both additional rows and columns<br>ùëá   | ùëñ, ùëó<br>ùëá<br>ùëñ, ùëó<br>(<br>ùëÇ .ùíú ‚äÇ<br>ùê¥.ùíú) ‚àß ( <br>ùëÇ .‚Ñõ <br><  <br>ùê¥.‚Ñõ )<br>ùëá<br>ùëá<br>ùëá<br>ùëá |

Table 2. The level-based taxonomy for TDA methods.

## <span id="page-6-0"></span>2.2 Pipeline of TDA and Task-based Taxonomy

In this section, we provide an overview of the key topics covered in the survey, structured around the TDA pipeline from a task-oriented perspective, as shown in Fig. [3.](#page-6-2) We categorize tasks by levels for finer classification. The pipeline highlights critical stages and procedures from the original training dataset to the augmented training dataset . We first overview the entire TDA pipeline, followed by a brief introduction to each pivotal procedure within it.

<span id="page-6-2"></span>![](_page_6_Figure_7.jpeg)

**Caption:** Figure 3 provides an overview of the TDA pipeline, detailing the three main procedures: pre-augmentation, augmentation, and post-augmentation. The data-centric stage includes preparation tasks to enhance augmentation, while the model-centric stage focuses on evaluating and optimizing the augmented dataset for improved machine learning performance, illustrating the comprehensive workflow of TDA.

Fig. 3. The overview of TDA pipeline and the task-based taxonomy for TDA approaches. The input and output of the TDA pipeline are the original table and the augmented table , respectively. The TDA pipeline comprises three main procedures: pre-augmentation, augmentation, and post-augmentation.

The TDA pipeline has two main stages:

- Data-centric stage includes pre-augmentation and augmentation procedures to transform the original data into augmented data. Pre-augmentation involves preparation to enhance augmentation, while augmentation details existing TDA methods, divided into retrieval-based and generation-based methods.
- Model-centric stage focuses on post-augmentation, primarily involving ML model training and evaluation to assess and optimize the augmented dataset. If the augmented dataset is not satisfactory, it is necessary to return to the data-centric stage for further augmentation.

(1) Pre-Augmentation. In the TDA pipeline, pre-augmentation encompasses preparation tasks to facilitate effective augmentation. For tabular data, issues like missing or incorrect cell values and unreliable metadata are common due to inappropriate data sharing and incompatible naming conventions [\[33\]](#page-39-1). For the table pool, with tables reaching millions or more, data preparation before TDA is crucial. Pre-augmentation aims to a) improve the quality of both the original table and the tables from the pool (in the case of retrieval-based methods) and b) better organize the table pool for improved acceleration and scalability. This can involve a range of tasks, applicable to both single-table and multi-table settings, as listed in Table [3.](#page-7-0) The table's right part presents their support for various different TDA tasks. The following observations can be drawn:

- (1) The tasks in single-table setting are applicable to all target TDA tasks, as all target TDA tasks require preprocessing of the original table.
- (2) The multi-table setting tasks are specifically for handling tables from the pool and may not be suitable for generation-based TDA tasks.
- (3) Entity matching, a pre-augmentation task that focuses on the relationships between rows, may not be much beneficial for the schema augmentation (sa<sup>r</sup> ), a TDA task at the column level.
- Section [3](#page-8-0) will provide a detailed introduction to relevant techniques for pre-augmentation tasks.

<span id="page-7-0"></span>Table 3. Overview of the pre-augmentation tasks and their target TDA tasks (see Table [4](#page-8-1) for the definitions): ea<sup>r</sup> (Entity Augmentation), sa<sup>r</sup> (Schema Augmentation), cc<sup>r</sup> (Cell Completion), ti<sup>r</sup> (Table Integration), rg<sup>g</sup> (Record Generation), fc<sup>g</sup> (Feature Construction), ci<sup>g</sup> (Cell Imputation), and ts<sup>g</sup> (Table Synthesis).

| Setting | Task Name            | Description                                          |     |     | Retrieval-based TDA |     |     |     |     | Generation-based TDA |
|---------|----------------------|------------------------------------------------------|-----|-----|---------------------|-----|-----|-----|-----|----------------------|
|         |                      |                                                      | ear | sar | ccr                 | tir | rgg | fcg | cig | tsg                  |
|         | Error Handling       | process dirty data in tables                         | !   | !   | !                   | !   | !   | !   | !   | !                    |
| Single  | Table Annotation     | infer table metadata information                     | !   | !   | !                   | !   | !   | !   | !   | !                    |
| table   | Table Simplification | streamline a table down to its essential elements    | !   | !   | !                   | !   | !   | !   | !   | !                    |
|         | Table Representation | encode table elements to a latent vector space       | !   | !   | !                   | !   | !   | !   | !   | !                    |
|         | Table Indexing       | assign a unique identifier to table elements         | !   | !   | !                   | !   |     |     |     |                      |
| Multi   | Table Navigation     | organize the table pool by connecting similar tables | !   | !   | !                   | !   |     |     |     |                      |
| table   | Schema Matching      | find matching pairs of columns in different tables   | !   | !   | !                   | !   |     |     |     |                      |
|         | Entity Matching      | find matching pairs of entities in different tables  | !   |     | !                   | !   |     |     |     |                      |

(2) Augmentation. Augmentation is the core procedure of the TDA pipeline, enhancing the original table with more data to improve downstream ML tasks. The techniques for TDA can be broadly divided into retrieval-based and generation-based methods. Retrieval-based methods are considered a data-driven TDA task based on the original table (called query table), with table pools as additional input. The key to this type of methods lies in properly modeling the similarity between the query table and the tables from the pool. Generation-based methods can effectively leverage pre-existing knowledge acquired from pre-training to augment the input tables, without requiring additional data sources. Both methods can be further subdivided by the level of the augmentation, as listed in Table [4.](#page-8-1) Section [4](#page-19-0) will cover the typical techniques for these TDA tasks.

| Method              | level                          | TDA Task                                                                                                           | Description                                                                                                                                                                                                               |
|---------------------|--------------------------------|--------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Retrieval<br>based  | row<br>column<br>cell<br>table | ear<br>Entity Augmentation,<br>sar<br>Schema Augmentation,<br>ccr<br>Cell Completion,<br>tir<br>Table Integration, | add rows retrieved from the table pool<br>add columns retrieved from the table pool<br>complete empty cells with values retrieved from the table pool<br>integrate retrieved tables from the pool with the original table |
| Generation<br>based | row<br>column<br>cell<br>table | rgg<br>Record Generation,<br>fcg<br>Feature Construction,<br>cig<br>Cell Imputation,<br>tsg<br>Table Synthesis,    | generate new records (rows) by generative models<br>construct new features (columns) by generative models<br>impute empty cells by generative models<br>synthesize rows and columns by generative models                  |

<span id="page-8-1"></span>Table 4. Overview of the TDA tasks. The superscripts r and g indicate the TDA task is whether retrieval-based and generation-based, respectively.

(3) Post-Augmentation. Finally, the TDA pipeline includes a post-augmentation procedure that occurs primarily after ML model training. This involves evaluating and optimizing the augmented TDA results to enhance the performance of the downstream ML task. We focus on three key aspects in post-augmentation: TDA datasets, evaluation policies, and optimization strategies. We first elaborate on the commonly used datasets associated with TDA and these datasets' characteristics. We then analyze evaluation policies from two perspective: original-table-based evaluation, which compares the augmented table with the original one (e.g, comparing their statistical distributions), and model-based evaluation, which compares the performance of ML models trained on augmented datasets versus baseline datasets. We finally delve into the optimization strategies. The downstream ML model can iteratively optimize augmented results by retaining data that improves performance until a target accuracy is reached. More complex strategies, like reinforcement learning (RL) based frameworks, can also guide the data optimization process. Section [5](#page-31-0) will detail post-augmentation techniques from these three aspects.

#### <span id="page-8-0"></span>3 TECHNIQUES IN PRE-AUGMENTATION

In this section, we review the techniques used in the pre-augmentation procedure, as introduced in Section [2.2.](#page-6-0) As shown in Table [5,](#page-9-0) we have selected a collection of representative TDA works and summarized the pre-augmentation tasks they involve. Most of the selected works are oriented to TDA and have been published in well-known conferences or journals with high citation counts, reflecting their significance within the field. We have also included several target tasks other than TDA (see the rightmost part of Table [5\)](#page-9-0), namely table search [\[15\]](#page-38-2) and semantics detection [\[105\]](#page-43-2), as these often serve as intermediate steps in TDA.

Our task-oriented approach, illustrated in Table [3,](#page-7-0) examines four pre-augmentation tasks for the single-table setting (Sections [3.1](#page-8-2) to [3.4\)](#page-12-0) and four for the multi-table setting (Sections [3.5](#page-14-0) to [3.8\)](#page-17-0). Pre-augmentation is essential for most TDA works, and the pre-augmentation tasks in Table [3](#page-7-0) are not mutually exclusive. A TDA work may involve one or more of these eight tasks. For example, Infogather [\[100\]](#page-42-5) (No.6 work in Table [5\)](#page-9-0) employs multiple pre-augmentation tasks (table representation, table annotations, etc.) to complete its entire TDA process.

#### <span id="page-8-2"></span>3.1 Error Handling

Error handling in pre-augmentation refers to the preprocessing of the dirty data in tables. Realworld tabular data often contain errors, such as mistakenly substituted proximal characters and unnecessary repetition of tokens in cell values. Generally, there are three types of errors that considered in pre-augmentation, missing values [\[14,](#page-38-5) [114\]](#page-43-3), misspellings [\[30,](#page-39-10) [42\]](#page-40-5), and numerical outliers [\[22\]](#page-39-8). When generating table representations based on token embedding with such errors,

<span id="page-9-0"></span>

| No.      | Reference                                                             | Year<br>Pub. | Handling<br>Error    | Annotation<br>Table | Simplification<br>Table        | Pre-augmentation<br>Representation<br>Table | Indexing<br>Table<br>Methods | Navigation<br>Table   | Matching<br>Schema    | Entity     | Augmentation | Tasks<br>Search<br>Target<br>Table | Detection<br>Semantics |
|----------|-----------------------------------------------------------------------|--------------|----------------------|---------------------|--------------------------------|---------------------------------------------|------------------------------|-----------------------|-----------------------|------------|--------------|------------------------------------|------------------------|
|          |                                                                       |              |                      |                     |                                |                                             |                              |                       |                       | Matching   |              |                                    |                        |
| 1<br>2   | [60]<br>al.<br>al.<br>et<br>et<br>Limaye<br>Venetis                   | 2010         | ‚àñ<br>‚àñ               | Ontology            | ‚àñ<br>‚àñ                         | Content                                     | ‚àñ<br>‚àñ                       | ‚àñ<br>‚àñ                | ‚àñ                     | ‚àñ<br>‚àñ     |              | !<br>!                             | !<br>!                 |
| 3        | [91]<br>[12]<br>MICE                                                  | 2011<br>2011 | ‚àñ                    | Ontology<br>‚àñ       | ‚àñ                              | Content<br>Content                          | ‚àñ                            | ‚àñ                     | Textual<br>‚àñ          | ‚àñ          | !            |                                    |                        |
| 4        | [86]<br>MissForest                                                    | 2012         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 5<br>6   | [27]<br>InfoGather<br>al.<br>et<br>Sarma                              | 2012         | ‚àñ<br>‚àñ               | ‚àñ<br>‚àñ              | Summarization<br>‚àñ             | Content+Metadata<br>Content+Metadata        | Inverted<br>‚àñ                | ‚àñ<br>‚àñ                | Textual+Metadata      | KB         | !            | !                                  |                        |
| 7        | [3]<br>[100]<br>al.<br>et<br>Ahmadov                                  | 2012<br>2015 | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content+Metadata                            | index<br>index<br>Inverted   | ‚àñ                     | Textual+Metadata<br>‚àñ | ‚àñ          | !            |                                    |                        |
| 8        | [7]<br>TabEL                                                          | 2015         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | DB         | !<br>!       | !                                  |                        |
| 9        | [47]<br>Veeramachaneni<br>al.<br>et<br>Christophides<br>and<br>Kanter | 2015         | ‚àñ<br>‚àñ               | ‚àñ<br>‚àñ              | ‚àñ<br>‚àñ                         | Content                                     | ‚àñ<br>‚àñ                       | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ                | KB<br>‚àñ    | !            | !                                  |                        |
| 10<br>11 | [23]<br>ExploreKit [48]                                               | 2015<br>2016 | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content<br>Content                          | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 12       | [119]<br>Ensemble<br>LSH                                              | 2016         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | LSH                          | ‚àñ                     | ‚àñ                     | ‚àñ          | !            | !                                  |                        |
| 13       | [109]<br>EntiTables                                                   | 2017         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | index<br>Inverted            | ‚àñ                     | ‚àñ                     | KB+DB      | !<br>!       | !                                  |                        |
| 14       | [75]<br>Aurum<br>TUS                                                  | 2018         | Implicit             | Supervised-learning | ‚àñ                              | Content                                     | LSH                          | ‚àñ                     | Textual               | ‚àñ          |              | !                                  |                        |
| 15       | table-GAN<br>[15]                                                     | 2018         | ‚àñ<br>‚àñ               | ‚àñ<br>‚àñ              | ‚àñ<br>‚àñ                         | Content<br>Content                          | ‚àñ<br>‚àñ                       | graph<br>Linkage<br>‚àñ | Numerical<br>‚àñ        | ‚àñ<br>‚àñ     | !            |                                    |                        |
| 16<br>17 | [78]<br>[103]<br>GAIN                                                 | 2018<br>2018 | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 18       | [108]<br>Table2Vec                                                    | 2019         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | KB+DB      | !            |                                    |                        |
| 19       | [118]<br>JOSIE                                                        | 2019         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | index<br>Inverted            | ‚àñ                     | ‚àñ                     | ‚àñ          | !<br>!       | !                                  |                        |
| 20       | [44]<br>Sherlock                                                      | 2019         | ‚àñ                    | Supervised-learning | Summarization<br>‚àñ             | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | KB+DB<br>‚àñ | !            |                                    |                        |
| 21       | CellAutoComplete [110]<br>MIWAE                                       | 2019         | ‚àñ<br>‚àñ               | ‚àñ<br>‚àñ              | ‚àñ                              | Content+Metadata                            | ‚àñ<br>‚àñ                       | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ                | ‚àñ          | !            |                                    |                        |
| 22       | [70]<br>ITS-GAN                                                       | 2019         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content<br>Content                          | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 23       | [19]<br>PATE-GAN                                                      | 2019         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 24<br>25 | [46]<br>CTGAN[99]                                                     | 2019<br>2019 | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 26       | [55]<br>MisGAN                                                        | 2019         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 27       | [76]<br>HI-VAE                                                        | 2020         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 28       | [9]<br>3L<br>D                                                        | 2020         | Implicit             | Supervised-learning | ‚àñ                              | Content                                     | LSH                          | ‚àñ                     | Textual+Numerical     | ‚àñ          | !<br>!       | !                                  |                        |
| 29       | [22]<br>EmbDI<br>ARDA                                                 | 2020         | Explicit             | ‚àñ<br>‚àñ              | Sampling<br>‚àñ                  | Content                                     | ‚àñ<br>‚àñ                       | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ     | !            |                                    |                        |
| 30       | [13]<br>Sato                                                          | 2020         | Explicit<br>Implicit | Supervised-learning | Summarization                  | Content+Context<br>Content+Context          | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          |              |                                    | !                      |
| 31<br>32 | [74]<br>al.<br>et<br>[105]<br>Nargesian                               | 2020<br>2020 | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | Linkage               | ‚àñ                     | ‚àñ          |              | !                                  |                        |
| 33       | [8]<br>DLN                                                            | 2021         | ‚àñ                    | ‚àñ                   | Sampling                       | Content+Metadata                            | ‚àñ                            | graph<br>‚àñ            | ‚àñ                     | ‚àñ          |              | !                                  |                        |
| 34       | [29]<br>PEXESO                                                        | 2021         | Implicit             | ‚àñ                   | ‚àñ                              | Content                                     | index<br>Inverted            | ‚àñ                     | Textual               | ‚àñ          | !<br>!       | !                                  |                        |
| 35       | [65]<br>al.<br>RONIN<br>et<br>Liu                                     | 2021         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | Hierarchical<br>‚àñ     | ‚àñ                     | ‚àñ          |              | !                                  |                        |
| 36       | [77]<br>cWGAN                                                         | 2021         | ‚àñ<br>‚àñ               | ‚àñ<br>‚àñ              | ‚àñ<br>‚àñ                         | Content                                     | ‚àñ<br>‚àñ                       | structure<br>‚àñ        | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ     | !            |                                    |                        |
| 37<br>38 | [31]<br>[4]<br>SIGRNN                                                 | 2021<br>2021 | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content<br>Content                          | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 39       | [26]<br>MIGAN                                                         | 2021         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 40       | [114]<br>Leva<br>MATE                                                 | 2022         | Explicit             | ‚àñ                   | ‚àñ                              | Content<br>Content                          | Inverted<br>‚àñ                | ‚àñ                     | ‚àñ                     | ‚àñ          | !<br>!       |                                    |                        |
| 41       | [32]<br>ALITE                                                         | 2022         | ‚àñ<br>‚àñ               | ‚àñ                   | ‚àñ<br>‚àñ                         |                                             | index<br>‚àñ                   | ‚àñ                     | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ     | !            |                                    |                        |
| 42<br>43 | [64]<br>AutoFeature<br>[50]                                           | 2022<br>2022 | ‚àñ                    | PLMs<br>‚àñ           | Sampling                       | Content<br>Content                          | ‚àñ                            | Clustering<br>‚àñ       | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 44       | [16]<br>al.<br>et<br>Chai                                             | 2022         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | Clustering            | ‚àñ                     | ‚àñ          | !            | !                                  |                        |
| 45       | [84]<br>al.<br>et<br>Santos                                           | 2022         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | Numerical             | ‚àñ          | !            | !<br>!                             |                        |
| 46       | [89]<br>StruBERT<br>TURL                                              | 2022         | Implicit             | ‚àñ<br>‚àñ              | Summarization<br>Summarization | Content+Context+Metadata                    | ‚àñ<br>‚àñ                       | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ     | !            |                                    | !                      |
| 47       | [73]<br>al.<br>et<br>Nargesian<br>[28]                                | 2022         | Implicit<br>‚àñ        | ‚àñ                   | ‚àñ                              | Content+Context+Metadata<br>Content         | ‚àñ                            | Linkage               | ‚àñ                     | ‚àñ          |              | !                                  |                        |
| 48<br>49 | [87]<br>Doduo                                                         | 2022<br>2022 | Implicit             | PLMs                | ‚àñ                              | Content+Context                             | ‚àñ                            | graph<br>‚àñ            | ‚àñ                     | ‚àñ          |              |                                    | !                      |
| 50       | [95]<br>TransTab                                                      | 2022         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | graph<br>Linkage      | ‚àñ                     | ‚àñ          | !<br>!       |                                    |                        |
| 51       | [52]<br>SOS                                                           | 2022         | Implicit<br>‚àñ        | PLMs<br>‚àñ           | Summarization<br>‚àñ             | Content                                     | ‚àñ<br>‚àñ                       | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ     |              |                                    | !                      |
| 52       | Watchog [71]                                                          | 2023         | Implicit             | ‚àñ                   | Sampling                       | Content+Context+Metadata<br>Content         | HNSW                         | ‚àñ                     | Textual               | ‚àñ          | !            | !                                  |                        |
| 54<br>53 | DeepJoin [30]<br>[49]<br>SANTOS                                       | 2023<br>2023 | Implicit             | Supervised-learning | ‚àñ                              | Content+Context                             | index<br>Inverted            | ‚àñ                     | Textual               | ‚àñ          | !            | !                                  |                        |
| 55       | [34]<br>Starmie                                                       | 2023         | Implicit             | ‚àñ                   | Sampling                       | Content+Context                             | LSH+HNSW                     | ‚àñ                     | Textual               | ‚àñ          | !            | !                                  |                        |
| 56       | [42]<br>AUTOTUS                                                       | 2023         | Implicit             | ‚àñ                   | Sampling                       | Content+Context                             | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !<br>!       | !                                  |                        |
| 57       | [40]<br>HYTREL<br>RADA                                                | 2023         | Implicit             | ‚àñ<br>‚àñ              | ‚àñ<br>‚àñ                         | Content+Context+Metadata                    | ‚àñ<br>‚àñ                       | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ     |              | !                                  | !                      |
| 58<br>59 | [21]<br>[67]<br>GOGGLE                                                | 2023<br>2023 | Implicit<br>‚àñ        | ‚àñ                   | ‚àñ                              | Content+Context<br>Content+Context          | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 60       | [113]<br>GANBLR                                                       | 2023         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 61       | STaSy [51]<br>CoDi                                                    | 2023         | ‚àñ<br>‚àñ               | ‚àñ<br>‚àñ              | ‚àñ<br>‚àñ                         | Content                                     | ‚àñ<br>‚àñ                       | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ     | !<br>!       |                                    |                        |
| 62       | [53]<br>TabCSDI                                                       | 2023         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content<br>Content                          | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 64<br>63 | [116]<br>[98]<br>GAINS                                                | 2023<br>2023 | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 65       | OmniMatch<br>[92]<br>MOAT                                             | 2023         | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            | !                                  |                        |
| 66       | [25]                                                                  | 2024         | Implicit<br>Implicit | ‚àñ<br>‚àñ              | ‚àñ<br>‚àñ                         | Content<br>Content                          | ‚àñ<br>‚àñ                       | graph<br>Linkage<br>‚àñ | ‚àñ<br>‚àñ                | ‚àñ<br>‚àñ     |              | !                                  |                        |
| 67<br>68 | FeatNavigator [59]<br>[66]<br>RelDDPM                                 | 2024<br>2024 | ‚àñ                    | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !            |                                    |                        |
| 69       | [62]<br>SMARTFEAT                                                     | 2024         | Implicit             | ‚àñ                   | ‚àñ                              | Content+Context                             | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          | !<br>!       |                                    |                        |
| 70       | [90]<br>DP-LLMTGen                                                    | 2024         | Implicit             | ‚àñ                   | ‚àñ                              | Content                                     | ‚àñ                            | ‚àñ                     | ‚àñ                     | ‚àñ          |              |                                    |                        |

Table 5. Overview of pre-augmentation tasks. The table contains a non-exhaustive list of representative TDA-related works (arranged in chronological order) and the corresponding pre-augmentation tasks they involve. Detailed categorization of each task is provided in the respective sections.

incorrect forecasts are inevitable [\[42\]](#page-40-5). Therefore, various methods have been developed to address tabular data errors, which can be categorized into explicit and implicit approaches.

Explicit error handling involves detecting and correcting errors directly. For example, ARDA [\[22\]](#page-39-8) handles missing data with a simple approach that uses uniform random sampling for categorical columns and the median value for numerical columns. Cappuzzo et al. [\[14\]](#page-38-5) handle missing values using classical database techniques like Skolemization [\[97\]](#page-42-15), which introduces a new constant or function symbol (called a Skolem function) to represent the unknown or missing value. Zhao and Fernandez [\[114\]](#page-43-3) detect missing data through a voting mechanism. They observe that missing values are usually covered by multiple attributes while common values typically only appear under very few. Thus, they identify value nodes with votes exceeding a predefined threshold for different attributes as missing data.

Implicit error handling does not directly detect errors but instead enhances the model's robustness to them. For example, Hu et al. [\[42\]](#page-40-5) propose a table noise generator in their automatic augmentation framework. The generator introduces artificial table noise to the training data to enhance the model's ability to handle potential errors. They generate cell value noise in three ways: (1) replacing/inserting characters with proximal characters, (2) deleting/repeating characters, and (3) changing the numeral display format such as using scientific notation. Furthermore, multiple studies [\[9,](#page-38-8) [21,](#page-39-19) [25,](#page-39-20) [28](#page-39-18)[‚Äì30,](#page-39-10) [34,](#page-39-2) [40,](#page-40-14) [42,](#page-40-5) [49,](#page-40-13) [59,](#page-41-16) [62,](#page-41-17) [71,](#page-41-14) [75,](#page-41-7) [87,](#page-42-11) [89,](#page-42-10) [90,](#page-42-1) [105\]](#page-43-2) adopt word embedding that can tolerate misspellings to some extent. For example, Dong et al. [\[30\]](#page-39-10) support semantic joins by using word table representation learning to join cells with similar meanings, thus handling data with misspellings and discrepancies in formats or terminologies.

Remarks. Real-world tables often contain various types of errors, making it essential for table augmentation models to be robust in combating such issues. Explicit methods require an established procedure of error detection and correction, adding additional steps and potentially leading to error propagation. On the other hand, implicit methods, while more streamlined, lack the interpretability of explicit methods. Moreover, both methods typically ignore addressing errors in numerical columns, such as out-of-bound values, which are often harder to detect and correct than textual errors. It is worth mentioning a recent trend of exploring the use of pre-trained language models (PLMs) for table representation. These approaches [\[34,](#page-39-2) [42\]](#page-40-5) show certain resistance to spelling errors and offer a promising direction for handling noisy data.

#### <span id="page-10-0"></span>3.2 Table Annotation

Table annotation involves inferring metadata information about a table, such as column types and the relationship between columns [\[87\]](#page-42-11). This task helps recover the semantic information within a table and is particularly useful for table augmentation by assessing the similarity between tables. Typically, metadata for tables is unreliable or incomplete due to inappropriate data sharing methods [\[33\]](#page-39-1). Even when metadata is available, tables from a wide range of sources can have incompatible metadata with different naming conventions and terminologies. Consequently, table annotation is crucial for retrieving syntactically and semantically relevant tables to augment the original table. These approaches are divided into the three following categories.

Ontology-based table annotation. In earlier research, ontologies were frequently used for annotating table elements. For example, as illustrated in Fig. [4](#page-11-0) (a), Limaye et al. [\[60\]](#page-41-6) develop a classic table discovery system that utilizes an ontology for table annotation at multiple levels. This includes cell-level annotation with ontology entities (e.g., labeling the cell "mechanical ape!" as a song name), column-level annotation with ontology types (e.g., identifying the column header "Title" as "Song"), and pairwise column annotation with ontology relationships (e.g., annotating the relationship

between "Artist" and "Song" as "written-by"). Similarly, Venetis et al. [\[91\]](#page-42-6) annotate columns using class labels from a "isA" database for column labels and binary relationships automatically extracted from the Web for relationship labels.

<span id="page-11-0"></span>![](_page_11_Figure_2.jpeg)

**Caption:** Figure 4 illustrates table annotation techniques, including ontology-based, supervised-learning-based, and PLM-based methods. It highlights how these approaches infer metadata information about tables, such as column types and relationships, which is crucial for enhancing the effectiveness of tabular data augmentation by improving the semantic understanding of the tables.

Fig. 4. The illustration of table annotation, including (a) ontology-based table annotation, (b) supervisedlearning-based table annotation, and (c) PLM-based table annotation.

Supervised-learning-based table annotation. Recent studies have leveraged labeled data and supervised learning techniques for table annotation. Specifically, researchers [\[44,](#page-40-8) [105\]](#page-43-2) train ML models on tables labeled with a fixed set of 78 types (address, description, year, etc.) and then use the trained models to annotate unknown tables, as illustrated in Fig. [4](#page-11-0) (b). On the other hand, feature-based approaches [\[9,](#page-38-8) [49,](#page-40-13) [75\]](#page-41-7) that compute several similarity signals (e.g., value overlap) to learn column representations have been used to detect semantic types. SATO [\[105\]](#page-43-2) further enhances semantic type detection at the column level by incorporating the topic of table (a.k.a. global context[6](#page-11-1) ) as a new similarity signal for column representation.

PLM-based table annotation. More recently, PLMs have been employed for table annotation due to their superior performance [\[50,](#page-40-11) [71,](#page-41-14) [87\]](#page-42-11). For example, ALITE [\[50\]](#page-40-11) first leverages PLMs to encode columns, and then these column embeddings are used for clustering to determine the column IDs. Suhara et al. [\[87\]](#page-42-11) propose Doduo, a multi-task learning framework, as illustrated in Fig. [4](#page-11-0) (c). Specifically, Doduo uses a single BERT model to complete both tasks of predicting the column types and the relationships. Additionally, Doduo incorporates the entire table as input to capture the table context. Despite recent efforts, existing methods rely heavily on large-scale and highquality labeled data. Miao and Wang [\[71\]](#page-41-14) propose Watchog, a lightweight framework for column annotation. Watchog employs contrastive learning techniques to learn robust representations for tables while maintaining minimal overhead, leveraging a large-scale unlabeled table pool.

Remarks. Real-world tables often have incomplete and incorrect metadata, necessitating table annotation to recover table semantics. Ontology-based table annotation can be limited by the ontology's coverage, particularly for domain-specific data, while supervised-learning table annotation depend heavily on large-scale, high-quality labeled data. Ontology-based methods typically offer high efficiency, whereas learning methods (including those based on supervised learning and those based on PLMs) achieve higher accuracy. One potential direction for improvement involves integrating these methods and exploring the efficient use of PLMs for table annotation. Additionally, integrating LLMs and Retrieval-Augmented Generation (RAG) [\[112\]](#page-43-12) with ontologies

<span id="page-11-1"></span><sup>6</sup> In the context of learning from tabular data, global context refers to the values from the entire table, whereas local context pertains to values from neighboring table elements. For example, for a specific row in the original table, the rows immediately above and below it represent the local context, while the entire original table represents the global context.

can lead to promising developments, particularly in domain-specific scenarios where leveraging external business knowledge is crucial.

#### <span id="page-12-1"></span>3.3 Table Simplification

Table simplification involves streamlining a table down to its essential elements, which can be addressed from both the content and semantic perspectives. From the table content perspective, this procedure, known as table sampling, involves selecting portions of the table to retain as much information as possible. This is particularly useful for fitting data into limited token lengths for language models. From the table semantics perspective, the procedure, referred to as table summarization, entails identifying the main topic or theme of the table to better understand its meaning. Accurate summarization helps in comparing tables for similarity and ensures that any added rows and columns remain consistent with the table's original theme. These two different perspectives are introduced as follows.

Table sampling selectively choose table content to preserve the original information as much as possible. An early work [\[8\]](#page-38-10) directly selects the top- samples for each column as input. AutoFeature [\[64\]](#page-41-1) and ARDA [\[22\]](#page-39-8) adopts the stratified sampling method that divides the samples into several subsets and randomly selects samples from each subset. Deepjoin[\[30\]](#page-39-10) adopts a frequency-based approach that samples the most frequent cell values for each column. Meanwhile, Starmie [\[34\]](#page-39-2) and AUTOTUS [\[42\]](#page-40-5) sample rows based on an importance score derived from the cell-level TF-IDF score for each row.

Table summarization aims to extract semantic themes or topics within tables. For example, Zhang et al. [\[105\]](#page-43-2) incorporate a topic-aware prediction module into their framework SATO, responsible for summarizing table semantics. In particular, this module produces a topic vector from the values across the entire table, representing the global context of that table. SATO's ablation experiments show that considering a table's global context improves table understanding and mitigates ambiguities. For web tables, there are often pre-existing table metadata, such as table captions that summarize the table's contents [\[111\]](#page-43-1). In this case, various works [\[28,](#page-39-18) [40,](#page-40-14) [71,](#page-41-14) [89,](#page-42-10) [100,](#page-42-5) [110\]](#page-43-9) directly leverage these metadata by converting them into vectors and concatenating them with the table's representation.

Remarks. Table simplification aims to extract the core information of a table either in content or semantic level. At the content level, table sampling selectively samples table content for constraints like limited token length for LLMs. Table sampling can also improve efficiency and scalability when facing large-scale tables. At present, most table sampling methods are statisticalbased which are fast and resource-efficient, yet they fall short in terms of precision. Sampling based solely on the value distribution (such as sampling the most frequent values), can lead to an incomplete representation of the original table. Therefore, there is a need to investigate more efficient and accurate sampling algorithms, potentially leveraging learning-based approaches. At the semantic level, table summarization serves both to extract the key semantic essence of the table and verify the coherence of any table augmentations. At present, as a preprocessing step, there are relatively few techniques that directly extract the primary topics or themes of tables. This is largely due to the challenge of achieving lightweight, efficient table summaries. Emerging LLMs may offer a promising solution in this regard.

#### <span id="page-12-0"></span>3.4 Table Representation

Table representation involves converting the table elements such as rows, columns, and cells into a latent vector space. This transformation prepares the table for robust use in subsequent TDA model. The past decade has witnessed the effectiveness of employing deep neural networks for table representation learning [\[9,](#page-38-8) [28](#page-39-18)[‚Äì30,](#page-39-10) [34,](#page-39-2) [42,](#page-40-5) [50,](#page-40-11) [71,](#page-41-14) [75,](#page-41-7) [87,](#page-42-11) [105\]](#page-43-2), thereby enhancing TDA. The basic idea of table representation is to create vector representations of tables that preserve their syntax and semantics. These vectors can then be compared using methods like cosine similarity to assess their relatedness. The effectiveness of these table representations depends on the information source they contain, which can be summarized as table content, table context, and metadata. We briefly introduce each type of approach as follows.

Content-based table representation. Typically, several works [\[9,](#page-38-8) [25,](#page-39-20) [75,](#page-41-7) [100\]](#page-42-5) derive features from table content, such as column distribution, column type, to create table representations. Also, many studies [\[19,](#page-39-13) [66,](#page-41-3) [78,](#page-41-8) [99,](#page-42-8) [103,](#page-43-6) [116\]](#page-43-11) directly transform table content into vectors. For example, discrete columns can be represented as one-hot vectors [\[99,](#page-42-8) [116\]](#page-43-11), analog bits vectors [\[66,](#page-41-3) [116\]](#page-43-11), and embedding vectors [\[116\]](#page-43-11). More recently, there are increasing studies [\[28,](#page-39-18) [30,](#page-39-10) [34,](#page-39-2) [89\]](#page-42-10) leveraging language models to encode tables. They first sequentialize table content, and then feed the sequential text into language models for encoding.

Context-based table representation. Recent research [\[13,](#page-38-9) [21,](#page-39-19) [34,](#page-39-2) [40,](#page-40-14) [42,](#page-40-5) [49,](#page-40-13) [71,](#page-41-14) [87,](#page-42-11) [105\]](#page-43-2) underscores the importance of contextual information, such as column relationships [\[42,](#page-40-5) [49,](#page-40-13) [87\]](#page-42-11), alongside table content. For instance, Khatiwada et al. [\[49\]](#page-40-13) enhance the SANTOS model by incorporating semantic relationships between column pairs, refining its comprehension of table contexts and filtering out tables with similar columns but different contexts during the TDA process. Hu et al. [\[42\]](#page-40-5) utilize PLMs (e.g., BERT) to capture the contextual relationships between columns, aiding in the identification of relevant tables in the table pool for TDA. SATO [\[105\]](#page-43-2) employs a hybrid model to utilize signals from both the global context (values from the entire table) and the local context (predicted types of neighboring columns). Similarly, several studies [\[34,](#page-39-2) [71,](#page-41-14) [87\]](#page-42-11) capture the global context of the table by feeding the entire table into column encoders, resulting in the encoded column vectors that contain distilled global table context. Several works [\[13,](#page-38-9) [21,](#page-39-19) [67\]](#page-41-15) leverage graphs to represent tables, with nodes representing cell values and edges indicating relationships between nodes (e.g., cell belonging to one specific column), aiming to better capture the table structure. These approaches have demonstrated high prediction accuracy, highlighting the effectiveness of integrating table context into table representations.

Metadata-based table representation. In addition to using data values organized in rows and columns, several studies [\[27,](#page-39-11) [28,](#page-39-18) [40,](#page-40-14) [71,](#page-41-14) [89,](#page-42-10) [100,](#page-42-5) [110\]](#page-43-9) also utilize metadata. Tables are often accompanied by secondary information such as the table captions and the containing webpage's title, forming part of the textual information [\[89\]](#page-42-10). Yakout et al. [\[100\]](#page-42-5) consider element-wise and cross-element similarities, where elements encompass both the table content (e.g., tuples and column values) and metadata (e.g., table captions and URLs). Watchog [\[71\]](#page-41-14) improves upon Starmie [\[34\]](#page-39-2) by incorporating metadata such as headers, captions, and topics.

Remarks. Table representation forms the basis of tabular data processing and is not limited solely to TDA tasks. Consequently, numerous studies have concentrated on this domain. Early approaches that relied on table content were found inadequate. Current research typically leverages both the content and contextual information of tables to create richer table representations, yielding improved outcomes. The use of LLMs has further improved the extraction of semantic information from tables. However, a notable challenge remains in accurately capturing the structural information of tables, such as row/column permutations invariance.

#### <span id="page-14-0"></span>3.5 Table Indexing

Table indexing involves assigning a unique identifier (index value) to table elements, allowing for quick and efficient lookup and retrieval based on their index values. Many retrieval-based TDA methods use indexes to enhance efficiency and scalability, particularly when dealing with large-scale table pools with millions of tables. Researchers have utilized various types of indexes, such as the inverted index [\[3,](#page-38-6) [29,](#page-39-14) [32,](#page-39-16) [49,](#page-40-13) [100,](#page-42-5) [109,](#page-43-5) [118\]](#page-43-8), Locality Sensitive Hashing (LSH) index [\[9,](#page-38-8) [15,](#page-38-2) [34,](#page-39-2) [75,](#page-41-7) [119\]](#page-43-4), and graph index such as Hierarchical Navigable Small World (HNSW) [\[30,](#page-39-10) [34,](#page-39-2) [69\]](#page-41-18).

<span id="page-14-1"></span>![](_page_14_Figure_3.jpeg)

**Caption:** Figure 5 depicts various table indexing techniques, including inverted index, LSH index, and HNSW index. These indexing methods are essential for improving the efficiency and scalability of table retrieval processes in TDA, particularly when dealing with large-scale table pools, showcasing their structural differences and operational mechanisms.

Fig. 5. The illustration of table indices, including (a) inverted index, (b) LSH index, and (c) HNSW index.

Inverted index is a structure of posting lists that maps each distinct value to its containing structures, such as tables (see Fig. [5](#page-14-1) (a)), rows, columns, or cells. When searching for candidates, the inverted index requires reading all or a large subset of these posting lists, which can lead to memory management issues and long read times in large-scale table pools. To address this, JOSIE [\[118\]](#page-43-8) incorporates a dictionary for quick access to the posting lists, avoiding the need to search the entire inverted index. Mate [\[32\]](#page-39-16) extends the single-attribute inverted index by adding a super key, a fixed-size hash value that aggregates all possible key value combinations into a single entry. This approach enables efficient multi-attribute join discovery without the prohibitive storage requirements of a full multi-attribute index.

LSH index has been widely adopted in approximate nearest neighbor search in high-dimensional spaces [\[75,](#page-41-7) [119\]](#page-43-4). The basic idea is that similar vectors are more likely to be hashed into the same bucket [\[34\]](#page-39-2) (see Fig. [5](#page-14-1) (b)), which significantly improves query time with minimal accuracy loss. For instance, D <sup>3</sup>L [\[9\]](#page-38-8) uses an extension of the LSH index to ensure that search time remains constant regardless of repository size. Starmie [\[34\]](#page-39-2) adjusts the LSH index using the simHash function to better estimate the similarity between column embedding vectors.

HNSW index is a graph-based approximate nearest neighbor search technique designed for high-dimensional data. HNSW involves a hierarchy of graphs, where each layer's graph is built on top of the previous layer, allowing for efficient search: a query can start in any layer and traverse the graph to find the nearest neighbors, as illustrated in Fig. [5](#page-14-1) (c). The HNSW index has recently been applied to accelerating the table retrieval process for TDA, reportedly achieving a 400√ó performance gain over the LSH index [\[34\]](#page-39-2).

Remarks. Table indexing substantially improves the efficiency and scalability of table retrieval for TDA. While inverted indexes provide high accuracy, they can encounter memory management issues and prohibitively long read times in large-scale table pools. To address these limitations, recent works [\[9,](#page-38-8) [34,](#page-39-2) [75\]](#page-41-7) have explored techniques like LSH and HNSW for table indexing, which can significantly improve efficiency and maintain accuracy. Furthermore, some study [\[34\]](#page-39-2) has adopted a hybrid approach, combining both LSH and HNSW indexing techniques to utilize the strengths of each method. However, most existing approaches are designed for static table pools. In today's information age, table pools are continuously evolving. Adapting table indexing methods to handle dynamic, ever-changing data remains a major challenge that has yet to be fully addressed.

#### 3.6 Table Navigation

Table navigation involves establishing a navigational framework over a table pool. Essentially, it refers to organizing the table pool in a way that highlights connections between similar tables, such as through edges in a graph or by clustering them together. With table navigation, the subsequent TDA can retrieve relevant data for augmentation more easily and efficiently. Existing works typically employ cluster structures [\[16,](#page-39-17) [50\]](#page-40-11), hierarchical structures [\[77\]](#page-41-12), or linkage graphs [\[15,](#page-38-2) [73,](#page-41-13) [74\]](#page-41-10) to manage tables in table pools.

<span id="page-15-0"></span>![](_page_15_Figure_4.jpeg)

**Caption:** Figure 6 illustrates table navigation techniques, including cluster structures, hierarchical structures, and linkage graphs. These methods organize table pools to facilitate efficient retrieval of similar tables, enhancing the effectiveness of retrieval-based TDA by establishing connections between related tables and improving data accessibility.

Fig. 6. The illustration of table navigation techniques, including (a) cluster structure, (b) hierarchical structure, and (c) linkage graph.

Cluster structure groups related tables in the table pool. For example, Chai et al. [\[16\]](#page-39-17) employ Multivariate Gaussian Mixture Model to cluster similar data points (rows), as illustrated in Fig. [6](#page-15-0) (a), aiding in the downstream data point selection and augmentation tasks. To facilitate the subsequent column integration, ALITE [\[50\]](#page-40-11) clusters similar columns using hierarchical clustering by iteratively merging the closest clusters based on Euclidean distances between column embeddings. This method can also be considered as a hierarchical structure to some extent.

Hierarchical structure allows navigation from broader concepts to more specific ones, as demonstrated in Fig. [6](#page-15-0) (b). For instance, Ouellette et al. [\[77\]](#page-41-12) propose a hierarchical structure called RONIN for organizing the tables in the pool. This model allows users to narrow down to potential datasets by navigating between groups of attributes sets connected by edges indicating subset relationships in a hierarchical manner. RONIN seamlessly integrates table search and table navigation, making the acquisition of related table more effective.

Linkage graph uses edges to indicate the relevance between tables in the table pool, as shown in Fig. [6](#page-15-0) (c). For instance, Nargesian et al. [\[73,](#page-41-13) [74\]](#page-41-10) make use of Directed Acyclic Graphs (DAG) to structure the table pool. In the DAG, nodes represent attribute sets, with the labels of a non-leaf node summarizing the content of the attribute sets in their corresponding subgraph. Another approach Aurum [\[15\]](#page-38-2) constructs an enterprise knowledge graph, with nodes representing attributes within the table pool, edges indicating relationships between two nodes. Specifically, Aurum also contains hyperedges connecting any number of nodes hierarchically related, such as attributes of the same table, or tables of the same table pool, thereby facilitating the exploration of relevant data items. More recently, OmniMatch [\[25\]](#page-39-20) constructs a similarity graph where columns are represented as nodes. These nodes are connected by different types of edges corresponding to various features, such as embedding similarity and distribution similarity. GNNs are then used to propagate these similarity signals to facilitate join discovery.

Remarks. Table navigation restructures the table pool, allowing retrieval-based TDA to locate and access similar tables more effectively. This concept is fairly new and continues to evolve. Clusters offer a relatively simple method for organizing table pools, indicating whether tables have similarity or belong to the same class; however, they lack the capability to convey more complex information, such as hierarchical relationships. For both cluster and hierarchical structure, they may not fully capture the relationships within table structures, such as the "entity-property" relationship between columns (e.g., entity "person" has the property "gender"). Both hierarchical structure and linkage graph face efficiency issues. Developing scalable and robust methods for navigating such vast table repositories remains a significant challenge in this field.

#### 3.7 Schema Matching

Schema matching involves evaluating the relatedness between two table columns. In this context, the set of column headers is typically referred to as the table schema [\[111\]](#page-43-1). Schema matching methods are frequently adopted in retrieval-based TDA for identifying and fetching those related columns and tables to expand the features in ML models. Due to the varied data types within tables, schema matching methods are categorized into textual matching, numerical matching, and metadata matching.

Textual matching is the most commonly used schema matching technique because textual columns usually contain more information than numerical ones. Below is a concise overview of some common textual matching methods.

- Value overlap [\[9,](#page-38-8) [25,](#page-39-20) [34,](#page-39-2) [75,](#page-41-7) [91,](#page-42-6) [100\]](#page-42-5): If there is a significant overlap in the value sets of two columns, then the columns are considered related.
- Semantic overlap [\[25,](#page-39-20) [27,](#page-39-11) [49,](#page-40-13) [59,](#page-41-16) [75\]](#page-41-7): When leveraging table annotations (see Section [3.2\)](#page-10-0) to derive labels describing column semantics, two columns are considered a match if there is a substantial overlap between their respective labels.
- Embedding similarity [\[9,](#page-38-8) [25,](#page-39-20) [29,](#page-39-14) [30,](#page-39-10) [49,](#page-40-13) [59,](#page-41-16) [75\]](#page-41-7): Related columns are identified by computing the similarity of their corresponding embeddings in vector space.

Numerical matching is concerned with numerical columns. These methods typically evaluate the value distribution [\[9,](#page-38-8) [84\]](#page-42-9) to derive insight from numerical data. For instance, D <sup>3</sup>L [\[9\]](#page-38-8) utilizes the Kolmogorov-Smirnov statistic to decide whether two numerical columns come from the same domain distribution and thus can be matched. Santos et al. [\[84\]](#page-42-9) propose the Quadrant Count Ratio (QCR) hashing scheme. This method divides the numerical values of two columns into four quadrants based on their sign (i.e., positive or negative). Only the points in the same quadrant would be assigned with the same hash value. Columns with a certain number of matching values are likely to be correlated.

Metadata matching is also commonly used, as tables are usually accompanied by secondary information such as column headers. For instance, several studies [\[9,](#page-38-8) [27,](#page-39-11) [71,](#page-41-14) [100\]](#page-42-5) consider two columns to be related when their column headers have a semantic overlap above a given threshold.

Remarks. Schema matching is a crucial step in retrieval-based TDA, used to determine the similarity between table attributes (columns) and infer the overall similarity between tables. This field has been extensively studied, and different types of schema matching methods are often adopted simultaneously, yielding promising results. However, there remain a wide range of research opportunities, particularly in the domain of numerical schema matching, such as handling different numerical display formats. Even current powerful LLMs cannot handle numbers well. Moreover, the vast majority of existing work in this area has focused solely on the similarity between single columns. The similarity between combined or composite columns is rarely addressed.

#### <span id="page-17-0"></span>3.8 Entity Matching

Entity matching involves identifying connections between entities in different tables, facilitating the localization of relevant entities and tables for retrieval-based TDA. These methods are particularly relevant in the context of horizontal tabular tables, where entities are typically represented as rows and their attributes are in columns. Based on the data source to which the entities are matched, the methods are categorized into KB-referenced entity matching and DB-referenced entity matching.

KB-referenced entity matching maps table entities to their referenced entity in a knowledge base (KB) [\[7\]](#page-38-7), aiming to enhance the semantic understanding of tables. Essentially, if two table entities are related to the same KB entity, the likelihood of these two table entities being related is high. For example, Sarma et al. [\[27\]](#page-39-11) link table entities to KB entities to acquire weighted label sets for representing table entities. However, not all table elements can be mapped to predefined types and relationships in the referenced KB. To address this, TabEL [\[7\]](#page-38-7) proposes an alternative way to weaken the strict mapping, using soft constraints based on graphical model to encode a higher preference for sets of related entities.

DB-referenced entity matching determines whether entities and their corresponding properties from table pools (or databases) refer to the same real-world object as the entities in the original table. Christophides et al. [\[23\]](#page-39-12) outline a general framework for this task, comprising two main components: (1) similarity metrics, which compare entity descriptions and (2) blocking techniques, which group tables in the table pool that are approximately similar for enhanced efficiency of this process. More recent efforts have begun to use iterative approaches, where previously discovered matching entities serve as input for computing similarities between further tables in the table pool [\[18\]](#page-39-7).

Hybrid entity matching combines both KB- and DB-referenced approaches to obtain a wider range of information sources. For example, Entitables [\[109\]](#page-43-5) identifies candidate entities from both sources. Entities in KB that share categories or types with the original table entities are considered good candidates. Similarly, entities in DB tables that contain the original table entities or have related captions to the original table are also considered candidates. Table2Vec [\[108\]](#page-43-7) enhances Entitables by incorporating word embeddings for table entities. CellAutoComplete [\[110\]](#page-43-9) further improves Entitables by carefully designing features that combine evidence from multiple sources Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI 19

(e.g., the similarity between table headings can be calculated using edit distance from DB or mapping probabilities from KB and DB).

Remarks. Entity matching uncovers relationships between entities across different tables, facilitating the retrieval of relevant entities and tables. For KB-referenced matching, knowledge bases may have limited coverage when applied to real-world table pools. To address this, several works [\[108](#page-43-7)[‚Äì110\]](#page-43-9) have integrated KB- and DB-referenced approaches to broaden the range of information sources. However, a common limitation in both KB- and DB-referenced entity matching is the assumption made in previous works that the leftmost column in a table contains the entity ID or name, which is not always the case. Indeed, entity matching has seen decreased use in the past few years compared to schema matching. An emerging field might be the combination of LLMs and RAG to replace traditional KB-based methods for entity matching. LLMs can better capture entity semantics, reducing the reliance on the leftmost column. Meanwhile, RAG techniques can effectively leverage and update the KB, counteracting the issue of limited scope.

# 3.9 Scenarios for Pre-augmentation Techniques

This section empirically summarizes the specific pre-augmentation tasks used in different TDA scenarios, as depicted in Fig. [7.](#page-18-0) The four single-table-setting pre-augmentation tasks apply to both retrieval and generation-based TDA, while the multi-table-setting tasks are only suitable for retrieval-based methods, which require an external table pool to handle multi-table relationships.

<span id="page-18-0"></span>![](_page_18_Figure_5.jpeg)

**Caption:** Figure 7 summarizes different TDA scenarios and their corresponding pre-augmentation tasks. It highlights the specific techniques applicable to both retrieval-based and generation-based TDA, emphasizing the importance of preparation tasks like error handling, table annotation, and schema matching in enhancing the overall TDA process.

Fig. 7. The illustration of different TDA scenarios and their suited pre-augmentation tasks.

EH Error Handling TA Table Annotation TS Table Simplification TR Table Representation TN Table Navigation Retrieval-based TDA operates on a table pool often possessing large-scale data, thus leading to issues such as inconsistent formatting and dynamic data. Common pre-augmentation methods in these scenarios include: (1) table simplification for reducing table information; (2) table indexing and table navigation for fast retrieval; (3) schema matching and entity matching for addressing inconsistent formatting; and (4) table indexing and table navigation for managing dynamic and large-scale table pools.

SM Schema Matching EM Entity Matching Generation-based TDA using a single original table may suffer from issues in data-scarce scenarios. In such scenarios, pre-augmentation techniques like table annotation (providing additional information or labels) and table summarization (extracting key information) are necessary. Furthermore, in privacy-preserving scenarios, generating synthetic data often benefits from table sampling, which involves using only partial data. Note that both table summarization and table sampling are part of table simplification, as discussed in Section [3.3.](#page-12-1)

Meanwhile, both retrieval- and generation-based TDA approaches face some common challenges, such as low-quality and imbalanced tables. In low-quality scenarios (e.g., tables with null or erroneous values), common preaugmentation techniques include error handling and table annotation (to annotate column when column names are missing). In imbalanced scenarios, table sampling within

table simplification is a common scheme. Additionally, TDA often requires semantic awareness; for these semantic-aware scenarios, and commonly used techniques include table annotation, table summarization within table simplification, and table representation.

#### <span id="page-19-0"></span>4 TECHNIQUES IN TABULAR DATA AUGMENTATION

This section will delve into the current state-of-the-art techniques for tabular data augmentation (TDA). As outlined in Table [4,](#page-8-1) we first classify TDA tasks into two primary categories: retrieval-based TDA (see Section [4.1\)](#page-19-1) and generation-based TDA (see Section [4.2\)](#page-26-0). Within these two categories, the approaches can be further divided into different levels: adding rows [\[78,](#page-41-8) [109\]](#page-43-5) or columns [\[30,](#page-39-10) [62\]](#page-41-17), augmenting individual cells [\[110\]](#page-43-9), and extending the original table with both rows and columns [\[50\]](#page-40-11). Thus, for each category, we will discuss the corresponding table augmentation work at the row, column, cell, or table level. Fig. [8](#page-20-0) provides a concise overview of the TDA works discussed in this section, along with their detailed taxonomy.

#### <span id="page-19-1"></span>4.1 Retrieval-based TDA

By retrieval-based, we refer to the process of enhancing the original table (query table ) with realistic data sourced from table pools <sup>T</sup> <sup>=</sup> { }. One important difference between augmenting tabular data and augmenting other data modalities lies in the availability of existing data resources (such as databases and data warehouses), which provide opportunities for discovering fresh, realistic data. In contrast, other data modalities (such as images) primarily rely on transforming the original data to generate new data that has not been seen before [\[17\]](#page-39-0). Retrieval-based TDA tasks are further divided into Entity Augmentation (ea<sup>r</sup> ) at the row level, Scheme Augmentation (sa<sup>r</sup> ) at the column level, Cell Completion (cc<sup>r</sup> ) at the cell level, and Table Integration (ti<sup>r</sup> ) at the table level. They will be introduced as follows.

4.1.1 Entity Augmentation (ea<sup>r</sup> ). Previous works [\[100,](#page-42-5) [111\]](#page-43-1) define retrieval-based TDA at the row-level as entity augmentation, as rows in tabular data generally correspond to specific entities. Entity augmentation extends a given table with more rows or row elements retrieved from table pools. Directly retrieving table rows without considering the context of the entire table is not feasible. Therefore, existing solutions typically first search for tables that are unionable with the input table and then select entities from these unionable tables to augment the original table. As we shall see, table search [\[15,](#page-38-2) [27,](#page-39-11) [89\]](#page-42-10) is inherently involved in this process, serving as an intermediate step that feeds into tabular data augmentation [\[111\]](#page-43-1).

To measure the unionability of two tables, existing solutions typically start by using table representation techniques (see Section [3.4\)](#page-12-0) to convert tabular data into latent-space vectors. These vectors, at the higher level, are then used to compute the relatedness between the source and target tables, with the resulting scores used to rank the target tables. For instance, Sarma et al. [\[27\]](#page-39-11) search for the top- related tables that are entity complements to the input table and then use these related tables to populate the input table. Most related works follow a similar approach as [\[27\]](#page-39-11), with distinctions mainly in the methodology employed to identify the top- unionable tables. These methodologies can be further categorized into four domains: statistical, KB-based, graph-based, and PLM-based.

Statistical entity augmentation methods use statistical models to estimate the unionability between two tables. For example, Infogather [\[100\]](#page-42-5) measures the context-to-context and tableto-context similarity by calculating the cosine similarity of their TF-IDF vectors. TUS [\[75\]](#page-41-7) utilizes three statistical models to statistically test the value overlap, semantic overlap, and embedding overlap between attributes (columns), and then aggregate the results to derive the unionability

<span id="page-20-0"></span>![](_page_20_Figure_1.jpeg)

**Caption:** Figure 8 categorizes TDA approaches from both task-oriented and table-level perspectives. It outlines key techniques within retrieval-based and generation-based TDA, illustrating the distinct methodologies and objectives at different levels of granularity, thereby providing a comprehensive overview of the current landscape in tabular data augmentation.

Fig. 8. The categorization of the TDA approaches, from both task-oriented and table-level perspectives. We also provide a concise introduction to the key TDA techniques within each category.

between tables. D <sup>3</sup>L [\[9\]](#page-38-8) extends TUS by incorporating additional statistical measures for numerical value distribution, which is based on Kolmogorov-Smirnov (KS) statistic.

KB-based entity augmentation methods consider a knowledge base (KB) for identifying potential unionable tables, instead of solely relying on tables within a table corpus or a table pool. For example, Das Sarma et al. [\[27\]](#page-39-11) represent table entities as weighted label sets from a knowledge

base (WebIsA [\[60\]](#page-41-6) or Freebase[7](#page-21-0) ) or from a table corpus, and take their dot product to compute the unionability between the input and candidate tables. EntiTables [\[109\]](#page-43-5) incorporates the DBpedia[8](#page-21-1) knowledge base to identify candidate entities. In their approach, they not only collect entities from similar tables, but also collect entities sharing the same types or categories from DBpedia with the input entities. They have proved that using related tables and using a KB are complementary when searching for candidate entities. Subsequently, Table2Vec [\[108\]](#page-43-7) further improves EntiTables for entity augmentation by incorporating Word2Vec to train table embeddings for entities. More recently, SANTOS [\[49\]](#page-40-13) leverages external knowledge bases, YAGO 4[9](#page-21-2) , to annotate both columns and the binary relationships between columns. To address the limited coverage of KB, SANTOS proposes a self-curated knowledge base on top of the table pool. To be more specific, SANTOS hypothesizes that columns with common semantics have overlap values. Thus, it first assigns a unique synthesized label to each column in the table pool with a confidence score 1; if two columns have overlap values, their synthesized labels can be assigned to each other with a confidence score based on the relatedness that are computed using the two columns' values.

Graph-based entity augmentation methods convert tables to a graph to compare the unionability between tables. An illustrative example is InfoGather [\[100\]](#page-42-5), which computes topic-sensitive pagerank (TSP) over a weighted graph, where nodes represent tables and edges indicate direct pairwise match between tables. EmbDI [\[13\]](#page-38-9) determines entity similarity from a table representation using compact tripartite graphs. Initially, EmbDI forms a heterogeneous graph based on the table, where cells, entities, and columns act as nodes, and the relationships derived from the table schema serve as edges. EmbDI then uses random walks to traverse the graph to construct sentences that can describe the table. The sentences are fed into word embedding algorithms like word2vec for entity representation and unionability computation. More recently, HYTREL [\[21\]](#page-39-19) models a table as a hypergraph and then encodes the hypergraph for table similarity prediction. In the hypergraph, nodes indicate table cells and three different types of hyperedges represent row, column, and the entire table, respectively.

PLM-based entity augmentation methods have been adapted for tabular data more recently, where TDA is one of the cases. PLMs often serve as table encoders, enhancing the capture of table semantics and structure for subsequent similarity calculations. Recently, Starmie [\[34\]](#page-39-2) leverages PLMs, i.e., RoBERTa, to encode columns. It then derives a unionability score between two tables using column aggregation algorithms. Recently, Hu et al. [\[42\]](#page-40-5) employ PLMs (BERT and RoBERTa) to obtain contextualized representation of column pair relationships, capturing nuanced table contexts to identify unionable tables in the pools.

Remarks. Entity augmentation can expand and diversify the samples for ML, making it a longstanding research focus. Statistical entity augmentation methods are typically based on fixed distribution assumptions and hard to refine. Meanwhile, it may encounter issues of computational efficiency and scalability when dealing with large-scale datasets. KB-based entity augmentation, on the other hand, suffers from limited KB coverage, often leading to low recall in practical scenarios. However, with the development of LLMs, the incorporation of RAG may be a future direction. RAG combines the dynamically retrieved information with the ability of generative models, reducing information loss and improving information relevance, thereby increasing recall rates. Graph-based entity augmentation, which converts tables to graphs, can better grasp the structure relationship of tables, but it also requires an additional encoding mechanism for

<span id="page-21-0"></span><sup>7</sup><http://www.freebase.com>

<span id="page-21-1"></span><sup>8</sup><https://www.dbpedia.org>

<span id="page-21-2"></span><sup>9</sup><http://yago-knowledge.org>

the constructed graph (e.g., HYTREL [\[21\]](#page-39-19) uses a transformer module to encode the hypergraphs). The entire process would be resource-intensive and time-consuming. For PLM-based entity augmentation, most methods directly sequentialize tabular data ignoring the table structure. These limitations highlight the need for more advanced, structure-aware methods that can effectively leverage the strengths of language models while addressing the unique challenges posed by tabular data. Furthermore, a significant portion of the existing research has focused primarily on table union search, without addressing the potential differences between the retrieved tables and the original data. Even when unionable tables are found, discrepancies may still need resolution before the tables can be combined.

4.1.2 Schema Augmentation (sa<sup>r</sup> ). We refer to retrieval-based TDA at the column level as schema augmentation, the process of extending the schema of the original table with additional columns from table pools. This process is somewhat similar to the join operation commonly executed in databases. To be specific, given a table pool <sup>T</sup> containing tables { } =1 , the task of schema augmentation sa<sup>r</sup> is to search <sup>T</sup> and find the columns [:, ] joinable to the query table . As a result, the result table <sup>=</sup> sa<sup>r</sup> ( , column, <sup>T</sup>) can boost the downstream ML task. To determine the joinability between two columns, the column values, column semantics, and table structure are the most considered three perspectives. The corresponding subcategories are introduced below.

<span id="page-22-0"></span>![](_page_22_Figure_3.jpeg)

**Caption:** Figure 9 illustrates schema augmentation techniques, including value-based joins, semantic-based joins, and structure-based joins. It highlights how these methods extend the schema of the original table by integrating additional columns from related tables, showcasing the evolution from traditional exact matches to more sophisticated semantic and structural approaches.

Fig. 9. The illustration of schema augmentation, including (a) value-based joins, (b) semantic-based joins, and (c) structure-based joins.

Value-based joins. Early approaches [\[118,](#page-43-8) [119\]](#page-43-4) for schema augmentation primarily focus on value-based join, where only exactly matching value can be joined [\[30\]](#page-39-10) (see Fig. [9](#page-22-0) (a)). For example, LSH Ensemble [\[119\]](#page-43-4) formulates the join problem as an overlap set similarity search by treating columns as sets and matching values as intersections between sets. JOSIE [\[118\]](#page-43-8) further improves LSH Ensemble by using intersection estimation to reduce the cost of set reads and the top set search. JOSIE starts by calculating the cost of reading posting lists and sets using statistical approximation techniques. It then uses an adaptive algorithm that switches between reading posting lists to gather candidates and reading sets to compute exact intersection sizes.

Semantic-based joins. Recent studies [\[22,](#page-39-8) [25,](#page-39-20) [29,](#page-39-14) [30\]](#page-39-10) have explored the concept of semantic joins, which join those columns with similar meanings (see Fig. [9](#page-22-0) (b)) instead of those ones having identical values. This approach is capable of handling misspellings and formatting variations, resulting in larger set of join results. For instance, ARDA [\[22\]](#page-39-8) performs joins on soft keys without requiring exact match. However, ARDA only consider one-hop semantic join (e.g., customer Z order, customer Z product), while AutoFeature [\[64\]](#page-41-1) and FeatNavigator [\[59\]](#page-41-16) consider multihop semantic join (e.g., customer Z order Z product). PEXESO [\[29\]](#page-39-14) targets the case that columns

are embedded into high-dimensional vectors and they are joined based on similarity predicates. DeepJoin [\[30\]](#page-39-10) enhances the capabilities of PEXESO [\[29\]](#page-39-14) by employing PLMs (e.g., BERT) as the column vector encoder. Meanwhile, DeepJoin can perform both value- and semantic-based joins for textual columns, breaking the drawback of previous methods [\[29,](#page-39-14) [118,](#page-43-8) [119\]](#page-43-4) that could only handle one type of join and surpassing these works in performance. However, DeepJoin can only handle textual columns with a relatively small cardinality. OmniMatch [\[25\]](#page-39-20) also detects both valueand semantic-based join between columns. OmniMatch incorporates Graph Neural Networks for similarity signals propagation to better capture column-pair similarity. Meanwhile, OmniMatch adopts a self-supervised learning approach by generating positive and negative join examples from the table pool, eliminating the need for large amounts of labeled data.

Structure-based joins. Most recent works tend to incorporate the table structure information in the column embeddings, such as using graphs to capture the table structure as illustrated in Fig. [9](#page-22-0) (c). For example, EmbDI [\[13\]](#page-38-9) first constructs a hypergraph capturing the table structure. The hypergraph contains one type of nodes representing columns. EmbDI then derives column embeddings from the column nodes containing the table structure information. Leva [\[114\]](#page-43-3) also uses a graph to capture the table structure, but the graph contains the information from the entire database with unique values in the table pool as nodes. Similarly, Bharadwaj et al. [\[8\]](#page-38-10) also construct a database-level graph, with columns as nodes and the relationships between columns as edges.

Remarks. Schema augmentation extends the original table with more features, thereby improving the downstream ML tasks. This area has evolved from traditional value-based joins, which depend on exact value matches, to more sophisticated semantic- and structure-based joins. Semanticbased joins have seen a trend of leveraging generative AI (e.g., PLMs) as encoders to capture the inherent semantics and contextual relationships within and across table columns. However, due to token length limitations, these methods are often restricted to handling table columns with relatively small cardinalities. When dealing with columns that have high cardinalities, additional sampling or chunking steps are required, which can lead to potential information loss and reduced join performance. In structure-based joins, most methods utilize graphs to capture the table structure. It has been observed that more complex graph structures can better represent the nuanced aspects of the table schema and relationships. However, this increased complexity often requires more resources. Therefore, it is important to explore a balanced approach that finds a careful equilibrium between the level of structural detail captured and the computational resources needed.

4.1.3 Cell Completion (cc<sup>r</sup> ). As with previous work [\[110\]](#page-43-9), we refer to retrieval-based TDA at the cell level as cell completion. This task involves filling in empty cells within the input table by leveraging information extracted from table pools. Cell completion can be divided into several subtasks based on cell types: populating attribute names, adding additional entity IDs/names, and filling values for table cells [\[110\]](#page-43-9). The approaches per category are introduced as follows.

Attribute name completion. This task is to populate the input table with additional possible column headers or labels. As shown in Fig. [10](#page-24-0) (a), the original song-related table is augmented with a new column header "Singer" retrieved from a related table 2 in the table pool, involving new features for ML. As a typical study, Yakout et al. [\[100\]](#page-42-5) search for similar tables and then match column labels. Their proposed approach for matching column labels utilize additional information, including similarities that are based on context, attribute names, and column values, respectively. EntiTables [\[109\]](#page-43-5) ranks a list of labels to be added as headings of new columns by using probabilistic models. The ranking is based on information obtained from a knowledge base and similar tables retrieved from a table corpus. A recent approach, RATA [\[40\]](#page-40-14), addresses this

<span id="page-24-0"></span>

| ùë∂<br>ùëª<br>(Original table) |                               |            |            |        |                 |                               |                        |                                          |           |          |                                                  |                                          |           |            |
|----------------------------|-------------------------------|------------|------------|--------|-----------------|-------------------------------|------------------------|------------------------------------------|-----------|----------|--------------------------------------------------|------------------------------------------|-----------|------------|
| Title                      | Album                         |            | Track Year |        |                 | Data lake                     |                        |                                          |           | ùë™<br>ùëªùüè  |                                                  |                                          |           | ùë™<br>ùëªùüê    |
| mechanical ape!            | charge!!                      |            | 10.0 2005  |        |                 | Song                          |                        | Album                                    |           | Artist   | Singer                                           | Album                                    |           | Track Date |
| believe me natalie         | hot fuss                      |            | 9.0 2004   |        |                 |                               | smile like you mean it | hot fuss                                 |           | killers  | northern voices                                  | faithful                                 |           | 1.0 1994   |
| mamma mia                  | abba                          |            | 1.0 1975   |        |                 |                               |                        |                                          |           |          |                                                  |                                          |           |            |
| sunday morning             | -                             |            | 3.0 2002   |        |                 |                               | mamma mia              | abba                                     |           | 1.0      | they might be giants severe tire damage 9.0 2006 |                                          |           |            |
| yeh yeh                    | mink car                      |            | 8.0 2001   |        | Cell Completion |                               |                        | sunday morning songs about jane maroon 5 |           |          | 3 doors down                                     | the better life                          |           | 5.0 2000   |
| purple toupee              | lincoln                       |            | 4.0 1993   |        | Model           |                               | mechanical ape!        | charge!!                                 |           | aquabats | michelle branch                                  | the spirit room                          | 10.0 2001 |            |
|                            |                               |            |            |        |                 |                               |                        |                                          |           |          |                                                  |                                          |           |            |
|                            | (a) Attribute name completion |            |            |        |                 | (b) Entity ID/name completion |                        |                                          |           |          |                                                  | (c) Cell value completion                |           |            |
| Title                      | Album                         | Track Year |            | Singer |                 | Title                         | Album                  | Track Year                               |           |          | Title                                            | Album                                    |           | Track Year |
| mechanical ape!            | charge!!                      |            | 10.0 2005  |        |                 | mechanical ape!               | charge!!               |                                          | 10.0 2005 |          | mechanical ape!                                  | charge!!                                 |           | 10.0 2005  |
| believe me natalie         | hot fuss                      |            | 9.0 2004   |        |                 | believe me natalie            | hot fuss               |                                          | 9.0 2004  |          | believe me natalie                               | hot fuss                                 |           | 9.0 2004   |
| mamma mia                  | abba                          |            | 1.0 1975   |        |                 | mamma mia                     | abba                   |                                          | 1.0 1975  |          | mamma mia                                        | abba                                     |           | 1.0 1975   |
| sunday morning             | -                             |            | 3.0 2002   |        |                 | sunday morning                | -                      |                                          | 3.0 2002  |          |                                                  | sunday morning songs about jane 3.0 2002 |           |            |
| yeh yeh                    | mink car                      |            | 8.0 2001   |        |                 | yeh yeh                       | mink car               |                                          | 8.0 2001  |          | yeh yeh                                          | mink car                                 |           | 8.0 2001   |
| purple toupee              | lincoln                       |            | 4.0 1993   |        |                 | purple toupee                 | lincoln                |                                          | 4.0 1993  |          | purple toupee                                    | lincoln                                  |           | 4.0 1993   |

Fig. 10. The illustration of three cell completion tasks: (a) attribute name completion, (b) entity ID/name completion, and (c) cell value completion.

problem using a retrieval-augmented self-trained transformer model. Specifically, RATA first indexes and searches tables from the table pool using a bi-encoder retrieval model, and then identifies augmentations from retrieved tables using a reader transformer.

Entity ID/name completion. Entity ID/name completion refers to populate the key column (i.e., the entity's ID or name) for a row (an entity). For example in Fig. [10](#page-24-0) (b), this procedure augments with a new entity name "smile like you mean it" (a song) retrieved from the table pool table 1 , thereby enriching the potential samples. Zhang and Balog [\[109\]](#page-43-5) populate rows with additional entities using a two-step approach: (1) candidate entity selection: search for table pool tables that contain the same or similar entities in the original table or search for KB entities with similar KB labels as the original table entities; and (2) entity ranking: leverage a probabilistic formulation based on Bayes' theorem to calculate the relatedness between entities. RATA [\[40\]](#page-40-14) tackles the entity ID/name completion in a similar manner to its attribute name population, using a retrieval-augmented strategy. This strategy pretrains the retrieval-based model by randomly removing entities from the corpus and then reconstructing the removed entities, aiming to better capture semantics and structure of tables.

Cell value completion. In this task of finding values for empty data cells, models are designed to estimate a specific value to fill in based on the information from the table pool. Referring to Fig. [10](#page-24-0) (c), this procedure fills in the empty cell in with a new value "songs about jane" retrieved from the table pool table 1 to avoid null values in learning tasks. A common approach is to retrieve tables from the table corpus and then extract values from those tables [\[100,](#page-42-5) [109\]](#page-43-5). Zhang and Balog [\[109,](#page-43-5) [110\]](#page-43-9) build upon this approach such that they consider integrating supplementary information from a knowledge base. Ahmadov et al. [\[3\]](#page-38-6) further improve these approaches [\[100,](#page-42-5) [109\]](#page-43-5) by introducing a hybrid method that combines a retrieval-based method with a generation-based method (a value prediction model). The prediction model is implemented using a black box approach that can automatically choose the best ML model (e.g., NN) and the corresponding parameters. More recently, Deng et al. [\[28\]](#page-39-18) propose the pre-trained TURL model, with cell value completion as one of its downstream tasks. Specifically, they pretrain a structure-aware transformer encoder with table pool tables to model the row-column structure by using a new Masked Entity Recovery (MER) objective. Likewise, RATA [\[40\]](#page-40-14) addresses the cell value completion problem with a transformer model. However, it incorporates a reader-or-selection component, that reads the retrieved table

and selects the most related ones. This component is based on an extractive approach, ensuring that the model's predictions are always based on existing data rather than speculative assumptions.

Remarks. Missing values are common and typically have a negative impact in practice, making cell completion crucial. This field has been actively studied for decades, and multiple works [\[40,](#page-40-14) [109\]](#page-43-5) consider addressing various cell types simultaneously. For attribute name and entity ID/name completion, an internal problem is that additional cell completion is required after these two operations to obtain a complete table, which may lead to problem propagation. Another issue is that retrieval-based methods may sometimes be outperformed by simple statistical techniques like averaging, especially in domain-specific tasks. Consequently, given the successful application of RAG in the NLP field, there is an opportunity to combine retrieval-based and generation-based methods [\[90,](#page-42-1) [95\]](#page-42-12), as generative approaches can better capture the inherent structure and patterns within tables. Future advancements could involve integrating generative AI models, such as language models, into this hybrid approach.

4.1.4 Table Integration (ti<sup>r</sup> ). We define retrieval-based TDA at the table level as table integration, the procedure of extending the original table with both rows and columns from related tables retrieved from the table pool via table search algorithms. There are two main methods for implementing table integration: one is compositional table integration, which combines retrieval-based TDA results at various levels (rows, columns, and/or cells), while the other is direct table integration, which directly enriches the original table with the content from the retrieved related tables.

Compositional table integration. An early work InfoGather [\[100\]](#page-42-5) presents a holistic augmentation framework that can perform column and/or row augmentation simultaneously. To be more specific, they introduce three core operations: entity augmentation by attribute name, entity augmentation by example, and attribute discovery. Entitables [\[109\]](#page-43-5) targets two tasks: augmenting rows with additional entities and augmenting columns with new headers. More recently, RATA [\[40\]](#page-40-14) integrates three TDA tasks: row population, column population, and cell completion. RATA functions as an end-to-end model that initially retrieves related tables and then extracts various table elements from these tables to perform different levels of TDA tasks.

Direct table integration. More recently, the concept of direct table integration was proposed by [\[72\]](#page-41-19), which aims to find the right operators (e.g., join, nest, group, link, and twist) to integrate tabular data into a desired form. A recent work [\[50\]](#page-40-11) has implemented this approach, proposing a model that extends the original table with both rows and columns simultaneously. They integrate two tables using Full Disjunction, which first connects two tables through an outer-join and then eliminates redundant rows. Leva [\[114\]](#page-43-3) takes a different approach by integrating tables in the latent space. To be specific, they construct a data-lake-level graph, and the embedding of this graph is used to featurize the original table.

Remarks. Table integration is a relatively new concept compared to other levels of retrievalbased TDA tasks, thus it still has a vast range of unexplored opportunities for further research and development. For compositional approaches that concatenate combining different levels of TDA, there is a risk of error propagation. The compounding of potential errors or biases from the individual TDA tasks can diminish the overall effectiveness and reliability of the integrated table. For direct approaches, there are relatively few works so far, possibly due to the lack of integration benchmarks. Most existing works rely on strong assumptions and are only applicable to small table pools. Researchers and practitioners may need to develop more robust and scalable integration techniques, as well as establish comprehensive benchmarking platforms.

#### <span id="page-26-0"></span>4.2 Generation-based TDA

Generation-based TDA refers to the augmentation of tabular data through the generation of synthetic data. Unlike retrieval-based methods, generation-based methods do not require external data sources and are often built upon generative models. Generation-based TDA tasks can be further categorized into the following sub-tasks: Record Generation (rg<sup>g</sup> ) at the row level, Feature Construction (fc<sup>g</sup> ) at the column level, Cell Imputation (ci<sup>g</sup> ) at the cell level, and Table Synthesis (ts<sup>g</sup> ) at the table level. These generation-based TDA tasks will be introduced and discussed in more detail in the following sections.

4.2.1 Record Generation (rg<sup>g</sup> ). Record generation aims at generating additional records (table rows) based on the original table and its associated information. In many practical scenarios, publicly released tables often contain only a small subset of the total available records due to various concerns such as legal constraints or privacy issues. They are referred to as sub-tables in this context. As a result, ML models trained on these limited sub-tables may suffer from suboptimal performance. Therefore, it is crucial to generate more synthetic records from the released sub-table. Based on the distribution statuses of the original and result tables, the goals of record generation approaches differ: one approach focuses on preserving the original distribution, while the other aims to address imbalanced tabular data through oversampling.

Distribution-preserving record generation ensures that the generated records maintain the same distribution as the original table. For example, early works use statistical approaches (e.g., Bayesian networks [\[106\]](#page-43-13) and Fourier decomposition [\[5\]](#page-38-12)) to model the distribution of the original table and then generate synthetic records by sampling from the distribution. Recently, table-GAN [\[78\]](#page-41-8) leverages generative adversarial networks (GAN) to generate synthetic records that are statistically similar to distribution of the original table. Specifically, table-GAN's framework comprises three neural networks: a generator, a discriminator, and a classifier, which collectively enhance the semantic coherence of the synthetic records. Similar to table-GAN, several other works such as PATE-GAN [\[46\]](#page-40-9), ITS-GAN [\[19\]](#page-39-13) and GANBLR [\[113\]](#page-43-10) also employ GAN to generate distribution-preserving records. PATE-GAN introduces differential privacy guarantees for privacy concern; ITS-GAN [\[19\]](#page-39-13) further maintains functional dependencies to capture the relationships between attributes; while GANBLR addresses the interpretation limitation of previous GAN-based methods and further consider explicit feature interactions. Diffsion models have also been applied for record generation. STaSy [\[51\]](#page-40-15) adopts a score-based generative model that uses the reverse diffusion process to generate records based on the score function aiming at enhancing sampling quality and diversity. CoDi [\[53\]](#page-40-16) leverages two diffusion models to process continuous and discrete columns separately and then co-evolve the two diffusion models by transforming conditions from and to each other during training. RelDDPM [\[66\]](#page-41-3) use diffusion models to generate tuples that not only cater to the original distribution but also meet specific conditions, such as satisfying a particular criterion for a given attribute. In particular, RelDDPM first trains an unconditional generative model (diffuser module) to capture the overall distribution of the original tabular data. Then, it uses controllers to measure how well the synthetic data matches the condition and guide the diffuser module to generate data that better satisfies the condition. GOGGLE [\[67\]](#page-41-15) further considers the relational structure of the original table when generating records. It first learns an approximate relational structure through the construction of a graph, which serves as the foundation of the generative modeling. Subsequently, it uses a variational autoencoders (VAE) architecture to gradually generate synthetic records similar to the original table through message passing over the constructed graph. Most recently, DP-LLMTGen [\[90\]](#page-42-1) explores LLMs for record generation. The LLM undergoes a two-stage fine-tuning process, one for capturing table formats, and the other for learning the feature distributions and dependencies.

Class-imbalance-aware record generation particularly focuses on oversampling imbalanced samples/instances (rows) within the tabular data, since class imbalance can significantly hinder the predictive performance of classification models. For example, CTGAN [\[99\]](#page-42-8) deals with the imbalanced tabular data by using a conditional generator that can condition on one of the discrete attributes. It also uses a training-by-sampling strategy, which ensures even sampling from the discrete attributes. Similarly, cWGAN [\[31\]](#page-39-4) first estimates the underlying distribution of the original table using Wasserstein GAN, a derivate of GAN optimizing GAN's training process, and then uses the trained generator to synthesize additional samples of the minority class. SIGRNN [\[4\]](#page-38-11) employs a sequence-to-sequence recurrent neural network (RNN) to generate minority class instances. The RNN is trained on the minority class instances to learn their data distribution, and then used to generate synthetic instances to augment the original dataset and balance the minority class. SOS [\[52\]](#page-40-12) proposes a transfer-based oversampling technique. SOS leverages a score-based generative model to transform majority class records to those "fake" minority class records.

Remarks. Record generation has a broad range of applications in ML, such as increasing sample size, oversampling imbalanced classes, and preserving privacy. This field has evolved from using statistical models to incorporating deep neural network models. Despite the progress made, a key challenge remains: how to effectively capture the complex attribute correlations present in the original tabular data. The synthetic records generated should not only reflect the overall statistical distribution of the source table but also accurately preserve the intricate relationships between different columns or features. Since the database community has extensively studied attribute correlations, integrating database techniques with newly proposed neural network models could be a promising direction. Additionally, tabular data often comes with a limited number of samples, making it challenging to train neural network models effectively and leading to a risk of overfitting. Techniques such as employing regularization methods (e.g., GOGGLE [\[67\]](#page-41-15)) and adopting self-supervised learning should be further explored to address this issue.

4.2.2 Feature Construction (fc<sup>g</sup> ). Feature construction refers to the task of generating additional columns, also known as features, to enhance the original table. Adequate features are essential for training ML models to achieve optimal performance, though good features alone are not always sufficient [\[64\]](#page-41-1). Consequently, numerous studies have focused on feature construction. Based on whether features are transformed into vectors and manipulated in hidden layers, feature construction methods can be categorized into two main groups: explicit and implicit.

<span id="page-27-0"></span>![](_page_27_Figure_4.jpeg)

**Caption:** Figure 10 depicts three cell completion tasks: (a) attribute name completion, (b) entity ID/name completion, and (c) cell value completion. It illustrates the processes involved in filling missing values in a table, emphasizing the importance of leveraging information from related tables to enhance the quality of the original dataset.

![](_page_27_Figure_5.jpeg)

**Caption:** Figure 11 provides an overview of explicit and implicit feature construction methods. It highlights how explicit methods generate new features by manipulating existing ones, while implicit methods derive new features through hidden layer manipulations, showcasing the diverse strategies employed to enhance feature sets in tabular data.

![](_page_27_Figure_6.jpeg)

**Caption:** Figure 12 illustrates the table synthesis process, emphasizing the generation of both rows and columns simultaneously. It highlights the need for integrated approaches that can effectively augment tables with low-quality features and limited samples, showcasing the potential for future research in this emerging area of tabular data augmentation.

Fig. 11. The illustration of feature construction: (a) explicit feature construction and (b) implicit feature construction.

Explicit feature construction generates new features by directly manipulating existing ones. For example, it can partition continuous columns into segments, as shown in Fig. [11](#page-27-0) (a), where

the "Age" column is divided into [0, 18], [19, 50], and [51, 80]. For instance, Kanter and Veeramachaneni [\[47\]](#page-40-6) use existing raw features and apply a variety of mathematical transformations (e.g., AVG, MIN, and MAX) to create a hierarchy of synthesized features that express more complex relationships in the data. Similarly, ExploreKit [\[48\]](#page-40-7) generates new features by applying predefined operators, including unary, binary and higher-order operators, to existing features. More recently, SMARTFEAT [\[62\]](#page-41-17) uses Foundation Models (FMs) to construct new features. It uses a prompt strategy to instruct the FMs to generate transformation functions for generating diverse and meaningful features.

Implicit feature construction generates new features by indirectly manipulating existing feature vectors in hidden layers, as illustrated in Fig. [11](#page-27-0) (b). For example, GAINS [\[98\]](#page-42-13) starts by training an encoder to map features into vectors, and then uses an evaluator to optimize the feature vector along the gradient direction. Finally, a decoder generates optimal feature subsets. Wang et al. [\[92\]](#page-42-14) combine the explicit and implicit methods by encoding feature transformation operation sequences into embedding vectors. They modify GAINS by replacing the original feature vectors with these operation sequence vectors to disclose a new feature space with discriminative patterns.

Remarks. ML is heavily based on high-quality features, which makes feature construction of great importance. For explicit methods, previous work typically relies on predefined operators that are hard to refine and often generate meaningless features. Recently, SMARTFEAT [\[62\]](#page-41-17) has begun to use foundation models for feature construction, indicating a potential future direction. For implicit methods, manipulating existing feature vectors in hidden layers lacks interpretability. Integrating explicit methods, such as GAINS [\[92\]](#page-42-14) may be a potential solution. Meanwhile, since implicit methods involve encoding features into vectors, using LLMs for feature encoding to better capture feature semantics is also a promising direction.

4.2.3 Cell Imputation (ci<sup>g</sup> ). Cell imputation is the procedure of generating assumed values to replace the unknown or missing values within the tabular data. Estimating such unseen values within the dataset is particularly challenging due to the high heterogeneity in data types and the large size of datasets, which often comprise millions of rows. Cell imputation methods can be divided into two main categories: statistical cell imputation and deep-learning-based cell imputation.

Statistical cell imputation identifies and computes missing cell values based on the statistical characteristics of the original table. For example, MICE [\[12\]](#page-38-3) creates a statistical model for each variable (i.e., a column in the tabular data setting) with missing values, filling in those missing values iteratively until convergence is reached. The statistical model can be a simple mean or median, or it can be a more complex statistical model like a regressor. MissForest [\[86\]](#page-42-7) trains a random forest on the observed parts of the dataset and then use this trained random forest to predict the missing values.

Deep-learning-based cell imputation has recently utilized a deep generative network to model tabular data for cell imputation, aiming to better capture the distribution and correlations between attributes in the original table. These methods can be further categorized on the basis of the type of generative model they use:

‚Ä¢ VAE-based methods typically employ an encoder to capture the underlying structure of table and a decoder to generate the imputed value. For example, MIWAE [\[70\]](#page-41-9) handles missing data using the encoder to approximate the posterior of the latent variables given the observed data and the decoder to reconstruct the complete data. HI-VAE [\[76\]](#page-41-4) modifies the VAE architecture to handle missing data by marginalizing out the missing data. To be specific, HI-VAE uses input dropout to make the encoder rely only on the observed data. Meanwhile, HI-VAE modifies the VAE decoder to factorize the likelihood into separate components for observed and missing data.

- GAN-based methods generally employ the generator to impute missing data and the discriminator to distinguish between real and imputed data. For instance, GAIN [\[103\]](#page-43-6) and MisGAN [\[55\]](#page-40-10) use the generator to impute missing data by analyzing real data vectors, while the discriminator, aided by a hint vector that provides additional information about the missing data, differentiates between observed and imputed components. However, GAIN and MisGAN are theoretically only supported under the Missing Completely at Random (MCAR) mechanism, where the probability of missing data is uniform across all cells regardless of their values. In contrast, the Missing at Random (MAR) mechanism, where the likelihood of missing data depends on the observed values, presents a greater challenge in practice. MIGAN [\[26\]](#page-39-15) addresses this by modeling the conditional distribution of missing values based on the observed data for each pattern of missing data.
- Diffusion-based methods use a reverse denoising process to learn the distribution of the data and perform the imputation. TabCSDI [\[116\]](#page-43-11) adapts the diffusion model to model the distribution of the missing parts given the observed parts by employing the reverse denoising process.

Remarks. Cell imputation addresses missing values in tabular data, a common issue in realworld datasets, and has been a longstanding concern for the database community. Statistical methods rely on analytically formulated techniques that often require human intervention and may not capture complex relationships between attributes. Recently, there has been a noticeable shift from these methods to deep-learning-based methods. However, deep-learning-based methods, while powerful, come with the challenges of more complex optimization and often necessitate fully observed datasets for training. A promising future direction is the adoption of self-supervised methods, which can leverage partially observed data without requiring complete datasets. Furthermore, some recent work, such as Hyperimpute [\[45\]](#page-40-17), utilizes an AutoML framework, presenting another promising direction for the field.

4.2.4 Table Synthesis (ts<sup>g</sup> ). We define generation-based TDA at the table level as table synthesis, which involves generating both rows and columns based on the original table. At present, most research focuses on generating only rows, columns, or cells individually. However, table synthesis as a whole could be a promising direction for future research. Tables with small size of samples and limited features are common, such as web tables. In this case, a simple and feasible approach is to combine previous works that generate rows or columns separately, but additional steps bring increased resource consumption and error propagation.

Therefore, table synthesis that generates rows and columns simultaneously in one pass is necessary and worth exploring. Currently, a transformer-based method TransTab [\[95\]](#page-42-12), which represents a combination of retrieval- and generation-based TDA. TransTab encodes tables into tokens to retain knowledge across the table pool, redistributes attention on these tokens to highlight important features, and ultimately synthesizes a new encoded table. Prompt engineering is a potential direct table synthesis strategy, where crafting appropriate prompts could guide LLMs in generating tables.

Remarks. Table synthesis aims to enhance the original table by generating both rows and columns, particularly useful when the training dataset has low-quality features and limited samples. This emerging field requires further exploration. Directly combining row and column generation methods can lead to error propagation. Therefore, more targeted solutions are necessary, such as leveraging large language models.

# <span id="page-30-2"></span>4.3 Retrieval vs. Generation in TDA

In this section, we summarize and analyze the aforementioned TDA techniques from the perspective of comparing the retrieval-based approaches and generation-based approaches. First, using the proposed level-based taxonomy, we will distinguish from the task objectives and methodologies of these two approaches at different levels of granularity in Section [4.3.1.](#page-30-0) Then, we will provide a general overview of the pros and cons of retrieval-based approaches and generation-based approaches in Section [4.3.2,](#page-30-1) enabling researchers to choose the approach that best matches their tasks and requirements.

<span id="page-30-0"></span>4.3.1 Comparison at Different Levels. We analyze the differences between retrieval-based and generation-based methods at each level:

- Row-level (ea<sup>r</sup> vs rg<sup>g</sup> ): Retrieval-based methods (ea<sup>r</sup> ) focus more on the similarity between the original table and candidate tables in the table pool. This method can introduce new and more diverse samples, but often overlooks the impact of retrieved records on the distribution of the original table. In contrast, generation-based methods (rg<sup>g</sup> ) pay attention to the original table (e.g., statistical distribution and the relationships between columns) to ensure that the generated data maintain the logic of the original table. However, these synthetic samples tend to be similar to the original ones and cannot introduce new, previously unseen samples.
- Column-level (sa<sup>r</sup> vs fc<sup>g</sup> ): Retrieval-based methods (sa<sup>r</sup> ) are akin to the database join operations, requiring the specification of the target column in the original table, which can introduce completely new related features. Generation-based methods (fc<sup>g</sup> ), similar to feature engineering, optimize existing features to generate new ones and then select the best among them.
- Cell-level (cc<sup>r</sup> vs ci<sup>g</sup> ): Retrieval-based methods (cc<sup>r</sup> ) target regular empty cells and special cells such as column headers. They identify candidate values by searching for similar tables in the table pool or KB entities and then rank these candidates. This process is repeated for each null value and may not be friendly to often-empty regular cells. In contrast, generation-based methods (ci<sup>g</sup> ) focus on regular empty cells, synthesizing new values by analyzing the statistical distribution and structural information of the original table. This method can handle multiple null values in one pass and can handle special cells with simple modifications.
- Table-level (ti<sup>r</sup> vs ts<sup>g</sup> ): Retrieval-based methods (ti<sup>r</sup> ) that integrate both rows and columns from external tables can easily introduce null values. While generation-based methods (ts<sup>g</sup> ) do not produce null values, their credibility is uncertain due to a lack of interpretability and potential model hallucination.

<span id="page-30-1"></span>4.3.2 Summary of Pros and Cons. We summarize the pros and cons of retrieval-based and generationbased TDA, highlighting their common challenges, as illustrated in Table [6.](#page-31-1)

Retrieval-based TDA first retrieves related tables from table pools and then uses the retrieved tables for augmentation. The row-and-column structure and the data abundance make this method uniquely suited for tabular data. This approach enriches the original table with real external data, improving interpretability and introducing new, related information. Although studied for over a decade, recent advancements in deep learning, such as PLMs [\[34,](#page-39-2) [42\]](#page-40-5), have revitalized the field. However, challenges remain. The retrieval process requires preprocessing and indexing potentially millions of tables, entailing improvements in efficiency and scalability. Additionally, the lack of labeled data in large-scale table pools suggests self-supervised approaches as a future direction. Furthermore, retrieval-based methods often struggle with generalization.

Generation-based TDA generates synthetic data for TDA. This approach does not require external data sources, eliminating the need for preprocessing and indexing numerous tables, thereby saving time and resources. Synthetic data generation also offers privacy protection. However, this field is nascent and faces challenges. Current deep generative models are often over-parameterized, leading to overfitting, especially with small tables. Moreover, the process lacks interpretability and may lead to models' hallucination.

In addition to the challenges of the two methods mentioned above, TDA faces some common issues due to the nature of tabular data: First, capturing the table semantics, such as differentiating the context of "apple" in a technology table versus a fruit table. Second, capturing the structural information, including row (column) invariance and relationships between columns (columns only depend on a subset of other columns). Additionally, incorporating generative AI models in a more effective and interpretable manner is a future direction for both retrieval- and generation-based TDA approaches. Given the respective pros and cons of retrieval- and generation-based methods, exploring their combination is also worthwhile.

Table 6. The comparison and analysis between retrieval- and generation-based TDA.

<span id="page-31-1"></span>

|                                                           | Pros                                                    | Cons                                                                                          | Common Challenge and Opportunities                                                                      |
|-----------------------------------------------------------|---------------------------------------------------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| Retrieval<br>(studied for long<br>and still has vitality) | interpretability<br>introduce brand new information     | face efficiency and scalability issues<br>lack of labeled data<br>lack generalization ability | capture the table semantics<br>capture the table structure<br>the incorporation of generative AI models |
| Generation<br>(a growing field)                           | privacy protection<br>not require external data sources | over-parameterized<br>lack interpretability and may cause model hallucination                 | the combination of retrieval- and generation-based methods                                              |

## <span id="page-31-0"></span>5 TECHNIQUES IN POST-AUGMENTATION

In this section, we mainly focus on three crucial aspects of post-augmentation: publicly available datasets used for TDA and its assessment (see Section [5.1\)](#page-31-2), policies for evaluating the performance of the augmentation methods (see Section [5.2\)](#page-33-0), and strategies for further optimizing the augmentation module (Section [5.3\)](#page-34-1).

### <span id="page-31-2"></span>5.1 TDA Datasets

This section delves into the classic datasets commonly used in TDA work, aiming to assist newcomers to the field. We only consider datasets that have been utilized in multiple TDA works, as these are likely to serve as strong benchmarks for the community. Key characteristics of each dataset are summarized in Table [7.](#page-32-0) These datasets can be broadly categorized into two main groups: retrieval-based TDA datasets and generation-based TDA datasets. The main difference between these categories lies in the input data requirements for the respective TDA approaches. Retrievalbased TDA necessitates the collection of additional external tables along with the original table, whereas generation-based TDA only requires the original table as input. Nevertheless, training a generative model for TDA might require a substantial quantity of tabular data. Yet, from a post-augmentation viewpoint, the extensive data used for training is not taken into account here.

Retrieval-based TDA datasets usually consist of a table pool with hundreds to thousands of tables. The earliest examples, Web\_Manual and Wiki\_Link, originate from the same study [\[60\]](#page-41-6). In the Web\_Manual dataset, the researchers use Wikipedia tables as their queries and retrieve 371 Web tables to serve as the target corpus. These Web tables are then manually annotated with entities, types, and inter-column relationships. In contrast, the Wiki\_Link dataset is designed for larger-scale use without extensive human annotation. It is created by selecting Wikipedia tables where at least 90% of the cell values were internally linked to entities in Wikipedia. While this automated approach leads to a larger dataset, the annotations are limited to only entity information, without the more detailed and accurate annotations found in Web\_Manual. Because of the trade-offs between dataset size and annotation quality, both Web\_Manual and Wiki\_Link are less frequently used in recent TDA research. Nargesian et al. [\[75\]](#page-41-7) focus on table union search, an immediate step before entity

<span id="page-32-0"></span>Table 7. Representative datasets used in TDA studies, including their basic properties and the specific TDA tasks they are suitable for. Some works provide either the number of columns (rows) or the average number of cols (rows). We manually calculate the missing values from the other and mark them in italics.

| No. | Dataset                                                                                                                              | Tables | Cols       | AVG. Cols | Rows   | AVG. Rows | Scenario       | Suitable tasks        | Used-by                                                                                                                    |
|-----|--------------------------------------------------------------------------------------------------------------------------------------|--------|------------|-----------|--------|-----------|----------------|-----------------------|----------------------------------------------------------------------------------------------------------------------------|
| 1   | Web_Manual<br>‚ô£                                                                                                                      | 371    | ‚àñ          | ‚àñ         | 18921  | 51        | Web            | ear<br>, sar          | Limaye et al. [60],<br>TabEL [7]                                                                                           |
| 2   | Wiki_Link<br>‚ô£                                                                                                                       | 6085   | ‚àñ          | ‚àñ         | 121700 | 20        | Web            | ear                   | Limaye et al. [60],<br>TabEL [7]                                                                                           |
| 3   | WDC Web table corpus<br>‚ô¶                                                                                                            | 50M    | 250M       | 5         | 700M   | 14        | Web            | sar                   | JOSIE [118], PEXESO [29], DeepJoin [30], Starmie [34]                                                                      |
| 4   | WikiTables corpus<br>‚ô•                                                                                                               |        | 1.6M 30.4M | 19        | ‚àñ      | ‚àñ         | Web            | ear<br>, sar<br>, ccr | TabEL [7], Entitables [109], Table2Vec [108],<br>CellAutoComplete [110], TURL [28], DeepJoin [30], RATA [40]               |
| 5   | TUS Small<br>‚ô†                                                                                                                       | 1,530  | 14,810     | 10        | 6.8M   | 4,466     | Relational     | ear                   | TUS [75], SANTOS [49], Starmie [34], AutoTUS [42]                                                                          |
| 6   | TUS Large<br>‚ô†                                                                                                                       | 5,043  | 54,923     | 11        | 9.7M   | 1,915     | Relational     | ear                   | 3L [9], SANTOS [49], Starmie [34], AutoTUS [42]<br>TUS [75], D                                                             |
| 7   | SANTOS Small<br>p                                                                                                                    | 550    | 6,322      | 11        | 3.8M   | 6,921     | Relational     | ear                   | SANTOS [49], Starmie [34], AutoTUS [42]                                                                                    |
| 8   | SANTOS Large<br>p                                                                                                                    | 11,090 | 123,477    | 11        | 70M    | 7,675     | Relational     | ear                   | SANTOS [49], Starmie [34], AutoTUS [42]                                                                                    |
| 9   | BTS<br>q                                                                                                                             | 1      | 30         | ‚àñ         | 1M     | ‚àñ         | Relational rgg |                       | table-GAN [78], ITS-GAN [19]                                                                                               |
| 10  | UCI datasets<br>r<br>(e.g., Adult, Covertype)                                                                                        | ‚àñ      | ‚àñ          | ‚àñ         | ‚àñ      | ‚àñ         | Relational rgg | , fcg<br>, cig        | CTGAN [99], SIGRNN [4], GOGGLE [67], RelDDPM [66], GAINS [98],<br>GAIN [103], HI-VAE [76], TabCSDI [116], HyperImpute [45] |
| 11  | Kaggle s<br>(e.g., Diabetes, Bank)                                                                                                   | ‚àñ      | ‚àñ          | ‚àñ         | ‚àñ      | ‚àñ         | Relational fcg |                       | GAINS [98], SMARTFEAT [62]                                                                                                 |
| 12  | OpenML repository<br>‚ãÜ<br>(e.g., Heart, Horce)<br>‚ô£ http://websail-fe.cs.northwestern.edu/TabEL/#Web_Manual, publication year: 2010. | ‚àñ      | ‚àñ          | ‚àñ         | ‚àñ      | ‚àñ         | Relational rgg |                       | ExploreKit [48], GAINS [98], RelDDPM [66]<br>‚ô¶ https://webdatacommons.org/webtables/#results-2015, publication year: 2015. |

‚ô• http://websail-fe.cs.northwestern.edu/TabEL/#WikiTables, publication year: 2015. ‚ô† [https://github.com/RJMillerLab/table-union-search-benchmark,](https://github.com/RJMillerLab/table-union-search-benchmark) publication year: 2018.

augmentation, and propose two synthesized datasets TUS Small and TUS Large. They identify high-quality base tables (with abundant rows and at least 5 textual columns) from Canadian and UK Open Data and then partition the base tables horizontally and vertically to obtain non-overlapping unionable tables to the base ones. Similarly, Khatiwada et al. [\[49\]](#page-40-13) use the same dataset synthesis technique as TUS to create SANTOS Small and SANTOS Large. These four datasets are commonly used for recent entity augmentation tasks. Additionally, open data repositories like the WDC Web table corpus and WikiTables corpus are frequently used. They contain a vast number of tables from various domains, adaptable for different types of retrieval-based TDA tasks (e.g., ea<sup>r</sup> , sa<sup>r</sup> , and cc<sup>r</sup> ).

Generation-based TDA datasets typically do not require a table pool or the collection and annotation of multiple similar tables, as only a single original table is necessary for generationbased TDA. Consequently, there are no datasets specifically designed for generation-based TDA tasks. Most generation-based TDA methods utilize multiple original tables from various fields, targeting different tasks such as binary classification, multi-class classification, and regression. For example, table-GAN and ITS-GAN adopt the BTS dataset, which is a single table containing 1 million records of domestic air ticket sales. These two methods take a portion of the BTS dataset as input and generate augmented tables. The effectiveness of these augmented tables is then assessed through regression tests on the "ticket price" attribute in the original BTS dataset. Common sources for generation-based TDA datasets are primarily from three public platforms: UCI, Kaggle, and OpenML. These platforms are popular partly due to their large scale and diverse range of tables.

Remarks. It is natural to observe that the same dataset, when processed differently, can be applied to different TDA tasks. For example, EntiTables [\[109\]](#page-43-5) adapts the WikiTables corpus for entity augmentation, while DeepJoin [\[30\]](#page-39-10) uses WikiTables corpus for schema augmentation. EntiTables filters out those tables that focus on entities, specifically those where the leftmost column contains unique entities, to create a dataset for the ea<sup>r</sup> task. Entities in related tables, such as those containing entities from the original table or having similar captions, are considered candidate entities. On the other hand, DeepJoin preprocesses datasets by stipulating the key columns as the ones with the most unique values in each table for subsequent joins. Another observation is that the current generation-based TDA methods do not have standardized benchmarks. These methods often choose their own datasets, which makes it challenging to

<sup>p</sup> [https://github.com/northeastern-datalab/santos,](https://github.com/northeastern-datalab/santos) [publicatio](http://websail-fe.cs.northwestern.edu/TabEL/#WikiTables)n year: 2023. <sup>q</sup> <https://www.transtats.bts.gov/DataIndex.asp> <sup>r</sup> <https://archive.ics.uci.edu>

<sup>s</sup> <https://www.kaggle.com> <sup>‚ãÜ</sup> <https://www.openml.org>

evaluate and compare their performance on a shared dataset, thereby hindering the assessment of their effectiveness and differences.

With the development of generative AI, an increasing number of TDA works are incorporating the use of pre-training. This has led to a growing need for the construction of robust pre-training tabular datasets. An ideal pre-training dataset should possess three main attributes: high quality, large scale, and wide coverage. Previous datasets often compromise between quality and scale, but combining human expertise with generative models might address this issue. Wide coverage means curating diverse datasets across multiple domains, such as finance and healthcare, and various data types like CSV, JSON, and Markdown. This diversity is essential for pre-training large-scale generative models to handle the variety of tabular data found in real-world scenarios. However, privacy concerns might pose challenges in developing high-quality TDA datasets.

#### <span id="page-33-0"></span>5.2 Evaluation Polices

We summarize the common evaluation policies for TDA works. These polices can be categorized into two main groups based on the involvement of models: original-table-based evaluation and modelbased evaluation. Many studies [\[19,](#page-39-13) [78\]](#page-41-8) have utilized both evaluation methods simultaneously.

Original-table-based evaluation refers to evaluating the augmented table by comparing it to the groundtruth, which is typically the original table or its derivatives. For example, several works [\[108,](#page-43-7) [109\]](#page-43-5) start from a base table and derive a subtable from it to serve as the original table , while the entities or columns outside this subtable are used as the ground truth. This method can efficiently construct the ground truth, but the range of truth values is rather limited, and there may be data related to the base table in the table pool that are not present in the base table (e.g., the base table about IT companies in one country with a table in the table pool about another country). Several works [\[19,](#page-39-13) [78\]](#page-41-8) calculate the cumulative distribution functions (CDFs) of the augmented and original tables to compare their statistic similarity. While this method is simple and effective, it only evaluates statistical distribution information and cannot capture more complex details, such as relationships between columns (e.g., the connection between "position" and "salary").

Model-based evaluation refers to feeding the augmented table alongside other baseline tables to a specific ML model and then evaluating the model's performance. These baseline tables generally include three primary datasets: (1) None [\[16,](#page-39-17) [19,](#page-39-13) [34,](#page-39-2) [39,](#page-40-18) [59,](#page-41-16) [64,](#page-41-1) [78,](#page-41-8) [114\]](#page-43-3) refers to the original dataset without any augmentation. For example, ITS-GAN [\[78\]](#page-41-8) feeds the augmented and original table to a specific classification model that performs a grid search over RandomForest, AdaBoost, and GradientBoosting, then comparing the corresponding classification results. (2) Random [\[16,](#page-39-17) [64\]](#page-41-1) refers to the original dataset augmented with randomly selected candidates. For instance, a schema augmentation work AutoFeature treats features in a table pool as independent entities and randomly selects a predefined number of features to augment the original table as the Random baseline. (3) All [\[16,](#page-39-17) [64,](#page-41-1) [114\]](#page-43-3) refers to the original dataset augmented with all possible candidates. For example, Leva involves a ALL baseline that joins the original table with as many tables as possible. In general, None baseline is suitable for both retrieval- and generation-based TDA while the other two are suitable only for retrieval-based TDA.

Remarks. The two evaluation methods, original-table-based and model-based, each come with their own set of pros and cons. The original-table-based evaluation is straightforward and efficient but may lack precision and be limited in scope, especially for specific augmentation needs like enhancing the minority class. The model-based evaluation offers greater accuracy but requires more time and resources. Additionally, using models introduces extra layers of uncertainty, such as model characteristics and hyperparameters, making it difficult to control variables and

ensure fairness. This also complicates determining whether issues arise from the model itself or the data augmentation method. To address these tradeoffs, several studies [\[19,](#page-39-13) [78\]](#page-41-8) adopt a hybrid approach, using both evaluation methods. This raises an interesting question: do these two evaluation methods yield conflicting results? It may be worth exploring the development of alternative evaluation methods, such as rule-based TDA evaluation policies, which could potentially strike a balance between efficiency and accuracy better than current methods.

#### <span id="page-34-1"></span>5.3 Optimization Strategies

Optimization strategies aim to further refine the augmented results based on the performance of specific downstream ML models. These techniques can be categorized into two main types: iteration-based and reinforcement-learning-based.

Iteration-based optimization involves a simple and direct method of using feedback from the downstream ML model to determine whether adding a candidate augmentation enhances the performance of the task. For instance, Chepurko et al. [\[22\]](#page-39-8) devise the random injection feature selection (RIFS) algorithm, which compares model performance using candidate features against deliberately constructed random features as a baseline. The objective is to identify and eliminate irrelevant features, finding a subset of features that contain signals relevant to the downstream ML task. Their subsequent work ARDA [\[22\]](#page-39-8) is an automated system that searches and joins data with the input table end-to-end. Similarly, Leva [\[114\]](#page-43-3) leverages the supervision signal from the downstream ML task to filter out unnecessary information. Leva uses a graph to capture information from the entire database, including both useful and potentially spurious relationships. During training, the downstream ML model will automatically focus on using the valuable information while ignoring or downweighting the non-useful parts. More recently, FeatNavigator [\[59\]](#page-41-16) assesses the actual utility gain of candidate features by running ML model on the original and augmented tables; it then iteratively selects features based on their utility gain and the feasibility of the join path.

Reinforcement-learning-based optimization employs reinforcement learning (RL) to explore the features that improve the performance of the ML model. For example, Liu et al. [\[65\]](#page-41-11) propose an RL-based automatic data search system that retrieves fresh training data from table pools and interacts with the downstream ML model. In this RL-based framework, each training data point and its corresponding influence score, calculated by the "Environment", serve as the "State". Given the "State", the "Agent" composed of a Search-Policy selects the optimal "Action" (a set of training data points retrieved from the table pool), and feeds them back into the "Environment". AutoFeature [\[64\]](#page-41-1) makes further improvement by not only exploring features that boost performance but also utilizing rarely selected ones to avoid local optima. Chai et al. [\[16\]](#page-39-17) further extend AutoFeature by broadening the scope of table pools, such as enterprise data warehouses, online repositories, and data markets.

Remarks. Optimization strategies have proven effective in improving downstream ML task performance, with a noticeable trend towards the use of RL. However, they often come with computational overhead, which can be a constraint in time-sensitive or resource-limited situations and can impact scalability. As such, a worthwhile research direction is to improve the iteration efficiency of these optimization strategies while maintaining their effectiveness. This could involve reducing the number of required iterations or exploring the use of lighter surrogate models in place of the original, specific ML models.

#### <span id="page-34-0"></span>6 TRENDS AND OPPORTUNITIES

This section examines the current landscape and future prospects of TDA techniques. We first explore emerging trends shaping the field (Section [6.1\)](#page-35-0), followed by a discussion of promising opportunities for further research and application (Section [6.2\)](#page-36-0). By highlighting these aspects, we aim to inspire continued interest in this vital area of study.

# <span id="page-35-0"></span>6.1 Major Trends in TDA Development

Based on our review of numerous research articles and our three-step pipeline, we have identified three significant trends in current TDA work:

- T1. Enhanced Table Representation. In the pre-augmentation phase, researchers are increasingly employing more advanced table representations to better capture both structural and semantic details within tables;
- T2. Confluence of Retrieval and Generation. In the augmentation phase, retrieval- and generationbased methods offer distinct advantages and disadvantages. A fruitful path forward may lie in the integration of these two methodologies;
- T3. Automated TDA. Looking at the entire TDA pipeline, a promising future direction is the creation of end-to-end automated TDA systems that can efficiently manage the entire TDA process. We proceed to elaborate on each of these trends.

T1. Enhanced Table Representation. The representation of tables is crucial for effective TDA. By accurately capturing content, semantic, and structural information in table representation, downstream augmentation processes can achieve better outcomes. Accordingly, there is a clear trend in TDA towards more complex and sophisticated table representations. Recent TDA research [\[28,](#page-39-18) [71\]](#page-41-14) includes diverse information such as table context and metadata, unlike earlier work [\[9,](#page-38-8) [75\]](#page-41-7) that focused solely on table content. Additionally, newer TDA approaches [\[67,](#page-41-15) [114\]](#page-43-3) utilize graph structures to capture relational information within tables more effectively, encoding these structures for improved table representation. Notably, many of the latest TDA techniques, both retrievalbased methods [\[34,](#page-39-2) [42\]](#page-40-5) and generation-based methods, utilize [\[90\]](#page-42-1) generative AI (e.g., PLMs) to generate robust table representations, leveraging their semantic understanding and generalization capabilities. While generative AI models have shown promise in tabular data representation, there remains ample opportunity for further development, especially compared to advancements in NLP and CV. In conclusion, the move towards more sophisticated and comprehensive table representations is a key trend driving progress in TDA research.

T2. Confluence of Retrieval and Generation. As discussed in Section [4.3,](#page-30-2) retrieval-based and generation-based TDA have their own pros and cons. Retrieval-based methods incorporate external data sources for better interpretability but struggle with efficiency and scalability as data increases. Generation-based methods do not use external data, thus lacking interpretability and potentially leading to model hallucination. Given the trade-offs between these two approaches, combining their strengths while mitigating their weaknesses should be the future direction for TDA research. This aligns with the prevalent use of RAG models in the field of NLP, where the benefits of both retrievaland generation-based techniques are harnessed. By blending these complementary methodologies, we can work towards more robust, efficient, and interpretable TDA solutions that can meet the evolving needs of diverse applications and domains.

T3. Automated TDA. A notable trend in TDA is the move towards automating the entire process, creating end-to-end platforms that integrate various operators. For example, Chepurko et al. [\[22\]](#page-39-8) propose ARDA, an end-to-end system that integrates multiple operators (e.g., imputation, hyperparameters optimization, feature selection, etc.) for automatic schema augmentation. Similarly, Hyperimpute [\[45\]](#page-40-17) automates cell imputation by proposing an AutoML framework that utilize search algorithms to automatically select candidate ML models. Despite these advancements, current solutions often focus on single subtasks of TDA, leaving room for further exploration and development. An area for improvement is to consider the TDA pipeline as a whole, integrating various

pre-augmentation and augmentation operators, and intelligently selecting suitable operators based on user requirements. Additionally, refining the automated TDA pipeline could involve an iterative step to identify and eliminate unnecessary or even harmful steps, based on augmentation evaluation and data characteristics. By advancing towards more holistic, end-to-end TDA automation, the field can unlock greater efficiency, scalability, and customization, ultimately enhancing the practical value and applicability of TDA techniques.

# <span id="page-36-0"></span>6.2 Emerging Opportunities for TDA

As we navigate the era of big data and the trends towards generative AI and autoML, we observe that obtaining high-quality data from massive amounts of data to facilitate generate AI is imperative. TDA is an important sub task of data quality and there is still a long way to go. Firstly, from a data perspective, the quantity and complexity of tabular data have significantly increased. Tables now typically have multiple patterns [\[115\]](#page-43-14) and contain millions of samples [\[33\]](#page-39-1), posing new challenges to TDA work. Secondly, from a model perspective, ML models, especially generative AI, have always faced interpretability issues and potential of privacy leakage. Below, we will elaborate on these key opportunities for further advancements in this field.

O1. Multimodal TDA. Current TDA works often assume that tables contain only textual and numerical values. However, the reality is that modern tables often encompass a broader range of modalities, such as images [\[115\]](#page-43-14). Handling these multimodal tables presents unique challenges that current table processing techniques may not adequately address. The representation and indexing of such heterogeneous tables, for instance, may require fundamentally different approaches compared to traditional text-based and numerical tables. Additionally, user needs might be expressed in modalities other than the tabular data itself [\[61,](#page-41-20) [104\]](#page-43-15). For instance, a user might use natural language to request the oversampling of a minor class. Addressing these multimodal user inputs and aligning them with TDA operations is another key challenge to tackle.

O2. Efficiency and Scalability. Another key challenge for TDA is the issue of efficiency and scalability. Retrieval-based TDA approaches often involve similarity comparisons at the table pool level, which can encompass millions of tables or more. Despite that, most existing methods [\[29,](#page-39-14) [49,](#page-40-13) [118\]](#page-43-8) are exact algorithms with a worst-case time complexity that is linear in relation to the product of the query column size and the table repository size, raising concerns about their scalability. This makes it a crucial research direction to explore. Meanwhile, the table itself may contain tens of thousands of records[10](#page-36-1). Moreover, both retrieval- and generation-based TDA methods are embracing the large-scale generative AI models such as PLMs and LLMs. The computational and memory requirements of training, fine-tuning, and deploying these models can hinder the efficiency and scalability of TDA techniques in practice. Addressing the scalability challenges associated with large-scale generative AI models in TDA is another crucial area of research. Potential solutions may include the development of more efficient model architectures the exploration of model compression [\[107\]](#page-43-16) and distillation techniques [\[94\]](#page-42-16).

O3. Domain-specific Tasks. Another promising direction for TDA research is the exploration of domain-specific applications. Domain-specific data often exhibit strong professionalism and have some unique characteristics [\[85\]](#page-42-17). For instance, medical data typically possesses specialized terminology and intricate data distributions that differ from more general tabular data. Developing TDA techniques tailored to these domain-specific characteristics could result in more effective and domain-friendly TDA strategies. This might involve incorporating domain knowledge, such as expertise from professionals [\[57\]](#page-40-1), knowledge base [\[2\]](#page-38-13) and knowledge graphs [\[1\]](#page-38-14), and utilizing

<span id="page-36-1"></span><sup>10</sup>Large-scale tables do not necessarily indicate high quality or no need for augmentation (e.g., many tables often have many records but few features for ML model training). Similarly, candidate tables in the table pool may also be large-scale.

specialized table representations [\[56\]](#page-40-19). Combining retrieval-based and generation-based TDA holds promise for domain-specific tasks, allowing for extensive use of limited domain knowledge. An interesting question arises as to whether the knowledge and techniques developed for TDA in one domain can be efficiently transferred to other domains. For example, can the insights and models derived from financial data augmentation be effectively applied to medical data, or vice versa? The ability to successfully transfer TDA across domains would be highly valuable, as it could accelerate research and development efforts, enable the reuse of existing resources, and promote the exchange of ideas.

O4. Interpretability. Existing TDA works face a notable challenge with respect to interpretability, particularly in generation-based TDA approaches. Retrieval-based TDA, on the other hand, has demonstrated better interpretability, as it can be interpreted as leveraging the retrieved relevant table information to augment the original data. One promising future direction for TDA research would be to explore the combination of retrieval- and generation-based TDA approaches. This hybrid method could improve the interpretability of generation-based TDA while also harness the respective advantages of both approaches. At the same time, the recent trend towards the incorporation of deep neural networks [\[30,](#page-39-10) [34,](#page-39-2) [42\]](#page-40-5), such as language models, into TDA workflows has introduced a new challenge regarding interpretability. The complex, black-box nature of these models can make it difficult to understand and explain the underlying rationale behind the augmented data. Addressing this interpretability challenge is crucial, as it impacts the trust, transparency, and practical adoption of TDA methods, particularly in domains where explainability is of paramount importance, such as high-stakes decision-making [\[82\]](#page-42-18) or regulated industries [\[63\]](#page-41-21).

O5. Privacy and Security. Last but not least, privacy and security represent critical opportunities for TDA. For retrieval-based TDA approaches, they carry out similarity comparisons between the original table and the tables in the pool, which may lead to information leakage of the user's original table. In contrast, generation-based TDA has inherent advantages in privacy protection, and several works [\[19,](#page-39-13) [35,](#page-39-21) [46\]](#page-40-9) have utilized record genration for privacy-preserving data publishing. However, the recent trend towards the incorporation of LLMs into TDA workflows introduces a new set of privacy and security challenges. While LLMs have powerful generative capabilities, they also carry the risk of information leakage [\[83,](#page-42-19) [102\]](#page-42-20), potentially exposing or memorizing sensitive data from their training corpora. Addressing the offensive and defensive aspects of LLMs behaviors, including the development of robust privacy-preserving training techniques [\[80,](#page-41-22) [101\]](#page-42-21), remains an active area of investigation.

### <span id="page-37-0"></span>7 CONCLUSION

This survey presents a thorough investigation of tabular data augmentation (TDA) for ML, with a particular emphasis on the recent advancements in leveraging prevalent generative AI models. Our work meticulously outlines the essential steps involved in TDA by constructing an end-toend pipeline encompassing three critical procedures: (1) pre-augmentation, where we summarize and analyze the commonly used preparation techniques for TDA; (2) augmentation, where we systematically compare current TDA techniques, including both retrieval-based and generationbased approaches; and (3) post-augmentation, where we delve into the evaluation and optimization processes following TDA. Additionally, we provide a comprehensive analysis of the pros and cons of current methodologies and outline future trends and opportunities for TDA.

The era of generative AI heralds a transformational phase for TDA. ML on tabular data is ubiquitous and demands a substantial amount of high-quality data ‚Äî a requirement that generative AI can significantly enhance. Despite the distinct characteristics of tabular data, generative AI models have predominantly been applied to fields like computer vision and natural language processing, with their application to tabular data still in its nascent stages. This leaves a vast landscape for further innovation and advancements in TDA through the use of generative AI. Our comprehensive survey aims to bridge this gap by providing a detailed, systematic overview of the current state of TDA and its potential future directions. We believe that this work can contribute to the community and serve as a valuable resource for researchers and practitioners alike. Our endeavor will be presented as a continuously updated literature repository, maintained online at [https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation.](https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation)

## ACKNOWLEDGMENTS

The authors would like to thank Yifan Wu, Jianfeng Zhang, Ankai Hao, Zhaoyi Yuan, Jiahui Long, and Hongwei Yuan for their invaluable feedback, which significantly improved the manuscript.

# REFERENCES

- <span id="page-38-14"></span>[1] Bilal Abu-Salih, Muhammad AL-Qurishi, Mohammed Alweshah, Mohammad AL-Smadi, Reem Alfayez, and Heba Saadeh. 2023. Healthcare knowledge graph construction: A systematic review of the state-of-the-art, open issues, and opportunities. J Big Data 10, 1 (2023), 81. <https://doi.org/10.1186/s40537-023-00774-9>
- <span id="page-38-13"></span>[2] Basant Agarwal. 2023. Financial sentiment analysis model utilizing knowledge-base and domain-specific representation. Multimed Tools Appl 82, 6 (2023), 8899‚Äì8920. <https://doi.org/10.1007/s11042-022-12181-y>
- <span id="page-38-6"></span>[3] Ahmad Ahmadov, Maik Thiele, Julian Eberius, Wolfgang Lehner, and Robert Wrembel. 2015. Towards a Hybrid Imputation Approach Using Web Tables. In 2015 IEEE/ACM 2nd International Symposium on Big Data Computing (BDC). 21‚Äì30. <https://doi.org/10.1109/BDC.2015.38>
- <span id="page-38-11"></span>[4] Reda Al-Bahrani, Dipendra Jha, Qiao Kang, Sunwoo Lee, Zijiang Yang, Wei-Keng Liao, Ankit Agrawal, and Alok Choudhary. 2021. SIGRNN: Synthetic Minority Instances Generation in Imbalanced Datasets using a Recurrent Neural Network:. In Proceedings of the 10th International Conference on Pattern Recognition Applications and Methods. 349‚Äì356. <https://doi.org/10.5220/0010348103490356>
- <span id="page-38-12"></span>[5] Boaz Barak, Kamalika Chaudhuri, Cynthia Dwork, Satyen Kale, Frank McSherry, and Kunal Talwar. 2007. Privacy, accuracy, and consistency too: a holistic solution to contingency table release. In Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems. 273‚Äì282. [https://doi.org/10.1145/1265530.](https://doi.org/10.1145/1265530.1265569) [1265569](https://doi.org/10.1145/1265530.1265569)
- <span id="page-38-1"></span>[6] Omar Benjelloun, Shiyu Chen, and Natasha F. Noy. 2020. Google Dataset Search by the Numbers. In The Semantic Web - ISWC 2020 - 19th International Semantic Web Conference, Athens, Greece, November 2-6, 2020, Proceedings, Part II (Lecture Notes in Computer Science, Vol. 12507). 667‚Äì682. [https://doi.org/10.1007/978-3-030-62466-8\\_41](https://doi.org/10.1007/978-3-030-62466-8_41)
- <span id="page-38-7"></span>[7] Chandra Sekhar Bhagavatula, Thanapon Noraset, and Doug Downey. 2015. TabEL: Entity Linking in Web Tables. In The Semantic Web - ISWC 2015. Vol. 9366. 425‚Äì441. [https://doi.org/10.1007/978-3-319-25007-6\\_25](https://doi.org/10.1007/978-3-319-25007-6_25)
- <span id="page-38-10"></span>[8] Sagar Bharadwaj, Praveen Gupta, Ranjita Bhagwan, and Saikat Guha. 2021. Discovering related data at scale. Proc. VLDB Endow. 14, 8 (2021), 1392‚Äì1400. <https://doi.org/10.14778/3457390.3457403>
- <span id="page-38-8"></span>[9] Alex Bogatu, Alvaro A. A. Fernandes, Norman W. Paton, and Nikolaos Konstantinou. 2020. Dataset Discovery in Data Lakes. In 2020 IEEE 36th International Conference on Data Engineering (ICDE). 709‚Äì720. [https://doi.org/10.1109/](https://doi.org/10.1109/ICDE48307.2020.00067) [ICDE48307.2020.00067](https://doi.org/10.1109/ICDE48307.2020.00067)
- <span id="page-38-4"></span>[10] Vadim Borisov, Tobias Leemann, Kathrin Sessler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. 2022. Deep Neural Networks and Tabular Data: A Survey. IEEE Trans. Neural Netw. Learning Syst. (2022), 1‚Äì21. [https:](https://doi.org/10.1109/TNNLS.2022.3229161) [//doi.org/10.1109/TNNLS.2022.3229161](https://doi.org/10.1109/TNNLS.2022.3229161)
- <span id="page-38-0"></span>[11] Vadim Borisov, Kathrin Se√üler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. 2023. Language Models are Realistic Tabular Data Generators. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. <https://openreview.net/forum?id=cEygmQNOeI>
- <span id="page-38-3"></span>[12] Stef Van Buuren and Karin Groothuis-Oudshoorn. 2011. MICE : Multivariate Imputation by Chained Equations in R. J. Stat. Soft. 45, 3 (2011). <https://doi.org/10.18637/jss.v045.i03>
- <span id="page-38-9"></span>[13] Riccardo Cappuzzo, Paolo Papotti, and Saravanan Thirumuruganathan. 2020. Creating Embeddings of Heterogeneous Relational Datasets for Data Integration Tasks. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. 1335‚Äì1349. <https://doi.org/10.1145/3318464.3389742>
- <span id="page-38-5"></span>[14] Riccardo Cappuzzo, Paolo Papotti, and Saravanan Thirumuruganathan. 2020. Creating Embeddings of Heterogeneous Relational Datasets for Data Integration Tasks. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. 1335‚Äì1349. <https://doi.org/10.1145/3318464.3389742>
- <span id="page-38-2"></span>[15] Raul Castro Fernandez, Ziawasch Abedjan, Famien Koko, Gina Yuan, Samuel Madden, and Michael Stonebraker. 2018. Aurum: A Data Discovery System. In 2018 IEEE 34th International Conference on Data Engineering (ICDE). 1001‚Äì1012.

<https://doi.org/10.1109/ICDE.2018.00094>

- <span id="page-39-17"></span>[16] Chengliang Chai, Jiabin Liu, Nan Tang, Guoliang Li, and Yuyu Luo. 2022. Selective data acquisition in the wild for model charging. Proc. VLDB Endow. 15, 7 (2022), 1466‚Äì1478. <https://doi.org/10.14778/3523210.3523223>
- <span id="page-39-0"></span>[17] Chengliang Chai, Jiayi Wang, Yuyu Luo, Zeping Niu, and Guoliang Li. 2022. Data Management for Machine Learning: A Survey. IEEE Trans. Knowl. Data Eng. (2022), 1‚Äì1. <https://doi.org/10.1109/TKDE.2022.3148237>
- <span id="page-39-7"></span>[18] Adriane Chapman, Elena Simperl, Laura Koesten, George Konstantinidis, Luis-Daniel Ib√°√±ez, Emilia Kacprzak, and Paul Groth. 2020. Dataset search: a survey. The VLDB Journal 29, 1 (2020), 251‚Äì272. [https://doi.org/10.1007/s00778-](https://doi.org/10.1007/s00778-019-00564-x) [019-00564-x](https://doi.org/10.1007/s00778-019-00564-x)
- <span id="page-39-13"></span>[19] Haipeng Chen, Sushil Jajodia, Jing Liu, Noseong Park, Vadim Sokolov, and V. S. Subrahmanian. 2019. FakeTables: Using GANs to Generate Functional Dependency Preserving Tables with Bounded Real Data. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. 2074‚Äì2080. <https://doi.org/10.24963/ijcai.2019/287>
- <span id="page-39-5"></span>[20] Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal, and Diyi Yang. 2023. An Empirical Survey of Data Augmentation for Limited Data Learning in NLP. Trans. Assoc. Comput. Linguistics 11 (2023), 191‚Äì211. [https://doi.org/10.1162/](https://doi.org/10.1162/TACL_A_00542) [TACL\\_A\\_00542](https://doi.org/10.1162/TACL_A_00542)
- <span id="page-39-19"></span>[21] Pei Chen, Soumajyoti Sarkar, Leonard Lausen, Balasubramaniam Srinivasan, Sheng Zha, Ruihong Huang, and George Karypis. 2023. HyTrel: Hypergraph-enhanced Tabular Data Representation Learning. Advances in Neural Information Processing Systems 36 (2023), 32173‚Äì32193.
- <span id="page-39-8"></span>[22] Nadiia Chepurko, Ryan Marcus, Emanuel Zgraggen, Raul Castro Fernandez, Tim Kraska, and David Karger. 2020. ARDA: automatic relational data augmentation for machine learning. Proc. VLDB Endow. 13, 9 (2020), 1373‚Äì1387. <https://doi.org/10.14778/3397230.3397235>
- <span id="page-39-12"></span>[23] Vassilis Christophides, Vasilis Efthymiou, and Kostas Stefanidis. 2015. Entity Resolution in the Web of Data. [https:](https://doi.org/10.2200/S00655ED1V01Y201507WBE013) [//doi.org/10.2200/S00655ED1V01Y201507WBE013](https://doi.org/10.2200/S00655ED1V01Y201507WBE013)
- <span id="page-39-9"></span>[24] Haoran Dai. 2018. Seattle House Prices Hedonic Model. <https://doi.org/10.18170/DVN/DHZ9VY>
- <span id="page-39-20"></span>[25] Yingjun Dai, Ahmed El-Roby, Elmira Adeeb, and Vivek Thaker. 2025. OmniMatch: Overcoming the Cold-Start Problem in Cross-Domain Recommendations using Auxiliary Reviews. , 80‚Äì91 pages. <https://doi.org/10.48786/EDBT.2025.07>
- <span id="page-39-15"></span>[26] Zongyu Dai, Zhiqi Bu, and Qi Long. 2021. Multiple Imputation via Generative Adversarial Network for Highdimensional Blockwise Missing Value Problems. In 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA). 791‚Äì798. <https://doi.org/10.1109/ICMLA52953.2021.00131>
- <span id="page-39-11"></span>[27] Anish Das Sarma, Lujun Fang, Nitin Gupta, Alon Halevy, Hongrae Lee, Fei Wu, Reynold Xin, and Cong Yu. 2012. Finding related tables. In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data. 817‚Äì828. <https://doi.org/10.1145/2213836.2213962>
- <span id="page-39-18"></span>[28] Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. 2022. TURL: Table Understanding through Representation Learning. SIGMOD Rec. 51, 1 (2022), 33‚Äì40. <https://doi.org/10.1145/3542700.3542709>
- <span id="page-39-14"></span>[29] Yuyang Dong, Kunihiro Takeoka, Chuan Xiao, and Masafumi Oyamada. 2021. Efficient Joinable Table Discovery in Data Lakes: A High-Dimensional Similarity-Based Approach. In 2021 IEEE 37th International Conference on Data Engineering (ICDE). 456‚Äì467. <https://doi.org/10.1109/ICDE51399.2021.00046>
- <span id="page-39-10"></span>[30] Yuyang Dong, Chuan Xiao, Takuma Nozawa, Masafumi Enomoto, and Masafumi Oyamada. 2023. DeepJoin: Joinable Table Discovery with Pre-Trained Language Models. Proc. VLDB Endow. 16, 10 (2023), 2458‚Äì2470. [https://doi.org/10.](https://doi.org/10.14778/3603581.3603587) [14778/3603581.3603587](https://doi.org/10.14778/3603581.3603587)
- <span id="page-39-4"></span>[31] Justin Engelmann and Stefan Lessmann. 2021. Conditional Wasserstein GAN-based oversampling of tabular data for imbalanced learning. Expert Systems with Applications 174 (2021), 114582. <https://doi.org/10.1016/j.eswa.2021.114582>
- <span id="page-39-16"></span>[32] Mahdi Esmailoghli, Jorge-Arnulfo Quian√©-Ruiz, and Ziawasch Abedjan. 2022. MATE: multi-attribute table extraction. Proc. VLDB Endow. 15, 8 (2022), 1684‚Äì1696. <https://doi.org/10.14778/3529337.3529353>
- <span id="page-39-1"></span>[33] Grace Fan, Jin Wang, Yuliang Li, and Ren√©e J. Miller. 2023. Table Discovery in Data Lakes: State-of-the-art and Future Directions. In Companion of the 2023 International Conference on Management of Data. 69‚Äì75. [https://doi.org/10.](https://doi.org/10.1145/3555041.3589409) [1145/3555041.3589409](https://doi.org/10.1145/3555041.3589409)
- <span id="page-39-2"></span>[34] Grace Fan, Jin Wang, Yuliang Li, Dan Zhang, and Ren√©e J. Miller. 2023. Semantics-aware Dataset Discovery from Data Lakes with Contextualized Column-based Representation Learning. , 1726‚Äì1739 pages. [https://doi.org/10.](https://doi.org/10.14778/3587136.3587146) [14778/3587136.3587146](https://doi.org/10.14778/3587136.3587146)
- <span id="page-39-21"></span>[35] Ju Fan, Junyou Chen, Tongyu Liu, Yuwei Shen, Guoliang Li, and Xiaoyong Du. 2020. Relational data synthesis using generative adversarial networks: a design space exploration. Proc. VLDB Endow. 13, 12 (2020), 1962‚Äì1975. <https://doi.org/10.14778/3407790.3407802>
- <span id="page-39-3"></span>[36] Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan H. Sengamedu, and Christos Faloutsos. 2024. Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey. <https://doi.org/10.48550/ARXIV.2402.17944>
- <span id="page-39-6"></span>[37] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard H. Hovy. 2021. A Survey of Data Augmentation Approaches for NLP. In Findings of the Association for Computational

Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 (Findings of ACL, Vol. ACL/IJCNLP 2021). 968‚Äì988. <https://doi.org/10.18653/V1/2021.FINDINGS-ACL.84>

- <span id="page-40-3"></span>[38] Joao Fonseca and Fernando Bacao. 2023. Tabular and latent space synthetic data generation: a literature review. J Big Data 10, 1 (2023), 115. <https://doi.org/10.1186/s40537-023-00792-7>
- <span id="page-40-18"></span>[39] Sainyam Galhotra, Yue Gong, and Raul Castro Fernandez. 2023. Metam: Goal-Oriented Data Discovery. In 2023 IEEE 39th International Conference on Data Engineering (ICDE). 2780‚Äì2793. <https://doi.org/10.1109/ICDE55515.2023.00213>
- <span id="page-40-14"></span>[40] Michael Glass, Xueqing Wu, Ankita Rajaram Naik, Gaetano Rossiello, and Alfio Gliozzo. 2023. Retrieval-Based Transformer for Table Augmentation. In Findings of the Association for Computational Linguistics: ACL 2023. 5635‚Äì5648. <https://doi.org/10.18653/v1/2023.findings-acl.348>
- <span id="page-40-0"></span>[41] Mikel Hernandez, Gorka Epelde, Ane Alberdi, Rodrigo Cilla, and Debbie Rankin. 2022. Synthetic data generation for tabular health records: A systematic review. Neurocomputing 493 (2022), 28‚Äì45. [https://doi.org/10.1016/j.neucom.](https://doi.org/10.1016/j.neucom.2022.04.053) [2022.04.053](https://doi.org/10.1016/j.neucom.2022.04.053)
- <span id="page-40-5"></span>[42] Xuming Hu, Shen Wang, Xiao Qin, Chuan Lei, Zhengyuan Shen, Christos Faloutsos, Asterios Katsifodimos, George Karypis, Lijie Wen, and Philip S. Yu. 2023. Automatic Table Union Search with Tabular Representation Learning. In Findings of the Association for Computational Linguistics: ACL 2023. 3786‚Äì3800. [https://doi.org/10.18653/v1/2023.](https://doi.org/10.18653/v1/2023.findings-acl.233) [findings-acl.233](https://doi.org/10.18653/v1/2023.findings-acl.233)
- <span id="page-40-4"></span>[43] Madelon Hulsebos, Xiang Deng, Huan Sun, and Paolo Papotti. 2023. Models and Practice of Neural Table Representations. In Companion of the 2023 International Conference on Management of Data. 83‚Äì89. [https:](https://doi.org/10.1145/3555041.3589411) [//doi.org/10.1145/3555041.3589411](https://doi.org/10.1145/3555041.3589411)
- <span id="page-40-8"></span>[44] Madelon Hulsebos, Kevin Hu, Michiel Bakker, Emanuel Zgraggen, Arvind Satyanarayan, Tim Kraska, √áagatay Demiralp, and C√©sar Hidalgo. 2019. Sherlock: A Deep Learning Approach to Semantic Data Type Detection. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1500‚Äì1508. <https://doi.org/10.1145/3292500.3330993>
- <span id="page-40-17"></span>[45] Daniel Jarrett, Bogdan C. Cebere, Tennison Liu, Alicia Curth, and Mihaela van der Schaar. 2022. HyperImpute: Generalized Iterative Imputation with Automatic Model Selection. In Proceedings of the 39th International Conference on Machine Learning. 9916‚Äì9937.
- <span id="page-40-9"></span>[46] James Jordon, Jinsung Yoon, and Mihaela van der Schaar. 2019. PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. <https://openreview.net/forum?id=S1zk9iRqF7>
- <span id="page-40-6"></span>[47] James Max Kanter and Kalyan Veeramachaneni. 2015. Deep feature synthesis: Towards automating data science endeavors. In 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA). 1‚Äì10. [https:](https://doi.org/10.1109/DSAA.2015.7344858) [//doi.org/10.1109/DSAA.2015.7344858](https://doi.org/10.1109/DSAA.2015.7344858)
- <span id="page-40-7"></span>[48] Gilad Katz, Eui Chul Richard Shin, and Dawn Song. 2016. ExploreKit: Automatic Feature Generation and Selection. In 2016 IEEE 16th International Conference on Data Mining (ICDM). 979‚Äì984. <https://doi.org/10.1109/ICDM.2016.0123>
- <span id="page-40-13"></span>[49] Aamod Khatiwada, Grace Fan, Roee Shraga, Zixuan Chen, Wolfgang Gatterbauer, Ren√©e J. Miller, and Mirek Riedewald. 2023. SANTOS: Relationship-based Semantic Table Union Search. Proc. ACM Manag. Data 1, 1 (2023), 1‚Äì25. <https://doi.org/10.1145/3588689>
- <span id="page-40-11"></span>[50] Aamod Khatiwada, Roee Shraga, Wolfgang Gatterbauer, and Ren√©e J. Miller. 2022. Integrating Data Lake Tables. Proc. VLDB Endow. 16, 4 (2022), 932‚Äì945. <https://doi.org/10.14778/3574245.3574274>
- <span id="page-40-15"></span>[51] Jayoung Kim, Chaejeong Lee, and Noseong Park. 2023. STaSy: Score-based Tabular data Synthesis. (2023). [https:](https://openreview.net/pdf?id=1mNssCWt_v) [//openreview.net/pdf?id=1mNssCWt\\_v](https://openreview.net/pdf?id=1mNssCWt_v)
- <span id="page-40-12"></span>[52] Jayoung Kim, Chaejeong Lee, Yehjin Shin, Sewon Park, Minjung Kim, Noseong Park, and Jihoon Cho. 2022. SOS: Score-based Oversampling for Tabular Data. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 762‚Äì772. <https://doi.org/10.1145/3534678.3539454>
- <span id="page-40-16"></span>[53] Chaejeong Lee, Jayoung Kim, and Noseong Park. 2023. CoDi: Co-evolving Contrastive Diffusion Models for Mixedtype Tabular Synthesis. In Proceedings of the 40th International Conference on Machine Learning. 18940‚Äì18956.
- <span id="page-40-2"></span>[54] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. 2024. Table-GPT: Table Fine-tuned GPT for Diverse Table Tasks. , 176 pages. [https:](https://doi.org/10.1145/3654979) [//doi.org/10.1145/3654979](https://doi.org/10.1145/3654979)
- <span id="page-40-10"></span>[55] Steven Cheng-Xian Li, Bo Jiang, and Benjamin M. Marlin. 2019. MisGAN: Learning from Incomplete Data with Generative Adversarial Networks. (2019). <https://openreview.net/forum?id=S1lDV3RcKm>
- <span id="page-40-19"></span>[56] Wei-Hong Li, Xialei Liu, and Hakan Bilen. 2024. Universal Representations: A Unified Look at Multiple Task and Domain Learning. Int J Comput Vis 132, 5 (2024), 1521‚Äì1545. <https://doi.org/10.1007/s11263-023-01931-6>
- <span id="page-40-1"></span>[57] Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. [n. d.]. ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge. Cureus 15 ([n. d.]), e40895. <https://doi.org/10.7759/cureus.40895>
- <span id="page-41-5"></span>[58] Yuliang Li, Xiaolan Wang, Zhengjie Miao, and Wang-Chiew Tan. 2021. Data augmentation for ML-driven data preparation and integration. Proc. VLDB Endow. 14, 12 (2021), 3182‚Äì3185. <https://doi.org/10.14778/3476311.3476403>
- <span id="page-41-16"></span>[59] Jiaming Liang, Chuan Lei, Xiao Qin, Jiani Zhang, Asterios Katsifodimos, Christos Faloutsos, and Huzefa Rangwala. 2024. FeatNavigator: Automatic Feature Augmentation on Tabular Data. CoRR abs/2406.09534 (2024). [https:](https://doi.org/10.48550/ARXIV.2406.09534) [//doi.org/10.48550/ARXIV.2406.09534](https://doi.org/10.48550/ARXIV.2406.09534)
- <span id="page-41-6"></span>[60] Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti. 2010. Annotating and searching web tables using entities, types and relationships. Proc. VLDB Endow. 3, 1-2 (2010), 1338‚Äì1347. <https://doi.org/10.14778/1920841.1921005>
- <span id="page-41-20"></span>[61] Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adri√† de Gispert, and Gonzalo Iglesias. 2023. LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. 1557‚Äì1566. <https://doi.org/10.18653/V1/2023.ACL-SHORT.133>
- <span id="page-41-17"></span>[62] Yin Lin, Bolin Ding, H. V. Jagadish, and Jingren Zhou. 2024. SMARTFEAT: Efficient Feature Construction through Feature-Level Foundation Model Interactions. (2024). <https://www.cidrdb.org/cidr2024/papers/p72-lin.pdf>
- <span id="page-41-21"></span>[63] P.J.G. Lisboa, S. Saralajew, A. Vellido, R. Fern√°ndez-Domenech, and T. Villmann. 2023. The coming of age of interpretable and explainable machine learning models. Neurocomputing 535 (2023), 25‚Äì39. [https://doi.org/10.1016/j.](https://doi.org/10.1016/j.neucom.2023.02.040) [neucom.2023.02.040](https://doi.org/10.1016/j.neucom.2023.02.040)
- <span id="page-41-1"></span>[64] Jiabin Liu, Chengliang Chai, Yuyu Luo, Yin Lou, Jianhua Feng, and Nan Tang. 2022. Feature Augmentation with Reinforcement Learning. In 2022 IEEE 38th International Conference on Data Engineering (ICDE). 3360‚Äì3372. [https:](https://doi.org/10.1109/ICDE53745.2022.00317) [//doi.org/10.1109/ICDE53745.2022.00317](https://doi.org/10.1109/ICDE53745.2022.00317)
- <span id="page-41-11"></span>[65] Jiabin Liu, Fu Zhu, Chengliang Chai, Yuyu Luo, and Nan Tang. 2021. Automatic data acquisition for deep learning. Proc. VLDB Endow. 14, 12 (2021), 2739‚Äì2742. <https://doi.org/10.14778/3476311.3476333>
- <span id="page-41-3"></span>[66] Tongyu Liu, Ju Fan, Nan Tang, Guoliang Li, and Xiaoyong Du. 2024. Controllable Tabular Data Synthesis Using Diffusion Models. Proc. ACM Manag. Data 2, 1 (2024), 1‚Äì29. <https://doi.org/10.1145/3639283>
- <span id="page-41-15"></span>[67] Tennison Liu, Zhaozhi Qian, Jeroen Berrevoets, and Mihaela van der Schaar. 2023. GOGGLE: Generative modelling for tabular data by learning relational structure. (2023). <https://openreview.net/pdf?id=fPVRcJqspu>
- <span id="page-41-0"></span>[68] Hui Luan and Chin-Chung Tsai. 2021. A Review of Using Machine Learning Approaches for Precision Education. Educational Technology & Society 24, 1 (2021), 250‚Äì266. <https://www.jstor.org/stable/26977871>
- <span id="page-41-18"></span>[69] Yu A. Malkov and D. A. Yashunin. 2020. Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs. IEEE Trans. Pattern Anal. Mach. Intell. 42, 4 (2020), 824‚Äì836. [https://doi.org/10.1109/](https://doi.org/10.1109/TPAMI.2018.2889473) [TPAMI.2018.2889473](https://doi.org/10.1109/TPAMI.2018.2889473)
- <span id="page-41-9"></span>[70] Pierre-Alexandre Mattei and Jes Frellsen. 2019. MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets. In Proceedings of the 36th International Conference on Machine Learning. 4413‚Äì4423.
- <span id="page-41-14"></span>[71] Zhengjie Miao and Jin Wang. 2023. Watchog: A Light-weight Contrastive Learning based Framework for Column Annotation. Proc. ACM Manag. Data 1, 4 (2023), 1‚Äì24. <https://doi.org/10.1145/3626766>
- <span id="page-41-19"></span>[72] Ren√©e J. Miller. 2018. Open data integration. Proc. VLDB Endow. 11, 12 (2018), 2130‚Äì2139. [https://doi.org/10.14778/](https://doi.org/10.14778/3229863.3240491) [3229863.3240491](https://doi.org/10.14778/3229863.3240491)
- <span id="page-41-13"></span>[73] Fatemeh Nargesian, Ken Qian Pu, Bahar Ghadiri Bashardoost, Erkang Zhu, and Renee J. Miller. 2022. Data Lake Organization. IEEE Trans. Knowl. Data Eng. (2022), 1‚Äì1. <https://doi.org/10.1109/TKDE.2021.3091101>
- <span id="page-41-10"></span>[74] Fatemeh Nargesian, Ken Q. Pu, Erkang Zhu, Bahar Ghadiri Bashardoost, and Ren√©e J. Miller. 2020. Organizing Data Lakes for Navigation. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. 1939‚Äì1950. <https://doi.org/10.1145/3318464.3380605>
- <span id="page-41-7"></span>[75] Fatemeh Nargesian, Erkang Zhu, Ken Q. Pu, and Ren√©e J. Miller. 2018. Table union search on open data. Proc. VLDB Endow. 11, 7 (2018), 813‚Äì825. <https://doi.org/10.14778/3192965.3192973>
- <span id="page-41-4"></span>[76] Alfredo Naz√°bal, Pablo M. Olmos, Zoubin Ghahramani, and Isabel Valera. 2020. Handling incomplete heterogeneous data using VAEs. Pattern Recognition 107 (2020), 107501. <https://doi.org/10.1016/j.patcog.2020.107501>
- <span id="page-41-12"></span>[77] Paul Ouellette, Aidan Sciortino, Fatemeh Nargesian, Bahar Ghadiri Bashardoost, Erkang Zhu, Ken Q. Pu, and Ren√©e J. Miller. 2021. RONIN: data lake exploration. Proc. VLDB Endow. 14, 12 (2021), 2863‚Äì2866. [https://doi.org/10.14778/](https://doi.org/10.14778/3476311.3476364) [3476311.3476364](https://doi.org/10.14778/3476311.3476364)
- <span id="page-41-8"></span>[78] Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu Park, and Youngmin Kim. 2018. Data synthesis based on generative adversarial networks. Proc. VLDB Endow. 11, 10 (2018), 1071‚Äì1083. [https:](https://doi.org/10.14778/3231751.3231757) [//doi.org/10.14778/3231751.3231757](https://doi.org/10.14778/3231751.3231757)
- <span id="page-41-2"></span>[79] Norman W. Paton, Jiaoyan Chen, and Zhenyu Wu. 2024. Dataset Discovery and Exploration: A Survey. ACM Comput. Surv. 56, 4 (2024), 1‚Äì37. <https://doi.org/10.1145/3626521>
- <span id="page-41-22"></span>[80] Charith Peris, Christophe Dupuy, Jimit Majmudar, Rahil Parikh, Sami Smaili, Richard Zemel, and Rahul Gupta. 2023. Privacy in the Time of Language Models. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining. 1291‚Äì1292. <https://doi.org/10.1145/3539597.3575792>

Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI 43

- <span id="page-42-0"></span>[81] Rundo, Trenta, Di Stallo, and Battiato. 2019. Machine Learning for Quantitative Finance Applications: A Survey. Applied Sciences 9, 24 (2019), 5574. <https://doi.org/10.3390/app9245574>
- <span id="page-42-18"></span>[82] Bukhoree Sahoh and Anant Choksuriwong. 2023. The role of explainable Artificial Intelligence in high-stakes decision-making systems: a systematic review. J Ambient Intell Human Comput 14, 6 (2023), 7827‚Äì7843. [https:](https://doi.org/10.1007/s12652-023-04594-w) [//doi.org/10.1007/s12652-023-04594-w](https://doi.org/10.1007/s12652-023-04594-w)
- <span id="page-42-19"></span>[83] June Sallou, Thomas Durieux, and Annibale Panichella. 2024. Breaking the Silence: the Threats of Using LLMs in Software Engineering. In Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results. 102‚Äì106. <https://doi.org/10.1145/3639476.3639764>
- <span id="page-42-9"></span>[84] Aecio Santos, Aline Bessa, Christopher Musco, and Juliana Freire. 2022. A Sketch-based Index for Correlated Dataset Search. In 2022 IEEE 38th International Conference on Data Engineering (ICDE). 2928‚Äì2941. [https://doi.org/10.1109/](https://doi.org/10.1109/ICDE53745.2022.00264) [ICDE53745.2022.00264](https://doi.org/10.1109/ICDE53745.2022.00264)
- <span id="page-42-17"></span>[85] Bowen Shi, Ke Xu, and Jichang Zhao. 2023. Domain-relevance of influence: characterizing variations in online influence across multiple domains on social media. J Big Data 10, 1 (2023), 69. <https://doi.org/10.1186/s40537-023-00764-x>
- <span id="page-42-7"></span>[86] Daniel J. Stekhoven and Peter B√ºhlmann. 2012. MissForest‚Äînon-parametric missing value imputation for mixed-type data. Bioinformatics 28, 1 (2012), 112‚Äì118. <https://doi.org/10.1093/bioinformatics/btr597>
- <span id="page-42-11"></span>[87] Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, √áaƒüatay Demiralp, Chen Chen, and Wang-Chiew Tan. 2022. Annotating Columns with Pre-trained Language Models. In Proceedings of the 2022 International Conference on Management of Data. 1493‚Äì1503. <https://doi.org/10.1145/3514221.3517906>
- <span id="page-42-2"></span>[88] Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark Gerstein. 2023. Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data? CoRR abs/2309.08963 (2023). <https://doi.org/10.48550/ARXIV.2309.08963>
- <span id="page-42-10"></span>[89] Mohamed Trabelsi, Zhiyu Chen, Shuo Zhang, Brian D. Davison, and Jeff Heflin. 2022. StruBERT: Structure-aware BERT for Table Search and Matching. In Proceedings of the ACM Web Conference 2022. 442‚Äì451. [https://doi.org/10.](https://doi.org/10.1145/3485447.3511972) [1145/3485447.3511972](https://doi.org/10.1145/3485447.3511972)
- <span id="page-42-1"></span>[90] Toan V. Tran and Li Xiong. 2024. Differentially Private Tabular Data Synthesis using Large Language Models. <https://doi.org/10.48550/ARXIV.2406.01457>
- <span id="page-42-6"></span>[91] Petros Venetis, Alon Halevy, Jayant Madhavan, Marius Pa≈üca, Warren Shen, Fei Wu, Gengxin Miao, and Chung Wu. 2011. Recovering semantics of tables on the web. Proc. VLDB Endow. 4, 9 (2011), 528‚Äì538. [https://doi.org/10.14778/](https://doi.org/10.14778/2002938.2002939) [2002938.2002939](https://doi.org/10.14778/2002938.2002939)
- <span id="page-42-14"></span>[92] Dongjie Wang, Meng Xiao, Min Wu, Pengfei Wang, Yuanchun Zhou, and Yanjie Fu. 2023. Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions. Advances in Neural Information Processing Systems 36 (2023), 43563‚Äì43578.
- <span id="page-42-4"></span>[93] Jiayi Wang, Chengliang Chai, Nan Tang, Jiabin Liu, and Guoliang Li. 2022. Coresets over multiple tables for feature-rich and data-efficient machine learning. Proc. VLDB Endow. 16, 1 (2022), 64‚Äì76. <https://doi.org/10.14778/3561261.3561267>
- <span id="page-42-16"></span>[94] Qiming Wang and Raul Castro Fernandez. 2023. Solo: Data Discovery Using Natural Language Questions Via A Self-Supervised Approach. Proc. ACM Manag. Data 1, 4 (2023), 262:1‚Äì262:27. <https://doi.org/10.1145/3626756>
- <span id="page-42-12"></span>[95] Zifeng Wang and Jimeng Sun. 2022. TransTab: Learning Transferable Tabular Transformers Across Tables. Advances in Neural Information Processing Systems 35 (2022), 2902‚Äì2915.
- <span id="page-42-3"></span>[96] Zaitian Wang, Pengfei Wang, Kunpeng Liu, Pengyang Wang, Yanjie Fu, Chang-Tien Lu, Charu C. Aggarwal, Jian Pei, and Yuanchun Zhou. 2024. A Comprehensive Survey on Data Augmentation. [https://doi.org/10.48550/ARXIV.2405.](https://doi.org/10.48550/ARXIV.2405.09591) [09591](https://doi.org/10.48550/ARXIV.2405.09591)
- <span id="page-42-15"></span>[97] Karina Wimmer, Ralf Wimmer, Christoph Scholl, and Bernd Becker. 2016. Skolem Functions for DQBF. In Automated Technology for Verification and Analysis. Vol. 9938. 395‚Äì411. [https://doi.org/10.1007/978-3-319-46520-3\\_25](https://doi.org/10.1007/978-3-319-46520-3_25)
- <span id="page-42-13"></span>[98] Meng Xiao, Dongjie Wang, Min Wu, Pengfei Wang, Yuanchun Zhou, and Yanjie Fu. 2023. Beyond Discrete Selection: Continuous Embedding Space Optimization for Generative Feature Selection. In 2023 IEEE International Conference on Data Mining (ICDM). 688‚Äì697. <https://doi.org/10.1109/ICDM58522.2023.00078>
- <span id="page-42-8"></span>[99] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. 2019. Modeling Tabular data using Conditional GAN. In Advances in Neural Information Processing Systems, Vol. 32. [https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2019/hash/254ed7d2de3b23ab10936522dd547b78-Abstract.html) [paper/2019/hash/254ed7d2de3b23ab10936522dd547b78-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/254ed7d2de3b23ab10936522dd547b78-Abstract.html)
- <span id="page-42-5"></span>[100] Mohamed Yakout, Kris Ganjam, Kaushik Chakrabarti, and Surajit Chaudhuri. 2012. InfoGather: entity augmentation and attribute discovery by holistic matching with web tables. In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data. 97‚Äì108. <https://doi.org/10.1145/2213836.2213848>
- <span id="page-42-21"></span>[101] Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, and Xiuzhen Cheng. 2024. On Protecting the Data Privacy of Large Language Models (LLMs): A Survey. CoRR abs/2403.05156 (2024). [https://doi.org/10.48550/](https://doi.org/10.48550/ARXIV.2403.05156) [ARXIV.2403.05156](https://doi.org/10.48550/ARXIV.2403.05156)
- <span id="page-42-20"></span>[102] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. 2024. A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly. High-Confidence Computing 4, 2 (2024), 100211.

<https://doi.org/10.1016/j.hcc.2024.100211>

- <span id="page-43-6"></span>[103] Jinsung Yoon, James Jordon, and Mihaela Schaar. 2018. GAIN: Missing Data Imputation using Generative Adversarial Nets. In Proceedings of the 35th International Conference on Machine Learning. 5689‚Äì5698.
- <span id="page-43-15"></span>[104] Bowen Yu, Cheng Fu, Haiyang Yu, Fei Huang, and Yongbin Li. 2023. Unified Language Representation for Question Answering over Text, Tables, and Images. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023. 4756‚Äì4765. <https://doi.org/10.18653/V1/2023.FINDINGS-ACL.292>
- <span id="page-43-2"></span>[105] Dan Zhang, Madelon Hulsebos, Yoshihiko Suhara, √áaƒüatay Demiralp, Jinfeng Li, and Wang-Chiew Tan. 2020. Sato: contextual semantic type detection in tables. Proc. VLDB Endow. 13, 12 (2020), 1835‚Äì1848. [https://doi.org/10.14778/](https://doi.org/10.14778/3407790.3407793) [3407790.3407793](https://doi.org/10.14778/3407790.3407793)
- <span id="page-43-13"></span>[106] Jun Zhang, Graham Cormode, Cecilia M. Procopiuc, Divesh Srivastava, and Xiaokui Xiao. 2014. PrivBayes: private data release via bayesian networks. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data. 1423‚Äì1434. <https://doi.org/10.1145/2588555.2588573>
- <span id="page-43-16"></span>[107] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. 2023. Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding. <https://doi.org/10.48550/ARXIV.2309.08168>
- <span id="page-43-7"></span>[108] Li Zhang, Shuo Zhang, and Krisztian Balog. 2019. Table2Vec: Neural Word and Entity Embeddings for Table Population and Retrieval. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 1029‚Äì1032. <https://doi.org/10.1145/3331184.3331333>
- <span id="page-43-5"></span>[109] Shuo Zhang and Krisztian Balog. 2017. EntiTables: Smart Assistance for Entity-Focused Tables. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. 255‚Äì264. <https://doi.org/10.1145/3077136.3080796>
- <span id="page-43-9"></span>[110] Shuo Zhang and Krisztian Balog. 2019. Auto-completion for Data Cells in Relational Tables. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 761‚Äì770. [https://doi.org/10.1145/3357384.](https://doi.org/10.1145/3357384.3357932) [3357932](https://doi.org/10.1145/3357384.3357932)
- <span id="page-43-1"></span>[111] Shuo Zhang and Krisztian Balog. 2020. Web Table Extraction, Retrieval, and Augmentation: A Survey. ACM Trans. Intell. Syst. Technol. 11, 2 (2020), 1‚Äì35. <https://doi.org/10.1145/3372117>
- <span id="page-43-12"></span>[112] Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gonzalez. 2024. RAFT: Adapting Language Model to Domain Specific RAG. CoRR abs/2403.10131 (2024). [https://doi.org/10.48550/](https://doi.org/10.48550/ARXIV.2403.10131) [ARXIV.2403.10131](https://doi.org/10.48550/ARXIV.2403.10131)
- <span id="page-43-10"></span>[113] Yishuo Zhang, Nayyar Zaidi, Jiahui Zhou, and Gang Li. 2023. Interpretable tabular data generation. Knowl Inf Syst 65, 7 (2023), 2935‚Äì2963. <https://doi.org/10.1007/s10115-023-01834-5>
- <span id="page-43-3"></span>[114] Zixuan Zhao and Raul Castro Fernandez. 2022. Leva: Boosting Machine Learning Performance with Relational Embedding Data Augmentation. In Proceedings of the 2022 International Conference on Management of Data. 1504‚Äì1517. <https://doi.org/10.1145/3514221.3517891>
- <span id="page-43-14"></span>[115] Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, and Weiping Wang. 2024. Multimodal Table Understanding. <https://doi.org/10.48550/ARXIV.2406.08100>
- <span id="page-43-11"></span>[116] Shuhan Zheng and Nontawat Charoenphakdee. 2022. Diffusion models for missing value imputation in tabular data. <https://doi.org/10.48550/ARXIV.2210.17128>
- <span id="page-43-0"></span>[117] Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, and Yuan Wu. 2024. A Survey on Data Augmentation in Large Model Era. <https://doi.org/10.48550/ARXIV.2401.15422>
- <span id="page-43-8"></span>[118] Erkang Zhu, Dong Deng, Fatemeh Nargesian, and Ren√©e J. Miller. 2019. JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes. In Proceedings of the 2019 International Conference on Management of Data. 847‚Äì864. <https://doi.org/10.1145/3299869.3300065>
- <span id="page-43-4"></span>[119] Erkang Zhu, Fatemeh Nargesian, Ken Q. Pu, and Ren√©e J. Miller. 2016. LSH ensemble: internet-scale domain search. Proc. VLDB Endow. 9, 12 (2016), 1185‚Äì1196. <https://doi.org/10.14778/2994509.2994534>