# GABO: Graph Augmentations with Bi-level Optimization

Heejung W. Chung <sup>∗</sup> Dept of Computer Science Stanford University hchung98@stanford.edu

Avoy Datta <sup>∗</sup> Dept of Electrical Engineering Stanford University avoy.datta@stanford.edu

Chris Waites <sup>∗</sup> Dept of Computer Science Stanford University waites@stanford.edu

# Abstract

Data augmentation refers to a wide range of techniques for improving model generalization by augmenting training examples. Oftentimes such methods require domain knowledge about the dataset at hand, spawning a plethora of recent literature surrounding automated techniques for data augmentation. In this work we apply one such method, bilevel optimization, to tackle the problem of graph classification on the ogbg-molhiv dataset. Our best performing augmentation achieved a test ROCAUC score of 77.77% with a GIN+virtual classifier, which makes it the most effective augmenter for this classifier on the leaderboard. This framework combines a GIN layer augmentation generator with a bias transformation and outperforms the same classifier augmented using the state-of-the-art FLAG augmentation.

# 1 Introduction

Data augmentation techniques characterize a well known class of approaches used in the context of machine learning to improve model generalization. In this class of techniques, training datasets are expanded by identifying invariances in the training data and exploiting these invariances to generate additional valid training points.

Automatic data augmentation techniques, broadly speaking, approach this by applying a large set of augmentation maps to a dataset (including those which may not work well), parameterizing them (for example, by the probability of their application), and aiming to find the values for these parameters which optimally improve generalization performance. Although this may produce a good performing data augmentation scheme given a set of known primitive augmentation maps, it still requires a priori knowledge of potential invariances in the data.

The goal of this project is to investigate methods for learning primitive augmentation maps end-to-end from a given classification problem. To the best of our knowledge, this is a new research direction which has not yet seen previous investigation in the context of graphs. Such a data augmentation algorithm could allow for free model generalization improvements for new unknown problems with nuanced invariance structure; this would reduce reliance on expert domain knowledge, proving to be a very important research direction.

In this regard, we have chosen to apply bilevel optimization [Mounsaveng et al.](#page-8-0) [\(2020\)](#page-8-0) to the task of automatic data augmentation. Bilevel optimization on a high level describes a metalearninginspired technique for training which optimizes for the generalization performance of a given model. It performs this by optimizing an augmentation network using the performance of a downstream

<sup>∗</sup>Equal contribution

![](_page_1_Figure_0.jpeg)

**Caption:** Figure 1 illustrates the GABO framework for graph augmentation using bilevel optimization. The inner loop updates classifier parameters based on augmented node embeddings, while the outer loop optimizes the augmentation generator using un-augmented examples, highlighting the novel integration of augmentation within graph classification tasks.

<span id="page-1-0"></span>Figure 1: GABO Framework. *Inner Loop:* For every training example, we obtain an atom embedding for each node. We then augment these embeddings using a frozen augmentation generator, and the augmented graph is then passed into the rest of our classifier. Our loss in the inner loop is used to update the parameters ω of our classifier. Note that we treat the atom encoder and GIN + virtual node as parts of a single classifier, but they are depicted separately here in order to clarify where the augmentation is performed within the forward operation. *Outer loop:* After freezing the graph classifier, we use un-augmented examples from a pseudo-validation set to optimize the parameters θ of our augmentation generator.

classification network evaluated on a held out validation set. This is distinct from other model robustness techniques used in the context of graph classification, for example FLAG [Kong et al.](#page-8-1) [\(2020\)](#page-8-1). FLAG approaches the task of model generalization in the form of adversarial training that is, it iteratively trains a given model on examples which have small and carefully constructed perturbations which change their label. The assumption is that these small perturbations should not change their label and that the classification model should be robust to such perturbations.

Our work is evaluated on the ogbg-molhiv dataset [Hu et al.](#page-7-0) [\(2021\)](#page-7-0), an adaptation from a previous dataset known as MoleculeNet [Wu et al.](#page-8-2) [\(2017\)](#page-8-2). In this setting, each graph represents a molecule in which nodes represent atoms and edges represent bonds between atoms. Node features are 9 dimensional and containing various information pertaining to atoms including their atomic number, chirality, formal charge, and whether or not the atom is in a ring. Molecules are assigned binary labels characterizing whether a molecule inhibits HIV virus replication or not. We believe this to be a relevant problem setting given that manually constructed augmentations are not immediately clear, highlighting the relevance of automatic techniques for data augmentation.

### 2 Related Work

Our work aims to improve on two existing methods: Free Large-scale Adversarial Augmentation on Graphs (FLAG) and Data Augmentation using Bilevel Optimization (DABO). Our data augmentation could be performed on any graph classification model; we choose to run all of our experiments with GIN virtual, given its position on the leaderboard despite its relatively simple implementation.

### 2.1 FLAG

The FLAG algorithm leverages the concept of "free" adversarial training [Shafahi et al.](#page-8-3) [\(2019\)](#page-8-3). Traditional adversarial training solves the min-max problem:

$$
\min_{\theta} \mathbb{E}_{(x,y)\sim D} \left[ \max_{\|\delta\|_p \leq \epsilon} L(f_{\theta}(x+\delta), y) \right]
$$
 (1)

The inner optimization problem is typically solved using Proximal Gradient Descent (PGD), which is strong but inefficent. "Free" adversarial training cuts down on the runtime by *simultaneously* computing gradients for both the perturbation δ and the model parameters θ. Optimization over δ is carried out M times for the same minibatch (ascent step), with the final gradient estimate averaged for robustness. The modified optimization problem is:

$$
\min_{\theta} \mathbb{E}_{(x,y)\sim D} \left[ \frac{1}{M} \sum_{t=1}^{M} \max_{||\delta||_p \leq \epsilon} L(f_{\theta}(x+\delta), y) \right]
$$
(2)

To make the optimization tractable in regard to runtime, the authors use gradient accumulation to update the model parameters θ. Essentially, the gradient w.r.t. θ is computed on every mini-batch, for M times, with the algorithm maintaining a cumulative count of gradients at every step (scaled down by M). At the end of each epoch, θ is updated using the cumulative scaled gradients, obtained by averaging over M iterations of the minibatch.

Some other features FLAG are worth noting:

- Unbounded attack: While realistic perturbations for image-based data is usually bounded by some L − p norm on the noise δ, this isn't the case for node features for most graph networks. FLAG addresses this by not imposing an explicit bound on δ (which is rather implicitly bounded by: step size α × # ascending steps M)
- Biased perturbation for node classification: For node inference problems *specifically*, FLAG biases perturbations for far-away nodes using larger step-sizes. The authors justify this approach using superior results in ablation studies.

#### 2.2 DABO: Data augmentation with bilevel optimization (computer vision)

[Mounsaveng et al.](#page-8-0) [\(2020\)](#page-8-0) presents a technique for automatic data augmentation map discovery, henceforth referred to as DABO. DABO learns a data augmentation map which, when applied to training data, maximizes the validation accuracy of an end task model. Although, the core limitation is that optimizing a parameterized data augmentation map for this objective is not possible through direct gradient based methods, as the validation loss on unknown holdout data is not a differentiable function of the data augmentation map parameters.

To get around this limitation, they propose an approach which makes use of techniques from online bilevel optimization. Withing this framework, they are able to learn transformations of the training data that minimize the validation loss while training the end task model. On a high level, their training procedure for the augmentation map makes use of an inner loop and an outer loop. In the inner loop, the classifier parameters ω are trained in the standard supervised way. In the outer loop, the data augmentation parameters θ are trained on the validation set using an online differentiable method.

$$
\theta \leftarrow \theta - \eta_{\theta} \nabla_{\theta} \mathcal{L}(X_{val}, \omega*)
$$
\n(3)

Given that augmentations by definitions are only applied to the training dataset, it's not immediately clear how the expression ∇θL(Xval, ω∗) is evaluated. To make it tractable, they make a key observation enabled by the fact that parameters of the classifier ω ∗ are shared between the validation and training loss. Given this, we can rewrite this expression as:

$$
\nabla_{\theta} \mathcal{L}(X_{val}, \omega^*) = \frac{\partial \mathcal{L}(X_{val}, \omega^*)}{\partial \omega^*} \frac{\partial \omega^*}{\partial \theta}
$$
(4)

Further, if we define the gradient of the training loss at iteration t as:

$$
\mathcal{G}^{(t)} = \nabla_{\omega} \mathcal{L}(\mathcal{A}_{\theta}(X_{train}), \omega^t)
$$
\n(5)

Then, finally we can write ∂ω<sup>∗</sup> ∂θ as:

$$
\frac{\partial \omega^*}{\partial \theta} = \sum_{i=1}^{T-1} \frac{\partial \omega^{(T)}}{\partial \omega^{(i)}} \frac{\partial \omega^{(i)}}{\partial \mathcal{G}^{(i)}} \frac{\partial \mathcal{G}^{(i)}}{\partial \theta} \tag{6}
$$

This is sufficient to optimize for using the bilevel optimization procedure described by rolling out the gradients observed during the inner loop. In practice, commonly the last j gradients are used to compute an estimator of the true gradient quantity just described. Additionally, gradient updates are performed on the augmenter every k steps instead of at the end of classifier training, as the latter would be computationally intractable. To the best of our knowledge, this class of augmentations has not been applied in the context of graphs before. Hence, we believe applying this approach to graphs is a novel contribution.

#### 2.3 GIN with virtual node

The graph isomorphism network (henceforth referred to as GIN) was first introduced in [Xu et al.](#page-8-4) [\(2018\)](#page-8-4). This model was first proposed as an investigation of the discriminative power of GNN variants (e.g. graph convolutional network and GraphSAGE). This work was able to show that such models were unable to distinguish simple graph structures. In response to this observation, GIN was developed as a simple architecture that was provably the most expressive among the class of GNNs, being as powerful as the Weisfeiler-Lehman graph isomorphism test. Hence, GIN demonstrates strong empirical performance in the context of many graph classification benchmarks. To briefly characterize this approach, GIN updates node representations as:

$$
h_v^{(k)} = \text{MLP}^{(k)}\left( (1+\varepsilon) \cdot h_v^{(k-1)} + \sum_{u \in \mathcal{N}(v)} h_u^{(k-1)} \right)
$$

In the context of graph classification, some approaches will additionally augment graphs by including "virtual" nodes - that is, nodes which are connected to all other nodes in the graph [Li et al.](#page-8-5) [\(2017\)](#page-8-5); [Gilmer et al.](#page-7-1) [\(2017\)](#page-7-1); [Hu et al.](#page-7-0) [\(2021\)](#page-7-0). GIN augmented with virtual nodes acts as the basis for the classification network within our following experimentation.

### 3 Methodology

#### 3.1 Baseline

To our knowledge, this is the first application of bilevel optimization to graph neural networks. We have thus decided to compare our model against two prior works:

- GIN+virtual node + FLAG: This model currently has the best performance of all GIN+virtual models on the OGB[\(Hu et al.](#page-7-0) [\(2021\)](#page-7-0)) leaderboard. Given our choice of classifier, and the use of FLAG [Kong et al.](#page-8-1) [\(2020\)](#page-8-1) as a state-of-the-art augmentation framework, this provides both a rigorous and fair state-of-the-art we can strive to beat.
- Random noise augmentation: Our second baseline was inspired by the addition of random noise to inputs in computer vision tasks. Experiments have shown that adding small amounts of input noise (jitter) to training data often aids generalization [\(Reed & Marks](#page-8-6) [\(1998\)](#page-8-6)). This has also been mathematically proven to have similar effects on loss objective optimization to Tikhonov Regularization [\(Bishop](#page-7-2) [\(1995\)](#page-7-2)). We sample our noise from a uniform distribution capped to the range [−1, +1].

#### 3.2 GABO: Graph augmentation with bilevel optimization

Figure [1](#page-1-0) exhibits our Graph Augmentation framework. Given that bilevel optimization has *not* been previously applied in the context of graphs before, our method is relatively immediate from the work of [Mounsaveng et al.](#page-8-0) [\(2020\)](#page-8-0). We extend their bilevel optimization framework to the task of graph augmentation. Importantly, the augmentation network is tasked with applying a *learned* transformation of the underlying graph to the orinal node features. Obtained this transformation entails consideration of two fundamental design choices:

• Inputs to the Augmentation Network: We use the term *augmentation generation type* to refer to our Augmenter inputs. We considered stochastic inputs (random noise), deterministic graph level features, as well as learned features from a *trainable* GIN Layer (which in turn ingested node features).

• Functional form of Augmentation: We refer to this using *node embedding transformation type*. This defines the degree to which the augmentation is allowed to impact the original node features. We used a simple bias addition, an elementwise multiplication transform and a *shifted* elementwise multiplication transform for experiments.

### 3.3 Augmentation generation types

Given that the Augmenter is responsible for sampling augmentation parameters, it remains a question what source of randomness is fed in and what information about the current batch the Augmenter is aware of. In this respect, throughout our experimentation we used three augmentation generation types, including: simple random noise, classic node features, and GIN features. Simple random noise corresponds to feeding the Augmenter purely random inputs, in particular randomly sampled noise from the uniform distribution in the range [−1, 1]. Classic node features corresponds to sampling noise as before but also concatenating deterministic graph level features to the noise vector, including measures of the betweenness centrality, closeness centrality, degree, and pagerank for each node. Finally, we also include a setting in which learned features from a trainable GIN Layer are concatenated to the noise vector and fed into the Augmenter, incorporating node-level feature information.

### 3.4 Node embedding transformation types

The augmentation generator outputs the parameters of a transformation, which is performed on nodes of training graphs. These transformations are written as f(h; φ) in Fig[.1,](#page-1-0) where h ∈ R d is the atom embedding of a node. We implement the following three types of transformations:

- Bias: f(h; φ) = h + φ
- Element-wise multiplication (with bias): f(h; φ) = φ<sup>1</sup>  h + φ<sup>2</sup>
- Shifted element-wise multiplication (with bias): f(h; φ) = (1 + φ1)  h + φ<sup>2</sup>

Note that we choose to perform element-wise multiplication rather than an affine transformation, because the latter would require the augmenter to generate a d × d weight matrix φ1. This would be computationally prohibitive given our resources for this project, and we believe we'd risk overfit on our pseudo-validation set.

# 4 Experimental Setup

# 4.1 Datasets

GABO requires partitioning the original full dataset into four splits (train, pseudo-validation, validation, and test) instead of the traditional three. We compute validation loss on the pseudo-validation set during the outer loop of our bilevel optimization, while our validation set is used to perform early stopping and model selection.

Throughout our experimentation, we try a number of distinct dataset splitting schemes to perform this partition. These are two fold - scaffolding, and randomized. The scaffold splitting attempts to separate molecules which differ structurally into distinct subsets, providing a more realistic estimate of model performance in experimental settings. The randomized method simply shuffles molecules plainly among the train, validation, and pseudo-validation sets, leaving the test set consistent with the OGB leaderboard. Although in theory the scaffolding approach would be expected to yield a more difficult learning setting, in practice we find that distinctions in dataset splitting do not yield significant differences in outcome.

Since GABO requires four data splits instead of three, our validation set is smaller than it would be using a traditional split, inducing more variance in our results. We, therefore, select three models which perform best on our validation set and report test performance for all of them in our performance table.

### 4.2 An important note about optimization

Though the models on the leaderboard which serve as our baselines are trained using the Adam optimizer [\(Kingma & Ba](#page-8-7) [\(2017\)](#page-8-7)), the use of the pytorch meta package restricted us to using a Stochastic Gradient Descent (SGD) with momentum for updates to both Augmenter and Classifier networks. pytorch meta is fairly new and so far does not support functionality for Adam. While SGD can sometimes generalize better than Adam for vision tasks [\(Zhou et al.](#page-8-8) [\(2020\)](#page-8-8)), Adam has been shown to perform consistently better than SGD on loss landscapes with sharp local minima [\(Wilson et al.](#page-8-9) [\(2018\)](#page-8-9)). We therefore have reason to believe we could have gotten higher scores had we had access to an Adam optimizer implementation within pytorch meta, and we leave this open as an avenue for future exploration.

### <span id="page-5-0"></span>4.3 Scheduling and hyperparameters

Each model is trained over a total period of 200 epochs, with a patience hyperparameter of 30 epochs for early stopping. The initial learning rate is set to 0.1, and we use a stepwise learning rate scheduler set to scale the initial rate with a constant factor <sup>1</sup> 5 over the schedule [60, 120, 160]. We also use L2 regularization over *both* augmenter and classifier loss objectives, with regularization constants of 0.01 for the augmenter and 5e-4 for the classifier. Additionally, we use a latent dimension for the random noise input into our augmenter of 10. Our node embeddings have dimension 256.

### 4.4 Computational Power

We ran a total of 53 experiments with the GABO framework, cumulating a total of roughly 165 GPU hours. All experiments were run on Tesla K80 GPUs on Amazon Web Services (AWS) p2.xlarge instances.

# 5 Results and Discussion

Our main result is that the best-performing GABO model, which combines a GIN embedding augmentation generator with a bias embedding transformation, outperforms GIN with virtual node and FLAG augmentation. This places us above all other GIN entries in the leaderboard.

| Method                   | Transform | Optimizer | Validation   | Test         |
|--------------------------|-----------|-----------|--------------|--------------|
| GIN+Virtual              | -         | Adam      | 84.79 ± 0.68 | 77.07±1.5    |
| GIN+Virtual+FLAG         | Bias      | Adam      | 84.38 ± 1.3  | 77.48 ± 0.96 |
| Baseline                 | Bias      | SGD+mom   | 68.70±0.26   | 68.88±0.42   |
| GABO w/ Noise Input      | Bias      | SGD+mom   | 84.83±1.2    | 76.17±2.0    |
| GABO w/ Classic Features | Bias      | SGD+mom   | 82.24±1.2    | 77.43±0.82   |
| GABO w/ GIN Embeddings   | Bias      | SGD+mom   | 81.15±3.3    | 77.77±.40    |

### Table 1: Performance Table

In the following sections, we describe trends observed in the rest of our experiments. Performance for different combinations of our augmentation generation types with our transformation types is summarized in Fig. [2.](#page-6-0)

## 5.1 Baseline performance

Our baseline, augmenting with random noise bias, performed poorly compared to leaderboard entries. It is clear that this model underfit, since performance on the training set similarly lags behind models with an augmentation network. The authors in [Bishop](#page-7-2) [\(1995\)](#page-7-2) showed that the standard deviation of noise added is equivalent to the regularizing factor in Tikhonov regularization; we suspect this

![](_page_6_Figure_0.jpeg)

**Caption:** Figure 2 presents an ablation study assessing the impact of transformation and augmentation generation types on test performance. Results indicate that GIN layer with noise outperforms other methods, emphasizing the importance of augmentation generation type in enhancing model generalization.

<span id="page-6-0"></span>Figure 2: Ablation study on the effects on transformation type and augmentation generation type on test performance. Given our limited computing capabilities, we perform training with five seeds for only experiments in the bottom row of this matrix. For all other cells, mean and standard deviation for ROCAUC are computed over experiments with different patience hyperparameters, which we determined did not make a significant difference on validation performance. We recognize that this is not a traditional method for computing mean and standard deviation of performance metrics, and, given more computing resources, we would have opted to run five seeds on all cells.

underfitting is due to over-regularization at choosing too large a standard deviation for the baseline noise distribution. This would lead to the magnitude of noise added to node embeddings hampering training. This over-regularization in our baseline highlights one of the benefits of our approach, which directly optimizes the bias to perform well on unseen data.

### 5.2 Effect of augmentation generation type

Of the three augmentation generation types we implemented, GIN layer with noise performs best, followed by classic node features with noise, then noise on its own. This makes intuitive sense, since the GIN layer augmenter makes use of node input features while classic node features do not.

One thing to note is that for all experiments where GIN is employed as an augmentation generator, we use a *single* GIN layer. This means transformation parameters are generated only with information from a node's immediate neighbors (1-hop neighborhoods). Future work on this project could include exploring deeper augmentation generators, which would capture information from larger k-hop neighborhoods.

### 5.3 Effect of transformation types

Of the three transformation functional forms, adding the augmentations as a bias term gives us the best performance. In general, performance steadily peaks for the transformation type set to "bias" for all "augmentation generation" types (figure [2,](#page-6-0) reading top to bottom). This makes slightly less intuitive sense than the preceding subsection, as we *expected* the model to learn effective dimension-specific scale factors for the original input node embeddings. In *reality*, our experiments tell us the optimal way to apply augmentations to input node embeddings with GABO is to treat them as a bias term.

![](_page_7_Figure_1.jpeg)

**Caption:** Figure 3 shows the L2-norm of the augmentation vector over training epochs, with red lines marking learning rate adjustments. The increasing magnitude of the bias vectors suggests effective learning, while the observed drops in norm correlate with learning rate changes, indicating dynamic adaptation during training.

<span id="page-7-3"></span>Figure 3: L2-norm of the augmentation vector over time. The red vertical lines denote epochs when the learning rate is dropped.

#### 5.4 Analysis of transformation weights (φ) over time

Unlike augmentations in computer vision, graph augmentations are harder to visualize and provide intuition for at inference time. We can, however, gauge how significant the augmentations are by looking at the L2 norm of the φ vector over time (figure [3\)](#page-7-3). Note that our model learns non-trivial bias vectors, as they increase in magnitude with more training. An interesting trend we noted is the L2 norms seem to drop every time the learning rate scheduler scales the learning rate (by <sup>1</sup> 5 , as described in subsection [4.3\)](#page-5-0).

### 6 Conclusion

In this work we show that bilevel optimization can be used to improve upon graph classification results for a relevant baseline on the ogbg-molhiv dataset. Further, this augmentation procedure can be followed with no a priori domain knowledge about the task at hand. Throughout our investigation we evaluate the efficacy of various design choices in our approach, including choices in augmentation generation types, transformation types, and data splitting schemes. We find that the best performing GABO framework combines a GIN layer augmentation generator with a bias transformation which outperforms GIN with virtual node and FLAG augmentation.

Going forward there exist a number of interesting directions for future work. As referenced previously, the augmentation network is currently optimized using SGD due to the constraints of the metalearning framework (pytorch-meta) we use. Given the strong empirical performance of the Adam optimizer, we believe swapping SGD for Adam could yield a reasonable performance improvement. In addition, in our experiments we only use GIN with a single layer. Although we provide graph-level information in the form of classic node features for certain experiments, increasing the depth of GIN to consider multi-hop information might improve performance. Furthermore, experimenting with other augmentation types beyond linear operations and other classification tasks for our dataset and other datasets would be worthwhile in future investigation.

## References

- <span id="page-7-2"></span>Bishop, C. M. Training with Noise is Equivalent to Tikhonov Regularization. *Neural Computation*, 7 (1):108–116, 01 1995. ISSN 0899-7667. doi: 10.1162/neco.1995.7.1.108. URL [https://doi.](https://doi.org/10.1162/neco.1995.7.1.108) [org/10.1162/neco.1995.7.1.108](https://doi.org/10.1162/neco.1995.7.1.108).
- <span id="page-7-1"></span>Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chemistry. *CoRR*, abs/1704.01212, 2017. URL <http://arxiv.org/abs/1704.01212>.
- <span id="page-7-0"></span>Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs, 2021.

<span id="page-8-7"></span>Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization, 2017.

- <span id="page-8-1"></span>Kong, K., Li, G., Ding, M., Wu, Z., Zhu, C., Ghanem, B., Taylor, G., and Goldstein, T. Flag: Adversarial data augmentation for graph neural networks, 2020.
- <span id="page-8-5"></span>Li, J., Cai, D., and He, X. Learning graph-level representation for drug discovery. *CoRR*, abs/1709.03741, 2017. URL <http://arxiv.org/abs/1709.03741>.
- <span id="page-8-0"></span>Mounsaveng, S., Laradji, I., Ayed, I. B., Vazquez, D., and Pedersoli, M. Learning data augmentation with online bilevel optimization for image classification, 2020.
- <span id="page-8-6"></span>Reed, R. D. and Marks, R. J. *Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks*. MIT Press, Cambridge, MA, USA, 1998. ISBN 0262181908.
- <span id="page-8-3"></span>Shafahi, A., Najibi, M., Ghiasi, A., Xu, Z., Dickerson, J., Studer, C., Davis, L. S., Taylor, G., and Goldstein, T. Adversarial training for free!, 2019.
- <span id="page-8-9"></span>Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. The marginal value of adaptive gradient methods in machine learning, 2018.
- <span id="page-8-2"></span>Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., Leswing, K., and Pande, V. S. Moleculenet: A benchmark for molecular machine learning. *CoRR*, abs/1703.00564, 2017. URL <http://arxiv.org/abs/1703.00564>.
- <span id="page-8-4"></span>Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? *CoRR*, abs/1810.00826, 2018. URL <http://arxiv.org/abs/1810.00826>.
- <span id="page-8-8"></span>Zhou, P., Feng, J., Ma, C., Xiong, C., HOI, S., and E, W. Towards theoretically understanding why sgd generalizes better than adam in deep learning, 2020.