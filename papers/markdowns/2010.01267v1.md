# WeMix: How to Better Utilize Data Augmentation

Yi Xu, Asaf Noy, Ming Lin, Qi Qian, Hao Li, Rong Jin Machine Intelligence Technology, Alibaba Group {yixu, asaf.noy, ming.l, qi.qian, lihao.lh, jinrong.jr}@alibaba-inc.com

First Version: October 2, 2020

#### Abstract

Data augmentation is a widely used training trick in deep learning to improve the network generalization ability. Despite many encouraging results, several recent studies did point out limitations of the conventional data augmentation scheme in certain scenarios, calling for a better theoretical understanding of data augmentation. In this work, we develop a comprehensive analysis that reveals pros and cons of data augmentation. The main limitation of data augmentation arises from the data bias, i.e. the augmented data distribution can be quite different from the original one. This data bias leads to a suboptimal performance of existing data augmentation methods. To this end, we develop two novel algorithms, termed "AugDrop" and "MixLoss", to correct the data bias in the data augmentation. Our theoretical analysis shows that both algorithms are guaranteed to improve the effect of data augmentation through the bias correction, which is further validated by our empirical studies. Finally, we propose a generic algorithm "WeMix" by combining AugDrop and MixLoss, whose effectiveness is observed from extensive empirical evaluations.

## 1 Introduction

Data augmentation [\[Baird,](#page-9-0) [1992](#page-9-0), [Schmidhuber](#page-11-0), [2015](#page-11-0)] has been a key to the success of deep learning in image classification [\[He et al.](#page-10-0), [2019\]](#page-10-0), and is becoming increasingly common in other tasks such as natural language processing [\[Zhang et al.](#page-12-0), [2015\]](#page-12-0) and object detection [\[Zoph et al.](#page-12-1), [2019](#page-12-1)]. The data augmentation expands training set by generating virtual instances through random augmentation to the original ones. This alleviates the overfitting [\[Shorten and Khoshgoftaar,](#page-11-1) [2019](#page-11-1)] problem when training large deep neural networks. Despite many encouraging results, it is not the case that data augmentation will always improve generalization errors [\[Min et al.](#page-11-2), [2020](#page-11-2), [Raghunathan et al.](#page-11-3), [2020](#page-11-3)]. In particular, [Raghunathan et al.](#page-11-3) [\[2020\]](#page-11-3) showed that training by augmented data will lead to a smaller robust error but potentially a larger standard error. Therefore, it is critical to answer the following two questions before applying data augmentation in deep learning:

- When will the deep models benefit from data augmentation?
- How to better leverage augmented data during training?

Several previous works [\[Raghunathan et al.,](#page-11-3) [2020,](#page-11-3) [Wu et al.](#page-12-2), [2020,](#page-12-2) [Min et al.,](#page-11-2) [2020\]](#page-11-2) tried to address the questions. Their analysis is limited to specific problems such as linear ridge regression therefore may not be applicable to deep learning. In this work, we aim to answer the two questions from a theoretical perspective under a more general non-convex setting. We address the first question in a more general form covering applications in deep learning. For the second question, we develop new approaches that are provably more effective than the conventional data augmentation approaches.

Most data augmentation operations alter the data distribution during the training progress. This imposes a data distribution bias (we simply use "data bias" in the rest of this paper) between the augmented data and the original data, which may make it difficult to fully leverage the augmented data. To be more concrete, let us consider label-mixing augmentation (e.g., mixup [\[Zhang et al.](#page-12-3), [2018,](#page-12-3) [Tokozume et al.](#page-11-4), [2018\]](#page-11-4)). Suppose we have n original data D = {(x<sup>i</sup> , yi), i = 1, . . . , n}, where the input-label pair (x<sup>i</sup> , yi) follows a distribution Pxy = (Px, Py(·|x)), P<sup>x</sup> is the marginal distribution of the inputs and Py(·|x) is the conditional distribution of the labels given inputs; we generate <sup>m</sup> augmented data <sup>D</sup><sup>e</sup> <sup>=</sup> {(xe<sup>i</sup> , <sup>y</sup>ei), i = 1, . . . , m}, where (xei , <sup>y</sup>ei) <sup>∼</sup> <sup>P</sup>xey<sup>e</sup> = (Pxe, <sup>P</sup>ye(·|xe)), and <sup>P</sup><sup>x</sup> <sup>=</sup> <sup>P</sup>x<sup>e</sup> but <sup>P</sup>y(·|x) <sup>6</sup><sup>=</sup> <sup>P</sup>ye(·|xe). Given <sup>x</sup> <sup>∼</sup> <sup>P</sup>x, the data bias is defined as <sup>δ</sup><sup>y</sup> = maxy,y<sup>e</sup> <sup>k</sup><sup>y</sup> <sup>−</sup> <sup>y</sup>ek. We will show that when the bias between <sup>D</sup> and <sup>D</sup><sup>e</sup> is large, directly training on the augmented data will not be as effective as training on the original data.

Given the fact that augmented data may hurt the performance, the next question is how to design better learning algorithms to leash out the power of augmented data. To this end, we develop two novel algorithms to alleviate the data bias. The first algorithm, termed AugDrop, corrects the data bias by introducing a constrained optimization problem. The second algorithm, termed MixLoss, corrects the data bias by introducing a modified loss function. We show that, both theoretically and empirically, even with a large data bias, the proposed algorithms can still improve the generalization performance by effectively leveraging the combination of augmented data and original data. We summarize the main contributions of this work as follows:

- We prove that in a conventional training scheme, a deep model can benefit from augmented data when the data bias is small.
- We design two algorithms termed AugDrop and MixLoss that can better leverage augmented data even when the data bias is large with theoretical guarantees.
- Based on our theoretical findings, we empirically propose a new efficient algorithm WeMix by combining AugDrop and MixLoss , which has better performances without extra training cost.

## 2 Related Work

A series of empirical works [\[Cubuk et al.](#page-9-1), [2019](#page-9-1), [Ho et al.,](#page-10-1) [2019,](#page-10-1) [Lim et al.,](#page-11-5) [2019,](#page-11-5) [Lin et al.](#page-11-6), [2019a,](#page-11-6) [Cubuk et al.,](#page-10-2) [2020,](#page-10-2) [Hataya et al.](#page-10-3), [2019\]](#page-10-3) on how to learn a good policy of using different data augmentations have been proposed without theoretical guarantees. In this section, we mainly focus on reviewing theoretical studies on data augmentation. For a survey of data augmentation, we refer readers to [\[Shorten and Khoshgoftaar,](#page-11-1) [2019\]](#page-11-1) and references therein for a comprehensive overview.

Several works have attempted to establish theoretical understandings of data augmentation from different perspectives [\[Dao et al.,](#page-10-4) [2019,](#page-10-4) [Chen et al.,](#page-9-2) [2019,](#page-9-2) [Rajput et al.](#page-11-7), [2019](#page-11-7)]. [Min et al.](#page-11-2) [\[2020\]](#page-11-2) shown that, with more training data, weak augmentation can improve performance while strong augmentation always hurts the performance. Later on, [Chen et al.](#page-9-3) [\[2020\]](#page-9-3) study the gap between the generalization error (please see the formal definition in [\[Chen et al.](#page-9-3), [2020\]](#page-9-3)) of adversarially-trained models and standard models. Both of their theoretical analyses were built on special linear binary classification model or linear regression model for label-preserving augmentation.

Recently, [Raghunathan et al.](#page-11-3) [\[2020\]](#page-11-3) studied label-preserving transformation in data augmentation, which is identical to the first case in this paper. Their analysis is restricted to linear least square regression under noiseless setting, which is not applicable to training deep neural networks. Besides, their analysis requires infinite unlabeled data. By contrast, we do not need original data is unlimited. [Wu et al.](#page-12-2) [\[2020\]](#page-12-2) considered linear data augmentations. There are several major differences between their work and ours. First, they focus on the ridge linear regression problem which is strongly convex, while we consider non-convex optimization problems, which is more applicable in deep learning. Second, we study more general data augmentations beyond linear transformation.

## 3 Preliminaries and Notations

We study a learning problem for finding a classifier to map an input x ∈ X onto a label y ∈ Y ⊂ R K, where K is the number of classes. We assume the input-label pair (x, y) is drawn from a distribution <sup>P</sup>xy = (Px, <sup>P</sup>y(·|x)). Since every augmented example (xe, <sup>y</sup>e) is generated by applying a certain transformation to either one or multiple examples, we will assume that (xe, <sup>y</sup>e) is drawn from a slightly different distribution <sup>P</sup>xey<sup>e</sup> = (Pxe, <sup>P</sup>ye(·|xe)), where <sup>P</sup>x<sup>e</sup> is the marginal distribution on the inputs <sup>x</sup><sup>e</sup> and <sup>P</sup>ye(·|xe)) (we can write it as <sup>P</sup>y<sup>e</sup> for simplicity) is the conditional distribution of the labels <sup>y</sup><sup>e</sup> given inputs <sup>x</sup>e. We sample <sup>n</sup> training examples (x<sup>i</sup> , <sup>y</sup>i), i = 1, . . . , n from distribution <sup>P</sup>xy and <sup>m</sup> training examples (xe<sup>i</sup> , <sup>y</sup>ei), i = 1, . . . , m from Pxeye. We assume that m ≫ n due to the data augmentation. We denote by D = {(x<sup>i</sup> , yi), i = 1, . . . , n} and <sup>D</sup><sup>e</sup> = (xe<sup>i</sup> , <sup>y</sup>ei), i = 1, . . . , m} the dataset sampled from <sup>P</sup>xy and <sup>P</sup>xeye, respectively. We denote by <sup>T</sup> (x) the set of augmented data transformed from x. We use the notation E(x,y)∼Pxy [·] to stand for the expectation that takes over a random variable (x, y) following a distribution Pxy. We denote by ∇wh(w) the gradient of a function h(w) in terms of variable w. When the variable to be taken a gradient is obvious, we use the notation ∇h(w) for simplicity. Let use k · k as the Euclidean norm for a vector or the Spectral norm for a matrix.

The augmented data De can be different from the original data D in two cases, according to [\[Raghunathan et al.](#page-11-3), [2020\]](#page-11-3). In the first case, often referred to as label-preserving, we consider

$$
\mathbb{P}_{\mathbf{y}}(\cdot|\mathbf{x}) = \mathbb{P}_{\widetilde{\mathbf{y}}}(\cdot|\widetilde{\mathbf{x}}), \ \forall \widetilde{\mathbf{x}} \in T(\mathbf{x}) \text{ but } \mathbb{P}_{\mathbf{x}} \neq \mathbb{P}_{\widetilde{\mathbf{x}}}.\tag{1}
$$

In the second case, often referred to as label-mixing, we consider

$$
\mathbb{P}_{\mathbf{x}} = \mathbb{P}_{\widetilde{\mathbf{x}}} \text{ but } \mathbb{P}_{\mathbf{y}}(\cdot|\mathbf{x}) \neq \mathbb{P}_{\widetilde{\mathbf{y}}}(\cdot|\widetilde{\mathbf{x}}), \exists \widetilde{\mathbf{x}} \in T(\mathbf{x}). \tag{2}
$$

Examples of label-preserving augmentation include translation, adding noises, small rotation, and brightness or contrast changes [\[Krizhevsky et al.](#page-11-8), [2012](#page-11-8), [Raghunathan et al.,](#page-11-3) [2020\]](#page-11-3). One important example of labelmixing augmentation is mixup [\[Zhang et al.](#page-12-3), [2018,](#page-12-3) [Tokozume et al.](#page-11-4), [2018](#page-11-4)]. Due to the space limitation, we will focus on the label-mixing case, and the related studies and analysis for the label-preserving case can be found in Appendix [A.](#page-13-0) To further quantify the difference between original data and augmented data when P<sup>x</sup> = Px<sup>e</sup> and P<sup>y</sup> 6= Py<sup>e</sup>, we introduce the data bias δ<sup>y</sup> given x ∼ P<sup>x</sup> as following:

<span id="page-2-3"></span><span id="page-2-2"></span><span id="page-2-0"></span>
$$
\delta_y := \max_{\mathbf{y}, \widetilde{\mathbf{y}}} \|\mathbf{y} - \widetilde{\mathbf{y}}\|.\tag{3}
$$

The equation in [\(3\)](#page-2-0) measures the difference between the label from original data and the label from augmented data given input x. We aim to learn a prediction function f(x; w) : R <sup>D</sup> ×X → R <sup>K</sup> that is as close as possible to y, where w ∈ R <sup>D</sup> is the parameter and R <sup>D</sup> is a closed convex set. We respectively define two objective functions for optimization problems over the original data and the augmented data as

$$
\mathcal{L}(\mathbf{w}) = \mathbf{E}_{(\mathbf{x}, \mathbf{y})} [\ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w}))], \quad \mathcal{L}(\mathbf{w}) = \mathbf{E}_{(\widetilde{\mathbf{x}}, \widetilde{\mathbf{y}})} [\ell(\widetilde{\mathbf{y}}, f(\widetilde{\mathbf{x}}; \mathbf{w}))], \tag{4}
$$

where ℓ is a cross-entropy loss function which is given by

$$
\ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w})) = \sum_{i=1}^{K} y_i p_i(\mathbf{x}; \mathbf{w}), \text{ where } p_i(\mathbf{x}; \mathbf{w}) = -\log \left( \frac{\exp(f_i(\mathbf{x}; \mathbf{w}))}{\sum_{j=1}^{K} \exp(f_j(\mathbf{x}; \mathbf{w}))} \right). \tag{5}
$$

We denote by <sup>w</sup><sup>∗</sup> and <sup>w</sup><sup>e</sup> <sup>∗</sup> the optimal solutions to min<sup>w</sup> <sup>L</sup>(w) and min<sup>w</sup> <sup>L</sup>e(w) respectively,

<span id="page-2-1"></span>
$$
\mathbf{w}_{*} \in \underset{\mathbf{w} \in \mathbb{R}^{D}}{\arg \min} \mathcal{L}(\mathbf{w}), \quad \widetilde{\mathbf{w}}_{*} \in \underset{\mathbf{w} \in \mathbb{R}^{D}}{\arg \min} \widetilde{\mathcal{L}}(\mathbf{w}). \tag{6}
$$

Taking L(w) as an example, we introduce some function properties used in our analysis.

<span id="page-2-4"></span>Definition 1. *The stochastic gradients of the objective functions* L(w) *is unbiased and bounded, if we have* E(x,y) [∇wℓ (y, f(x; w))] = ∇L(w)*, and there exists a constant* G > 0*, such that* k∇wp(x; w)k ≤ G, ∀x ∈ X, ∀w ∈ R <sup>D</sup>*, where* p(x; w) = (p1(x; w), . . . , pK(x; w)) *is a vector.*

<span id="page-2-5"></span>Definition 2. L(w) *is smooth with an* L*-Lipchitz continuous gradient, if there exists a constant* L > 0 *such that* k∇L(w) − ∇L(u)k ≤ Lkw − uk, ∀w, u ∈ R <sup>D</sup>*, or equivalently,* L(w) − L(u) ≤ h∇L(u), w − ui + <sup>L</sup> 2 kw − uk 2 , ∀w, u ∈ R D*.*

The above properties are standard and widely used in the literature of non-convex optimization [\[Ghadimi and Lan,](#page-10-5) [2013,](#page-10-5) [Yan et al.,](#page-12-4) [2018,](#page-12-4) [Yuan et al.](#page-12-5), [2019](#page-12-5), [Wang et al.,](#page-12-6) [2019,](#page-12-6) [Li et al.,](#page-11-9) [2020\]](#page-11-9). We introduce an important property termed Polyak- Lojasiewicz (PL) condition [\[Polyak,](#page-11-10) [1963\]](#page-11-10) on the objective function L(w).

<span id="page-3-1"></span>Definition 3. *(PL condition)* L(w) *satisfies the PL condition, if there exists a constant* µ > 0 *such that* 2µ(L(w) − L(w∗)) ≤ k∇L(w)k 2 , ∀w ∈ R <sup>D</sup>*, where* w<sup>∗</sup> *is defined in [\(6\)](#page-2-1).*

The PL condition has been observed in training deep and shallow neural networks [\[Allen-Zhu et al.,](#page-9-4) [2019](#page-9-4), [Xie et al.](#page-12-7), [2017\]](#page-12-7), and is widely used in many non-convex optimization studies [\[Karimi et al.,](#page-10-6) [2016](#page-10-6), [Li and Li](#page-11-11), [2018,](#page-11-11) [Charles and Papailiopoulos,](#page-9-5) [2018,](#page-9-5) [Yuan et al.,](#page-12-5) [2019,](#page-12-5) [Li et al.,](#page-11-9) [2020\]](#page-11-9). It is also theoretically verified in [\[Allen-Zhu et al.,](#page-9-4) [2019\]](#page-9-4) and empirically estimated in [\[Yuan et al.](#page-12-5), [2019\]](#page-12-5) for deep neural networks. It is worth noting that PL condition is weaker than many conditions such as strong convexity, restricted strong convexity and weak strong convexity [\[Karimi et al.](#page-10-6), [2016](#page-10-6)].

Finally, we will refer to κ = L µ as condition number throughout this study.

## <span id="page-3-4"></span>4 Main Results

In this section, we present the main results for label-mixing augmentation satisfying [\(2\)](#page-2-2). Due to the space limitation, we present the results of label-preserving augmentation satisfying [\(1\)](#page-2-3) in Appendix [A.](#page-13-0) Since we have access to m ≫ n augmented data, it is natural to fully leverage the augmented data De during training. But on the other hand, due to the data bias δy, the prediction model learned from augmented data De could be even worse than training the prediction model directly from the original data D, as revealed by Lemma [1](#page-3-0) (its proof can be found in Appendix [C\)](#page-15-0) and its remark. Throughout this section, suppose that a mini-batch SGD is used for optimization, i.e. to optimize L(w), we have

$$
\mathbf{w}_{t+1} = \mathbf{w}_t - \frac{\eta}{m_0} \sum_{k=1}^{m_0} \nabla_{\mathbf{w}} \ell \left( \mathbf{y}_{k,t}, f(\mathbf{x}_{k,t}; \mathbf{w}_t) \right), \tag{7}
$$

where η is the step size, m<sup>0</sup> is the batch size, and (xk,t, yk,t), k = 1, . . . , m<sup>0</sup> are sampled from D. A similar mini-batch SGD algorithm can be developed for the augmented data.

<span id="page-3-0"></span>Lemma 1. *Assume that* L *and* Le *satisfy properties in Definition [1,](#page-2-4) [2](#page-2-5) and [3,](#page-3-1) by setting* η = 1/L *and* m<sup>0</sup> ≥ 8 δ 2 y *, when* t ≥ <sup>L</sup> µ log 4(L(w1)−L(w∗))<sup>µ</sup> δ 2 yG<sup>2</sup> *, we have*

<span id="page-3-3"></span>
$$
\mathbf{E}[\mathcal{L}(\mathbf{w}_{t+1}) - \mathcal{L}(\mathbf{w}_*)] \le \delta_y^2 G^2 / \mu \le O(\delta_y^2 / \mu),\tag{8}
$$

*where* wt+1 *is output of mini-batch SGD trained on* De*,* δ<sup>y</sup> *is defined in [\(3\)](#page-2-0).*

Remark: It is easy to verify (see the details of proof in Appendix [D\)](#page-17-0) that if we simply train the learning model by the original data D, we have

<span id="page-3-2"></span>
$$
\mathbf{E}\left[\mathcal{L}(\mathbf{w}_{n+1}) - \mathcal{L}(\mathbf{w}_{*})\right] \le O\left(L\log(n)/(n\mu^{2})\right). \tag{9}
$$

Comparing the result in [\(9\)](#page-3-2) with the result of [\(8\)](#page-3-3) in Lemma [1,](#page-3-0) it is easy to show that, when the data bias is too large, i.e., δ 2 <sup>y</sup> ≥ Ω(L log(n)/(nµ)), we have O L log(n)/(nµ<sup>2</sup> ) ≤ O(δ 2 y /µ). This implies that training the deep model directly on the original data D is more effective than on the augmented data De. Hence, in order to better leverage the augmented data in the presence of large data bias (δ 2 <sup>y</sup> ≥ Ω(κ log(n)/n), where κ = L/µ), we need to come up with approaches that automatically correct the data bias. Below, we develop two approaches to correct the data bias. The first approach, termed "AugDrop", corrects the data bias by introducing a constrained optimization approach, and the second approach, termed "MixLoss", addresses the problem by introducing a modified loss function.

#### 4.1 AugDrop: Correcting Data Bias by Constrained Optimization

To address this challenge, we propose a constrained optimization problem, i.e.

<span id="page-4-4"></span><span id="page-4-3"></span><span id="page-4-1"></span>
$$
\min_{w \in \mathbb{R}^D} \mathcal{L}(\mathbf{w}) \quad \text{s.t.} \quad \widetilde{\mathcal{L}}(\mathbf{w}) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_*) \le \gamma,\tag{10}
$$

where γ > 0 is a positive constant, <sup>w</sup><sup>e</sup> <sup>∗</sup> is defined in [\(6\)](#page-2-1). The key idea is that by utilizing the augmented data to constrain the solution in a small region, we will be able to enjoy a smaller condition number, leading to a better performance in optimizing L(w). To make it concrete, we first define three important terms:

$$
\gamma_0 := \delta_y^2 G^2 / (2\mu), \quad \mathcal{A}(\gamma) = \left\{ \mathbf{w} : \widetilde{\mathcal{L}}(\mathbf{w}) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_*) \le \gamma \right\},\tag{11}
$$

$$
\mu(\gamma) = \max_{\mu'} \left\{ \mathcal{L}(\mathbf{w}) - \mathcal{L}(\mathbf{w}_*) \le ||\nabla \mathcal{L}(\mathbf{w})||^2 / (2\mu'), \mathbf{w} \in \mathcal{A}(\gamma) \right\}.
$$
 (12)

We then present a proposition about A(γ) and µ(γ), whose proof is included in Appendix [E.](#page-18-0)

<span id="page-4-0"></span>Proposition 1. *If* γ ∈ [γ0, 8γ0]*, we have* w<sup>∗</sup> ∈ A(γ) *and* µ(γ) ≥ µ*.*

According to Proposition [1,](#page-4-0) by restricting our solutions in A(γ), we have a smaller condition number (since µ(γ) ≥ µ) and consequentially a smaller optimization error. It is worth mentioning that the restriction of solutions in A(γ) is reasonable due to the optimal solution w<sup>∗</sup> ∈ A(γ). The idea of using augmentation transformation to restrict the candidate solution was recognized by several earlier studies, e.g. [\[Raghunathan et al.,](#page-11-3) [2020\]](#page-11-3). But none of these studies cast it into a constrained optimization problem, a key contribution of our work.

The next question is how to solve the constrained optimization problem in [\(10\)](#page-4-1). It is worth noting that neither L(w) nor Le(w) is convex. Although multiple approaches can be used to solve non-convex constrained optimization problems [\[Cartis et al.,](#page-9-6) [2011](#page-9-6), [Lin et al.](#page-11-12), [2019b](#page-11-12), [Birgin and Mart´ınez,](#page-9-7) [2020](#page-9-7), [Grapiglia and Yuan](#page-10-7), [2019,](#page-10-7) [Wright](#page-12-8), [2001,](#page-12-8) [O'Neill and Wright](#page-11-13), [2020](#page-11-13), [Boob et al.,](#page-9-8) [2019,](#page-9-8) [Ma et al.](#page-11-14), [2019\]](#page-11-14), they are too complicated to be implemented in deep learning. Instead, we present a simple approach that divides the optimization into two stages, which is referred to as AugDrop (Please see the details of update steps from Algorithm [2](#page-18-1) in Appendix [F\)](#page-18-2).

- Stage I. We minimize <sup>L</sup>e(w) over the augmented data <sup>D</sup>e. It runs a mini-batch SGD against <sup>D</sup><sup>e</sup> at least T<sup>1</sup> iterations with the size of mini-batch being m1. We denote by w<sup>T</sup>1+1 the final output solution of this stage.
- Stage II. We minimize L(w) using the original data D. It initializes the solution w<sup>T</sup>1+1 and runs a mini-batch SGD against D in n/m<sup>2</sup> iterations with mini-batch size being m2.

We notice that AugDrop is closely related to TSLA by [\[Xu et al.](#page-12-9), [2020\]](#page-12-9) where the first stage trains the data with label smoothing and the second stage trains the data without label smoothing. However, they study the problem how to reduce the variance of stochastic gradient in using label smoothing, while we study how to correct bias in data augmentation by solving a constrained optimization problem. The following theorem states that if we run this two stage optimization algorithm, we could achieve a better performance since µ(8γ0) is larger than µ. We include its proof in Appendix [F.](#page-18-2)

<span id="page-4-2"></span>Theorem 1. *Define* µ<sup>c</sup> = µ(8γ0)*. Assume that* L *and* Le *satisfy properties in Definition [1,](#page-2-4) [2](#page-2-5) and [3,](#page-3-1) set learing rate* η<sup>1</sup> = 1/L *in Stage I and learning rate* η<sup>2</sup> = 1 2nµ<sup>c</sup> log 8nµ 2 c (L(w1)−L(w∗)) G2L *in Stage II for AugDrop. Let* w<sup>1</sup> *be the initial solution in Stage I of AugDrop and* wT1+2, . . . , wT1+n/m2+1 *be the intermediate solutions obtained by the mini-batch SGD in Stage II of AugDrop. Choose* T<sup>1</sup> = 1 η1µ log 2(Le(w1)−Le(w<sup>e</sup> <sup>∗</sup>))<sup>µ</sup> δ 2 yG<sup>2</sup> *,* m<sup>1</sup> =

$$
\left(1+\sqrt{3\log\frac{2T_1}{\delta}}\right)^2\frac{8}{\delta_y^2} \text{ and } m_2 = \left(1+\sqrt{3\log\frac{2n}{\delta}}\right)^2\frac{4}{\delta_y^2}, \text{ with a probability } 1-\delta, \text{ we have } \mathbf{w}_t \in \mathcal{A}(8\gamma_0), \forall t \in \mathcal{A}(8\gamma_0).
$$

{T<sup>1</sup> + 2, . . . , T<sup>1</sup> + n/m<sup>2</sup> + 1} *and*

$$
\mathcal{E}\left[\mathcal{L}(\widehat{\mathbf{w}}) - \mathcal{L}(\mathbf{w}_{*})\right] \le \frac{G^2 L}{4n\mu_c^2} \left(1 + \log\left(\frac{4n\mu_c^2(\mathcal{L}(\mathbf{w}_1) - \mathcal{L}(\mathbf{w}_{*}))}{G^2 L}\right)\right) \le O\left(\frac{L\log(n)}{n\mu_c^2}\right),\tag{13}
$$

*where* <sup>w</sup><sup>b</sup> <sup>=</sup> <sup>w</sup>T1+n/m2+1 *and* <sup>δ</sup><sup>y</sup> *is defined in in [\(3\)](#page-2-0).*

Remark. Theorem [1](#page-4-2) shows that all intermediate solutions w<sup>t</sup> obtained in Stage II of AugDrop satisfy the constraint <sup>L</sup>e(wt) <sup>−</sup> <sup>L</sup>e(w<sup>e</sup> <sup>∗</sup>) <sup>≤</sup> <sup>8</sup>γ0, that is to say, <sup>w</sup><sup>t</sup> ∈ A(8γ0). Based on Proposition [1,](#page-4-0) we will enjoy a larger µ<sup>c</sup> than µ. Comparing the result of [\(13\)](#page-5-0) in Theorem [1](#page-4-2) with [\(9\)](#page-3-2), training by using AugDrop will result in a better performance than directly training on D due to µ<sup>c</sup> ≥ µ. Besides, when the data bias is large, i.e., δ 2 <sup>y</sup> ≥ Ω(L log(n)/(nµ)), we know O(L log(n)/(nµ<sup>2</sup> c )) ≤ O(µδ<sup>2</sup> y/µ<sup>2</sup> c ) ≤ O(δ 2 <sup>y</sup>/µ), where the last inequality holds due to µ<sup>c</sup> ≥ µ. By comparing [\(13\)](#page-5-0) with the result of [\(8\)](#page-3-3) in Lemma [1,](#page-3-0) we know that training by using AugDrop has a better performance than directly training on De when the data bias is large. By solving a constrained problem, the AugDrop algorithm can correct the data bias and thus can enjoy an better performance.

#### 4.2 MixLoss: Correcting Data Bias by Modified Loss Function

Without loss of generality, we set L(w∗) = 0, a common property observed in training deep neural networks [\[Zhang et al.](#page-12-10), [2016,](#page-12-10) [Allen-Zhu et al.,](#page-9-4) [2019,](#page-9-4) [Du et al.,](#page-10-8) [2018](#page-10-8), [2019,](#page-10-9) [Arora et al.,](#page-9-9) [2019,](#page-9-9) [Chizat et al.](#page-9-10), [2019,](#page-9-10) [Hastie et al.](#page-10-10), [2019](#page-10-10), [Yun et al.,](#page-12-11) [2019,](#page-12-11) [Zou et al.,](#page-12-12) [2020](#page-12-12)]. Since <sup>k</sup><sup>y</sup> <sup>−</sup> <sup>y</sup>ek ≤ <sup>δ</sup><sup>y</sup> for any <sup>y</sup> and <sup>y</sup><sup>e</sup> and given <sup>x</sup>, we define a new loss function <sup>ℓ</sup>a(ye, f(xe; <sup>w</sup>)) as

<span id="page-5-3"></span><span id="page-5-1"></span><span id="page-5-0"></span>
$$
\ell_a(\widetilde{\mathbf{y}}, f(\widetilde{\mathbf{x}}; \mathbf{w})) = \min_{\|\mathbf{z} - \widetilde{\mathbf{y}}\| \le \delta_y} \ell(\mathbf{z}, f(\widetilde{\mathbf{x}}; \mathbf{w})).
$$
\n(14)

It has been shown that since the cross-entropy loss ℓ(z, ·) is convex in terms of z ∈ Y, then the minimization problem [\(14\)](#page-5-1) is a convex optimization problem and has a closed form solution [\[Boyd and Vandenberghe](#page-9-11), [2004\]](#page-9-11). Using this new loss, we define a new objective function La(w)

$$
\mathcal{L}_a(\mathbf{w}) = \mathrm{E}_{(\widetilde{\mathbf{x}}, \widetilde{\mathbf{y}})} [\ell_a(\widetilde{\mathbf{y}}, f(\widetilde{\mathbf{x}}; \mathbf{w}))] = \mathrm{E}_{(\widetilde{\mathbf{x}}, \widetilde{\mathbf{y}})} \left[ \min_{\|\mathbf{z} - \widetilde{\mathbf{y}}\| \le \delta_y} \ell(\mathbf{z}, f(\widetilde{\mathbf{x}}; \mathbf{w})) \right].
$$
\n(15)

It is easy to verify that La(w∗) = 0 and therefore w<sup>∗</sup> also minimizes La(w) (see Appendix [G\)](#page-21-0). In contrast, <sup>w</sup><sup>e</sup> <sup>∗</sup>, the minimizer of <sup>L</sup>e(w), can be very different from <sup>w</sup>∗. Hence, we can correct the data bias arising from the augmented data by replacing Le(w) with La(w), leading to the following optimization problem:

<span id="page-5-2"></span>
$$
\min_{\mathbf{w}\in\mathbb{R}^D} \mathcal{L}_c(\mathbf{w}) = \lambda \mathcal{L}(\mathbf{w}) + (1 - \lambda) \mathcal{L}_a(\mathbf{w}),\tag{16}
$$

where λ ∈ (0, 1). Since Lc(w) shares the same minimizer with L(w) (see Appendix [G\)](#page-21-0), it is sufficiently to optimize Lc(w), instead of optimizing L(w). The main advantage of minimizing Lc(w) over L(w) is that by introducing a small λ, we will be able to reduce the variance in computing the gradient of Lc(w), and therefore improve the overall convergence. More specifically, our SGD method is given as follows: at each iteration t, we compute the approximate gradient as

$$
\widehat{\mathbf{g}}_t = \lambda \nabla \ell(\mathbf{y}_t, f(\mathbf{x}_t; \mathbf{w}_t)) + (1 - \lambda) \frac{1}{m_0} \sum_{i=1}^{m_0} \nabla \ell_a(\widetilde{\mathbf{y}}_{t,i}, f(\widetilde{\mathbf{x}}_{t,i}; \mathbf{w}_t)),
$$
\n(17)

where (xt, yt) is an example sampled from D at iteration t. We refer to this approach as MixLoss (Please see the details of update steps from Algorithm [3](#page-21-1) in Appendix [H\)](#page-21-2). We then give the convergence result in the following theorem, whose proof is included in Appendix [H.](#page-21-2)

#### <span id="page-6-1"></span>Algorithm 1 WeMix

1: Input: T1, T2, stochastic algorithms A1, A<sup>2</sup> (e.g., momentum SGD, SGD) 2: Initialize: w<sup>1</sup> ∈ R <sup>D</sup>, λ ∈ (0, 1), η1, η<sup>2</sup> > 0 // First stage: Weighted Mixed Losses 3: for t = 1, 2, . . . , T<sup>1</sup> do 4: draw examples (xi<sup>t</sup> , yi<sup>t</sup> ) at random from training data ⋄ construct stochastic gradient of L 5: generate augmented examples (xej<sup>t</sup> , <sup>y</sup>ej<sup>t</sup> ) ⋄ construct stochastic gradient of L<sup>a</sup> 6: compute stochastic gradient <sup>g</sup>b<sup>t</sup> <sup>=</sup> <sup>λ</sup>∇ℓ(yi<sup>t</sup> , f(xi<sup>t</sup> ; <sup>w</sup>t)) + (1 <sup>−</sup> <sup>λ</sup>)∇ℓa(yei<sup>t</sup> , f(xei<sup>t</sup> ; wt)) 7: <sup>w</sup>t+1 <sup>=</sup> <sup>A</sup>1(wt; <sup>g</sup>bt, η1) <sup>⋄</sup> update one step of <sup>A</sup><sup>1</sup> 8: end for // Second stage: Augmentation Dropping 9: for t = T<sup>1</sup> + 1, T<sup>1</sup> + 2, . . . , T<sup>1</sup> + T<sup>2</sup> do 10: draw examples (xi<sup>t</sup> , yi<sup>t</sup> ) at random from training data ⋄ construct stochastic gradient of L 11: compute stochastic gradient <sup>g</sup>b<sup>t</sup> <sup>=</sup> <sup>∇</sup>ℓ(yi<sup>t</sup> , f(xi<sup>t</sup> ; wt)) 12: <sup>w</sup>t+1 <sup>=</sup> <sup>A</sup>2(wt; <sup>g</sup>bt, η2) <sup>⋄</sup> update one step of <sup>A</sup><sup>2</sup> 13: end for 14: Output: wT1+T2+1.

<span id="page-6-2"></span>Theorem 2. *Assume that* L*,* Le *and* L<sup>a</sup> *satisfy properties in Definition [1,](#page-2-4) [2](#page-2-5) and [3,](#page-3-1) by setting* m<sup>0</sup> ≥ 72(1−λ) 2 λ2 *and* η = 1 µn log nµ2L(w1) <sup>λ</sup>2LG<sup>2</sup> ≤ 1 2L *in MixLoss, we have*

$$
\mathbf{E}\left[\mathcal{L}(\mathbf{w}_{n+1}) - \mathcal{L}(\mathbf{w}_{*})\right] \le \frac{\lambda LG^2}{n\mu^2} \left(1 + 5\log\frac{n\mu^2 \mathcal{L}(\mathbf{w}_1)}{\lambda^2 LG^2}\right) \le O\left(\frac{\lambda L\log(n/\lambda^2)}{n\mu^2}\right). \tag{18}
$$

<span id="page-6-0"></span>

Remark. According to the results in [\(18\)](#page-6-0) and [\(9\)](#page-3-2), we know that O λL log(n/λ<sup>2</sup> )/(nµ<sup>2</sup> ) ≤ O L log(n)/(nµ<sup>2</sup> ) when an appropriate λ ∈ (0, 1) is selected, leading to a better performance by using MixLoss compared with the performance trained on the original data D. For example, one can simply use λ = O(µ/L). On the other hand, when the data bias is large where δ 2 y satisfying δ 2 <sup>y</sup> ≥ Ω(L log(n)/(nµ))), we know O(L log(n)/(nµ<sup>2</sup> )) ≤ O(δ 2 <sup>y</sup>/µ). Based on previous discussion, by choosing an appropriate λ ∈ (0, 1) (e.g., λ = O(µ/L)), we will have O λL log(n/λ<sup>2</sup> )/(nµ<sup>2</sup> ) ≤ O(δ 2 <sup>y</sup>/µ). Then by comparing [\(18\)](#page-6-0) with [\(8\)](#page-3-3), we know that training by using MixLoss has a better performance than directly training on De when the data bias is large. Therefore, by solving the problem with a modified loss function, the MixLoss algorithm can enjoy a better performance by correcting the data bias.

#### 4.3 WeMix: A Generic Weighted Mixed Losses with Augmentation Dropping Algorithm

Inspired by previous theoretical analysis of using augmented data, we propose a generic framework of weighted mixed losses with augmentation dropping that builds upon two algorithms, AugDrop and MixLoss. Algorithm [1](#page-6-1) describes our procedure in detail, which is referred to as WeMix. It consists of two stages, wherein the first stage it runs a stochastic algorithm A<sup>1</sup> (e.g., momentum SGD, SGD) for solving weighted mixed losses [\(16\)](#page-5-2) and the second stage it runs another/same stochastic algorithm A<sup>2</sup> (e.g., momentum SGD, SGD) for solving the problem over original data. The notation A(·; ·, η) is one update step of a stochastic algorithm A with learning rate η. For example, if we select SGD as algorithm A, then

$$
\text{SGD}(\mathbf{w}_t; \mathbf{\hat{g}}_t, \eta) = \mathbf{w}_t - \eta \mathbf{\hat{g}}_t.
$$

The proposed WeMix is a generic strategy where the subroutine algorithm A1/A<sup>2</sup> can be replaced by any stochastic algorithms such as stochastic versions of momentum methods [\[Polyak](#page-11-15), [1964](#page-11-15), [Nesterov](#page-11-16), [1983](#page-11-16), [Yan et al.,](#page-12-4) [2018\]](#page-12-4) and adaptive methods [\[Duchi et al.,](#page-10-11) [2011,](#page-10-11) [Hinton et al.,](#page-10-12) [2012,](#page-10-12) [Zeiler,](#page-12-13) [2012,](#page-12-13) [Kingma and Ba](#page-10-13),

| Method           | CIFAR-100    | CIFAR-10     |
|------------------|--------------|--------------|
| without mixup    | 76.97 ± 0.27 | 94.95 ± 0.17 |
| mixup            | 78.31 ± 0.18 | 95.67 ± 0.09 |
| AugDrop (ours)   | 80.24 ± 0.34 | 96.03 ± 0.12 |
| MixLoss (ours)   | 79.70 ± 0.31 | 95.94 ± 0.11 |
| WeMix (ours)     | 80.61 ± 0.10 | 96.11 ± 0.11 |
| MixLoss-s (ours) | 79.53 ± 0.13 | 95.87 ± 0.14 |
| WeMix-s (ours)   | 80.29 ± 0.22 | 96.06 ± 0.16 |

<span id="page-7-1"></span>Table 1: Comparison of Testing Top-1 Accuracy (mean ± standard deviation, in %) using Different Methods on ResNet-18 over CIFAR-10 and CIFAR-100 for mixup

[2015,](#page-10-13) [Dozat,](#page-10-14) [2016,](#page-10-14) [Reddi et al.](#page-11-17), [2018\]](#page-11-17). We can also replace ℓ<sup>a</sup> by ℓ to avoid solving a minimization problem. The last solution of the first stage will be used as the initial solution of the second stage. If λ = 0 and ℓ<sup>a</sup> = ℓ, then WeMix reduces to the AugDrop; while if T<sup>2</sup> = 0, WeMix becomes to MixLoss. For label-preserving case, we only need to simply use ℓ<sup>a</sup> = ℓ (i.e, δ<sup>f</sup> = 0) in WeMix.

## 5 Experiments

To evaluate the performance of the proposed methods, we trained deep neural networks on two benchmark data sets, CIFAR-10 and CIFAR-100[1](#page-7-0) [\[Krizhevsky and Hinton](#page-10-15), [2009\]](#page-10-15) for the image classification task. Both CIFAR-10 and CIFAR-100 have 50,000 training images and 10,000 testing images of 32×32 resolutions. CIFAR-10 has 10 classes containing 6000 images each, while CIFAR-100 has 100 classes. We use mixup [\[Zhang et al.,](#page-12-3) [2018\]](#page-12-3) as an example of lable-mixing augmentation and Contrast as an example of lable-preserving augmentation and. For the choice of backbone, we use ResNet-18 model [\[He et al.](#page-10-16), [2016\]](#page-10-16) in mixup, and Wide-ResNet-28-10 model [\[Zagoruyko and Komodakis,](#page-12-14) [2016\]](#page-12-14) is applied in the Contrast experiment following by [\[Cubuk et al.](#page-9-1), [2019,](#page-9-1) [2020\]](#page-10-2). To verify our theoretical results, we compare the proposed AugDrop and MixLoss with two baselines, SGD with mixup/Contrast and SGD without mixup/Contrast (baseline). We also include WeMix in the comparison. The mini-batch size of training instances for all methods is 256 as suggested by [He et al.](#page-10-0) [\[2019\]](#page-10-0) and [He et al.](#page-10-16) [\[2016](#page-10-16)]. The momentum parameter of 0.9 is used. The weight decay with the parameter value is set to be 5×10<sup>−</sup><sup>4</sup> . The total epochs of training progress is fixed as 200. Followed by [\[He et al.,](#page-10-16) [2016,](#page-10-16) [Zagoruyko and Komodakis,](#page-12-14) [2016](#page-12-14)], we use 0.1 as the initial learning rates for all algorithms and divide them by 10 every 60 epochs.

For AugDrop, we drop off the augmentation after s-th epoch, where s ∈ {150, 160, 170, 180, 190} is tuned. For example, if s = 160, then it means that we run the first stage of AugDrop 160 epochs and the second stage 40 epochs. For MixLoss, we tune the parameter δ<sup>y</sup> from {0.5, 0.05, 0.005, 0.0005} and the best performance is reported. For WeMix, we use the value of δ<sup>y</sup> with the best performance in MixLoss, and we tune the dropping off epochs s same as AugDrop. We fix the convex combination parameter λ = 0.1 both for MixLoss and WeMix. We use top-1 accuracy to evaluate the performance. All top-1 accuracy on the testing data set are averaged over 5 independent random trails with their standard deviations.

#### 5.1 mixup

Given two examples (x<sup>i</sup> , yi) and (x<sup>j</sup> , y<sup>j</sup> ) that are drawn at random from the training data, mixup creates a virtual training example as follows x ′ = βx<sup>i</sup> + (1 − β)x<sup>j</sup> , y ′ = βy<sup>i</sup> + (1 − β)y<sup>j</sup> , where β ∈ [0, 1] is sampled from a Beta distribution β(α, α). We use α = 1 in the experiments as suggested in [\[Zhang et al.](#page-12-3), [2018\]](#page-12-3). In this subsection, we want to empirically verify that our theoretical findings for label-mixing augmentation in

<span id="page-7-0"></span><sup>1</sup><https://www.cs.toronto.edu/~kriz/cifar.html>

<span id="page-8-0"></span>Table 2: Comparison of Testing Top-1 Accuracy (mean ± standard deviation, in %) using Different Methods on ResNet-18 over CIFAR-100 for mixup of three images and ten images

| Method  | 3 images     | 10 images    |
|---------|--------------|--------------|
| Mixup   | 76.56 ± 0.23 | 60.36 ± 0.88 |
| AugDrop | 80.18 ± 0.19 | 76.35 ± 0.27 |
| MixLoss | 79.61 ± 0.09 | 75.41 ± 0.19 |
| WeMix   | 80.41 ± 0.22 | 78.08 ± 0.11 |

<span id="page-8-1"></span>Table 3: Comparison of Testing Top-1 Accuracy (mean ± standard deviation, in %) using Different Methods on WideResNet-28-10 over CIFAR-10 and CIFAR-100 for Contrast Transformation

| Method           | CIFAR-100    | CIFAR-10     |
|------------------|--------------|--------------|
| without Contrast | 78.07 ± 0.27 | 95.51 ± 0.14 |
| Contrast         | 77.90 ± 0.26 | 95.66 ± 0.05 |
| AugDrop (ours)   | 78.40 ± 0.24 | 95.93 ± 0.21 |
| MixLoss (ours)   | 78.17 ± 0.20 | 95.70 ± 0.11 |
| WeMix (ours)     | 78.79 ± 0.18 | 95.81 ± 0.11 |

Section [4.](#page-3-4) The experimental results conducted on CIFAR-10 and CIFAR-100 are listed in Table [1.](#page-7-1) We can see from the results that both AugDrop and MixLoss are better than two baselines, with and without mixup, which matches the theory found in Section [4.](#page-3-4) The performance of MixLoss is slightly worse than that of AugDrop, but they are comparable. Besides, the proposed WeMix enjoys both improvements, leading to the best performance among all algorithms although its convergence theoretical guarantee is unclear.

Next, we implement MixLoss and WeMix with δ<sup>y</sup> = 0 (i.e., use ℓ<sup>a</sup> = ℓ), which are denoted by MixLoss-s and WeMix-s, respectively. We summarize the results in Table [1,](#page-7-1) showing that both MixLoss-s and WeMix-s drop performance, comparing with MixLoss and WeMix, respectively.

Besides, we use more than two images in mixup such as three and ten images and the results are shown in Table [2.](#page-8-0) Although the top-1 accuracy of mixup reduces dramatically, we find that the proposed WeMix can still improve the performance when it comparing with mixup itself, showing the robustness of WeMix.

#### 5.2 Contrast

As a simple label-preserving augmentation, Contrast controls the contrast of the image. Its transformation magnitude is randomly selected from a uniform distribution [0.1, 1.9] following by [\[Cubuk et al.,](#page-9-1) [2019\]](#page-9-1). Despite its simplicity, we choose it to demonstrate our theory for the considered case in Appendix [A.](#page-13-0) The results of highest top-1 accuracy on the testing data sets for different methods are presented in Table [3.](#page-8-1) We find that by directly training on data with Contrast, it will drop the performance a little bit. Even so, the result shows that AugDrop has better performance than two baselines, which is consistent with the theoretical findings for label-preserving augmentation in Appendix [A](#page-13-0) that we need use the data augmentation at the early training stage but drop it at the end of training. Although there is no theoretical guarantee for the label-preserving transformation case, we implement MixLoss and WeMix by setting δ<sup>f</sup> = 0, i.e., using ℓ<sup>a</sup> = ℓ in [\(14\)](#page-5-1). The results show that MixLoss and WeMix are better than two baselines but are slightly worse than AugDrop.

## 6 Conclusions and Future Work

In this paper, we have studied how to better utilize data augmentation in training deep neural networks by designing two training schemes with the first one switches augmented data to original data during the training progress and the second one training on a convex combination of original loss and augmented loss. We have provided theoretical analyses of these two training schemes in non-convex smooth optimization setting. With the insights of theoretical results, we have designed a generic algorithm WeMix that can well leverage data augmentation in practice. We have verified our theoretical finding throughout extensive experimental evaluations on training ResNet and WideResNet models over benchmark data sets. Despite the effectiveness of WeMix, its theoretical guarantee is still not fully understand. We would like to leave this open problem as future work.

## References

- <span id="page-9-4"></span>Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In *International Conference on Machine Learning*, pages 242–252, 2019.
- <span id="page-9-9"></span>Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. *arXiv preprint arXiv:1901.08584*, 2019.
- <span id="page-9-0"></span>Henry S Baird. Document image defect models. In *Structured Document Image Analysis*, pages 546–556. Springer, 1992.
- <span id="page-9-7"></span>EG Birgin and JM Mart´ınez. Complexity and performance of an augmented lagrangian algorithm. *Optimization Methods and Software*, pages 1–36, 2020.
- <span id="page-9-8"></span>Digvijay Boob, Qi Deng, and Guanghui Lan. Proximal point methods for optimization with nonconvex functional constraints. *arXiv preprint arXiv:1908.02734*, 2019.
- <span id="page-9-11"></span>Stephen Boyd and Lieven Vandenberghe. *Convex Optimization*. Cambridge University Press, 2004.
- <span id="page-9-6"></span>Coralia Cartis, Nicholas IM Gould, and Philippe L Toint. On the evaluation complexity of composite function minimization with applications to nonconvex nonlinear programming. *SIAM Journal on Optimization*, 21 (4):1721–1739, 2011.
- <span id="page-9-5"></span>Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. In *International Conference on Machine Learning*, pages 745–754, 2018.
- <span id="page-9-3"></span>Lin Chen, Yifei Min, Mingrui Zhang, and Amin Karbasi. More data can expand the generalization gap between adversarially robust and standard models. *arXiv preprint arXiv:2002.04725*, 2020.
- <span id="page-9-2"></span>Shuxiao Chen, Edgar Dobriban, and Jane H Lee. Invariance reduces variance: Understanding data augmentation in deep learning and beyond. *arXiv preprint arXiv:1907.10905*, 2019.
- <span id="page-9-10"></span>Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In *Advances in Neural Information Processing Systems*, pages 2937–2947, 2019.
- <span id="page-9-12"></span>Imre Csiszar and J´anos K¨orner. *Information theory: coding theorems for discrete memoryless systems*. Cambridge University Press, 2011.
- <span id="page-9-1"></span>Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 113–123, 2019.
- <span id="page-10-2"></span>Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops*, pages 702–703, 2020.
- <span id="page-10-4"></span>Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher Re. A kernel theory of modern data augmentation. In *International Conference on Machine Learning*, pages 1528–1537, 2019.
- <span id="page-10-14"></span>Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
- <span id="page-10-9"></span>Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In *International Conference on Machine Learning*, pages 1675–1685, 2019.
- <span id="page-10-8"></span>Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes overparameterized neural networks. *arXiv preprint arXiv:1810.02054*, 2018.
- <span id="page-10-11"></span>John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. *Journal of Machine Learning Research*, 12:2121–2159, 2011.
- <span id="page-10-5"></span>Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. *SIAM Journal on Optimization*, 23(4):2341–2368, 2013.
- <span id="page-10-17"></span>Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. *Mathematical Programming*, 155(1-2):267–305, 2016.
- <span id="page-10-7"></span>Geovani N Grapiglia and Ya-xiang Yuan. On the complexity of an augmented lagrangian method for nonconvex optimization. *arXiv preprint arXiv:1906.05622*, 2019.
- <span id="page-10-10"></span>Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. *arXiv preprint arXiv:1903.08560*, 2019.
- <span id="page-10-3"></span>Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster autoaugment: Learning augmentation strategies using backpropagation. *arXiv preprint arXiv:1911.06987*, 2019.
- <span id="page-10-16"></span>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 770–778, 2016.
- <span id="page-10-0"></span>Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classification with convolutional neural networks. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 558–567, 2019.
- <span id="page-10-12"></span>Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. 2012.
- <span id="page-10-1"></span>Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efficient learning of augmentation policy schedules. In *International Conference on Machine Learning*, pages 2731– 2741. PMLR, 2019.
- <span id="page-10-6"></span>Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak- lojasiewicz condition. In *Joint European Conference on Machine Learning and Knowledge Discovery in Databases*, pages 795–811. Springer, 2016.
- <span id="page-10-13"></span>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In *International Conference on Learning Representations*, 2015.
- <span id="page-10-15"></span>Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. *Master's thesis, Technical report, University of Tronto*, 2009.
- <span id="page-11-8"></span>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In *Advances in Neural Information Processing Systems*, pages 1097–1105, 2012.
- <span id="page-11-9"></span>Xiaoyu Li, Zhenxun Zhuang, and Francesco Orabona. Exponential step sizes for non-convex optimization. *arXiv preprint arXiv:2002.05273*, 2020.
- <span id="page-11-11"></span>Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex optimization. In *Advances in Neural Information Processing Systems*, pages 5564–5574, 2018.
- <span id="page-11-5"></span>Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. In *Advances in Neural Information Processing Systems*, pages 6665–6675, 2019.
- <span id="page-11-6"></span>Chen Lin, Minghao Guo, Chuming Li, Xin Yuan, Wei Wu, Junjie Yan, Dahua Lin, and Wanli Ouyang. Online hyper-parameter learning for auto-augmentation strategy. In *Proceedings of the IEEE International Conference on Computer Vision*, pages 6579–6588, 2019a.
- <span id="page-11-12"></span>Qihang Lin, Runchao Ma, and Yangyang Xu. Inexact proximal-point penalty methods for non-convex optimization with non-convex constraints. *arXiv preprint arXiv:1908.11518*, 2019b.
- <span id="page-11-14"></span>Runchao Ma, Qihang Lin, and Tianbao Yang. Proximally constrained methods for weakly convex optimization with weakly convex constraints. *arXiv preprint arXiv:1908.01871*, 2019.
- <span id="page-11-2"></span>Yifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More data can help, double descend, or hurt generalization. *arXiv preprint arXiv:2002.11080*, 2020.
- <span id="page-11-16"></span>Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k<sup>2</sup> ). *Soviet Mathematics Doklady*, 27:372–376, 1983.
- <span id="page-11-18"></span>Yurii Nesterov. *Introductory lectures on convex optimization : a basic course*. Applied optimization. Kluwer Academic Publ., 2004. ISBN 1-4020-7553-7.
- <span id="page-11-13"></span>Michael O'Neill and Stephen J Wright. A log-barrier newton-cg method for bound constrained optimization with complexity guarantees. *IMA Journal of Numerical Analysis*, 2020.
- <span id="page-11-15"></span>Boris T Polyak. Some methods of speeding up the convergence of iteration methods. *USSR Computational Mathematics and Mathematical Physics*, 4(5):1–17, 1964.
- <span id="page-11-10"></span>Boris Teodorovich Polyak. Gradient methods for minimizing functionals. *Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki*, 3(4):643–653, 1963.
- <span id="page-11-3"></span>Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding and mitigating the tradeoff between robustness and accuracy. *arXiv preprint arXiv:2002.10716*, 2020.
- <span id="page-11-7"></span>Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris Papailiopoulos. Does data augmentation lead to positive margin? In *International Conference on Machine Learning*, pages 5321–5330, 2019.
- <span id="page-11-17"></span>Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In *International Conference on Learning Representations*, 2018.
- <span id="page-11-0"></span>J¨urgen Schmidhuber. Deep learning in neural networks: An overview. *Neural networks*, 61:85–117, 2015.
- <span id="page-11-1"></span>Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. *Journal of Big Data*, 6(1):60, 2019.
- <span id="page-11-4"></span>Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Between-class learning for image classification. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 5486–5494, 2018.

<span id="page-12-15"></span>Alexandre B Tsybakov. *Introduction to nonparametric estimation*. Springer Science & Business Media, 2008.

- <span id="page-12-6"></span>Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum: Faster variance reduction algorithms. In *Advances in Neural Information Processing Systems*, pages 2403–2413, 2019.
- <span id="page-12-8"></span>Stephen J Wright. On the convergence of the newton/log-barrier method. *Mathematical Programming*, 90 (1):71–100, 2001.
- <span id="page-12-2"></span>Sen Wu, Hongyang R Zhang, Gregory Valiant, and Christopher R´e. On the generalization effects of linear transformations in data augmentation. *arXiv preprint arXiv:2005.00695*, 2020.
- <span id="page-12-7"></span>Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In *Artificial Intelligence and Statistics*, pages 1216–1224, 2017.
- <span id="page-12-9"></span>Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, and Rong Jin. Towards understanding label smoothing. *arXiv preprint arXiv:2006.11653*, 2020.
- <span id="page-12-4"></span>Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A unified analysis of stochastic momentum methods for deep learning. In *International Joint Conference on Artificial Intelligence*, pages 2955–2961, 2018.
- <span id="page-12-5"></span>Zhuoning Yuan, Yan Yan, Rong Jin, and Tianbao Yang. Stagewise training accelerates convergence of testing error over sgd. In *Advances in Neural Information Processing Systems*, pages 2604–2614, 2019.
- <span id="page-12-11"></span>Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small relu networks are powerful memorizers: a tight analysis of memorization capacity. In *Advances in Neural Information Processing Systems*, pages 15558–15569, 2019.
- <span id="page-12-14"></span>Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. *arXiv preprint arXiv:1605.07146*, 2016.
- <span id="page-12-13"></span>Matthew D Zeiler. Adadelta: an adaptive learning rate method. *arXiv preprint arXiv:1212.5701*, 2012.
- <span id="page-12-10"></span>Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. *arXiv preprint arXiv:1611.03530*, 2016.
- <span id="page-12-3"></span>Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In *International Conference on Learning Representations*, 2018.
- <span id="page-12-0"></span>Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In *Advances in neural information processing systems*, pages 649–657, 2015.
- <span id="page-12-1"></span>Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, and Quoc V Le. Learning data augmentation strategies for object detection. *arXiv preprint arXiv:1906.11172*, 2019.
- <span id="page-12-12"></span>Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-parameterized deep relu networks. *Machine Learning*, 109(3):467–492, 2020.

## <span id="page-13-0"></span>A Main Results for label-preserving Augmentation

We consider label-preserving augmentation case [\(1\)](#page-2-3), that is,

$$
\mathbb{P}_{\mathbf{y}}(\cdot|\mathbf{x}) = \mathbb{P}_{\widetilde{\mathbf{y}}}(\cdot|\widetilde{\mathbf{x}}), \ \forall \widetilde{\mathbf{x}} \in T(\mathbf{x}) \ \mathrm{but} \ \mathbb{P}_{\mathbf{x}} \neq \mathbb{P}_{\widetilde{\mathbf{x}}}.
$$

It covers many image data augmentations including translation, adding noises, small rotation, and brightness or contrast changes [\[Krizhevsky et al.,](#page-11-8) [2012,](#page-11-8) [Raghunathan et al.,](#page-11-3) [2020\]](#page-11-3). It is worth mentioning that the compositions of label-preserving augmentation could also be label-preserving. Similar to the case of labelmixing augmentation, we measure the following difference between P<sup>x</sup> and Px<sup>e</sup> by a KL divergence:

$$
\delta_P := D_{KL}(\mathbb{P}_{\mathbf{x}} || \mathbb{P}_{\widetilde{\mathbf{x}}}) = \mathbf{E}_{\mathbf{x} \sim \mathbb{P}_x} \left[ \log \frac{\mathbb{P}_{\mathbf{x}}(\mathbf{x})}{\mathbb{P}_{\widetilde{\mathbf{x}}}(\mathbf{x})} \right]. \tag{19}
$$

Due to the data bias δ<sup>P</sup> , the prediction model learned from augmented data De could be even worse than training the prediction model directly from the original data D, as revealed by the following lemma and its remark.

<span id="page-13-3"></span>Lemma 2. *(label-preserving augmentation) Assume that* L *and* Le *satisfy properties in Definition [1,](#page-2-4) [2](#page-2-5) and [3,](#page-3-1) by setting* η = 1/L *and* m<sup>0</sup> ≥ <sup>4</sup> ηδ<sup>P</sup> *, when* t ≥ t<sup>0</sup> = <sup>L</sup> µ log (L(w1)−L(w∗))<sup>µ</sup> <sup>2</sup>δ<sup>P</sup> <sup>G</sup><sup>2</sup> *, we have*

$$
\mathcal{E}_{(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t)}[\mathcal{L}(\mathbf{w}_{t+1}) - \mathcal{L}(\mathbf{w}_*)] \le \frac{4\delta_P G^2}{\mu} \le O\left(\frac{\delta_P}{\mu}\right),\tag{20}
$$

*where* wt+1 *is output of mini-batch SGD trained on* De*,* δ<sup>P</sup> *is defined in [\(19\)](#page-13-1).*

*Proof.* See Appendix [I.1.](#page-23-0)

Remark: Comparing the result in [\(9\)](#page-3-2) with the result of [\(20\)](#page-13-2) in Lemma [2,](#page-13-3) it is easy to show that, when the data bias is too large, i.e., δ<sup>P</sup> ≥ Ω(L log(n)/(nµ)), we have O L log(n)/(nµ<sup>2</sup> ) ≤ O(δ<sup>P</sup> /µ). This implies that training the deep model directly on the original data D is more effective than on the augmented data De. Hence, in order to better leverage the augmented data in the presence of large data bias (δ<sup>P</sup> ≥ Ω(κ log(n)/n), where κ = L/µ), we need to come up with an approach that automatically correct the data bias in De. Below, we use AugDrop to correct the data bias by solving a constrained optimization problem.

#### A.1 AugDrop: Correcting Data Bias by Constrained Optimization

To correct data bias, we consider to solve the constrained optimization problem [\(10\)](#page-4-1). The key idea is to shrink the solution in a small region by using utilize augmented data to enjoy a smaller condition number, leading to an improved convergence in optimizing L(w). By introducing a term that

$$
\gamma_1 := \delta_P G^2 / \mu,
$$

we can present a proposition about A(γ) and µ(γ), showing that we have a smaller condition number and consequentially a smaller optimization error by restricting our solutions to A(γ).

<span id="page-13-4"></span>Proposition 2. *If* γ ∈ [γ1, 4γ1]*, we have* w<sup>∗</sup> ∈ A(γ) *and* µ(γ) ≥ µ*, where* A(γ) *and* µ(γ) *are defined in [\(11\)](#page-4-3) and [\(12\)](#page-4-4), respectively.*

*Proof.* See Appendix [I.2.](#page-24-0)

The following theorem shows the convergence result of AugDrop for label-preserving augmentation.

<span id="page-13-2"></span><span id="page-13-1"></span>

<span id="page-14-0"></span>Theorem 3. *Define* γ = 4γ1, µ<sup>e</sup> = µ(4γ1)*. Assume that* L *and* Le *satisfy properties in Definition [1,](#page-2-4) [2](#page-2-5) and [3,](#page-3-1) set learning rate* η<sup>1</sup> = 1/L *in Stage I and learning rate* η<sup>2</sup> = 1 2nµ<sup>e</sup> log 8nµ<sup>2</sup> e (L(w1)−L(w∗)) G2L *in Stage II for Aug-Drop. Let* w<sup>1</sup> *be the initial solution in Stage I of AugDrop and* wT1+2, . . . , wT1+n/m2+1 *be the intermediate solutions obtained by the mini-batch SGD in Stage II of AugDrop. Choose* T<sup>1</sup> = <sup>L</sup> µ log 2(Le(w1)−Le(w<sup>e</sup> <sup>∗</sup>))<sup>µ</sup> <sup>δ</sup><sup>P</sup> <sup>G</sup><sup>2</sup> *,* m<sup>1</sup> = 1 + <sup>q</sup> 3 log <sup>2</sup>T<sup>1</sup> δ <sup>2</sup> 8 δ<sup>P</sup> *and* m<sup>2</sup> = 1 + <sup>q</sup> 3 log <sup>2</sup><sup>n</sup> δ 2 8 δ<sup>P</sup> *, with a probability* 1 − δ*, we have* w<sup>t</sup> ∈

$$
\mathcal{A}(4\gamma_1), \forall t \in \{T_1 + 2, \ldots, T_1 + n/m_2 + 1\} \text{ and}
$$

$$
\mathcal{E}\left[\mathcal{L}(\widehat{\mathbf{w}}) - \mathcal{L}(\mathbf{w}_*)\right] \le \frac{G^2 L}{8n\mu_e^2} + \frac{G^2 L}{8n\mu_e^2} \log\left(\frac{8n\mu_e^2(\mathcal{L}(\mathbf{w}_1) - \mathcal{L}(\mathbf{w}_*))}{G^2 L}\right) \le O\left(\frac{L\log(n)}{n\mu_e^2}\right),\tag{21}
$$

*where* <sup>w</sup><sup>b</sup> <sup>=</sup> <sup>w</sup>T1+n/m2+1 *and* <sup>δ</sup><sup>P</sup> *is defined in [\(19\)](#page-13-1).*

*Proof.* See Appendix [I.3.](#page-24-1)

<span id="page-14-3"></span><span id="page-14-1"></span>

Remark. Theorem [3](#page-14-0) shows that all intermediate solutions w<sup>t</sup> obtained in Stage II of AugDrop satisfy the constraint <sup>L</sup>e(wt) <sup>−</sup> <sup>L</sup>e(w<sup>e</sup> <sup>∗</sup>) <sup>≤</sup> <sup>4</sup>γ1, that is to say, <sup>w</sup><sup>t</sup> ∈ A(4γ1). Based on Proposition [2,](#page-13-4) we will enjoy a larger µ<sup>e</sup> than µ. Comparing the result of [\(21\)](#page-14-1) in Theorem [3](#page-14-0) with [\(9\)](#page-3-2), training by using AugDrop will result in a better performance than directly training on D due to µ<sup>e</sup> ≥ µ. Besides, when the data bias is large, i.e., δ<sup>P</sup> ≥ Ω(L log(n)/(nµ)), we know O(L log(n)/(nµ<sup>2</sup> e )) ≤ O(µδ<sup>P</sup> /µ<sup>2</sup> e ) ≤ O(δ<sup>P</sup> /µ), where the last inequality holds due to µ<sup>e</sup> ≥ µ. By comparing [\(21\)](#page-14-1) with the result of [\(20\)](#page-13-2) in Lemma [2,](#page-13-3) we know that training by using AugDrop has a better performance than directly training on De when the data bias is large. By solving a constrained problem, the AugDrop algorithm can correct the data bias and thus enjoy an better performance.

## B Technical Results for Cross-entropy Loss

<span id="page-14-2"></span>Lemma 3. *Assume that* L(w) = E[ℓ(y, f(x; w))] *satisfies property in Definition [1,](#page-2-4) where* ℓ *is a cross-entropy loss, then we have*

$$
\|\nabla_{\mathbf{w}}\ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w})) - \nabla_{\mathbf{w}}\ell(\widetilde{\mathbf{y}}, f(\mathbf{x}; \mathbf{w}))\| \le G \|\mathbf{y} - \widetilde{\mathbf{y}}\|,
$$
\n(22)

*and*

$$
\|\nabla_{\mathbf{w}}\ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w}))\| \le G. \tag{23}
$$

*Proof.* The objective function is

$$
\mathcal{L}(\mathbf{w}) = \mathbf{E}_{(\mathbf{x}, \mathbf{y})} [\ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w}))],
$$
\n(24)

where the cross-entropy loss function ℓ is given by

$$
\ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w})) = \sum_{i=1}^{K} -y_i \log \left( \frac{\exp(f_i(\mathbf{x}; \mathbf{w}))}{\sum_{j=1}^{K} \exp(f_j(\mathbf{x}; \mathbf{w}))} \right).
$$
(25)

Let set

$$
p(\mathbf{x}; \mathbf{w}) = (p_1(\mathbf{x}; \mathbf{w}), \dots, p_K(\mathbf{x}; \mathbf{w})), \quad p_i(\mathbf{x}; \mathbf{w}) = -\log\left(\frac{\exp(f_i(\mathbf{x}; \mathbf{w}))}{\sum_{j=1}^K \exp(f_j(\mathbf{x}; \mathbf{w}))}\right),
$$
(26)

then the gradient of ℓ with respective to w is

$$
\nabla \ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w})) = \langle \mathbf{y}, \nabla p(\mathbf{x}; \mathbf{w}) \rangle.
$$
 (27)

Therefore, ∀x ∈ X and w ∈ R <sup>D</sup> we have

$$
\|\nabla_{\mathbf{w}} \ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w})) - \nabla_{\mathbf{w}} \ell(\widetilde{\mathbf{y}}, f(\mathbf{x}; \mathbf{w}))\|
$$
  
\n
$$
= \|\langle \mathbf{y} - \widetilde{\mathbf{y}}, \nabla p(\mathbf{x}; \mathbf{w}) \rangle\|
$$
  
\n
$$
\leq \|\nabla p(\mathbf{x}; \mathbf{w})\| \|\mathbf{y} - \widetilde{\mathbf{y}}\|
$$
  
\n
$$
\leq G \|\mathbf{y} - \widetilde{\mathbf{y}}\|,
$$
\n(28)

and

$$
\|\nabla_{\mathbf{w}}\ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w}))\| = \|\langle \mathbf{y}, \nabla p(\mathbf{x}; \mathbf{w}) \rangle\| \le \|\nabla p(\mathbf{x}; \mathbf{w})\| \|\mathbf{y}\| \le G,\tag{29}
$$

where uses the facts that k∇p(x; w)k ≤ G and kyk ≤ kyk<sup>1</sup> = 1, here k · k is a Euclidean norm (ℓ<sup>2</sup> norm) and k · k<sup>1</sup> is ℓ<sup>1</sup> norm.

## <span id="page-15-0"></span>C Proof of Lemma [1](#page-3-0)

*Proof.* Recall that the update of mini-batch SGD is given by

$$
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \widetilde{\mathbf{g}}_t.
$$

Let set the averaged mini-batch stochastic gradients of Le(wt) as

$$
\widetilde{\mathbf{g}}_t := \frac{1}{m_0} \sum_{i=1}^{m_0} \nabla \ell \left( \widetilde{\mathbf{y}}_{t,i}, f(\widetilde{\mathbf{x}}_{t,i}; \mathbf{w}_t) \right),
$$

then by the Assumption of Le satisfying the property in Definition [1,](#page-2-4) we know that

$$
\mathbf{E}_{(\widetilde{\mathbf{x}}_{t,i}, \widetilde{\mathbf{y}}_{t,i})} \left[ \nabla \ell \left( \widetilde{\mathbf{y}}_{t,i}, f(\widetilde{\mathbf{x}}_{t,i}; \mathbf{w}_t) \right) \right] = \nabla \widetilde{\mathcal{L}}(\mathbf{w}_t), \quad \forall i \in \{1, \dots, m_0\}
$$
\n(30)

and thus

<span id="page-15-3"></span><span id="page-15-2"></span><span id="page-15-1"></span>
$$
E_{(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t)}[\widetilde{\mathbf{g}}_t] = \nabla \widetilde{\mathcal{L}}(\mathbf{w}_t),
$$
\n(31)

where we write E(xet,yet) [ge<sup>t</sup>] as E(xet,1,yet,1) [. . .E(xet,m<sup>0</sup> ,yet,m<sup>0</sup> ) [ge<sup>t</sup>]] for simplicity. Then the norm variance of <sup>g</sup>e<sup>t</sup> is given by

$$
E_{\left(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t\right)}\left[\left\|\widetilde{\mathbf{g}}_t - \nabla \widetilde{\mathcal{L}}(\mathbf{w}_t)\right\|^2\right]
$$
\n
$$
= E_{\left(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t\right)}\left[\left\|\frac{1}{m_0} \sum_{i=1}^{m_0} \nabla \ell\left(\widetilde{\mathbf{y}}_{t,i}, f(\widetilde{\mathbf{x}}_{t,i}; \mathbf{w}_t)\right) - \nabla \widetilde{\mathcal{L}}(\mathbf{w}_t)\right\|^2\right]
$$
\n
$$
\stackrel{(a)}{=} \frac{1}{m_0^2} \sum_{i=1}^{m_0} E_{\left(\widetilde{\mathbf{x}}_{t,i}, \widetilde{\mathbf{y}}_{t,i}\right)}\left[\left\|\nabla \ell\left(\widetilde{\mathbf{y}}_{t,i}, f(\widetilde{\mathbf{x}}_{t,i}; \mathbf{w}_t)\right) - \nabla \widetilde{\mathcal{L}}(\mathbf{w}_t)\right\|^2\right]
$$
\n
$$
\stackrel{(b)}{\leq} \frac{4G^2}{m_0},\tag{32}
$$

where (a) is due to [\(30\)](#page-15-1) and the pairs (xet,<sup>1</sup>, <sup>y</sup>et,<sup>1</sup>), . . . ,(xet,m<sup>0</sup> , <sup>y</sup>et,m<sup>0</sup> ) are independently sampled from De; (b) is due to the facts that the Assumption of Le satisfying the property in Definition [1](#page-2-4) and Lemma [3,](#page-14-2) and then by Jensen's inequality, we also have k∇wLe(w)k ≤ G, implying that <sup>∇</sup>ℓ(ye, f(xe; <sup>w</sup>)) − ∇Le(w) 2 ≤ 4G<sup>2</sup> . On the other hand, by the Assumption of L satisfying the property in Definition [2,](#page-2-5) we have

$$
\begin{split}\n& \mathbf{E}_{(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t)}[\mathcal{L}(\mathbf{w}_{t+1}) - \mathcal{L}(\mathbf{w}_t)] \\
&\leq & \mathbf{E}_{(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t)}\left[\langle \nabla \mathcal{L}(\mathbf{w}_t), \mathbf{w}_{t+1} - \mathbf{w}_t \rangle + \frac{L}{2} ||\mathbf{w}_{t+1} - \mathbf{w}_t||^2\right] \\
& \stackrel{(a)}{=} & \frac{\eta}{2} \mathbf{E}_{(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t)}\left[||\nabla \mathcal{L}(\mathbf{w}_t) - \widetilde{\mathbf{g}}_t||^2 - ||\nabla \mathcal{L}(\mathbf{w}_t)||^2 - (1 - \eta L) ||\widetilde{\mathbf{g}}_t||^2\right] \\
& \stackrel{(b)}{=} & \frac{\eta}{2} \left(||\nabla \mathcal{L}(\mathbf{w}_t) - \nabla \widetilde{\mathcal{L}}(\mathbf{w}_t)||^2 + \mathbf{E}_{(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t)}\left[||\nabla \widetilde{\mathcal{L}}(\mathbf{w}_t) - \widetilde{\mathbf{g}}_t||^2\right] - ||\nabla \mathcal{L}(\mathbf{w}_t)||^2 \\
&- (1 - \eta L) \mathbf{E}_{(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t)}\left[||\widetilde{\mathbf{g}}_t||^2\right]\right) \\
&\leq & \frac{\eta}{2} \left(||\nabla \mathcal{L}(\mathbf{w}_t) - \nabla \widetilde{\mathcal{L}}(\mathbf{w}_t)||^2 + \frac{4G^2}{m_0} - ||\nabla \mathcal{L}(\mathbf{w}_t)||^2\right)\n\end{split} \tag{33}
$$

where the (a) is due to the update of <sup>w</sup>t+1 <sup>=</sup> <sup>w</sup><sup>t</sup> <sup>−</sup> <sup>η</sup>ge<sup>t</sup>; (b) is due to [\(31\)](#page-15-2); (c) is due to <sup>η</sup> = 1/L and [\(32\)](#page-15-3). By using the Assumption of <sup>L</sup><sup>e</sup> and <sup>L</sup> satisfying the property in Definition [1](#page-2-4) and <sup>P</sup><sup>x</sup> <sup>=</sup> <sup>P</sup>xe, we have

<span id="page-16-1"></span>
$$
\|\nabla \mathcal{L}(\mathbf{w}_t) - \nabla \widetilde{\mathcal{L}}(\mathbf{w}_t)\| \n= \|\mathbf{E}_{(\mathbf{x}, \mathbf{y})}[\nabla_{\mathbf{w}} \ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w}_t))] - \mathbf{E}_{(\widetilde{\mathbf{x}}, \widetilde{\mathbf{y}})}[\nabla_{\mathbf{w}} \ell(\widetilde{\mathbf{y}}, f(\widetilde{\mathbf{x}}; \mathbf{w}_t))]\| \n\overset{(a)}{\leq} \mathbf{E}_{(\mathbf{x}, \mathbf{y}, \widetilde{\mathbf{y}})}[\|\nabla_{\mathbf{w}} \ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w}_t)) - \nabla_{\mathbf{w}} \ell(\widetilde{\mathbf{y}}, f(\mathbf{x}; \mathbf{w}_t))\|] \n\overset{(22)}{\leq} G \mathbf{E}_{(\mathbf{y}, \widetilde{\mathbf{y}})}[\|\mathbf{y} - \widetilde{\mathbf{y}}\|] \n\overset{(b)}{\leq} G \delta_y,
$$
\n(34)

where (a) uses Jensen's inequality; (b) is due to [\(3\)](#page-2-0). By using the Assumption of L satisfying the property in Definition [3](#page-3-1) and [\(34\)](#page-16-0), inequality [\(33\)](#page-16-1) becomes

<span id="page-16-0"></span>
$$
\begin{split} & \mathbf{E}_{(\widetilde{\mathbf{x}}_t,\widetilde{\mathbf{y}}_t)}[\mathcal{L}(\mathbf{w}_{t+1}) - \mathcal{L}(\mathbf{w}_t)] \\ &\leq \frac{\eta G^2 \delta_y^2}{2} + \frac{2\eta G^2}{m_0} - \frac{\eta}{2} \|\nabla \mathcal{L}(\mathbf{w}_t)\|^2 \\ &\leq \frac{\eta G^2 \delta_y^2}{2} + \frac{2\eta G^2}{m_0} - \eta \mu \left(\mathcal{L}(\mathbf{w}_t) - \mathcal{L}(\mathbf{w}_*)\right), \end{split}
$$

which implies

$$
\begin{split} & \mathbf{E}_{(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t)}[\mathcal{L}(\mathbf{w}_{t+1}) - \mathcal{L}(\mathbf{w}_*)] \\ &\leq (1 - \eta \mu) \left( \mathcal{L}(\mathbf{w}_t) - \mathcal{L}(\mathbf{w}_*) \right) + \frac{\eta G^2 \delta_y^2}{2} + \frac{2\eta G^2}{m_0} \\ &\leq (1 - \eta \mu)^t \left( \mathcal{L}(\mathbf{w}_1) - \mathcal{L}(\mathbf{w}_*) \right) + \left( \frac{\eta G^2 \delta_y^2}{2} + \frac{2\eta G^2}{m_0} \right) \sum_{i=0}^{t-1} (1 - \eta \mu)^i. \end{split}
$$

Due to (1 − ηµ) <sup>t</sup> ≤ exp(−tηµ) and Pt−<sup>1</sup> <sup>i</sup>=0(1 − ηµ) <sup>i</sup> ≤ 1 ηµ , when

$$
m_0 \ge \frac{8}{\delta_y^2}
$$

and

$$
t \geq \frac{L}{\mu} \log \frac{4(\mathcal{L}(\mathbf{w}_1) - \mathcal{L}(\mathbf{w}_*)) \mu}{\delta_y^2 G^2},
$$

we know

$$
\mathrm{E}_{(\widetilde{\mathbf{x}}_t,\widetilde{\mathbf{y}}_t)}[\mathcal{L}(\mathbf{w}_{t+1})-\mathcal{L}(\mathbf{w}_*)] \leq \frac{\delta_y^2 G^2}{\mu}.
$$

# <span id="page-17-0"></span>D Proof of [\(9\)](#page-3-2)

We first put the full statement of [\(9\)](#page-3-2) in the following lemma.

Lemma 4. *Assume that* L *satisfies the properties in Definition [1,](#page-2-4) [2](#page-2-5) and [3,](#page-3-1) by setting* η = 1 2nµ log 8nµ<sup>2</sup> (L(w1)−L(w∗)) G2L *, we have* E(xn,yn) [L(wn+1) − L(w∗)] ≤ G2L <sup>8</sup>nµ<sup>2</sup> + G2L <sup>8</sup>nµ<sup>2</sup> log 8nµ<sup>2</sup> (L(w1)−L(w∗)) G2L *, where* wn+1 *is output of SGD trained on* D*.*

*Proof.* By the Assumption of L satisfying the property in Definition [2,](#page-2-5) we have

$$
E_{(\mathbf{x}_n, \mathbf{y}_n)}[\mathcal{L}(\mathbf{w}_{n+1}) - \mathcal{L}(\mathbf{w}_n)]
$$
  
\n
$$
\leq E_{(\mathbf{x}_n, \mathbf{y}_n)}[\langle \nabla \mathcal{L}(\mathbf{w}_n), \mathbf{w}_{n+1} - \mathbf{w}_n \rangle] + \frac{L}{2} E_{(\mathbf{x}_n, \mathbf{y}_n)} [\|\mathbf{w}_{t+n} - \mathbf{w}_n\|^2]
$$
  
\n
$$
\stackrel{(a)}{=} - \eta E_{(\mathbf{x}_n, \mathbf{y}_n)}[\langle \nabla \mathcal{L}(\mathbf{w}_n), \nabla \ell(\mathbf{y}_n, f(\mathbf{x}_n; \mathbf{w}_n)) \rangle] + \frac{L}{2} E_{(\mathbf{x}_n, \mathbf{y}_n)} [\|\nabla \ell(\mathbf{y}_n, f(\mathbf{x}_n; \mathbf{w}_n))\|^2]
$$
  
\n
$$
\stackrel{(b)}{=} - \eta \|\nabla \mathcal{L}(\mathbf{w}_n)\|^2 + \frac{\eta^2 L}{2} E_{(\mathbf{x}_n, \mathbf{y}_n)} [\|\nabla \ell(\mathbf{y}_n, f(\mathbf{x}_n; \mathbf{w}_n))\|^2],
$$

where (a) is due to the update of wn+1 = w<sup>n</sup> − η∇ℓ (yn, f(xn; wn)); (b) is due to the Assumption of L satisfying the property in Definition [1](#page-2-4) that E(x,y) [∇wℓ (y, f(x; w))] = ∇L(w). By using the Assumption of L satisfying the property in Definition [1](#page-2-4) that k∇wℓ(y, f(x; w))k ≤ G and the Assumption of L satisfying the property in Definition [3,](#page-3-1) we have

$$
\begin{aligned} &\mathbf{E}_{(\mathbf{x}_n,\mathbf{y}_n)}[\mathcal{L}(\mathbf{w}_{n+1}) - \mathcal{L}(\mathbf{w}_n)] \\ &\leq \frac{\eta^2 LG^2}{2} - \eta \|\nabla \mathcal{L}(\mathbf{w}_n)\|^2 \\ &\leq \frac{\eta^2 LG^2}{2} - 2\eta\mu \left(\mathcal{L}(\mathbf{w}_n) - \mathcal{L}(\mathbf{w}_*)\right), \end{aligned}
$$

which implies

$$
\begin{aligned} & \mathbf{E}_{(\mathbf{x}_n, \mathbf{y}_n)}[\mathcal{L}(\mathbf{w}_{n+1}) - \mathcal{L}(\mathbf{w}_*)] \\ &\leq (1 - 2\eta\mu) \mathbf{E}_{(\mathbf{x}_{n-1}, \mathbf{y}_{n-1})}[\mathcal{L}(\mathbf{w}_n) - \mathcal{L}(\mathbf{w}_*)] + \frac{\eta^2 LG^2}{2} \\ &\leq (1 - 2\eta\mu)^n \left(\mathcal{L}(\mathbf{w}_1) - \mathcal{L}(\mathbf{w}_*)\right) + \frac{\eta^2 LG^2}{2} \sum_{i=0}^{n-1} (1 - 2\eta\mu)^i. \end{aligned}
$$

Due to (1 − 2ηµ) <sup>n</sup> ≤ exp(−2ηµn) and P<sup>n</sup>−<sup>1</sup> <sup>i</sup>=0 (1 − 2ηµ) <sup>i</sup> ≤ <sup>1</sup> 2ηµ , then by using the setting of

$$
\eta = \frac{1}{2n\mu} \log \left( \frac{8n\mu^2 (\mathcal{L}(\mathbf{w}_1) - \mathcal{L}(\mathbf{w}_*))}{G^2 L} \right),
$$

we have

$$
E_{(\mathbf{x}_n, \mathbf{y}_n)} [\mathcal{L}(\mathbf{w}_{n+1}) - \mathcal{L}(\mathbf{w}_*))]
$$
  
\n
$$
\leq \exp(-2\eta \mu n) (\mathcal{L}(\mathbf{w}_1) - \mathcal{L}(\mathbf{w}_*)) + \frac{\eta G^2 L}{4\mu}
$$
  
\n
$$
= \frac{G^2 L}{8n\mu^2} + \frac{G^2 L}{8n\mu^2} \log \left( \frac{8n\mu^2 (\mathcal{L}(\mathbf{w}_1) - \mathcal{L}(\mathbf{w}_*))}{G^2 L} \right)
$$
  
\n
$$
\leq O\left(\frac{L}{n\mu^2} \log(n)\right).
$$

## <span id="page-18-0"></span>E Proof of Proposition [1](#page-4-0)

*Proof.* By using the Assumption of Le satisfying the property in Definition [3,](#page-3-1) we have

$$
\tilde{\mathcal{L}}(\mathbf{w}_{*}) - \tilde{\mathcal{L}}(\widetilde{\mathbf{w}}_{*})
$$
\n
$$
\leq \frac{\|\nabla \tilde{\mathcal{L}}(\mathbf{w}_{*})\|^{2}}{2\mu}
$$
\n
$$
\stackrel{(a)}{=} \frac{\|\nabla \tilde{\mathcal{L}}(\mathbf{w}_{*}) - \nabla \mathcal{L}(\mathbf{w}_{*})\|^{2}}{2\mu}
$$
\n
$$
\stackrel{(b)}{\leq} \frac{\delta_{y}^{2} G^{2}}{2\mu}
$$

where (a) is due to the definition of w<sup>∗</sup> in [\(6\)](#page-2-1) so that ∇L(w∗) = 0; (b) follows the same analysis of [\(34\)](#page-16-0) in Lemma [1.](#page-3-0) Thus we know w<sup>∗</sup> ∈ A(γ) when γ ≥ γ<sup>0</sup> := δ 2 yG<sup>2</sup> 2µ . On the other hand, by the definition of µ(γ) in [\(12\)](#page-4-4) and the Assumption of L satisfying the property in Definition [3,](#page-3-1) we know µ(γ) ≥ µ when γ ≤ 8µ0.

## <span id="page-18-2"></span>F Algorithm AugDrop and Proof of Theorem [1](#page-4-2)

We present the details of update steps for AugDrop and its convergence analysis in this section.

<span id="page-18-1"></span>Algorithm 2 AugDrop 1: Input: T<sup>1</sup> 2: Initialize: w<sup>1</sup> ∈ R <sup>D</sup>, η1, η<sup>2</sup> > 0 // Stage I: Train Augmented Data 3: for t = 1, 2, . . . , T<sup>1</sup> do 4: draw <sup>m</sup><sup>1</sup> examples (xet,<sup>1</sup>, <sup>y</sup>et,<sup>1</sup>), . . . ,(xet,m<sup>1</sup> , <sup>y</sup>et,m<sup>1</sup> ) at random from augmented data 5: update wt+1 = w<sup>t</sup> − η1 m1 P<sup>m</sup><sup>1</sup> <sup>i</sup>=1 <sup>∇</sup>w<sup>ℓ</sup> (yet,i, f(xet,i; <sup>w</sup>t)) 6: end for // Stage II: Train Original Data 7: for t = T<sup>1</sup> + 1, T<sup>1</sup> + 2, . . . , T<sup>1</sup> + n/m<sup>2</sup> do 8: draw m<sup>2</sup> examples (xt,<sup>1</sup>, yt,<sup>1</sup>), . . . ,(xt,m<sup>2</sup> , yt,m<sup>2</sup> ) without replacement at random from original data 9: update wt+1 = w<sup>t</sup> − η2 m2 P<sup>m</sup><sup>2</sup> <sup>i</sup>=1 ∇wℓ (yt,i, f(xt,i; wt)) 10: end for 11: Output: w<sup>T</sup>1+n/m2+1.

*Proof.* In the first stage of the proposed algorithm, we run a mini-batch SGD over the augmented data <sup>D</sup><sup>e</sup> with <sup>m</sup><sup>1</sup> as the size of mini-batch. Let (xet,i, <sup>y</sup>et,i), i = 1, . . . , m<sup>1</sup> be the <sup>m</sup><sup>1</sup> examples sampled in the <sup>t</sup>th iteration. Let <sup>g</sup>e<sup>t</sup> be the average gradient for the <sup>t</sup> iteration, i.e.

<span id="page-18-3"></span>
$$
\widetilde{\mathbf{g}}_t = \frac{1}{m_1} \sum_{i=1}^{m_1} \nabla_{\mathbf{w}} \ell(\widetilde{\mathbf{y}}_{t,i}, f(\widetilde{\mathbf{x}}_{t,i}; \mathbf{w}_t))
$$

We then update the solution by mini-batch SGD: <sup>w</sup>t+1 <sup>=</sup> <sup>w</sup><sup>t</sup> <sup>−</sup> <sup>η</sup>1ge<sup>t</sup>. By using Lemma 4 of [\[Ghadimi et al.](#page-10-17), [2016\]](#page-10-17), with a probability 1 − δ ′ , we have

$$
\left\| \widetilde{\mathbf{g}}_t - \nabla \widetilde{\mathcal{L}}(\mathbf{w}_t) \right\| \le \left( 1 + \sqrt{3 \log \frac{1}{\delta'}} \right) \sqrt{\frac{8G^2}{m_1}}.
$$
\n(35)

By the Assumption of <sup>L</sup><sup>e</sup> satisfying the property in Definition [2](#page-2-5) and the update of <sup>w</sup>t+1 <sup>=</sup> <sup>w</sup><sup>t</sup> <sup>−</sup>η1get, we have

$$
\tilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \tilde{\mathcal{L}}(\mathbf{w}_t)
$$
\n
$$
\leq -\eta_1 \langle \nabla \tilde{\mathcal{L}}(\mathbf{w}_t), \tilde{\mathbf{g}}_t \rangle + \frac{\eta_1^2 L}{2} ||\tilde{\mathbf{g}}_t||^2
$$
\n
$$
= \frac{\eta_1}{2} ||\nabla \tilde{\mathcal{L}}(\mathbf{w}_t) - \tilde{\mathbf{g}}_t||^2 - \frac{\eta_1}{2} ||\nabla \tilde{\mathcal{L}}(\mathbf{w}_t)||^2 - \frac{\eta_1 (1 - \eta_1 L)}{2} ||\tilde{\mathbf{g}}_t||^2
$$
\n
$$
\leq \frac{\eta_1}{2} \left( 1 + \sqrt{3 \log \frac{1}{\delta'}} \right)^2 \frac{8G^2}{m_1} - \eta_1 \mu (\tilde{\mathcal{L}}(\mathbf{w}_t) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*)),
$$

where (a) uses the facts that [\(35\)](#page-18-3), η<sup>1</sup> = 1/L and the Assumption of Le satisfying the property in Definition [3.](#page-3-1) Thus, with a probability (1 − δ ′ ) t , using the recurrence relation, we have

$$
\tilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*)
$$
\n
$$
\leq (1 - \eta_1 \mu)(\tilde{\mathcal{L}}(\mathbf{w}_t) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*)) + \frac{\eta_1}{2} \left(1 + \sqrt{3 \log \frac{1}{\delta'}}\right)^2 \frac{8G^2}{m_1}
$$
\n
$$
\leq (1 - \eta_1 \mu)^t \left(\tilde{\mathcal{L}}(\mathbf{w}_1) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*)\right) + \frac{\eta_1}{2} \left(1 + \sqrt{3 \log \frac{1}{\delta'}}\right)^2 \frac{8G^2}{m_1} \sum_{i=0}^{t-1} (1 - \eta_1 \mu)^i. \tag{36}
$$

Due to (1 − η1µ) <sup>t</sup> ≤ exp(−tη1µ) and P<sup>t</sup>−<sup>1</sup> <sup>i</sup>=0(1 − η1µ) <sup>i</sup> ≤ <sup>1</sup> η1µ , when

<span id="page-19-1"></span>
$$
t \geq T_1 := \frac{1}{\eta_1 \mu} \log \frac{2(\mathcal{L}(\mathbf{w}_1) - \mathcal{L}(\widetilde{\mathbf{w}}_*)) \mu}{\delta_y^2 G^2},
$$

we have

$$
\tilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*)
$$
\n
$$
\leq \exp(-t\eta_1\mu) \left( \tilde{\mathcal{L}}(\mathbf{w}_1) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*) \right) + \left( 1 + \sqrt{3 \log \frac{1}{\delta'}} \right)^2 \frac{4G^2}{\mu m_1}
$$
\n
$$
\leq \frac{\delta_y^2 G^2}{2\mu} + \left( 1 + \sqrt{3 \log \frac{1}{\delta'}} \right)^2 \frac{4G^2}{\mu m_1}.
$$
\n(37)

Let δ ′ = δ 2T1 , if we choose m<sup>1</sup> such that

$$
m_1 = \left(1 + \sqrt{3\log\frac{2T_1}{\delta}}\right)^2 \frac{8}{\delta_y^2},
$$

then for any t ≥ T1, then with a probability 1 − δ/2 we have

<span id="page-19-0"></span>
$$
\widetilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_{*}) \le \frac{\delta_y^2 G^2}{\mu}.
$$
\n(38)

In the second stage of the proposed algorithm, we run a mini-batch SGD over the original data D with m<sup>2</sup> as the size of mini-batch. Let (xt,i, yt,i), i = 1, . . . , m<sup>2</sup> be the m<sup>2</sup> examples sampled in the tth iteration. Let <sup>g</sup>b<sup>t</sup> be the average gradient for the <sup>t</sup> iteration, i.e.

$$
\widehat{\mathbf{g}}_t = \frac{1}{m_2} \sum_{i=1}^{m_2} \nabla_{\mathbf{w}} \ell(\mathbf{y}_{t,i}, f(\mathbf{x}_{t,i}; \mathbf{w}_t))
$$

We then update the solution <sup>w</sup>t+1 <sup>=</sup> <sup>w</sup><sup>t</sup> <sup>−</sup> <sup>η</sup>2gbt. By using Lemma 4 of [\[Ghadimi et al.,](#page-10-17) [2016\]](#page-10-17), with a probability 1 − δ ′′, we have

<span id="page-20-1"></span><span id="page-20-0"></span>
$$
\|\widehat{\mathbf{g}}_t - \nabla \mathcal{L}(\mathbf{w}_t)\| \le \left(1 + \sqrt{3\log\frac{1}{\delta''}}\right)\sqrt{\frac{8G^2}{m_2}}.\tag{39}
$$

By the smoothness of <sup>L</sup>e(w) and the update of <sup>w</sup>t+1 <sup>=</sup> <sup>w</sup><sup>t</sup> <sup>−</sup> <sup>η</sup>2gbt, we have

$$
\tilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \tilde{\mathcal{L}}(\mathbf{w}_t) \n\leq -\eta_2 \langle \nabla \tilde{\mathcal{L}}(\mathbf{w}_t), \hat{\mathbf{g}}_t \rangle + \frac{\eta_2^2 L}{2} ||\hat{\mathbf{g}}_t||^2 \n= \frac{\eta_2}{2} ||\nabla \tilde{\mathcal{L}}(\mathbf{w}_t) - \hat{\mathbf{g}}_t||^2 - \frac{\eta_2}{2} ||\nabla \tilde{\mathcal{L}}(\mathbf{w}_t)||^2 - \frac{\eta_2 (1 - \eta_2 L)}{2} ||\hat{\mathbf{g}}_t||^2 \n\stackrel{(a)}{\leq} \eta_2 ||\nabla \tilde{\mathcal{L}}(\mathbf{w}_t) - \nabla \mathcal{L}(\mathbf{w}_t)||^2 + \eta_2 ||\hat{\mathbf{g}}_t - \nabla \mathcal{L}(\mathbf{w}_t)||^2 - \eta_2 \mu (\tilde{\mathcal{L}}(\mathbf{w}_t) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*)) \n\stackrel{(b)}{\leq} \eta_2 \left( 2 \delta_y^2 G^2 + \left( 1 + \sqrt{3 \log \frac{1}{\delta''}} \right)^2 \frac{8G^2}{m_2} \right) - \eta_2 \mu (\tilde{\mathcal{L}}(\mathbf{w}_t) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*)),
$$
\n(40)

where (a) uses the facts that Young's inequality, η<sup>2</sup> ≤ 1/L and the Assumption of Le satisfying the property in Definition [3;](#page-3-1) (b) uses inequality [\(39\)](#page-20-0) and the same analysis of [\(34\)](#page-16-0) in Lemma [1.](#page-3-0) It is easy to verify that for any t ∈ {T<sup>1</sup> + 1, . . . , T<sup>1</sup> + n/m2}, we have, with a probability (1 − δ ′′) n/m2 , we have

$$
\tilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*)
$$
\n
$$
\leq (1 - \eta_2 \mu) \left( \tilde{\mathcal{L}}(\mathbf{w}_t) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*) \right) + \eta_2 \left( G^2 \delta_y^2 + \left( 1 + \sqrt{3 \log \frac{1}{\delta''}} \right)^2 \frac{8G^2}{m_2} \right)
$$
\n
$$
\leq (1 - \eta_2 \mu)^t \left( \tilde{\mathcal{L}}(\mathbf{w}_{T_1 + 1}) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*) \right) + \eta_2 \left( G^2 \delta_y^2 + \left( 1 + \sqrt{3 \log \frac{1}{\delta''}} \right)^2 \frac{8G^2}{m_2} \right) \sum_{i=0}^{t-1} (1 - \eta_2 \mu)^i
$$
\n
$$
\leq \frac{\delta_y^2 G^2}{\mu} + \frac{1}{\mu} \left( G^2 \delta_y^2 + \left( 1 + \sqrt{3 \log \frac{1}{\delta''}} \right)^2 \frac{8G^2}{m_2} \right),
$$

where the last inequality is due to (1 − η2µ) <sup>t</sup> ≤ 1, P<sup>t</sup>−<sup>1</sup> <sup>i</sup>=0(1 − η2µ) <sup>i</sup> ≤ 1 η2µ , and [\(38\)](#page-19-0). Let δ ′′ = δ 2n/m2 , if we choose m<sup>2</sup> such that m<sup>2</sup> ≥ 1 + <sup>q</sup> 3 log <sup>2</sup><sup>n</sup> m2δ 2 4 δ 2 y , for example,

$$
m_2 = \left(1 + \sqrt{3\log\frac{2n}{\delta}}\right)^2 \frac{4}{\delta_y^2},
$$

then with a probability 1 − δ, for any t ∈ {T<sup>1</sup> + 1, . . . , T<sup>1</sup> + n/m2} we have

$$
\widetilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_*) \le \frac{4\delta_y^2 G^2}{\mu}.
$$

Therefore, w<sup>t</sup> ∈ A(8γ0) for any t ∈ {T<sup>1</sup> + 2, . . . , T<sup>1</sup> + n/m<sup>2</sup> + 1}. Following the standard analysis in Appendix [D,](#page-17-0) we have

$$
\mathrm{E}\left[\mathcal{L}(\mathbf{w}_{T_1+n/m_2+1})-\mathcal{L}(\mathbf{w}_*)\right] \leq \frac{G^2L}{4n\mu_c^2} + \frac{G^2L}{4n\mu_c^2}\log\left(\frac{4n\mu_c^2(\mathcal{L}(\mathbf{w}_1)-\mathcal{L}(\mathbf{w}_*))}{G^2L}\right),
$$

where µ<sup>c</sup> = µ(8γ0).

# <span id="page-21-0"></span>G Optimal Solutions of La(w) and Lc(w)

By the definition of ℓ<sup>a</sup> in [\(14\)](#page-5-1) and P<sup>x</sup> = Pxe, we know

<span id="page-21-3"></span>
$$
\ell_a(\widetilde{\mathbf{y}}, f(\widetilde{\mathbf{x}}; \mathbf{w}))
$$
\n
$$
= \min_{\|\mathbf{z} - \widetilde{\mathbf{y}}\| \le \delta_y} \ell(\mathbf{z}, f(\mathbf{x}; \mathbf{w}))
$$
\n
$$
\le \ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w}))
$$
\n(41)

since <sup>k</sup><sup>y</sup> <sup>−</sup> <sup>y</sup>ek ≤ <sup>δ</sup>y. Therefore, by [\(15\)](#page-5-3), [\(41\)](#page-21-3) and <sup>P</sup><sup>x</sup> <sup>=</sup> <sup>P</sup>x<sup>e</sup> we have

$$
\mathcal{L}_a(\mathbf{w})
$$
\n
$$
= \mathbf{E}_{\mathbf{y}}[\mathcal{L}_a(\mathbf{w})]
$$
\n
$$
= \mathbf{E}_{(\mathbf{x}, \widetilde{\mathbf{y}}, \mathbf{y})} [\ell_a(\widetilde{\mathbf{y}}, f(\mathbf{x}; \mathbf{w}))]
$$
\n
$$
\leq \mathbf{E}_{(\mathbf{x}, \widetilde{\mathbf{y}}, \mathbf{y})} [\ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w}))]
$$
\n
$$
= \mathbf{E}_{(\mathbf{x}, \mathbf{y})} [\ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w}))]
$$
\n
$$
= \mathcal{L}(\mathbf{w}).
$$
\n(42)

Since ℓ is a non-negative loss function, then we know

$$
0 \leq \mathcal{L}_a(\mathbf{w}_*) \leq \mathcal{L}(\mathbf{w}_*) = 0,
$$

which implies that

$$
\mathcal{L}_a(\mathbf{w}_*) = 0,
$$

and thus

$$
\mathcal{L}_a(\mathbf{w}_*) \leq \mathcal{L}_a(\mathbf{w}), \quad \forall \mathbf{w}.
$$

Therefore, w<sup>∗</sup> also minimizes La(w).

On the other hand, by [\(16\)](#page-5-2) we know

$$
\mathcal{L}_c(\mathbf{w}_*) = \lambda \mathcal{L}(\mathbf{w}_*) + (1 - \lambda) \mathcal{L}_a(\mathbf{w}_*) = 0.
$$

Therefore,

$$
\mathcal{L}_c(\mathbf{w}_*) \leq \mathcal{L}_c(\mathbf{w}), \quad \forall \mathbf{w},
$$

i.e, w<sup>∗</sup> also minimizes Lc(w), indicating that Lc(w) shares the same minimizer as L(w).

## <span id="page-21-2"></span>H Algorithm MixLoss and Proof of Theorem [2](#page-6-2)

We present the details of update steps for MixLoss and its convergence analysis in this section.

#### <span id="page-21-1"></span>Algorithm 3 MixLoss

1: Input: λ 2: Initialize: w<sup>1</sup> ∈ R <sup>D</sup>, η > 0 3: for t = 1, 2, . . . , n do 4: draw an example (xt, yt) without replacement at random from original data 5: draw <sup>m</sup><sup>0</sup> examples (xet,<sup>1</sup>, <sup>y</sup>et,<sup>1</sup>), . . . ,(xet,m<sup>0</sup> , <sup>y</sup>et,m<sup>0</sup> ) at random from augmented data 6: compute <sup>g</sup>b<sup>t</sup> <sup>=</sup> <sup>λ</sup>∇ℓ(yt, f(xt; <sup>w</sup>t)) + (1 <sup>−</sup> <sup>λ</sup>) 1 m0 Pm<sup>0</sup> <sup>i</sup>=1 <sup>∇</sup>ℓa(yet,i, f(xet,i; <sup>w</sup>t)) 7: update <sup>w</sup>t+1 <sup>=</sup> <sup>w</sup><sup>t</sup> <sup>−</sup> <sup>η</sup>gb<sup>t</sup> 8: end for 9: Output: wn+1.

*Proof.* Recall that

<span id="page-22-1"></span><span id="page-22-0"></span>
$$
\mathcal{L}_c(\mathbf{w}) = \lambda \mathcal{L}(\mathbf{w}) + (1 - \lambda) \mathcal{L}_a(\mathbf{w}),
$$
\n(43)

where La(w) = E(xe,ye) [ℓa(ye, f(xe; <sup>w</sup>))] = E(xe,ye) min kz−yek≤δ<sup>y</sup> <sup>ℓ</sup>(z, f(xe; <sup>w</sup>)) and

$$
\widehat{\mathbf{g}}_t = \lambda \nabla \ell(\mathbf{y}_t, f(\mathbf{x}_t; \mathbf{w}_t)) + (1 - \lambda) \frac{1}{m_0} \sum_{i=1}^{m_0} \nabla \ell_a(\widetilde{\mathbf{y}}_{t,i}, f(\widetilde{\mathbf{x}}_{t,i}; \mathbf{w}_t)).
$$
\n(44)

By the update of <sup>w</sup>t+1 <sup>=</sup> <sup>w</sup><sup>t</sup> <sup>−</sup> <sup>η</sup>gb<sup>t</sup> and by the Assumption of <sup>L</sup><sup>c</sup> satisfying the property in Definition [2,](#page-2-5) we have [\[Nesterov,](#page-11-18) [2004\]](#page-11-18)

$$
E_t \left[ \mathcal{L}_c(\mathbf{w}_{t+1}) - \mathcal{L}_c(\mathbf{w}_t) \right]
$$
  
\n
$$
\leq - \eta E_t \left[ \langle \nabla \mathcal{L}_c(\mathbf{w}_t), \hat{\mathbf{g}}_t \rangle \right] + \frac{\eta^2 L}{2} E_t \left[ \| \hat{\mathbf{g}}_t \|^2 \right]
$$
  
\n(a)  
\n
$$
\leq - \eta (1 - \eta L) E_t \left[ \| \nabla \mathcal{L}_c(\mathbf{w}_t) \|^2 \right] + \eta^2 L E_t \left[ \| \hat{\mathbf{g}}_t - \nabla \mathcal{L}_c(\mathbf{w}_t) \|^2 \right]
$$
  
\n(b)  
\n
$$
\leq - \eta (1 - \eta L) E_t \left[ \| \nabla \mathcal{L}_c(\mathbf{w}_t) \|^2 \right] + \frac{9}{8} \lambda^2 \eta^2 L E_t \left[ \| \nabla \ell(\mathbf{y}_t, f(\mathbf{x}_t; \mathbf{w}_t)) - \nabla \mathcal{L}(\mathbf{w}_t) \|^2 \right]
$$
  
\n
$$
+ 9(1 - \lambda)^2 \eta^2 L E_t \left[ \left\| \frac{1}{m_0} \sum_{i=1}^{m_0} \nabla \ell_a(\tilde{\mathbf{y}}_{t,i}, f(\tilde{\mathbf{x}}_{t,i}; \mathbf{w}_t)) - \nabla \mathcal{L}_a(\mathbf{w}_t) \right\|^2 \right]
$$
  
\n(c)  
\n
$$
\leq - \eta (1 - \eta L) E_t \left[ \| \nabla \mathcal{L}_c(\mathbf{w}_t) \|^2 \right] + \frac{9}{2} \lambda^2 \eta^2 L G^2 + \frac{36(1 - \lambda)^2 \eta^2 L G^2}{m_0}
$$
  
\n(d)  
\n
$$
\leq - \eta (1 - \eta L) E_t \left[ \| \nabla \mathcal{L}_c(\mathbf{w}_t) \|^2 \right] + 5 \lambda^2 \eta^2 L G^2,
$$
  
\n(45)

where Et[·] is taken over random variables (xt, <sup>y</sup>t),(xet,<sup>1</sup>, <sup>y</sup>et,<sup>1</sup>), . . . ,(xet,m<sup>0</sup> , <sup>y</sup>et,m<sup>0</sup> ); (a) uses the facts that Young's inequality ka − bk <sup>2</sup> ≤ 2kak <sup>2</sup> + 2kbk <sup>2</sup> and E[gb<sup>t</sup>] = ∇Lc(wt); (b) uses the facts that [\(43\)](#page-22-0) [\(44\)](#page-22-1) and Young's inequality ka + bk <sup>2</sup> ≤ (1 + 1/c)kak <sup>2</sup> + (1 + c)kbk <sup>2</sup> with a = 8; (c) use the same analysis in [\(32\)](#page-15-3) from the proof of Lemma [1,](#page-3-0) the facts that the Assumption of L satisfying the property in Definition [1](#page-2-4) and by Jensen's inequality, we also have k∇L(w)k ≤ G, implying that k∇ℓ(y, f(x; w)) − ∇L(w)k <sup>2</sup> ≤ 4G<sup>2</sup> ; (d) holds by setting m<sup>0</sup> ≥ 72(1−λ) 2 <sup>λ</sup><sup>2</sup> since we have sufficiently large number of augmented examples. Thus, since η ≤ <sup>1</sup> 2L and by using the Assumption of L<sup>c</sup> satisfying the property in Definition [3,](#page-3-1) we have

$$
\mathrm{E}_{t}\left[\mathcal{L}_{c}(\mathbf{w}_{t+1})-\mathcal{L}_{c}(\mathbf{w}_{t})\right]\leq-\eta\mu\mathrm{E}_{t}\left[\mathcal{L}_{c}(\mathbf{w}_{t})\right]+5\lambda^{2}\eta^{2}LG^{2},
$$

and therefore

$$
E_n \left[ \mathcal{L}_c(\mathbf{w}_{n+1}) \right]
$$
  
\n
$$
\leq \exp(-\eta \mu n) \mathcal{L}_c(\mathbf{w}_1) + \frac{5\lambda^2 \eta LG^2}{\mu}
$$
  
\n
$$
\leq \exp(-\eta \mu n) \mathcal{L}(\mathbf{w}_1) + \frac{5\lambda^2 \eta LG^2}{\mu},
$$
\n(46)

where last inequality is due to the fact that Lc(w) ≤ L(w). In [\(46\)](#page-22-2), by choosing

<span id="page-22-3"></span><span id="page-22-2"></span>
$$
\eta = \frac{1}{\mu n} \log \frac{n \mu^2 \mathcal{L}(\mathbf{w}_1)}{\lambda^2 LG^2},
$$

we have

$$
\mathcal{E}_n\left[\mathcal{L}_c(\mathbf{w}_{n+1})\right] \le \frac{\lambda^2 LG^2}{n\mu^2} + \frac{5\lambda^2 LG^2}{n\mu^2} \log \frac{n\mu^2 \mathcal{L}(\mathbf{w}_1)}{\lambda^2 LG^2}.
$$
\n(47)

Since L(w) = <sup>1</sup> λ Lc(w) − 1−λ λ La(w), then [\(47\)](#page-22-3) becomes

$$
E_n \left[ \mathcal{L}(\mathbf{w}_{n+1}) \right]
$$
  
\n
$$
\leq \frac{\lambda LG^2}{n\mu^2} + \frac{5\lambda LG^2}{n\mu^2} \log \frac{n\mu^2 \mathcal{L}(\mathbf{w}_1)}{\lambda^2 LG^2} - \frac{1-\lambda}{\lambda} E \left[ \mathcal{L}_a(\mathbf{w}_{n+1}) \right]
$$
  
\n
$$
\leq \frac{\lambda LG^2}{n\mu^2} + \frac{5\lambda LG^2}{n\mu^2} \log \frac{n\mu^2 \mathcal{L}(\mathbf{w}_1)}{\lambda^2 LG^2},
$$
\n(48)

where the last inequality is due to λ ∈ (0, 1) and La(wn+1) ≥ La(w∗) = 0.

# I Proofs in Appendix [A](#page-13-0)

We include the proofs for Appendix section "Main Results for label-preserving Augmentation".

#### <span id="page-23-0"></span>I.1 Proof of Lemma [2](#page-13-3)

The analysis is similar to that for Lemma [1.](#page-3-0) For completeness, we include it here.

*Proof.* Following the same analysis in Lemma [1,](#page-3-0) we can have the same result as in [\(33\)](#page-16-1). That is to say, we have

$$
\mathbf{E}_{(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t)}[\mathcal{L}(\mathbf{w}_{t+1}) - \mathcal{L}(\mathbf{w}_t)] \leq \frac{\eta}{2} \left( \|\nabla \mathcal{L}(\mathbf{w}_t) - \nabla \widetilde{\mathcal{L}}(\mathbf{w}_t)\|^2 + \frac{4G^2}{m_0} - \|\nabla \mathcal{L}(\mathbf{w}_t)\|^2 \right).
$$
(49)

We have

<span id="page-23-2"></span>
$$
\|\nabla \mathcal{L}(\mathbf{w}_t) - \nabla \tilde{\mathcal{L}}(\mathbf{w}_t)\|
$$
  
\n
$$
\leq \int d\mathbf{x} \mathbf{y} \|\mathbb{P}_{\mathbf{x}}(\mathbf{x}) - \mathbb{P}_{\tilde{\mathbf{x}}}(\mathbf{x})\| \|\nabla \ell(\mathbf{y}, f(\mathbf{x}; \mathbf{w}_t))\|
$$
  
\n
$$
\leq G \int d\mathbf{x} \|\mathbb{P}_{\mathbf{x}}(\mathbf{x}) - \mathbb{P}_{\tilde{\mathbf{x}}}(\mathbf{x})\|
$$
  
\n
$$
\leq G \sqrt{2D_{KL}(\mathbb{P}_{\mathbf{x}} \|\mathbb{P}_{\tilde{\mathbf{x}}})}
$$
  
\n
$$
\stackrel{(c)}{=} G \sqrt{2\delta_P},
$$
\n(50)

where (a) is due to the Assumption of L satisfying the property in Definition [1;](#page-2-4) (b) uses Pinsker's inequality [\[Csiszar and K¨orner](#page-9-12), [2011,](#page-9-12) [Tsybakov,](#page-12-15) [2008\]](#page-12-15); (c) is due to [\(19\)](#page-13-1). With inequality [\(50\)](#page-23-1), by using the facts that η = 1/L and the Assumption of L satisfying the property in Definition [3,](#page-3-1) inequality [\(49\)](#page-23-2) becomes

<span id="page-23-1"></span>
$$
\begin{aligned} &\mathbf{E}_{(\widetilde{\mathbf{x}}_t,\widetilde{\mathbf{y}}_t)}[\mathcal{L}(\mathbf{w}_{t+1}) - \mathcal{L}(\mathbf{w}_t)] \\ &\leq &\eta \delta_P G^2 + \frac{4G^2}{m_0} - \frac{\eta}{2} \|\nabla \mathcal{L}(\mathbf{w}_t)\|^2 \\ &\leq &\eta \delta_P G^2 + \frac{4G^2}{m_0} - \eta \mu \left( \mathcal{L}(\mathbf{w}_t) - \mathcal{L}(\mathbf{w}_*) \right) \\ &\leq &2\eta \delta_P G^2 - \eta \mu \left( \mathcal{L}(\mathbf{w}_t) - \mathcal{L}(\mathbf{w}_*) \right), \end{aligned}
$$

where the last inequality is due to the selection of m<sup>0</sup> ≥ 4 ηδ<sup>P</sup> . Then we have

$$
\begin{split} & \mathbf{E}_{(\widetilde{\mathbf{x}}_t, \widetilde{\mathbf{y}}_t)}[\mathcal{L}(\mathbf{w}_{t+1}) - \mathcal{L}(\mathbf{w}_*)] \\ &\leq (1 - \eta \mu) \mathbf{E}_{(\widetilde{\mathbf{x}}_{t-1}, \widetilde{\mathbf{y}}_{t-1})}[\mathcal{L}(\mathbf{w}_t) - \mathcal{L}(\mathbf{w}_*)] + 2\eta \delta_P G^2 \\ &\leq (1 - \eta \mu)^t \left( \mathcal{L}(\mathbf{w}_1) - \mathcal{L}(\mathbf{w}_*) \right) + 2\eta \delta_P G^2 \sum_{i=0}^{t-1} (1 - \eta \mu)^i. \end{split}
$$

Due to (1 − ηµ) <sup>t</sup> ≤ exp(−tηµ) and Pt−<sup>1</sup> <sup>i</sup>=0(1 − ηµ) <sup>i</sup> ≤ 1 ηµ , when

$$
t \geq \frac{L}{\mu} \log \frac{(\mathcal{L}(\mathbf{w}_1) - \mathcal{L}(\mathbf{w}_*)) \mu}{2 \delta_P G^2},
$$

we know

$$
\mathrm{E}_{(\widetilde{\mathbf{x}}_t,\widetilde{\mathbf{y}}_t)}[\mathcal{L}(\mathbf{w}_{t+1})-\mathcal{L}(\mathbf{w}_*)] \leq \frac{4\delta_P G^2}{\mu}.
$$

#### <span id="page-24-0"></span>I.2 Proof of Proposition [2](#page-13-4)

*Proof.* By using the Assumption of Le satisfying the property in Definition [3,](#page-3-1) we have

$$
\tilde{\mathcal{L}}(\mathbf{w}_{*}) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_{*})
$$
\n
$$
\leq \frac{\|\nabla \tilde{\mathcal{L}}(\mathbf{w}_{*})\|^{2}}{2\mu}
$$
\n
$$
\stackrel{(a)}{=} \frac{\|\nabla \tilde{\mathcal{L}}(\mathbf{w}_{*}) - \nabla \mathcal{L}(\mathbf{w}_{*})\|^{2}}{2\mu}
$$
\n
$$
\stackrel{(b)}{\leq} \frac{\delta_{P}G^{2}}{\mu}
$$

where (a) is due to the definition of w<sup>∗</sup> in [\(6\)](#page-2-1) so that ∇L(w∗) = 0; (b) follows the same analysis of [\(50\)](#page-23-1) in Lemma [2.](#page-13-3) Thus we know <sup>w</sup><sup>∗</sup> ∈ A(γ) when <sup>γ</sup> <sup>≥</sup> <sup>γ</sup><sup>1</sup> := <sup>δ</sup><sup>P</sup> <sup>G</sup><sup>2</sup> µ . On the other hand, by the definition of µ(γ) in [\(12\)](#page-4-4) and the Assumption of L satisfying the property in Definition [3,](#page-3-1) we know µ(γ) ≥ µ when γ ≤ 4µ1.

#### <span id="page-24-1"></span>I.3 Proof of Theorem [3](#page-14-0)

This proof is similar to the proof of Theorem [1.](#page-4-2) For completeness, we include it here.

*Proof.* In the first stage of the proposed algorithm, we run a mini-batch SGD over the augmented data De with m<sup>1</sup> as the size of mini-batch. Using the similar analysis in [\(36\)](#page-19-1) from Theorem [1,](#page-4-2) we have, with a probability (1 − δ ′ ) t ,

$$
\widetilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_*) \leq (1 - \eta_1 \mu)^t \left( \widetilde{\mathcal{L}}(\mathbf{w}_1) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_*) \right) + \frac{\eta_1}{2} \left( 1 + \sqrt{3 \log \frac{1}{\delta'}} \right)^2 \frac{8G^2}{m_1} \sum_{i=0}^{t-1} (1 - \eta_1 \mu)^i.
$$

Due to (1 − η1µ) <sup>t</sup> ≤ exp(−tη1µ) and Pt−<sup>1</sup> <sup>i</sup>=0(1 − η1µ) <sup>i</sup> ≤ <sup>1</sup> η1µ , when

$$
t \geq T_1 := \frac{L}{\mu} \log \frac{2(\widetilde{\mathcal{L}}(\mathbf{w}_1) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_*)) \mu}{\delta_P G^2}
$$

we have

$$
\widetilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_{*}) \leq \frac{\delta_P G^2}{2\mu} + \left(1 + \sqrt{3\log\frac{1}{\delta'}}\right)^2 \frac{8G^2}{2\mu m_1}.
$$
\n(51)

,

Let δ ′ = <sup>δ</sup> 2T1 , if we choose m<sup>1</sup> such that

$$
m_1 = \left(1 + \sqrt{3\log\frac{2T_1}{\delta}}\right)^2 \frac{8}{\delta_P},
$$

then for any t ≥ T1, we have

<span id="page-25-0"></span>
$$
\widetilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_{*}) \le \frac{\delta_P G^2}{\mu}.
$$
\n(52)

In the second stage of the proposed algorithm, we run a mini-batch SGD over the original data D with m<sup>2</sup> as the size of mini-batch. Using the same analysis in [\(40\)](#page-20-1) from Theorem [1,](#page-4-2) we have

$$
\widetilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \widetilde{\mathcal{L}}(\mathbf{w}_t) \leq \eta_2 \|\nabla \widetilde{\mathcal{L}}(\mathbf{w}_t) - \nabla \mathcal{L}(\mathbf{w}_t)\|^2 + \eta_2 \|\widehat{\mathbf{g}}_t - \nabla \mathcal{L}(\mathbf{w}_t)\|^2 - \eta_2 \mu(\widetilde{\mathcal{L}}(\mathbf{w}_t) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_*)).
$$

Then by [\(39\)](#page-20-0) in the proof of Theorem [1](#page-4-2) and [\(50\)](#page-23-1) in the proof of Lemma [2,](#page-13-3) we have, with a probability 1−δ ′′ ,

$$
\widetilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \widetilde{\mathcal{L}}(\mathbf{w}_t) \leq 2\eta_2 \delta_P G^2 + \eta_2 \left(1 + \sqrt{3\log \frac{1}{\delta''}}\right)^2 \frac{8G^2}{m_2} - \eta_2 \mu(\widetilde{\mathcal{L}}(\mathbf{w}_t) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_*)).
$$

It is easy to verify that for any t ∈ {T<sup>1</sup> + 1, . . . , T<sup>1</sup> + n/m2}, we have, with a probability (1 − δ ′′) n/m2 , we have

$$
\tilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*)
$$
\n
$$
\leq (1 - \eta_2 \mu) \left( \tilde{\mathcal{L}}(\mathbf{w}_t) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*) \right) + \eta_2 \left( 2 \delta_P G^2 + \left( 1 + \sqrt{3 \log \frac{1}{\delta''}} \right)^2 \frac{8G^2}{m_2} \right)
$$
\n
$$
\leq (1 - \eta_2 \mu)^{n/m_2} \left( \tilde{\mathcal{L}}(\mathbf{w}_{T_1+1}) - \tilde{\mathcal{L}}(\tilde{\mathbf{w}}_*) \right) + \eta_2 \left( 2 \delta_P G^2 + \left( 1 + \sqrt{3 \log \frac{1}{\delta''}} \right)^2 \frac{8G^2}{m_2} \right)^n \sum_{i=0}^{n/m_2 - 1} (1 - \eta_2 \mu)^i
$$
\n
$$
\leq \frac{\delta_P G^2}{\mu} + \frac{1}{\mu} \left( 2 \delta_P G^2 + \left( 1 + \sqrt{3 \log \frac{1}{\delta''}} \right)^2 \frac{8G^2}{m_2} \right),
$$

where the last inequality is due to (1 − η2µ) <sup>t</sup> ≤ 1, P<sup>t</sup>−<sup>1</sup> <sup>i</sup>=0(1 − η2µ) <sup>i</sup> ≤ 1 η2µ , and [\(52\)](#page-25-0). Let δ ′′ = δ 2n/m2 , if we choose m<sup>2</sup> such that m<sup>2</sup> ≥ 1 + <sup>q</sup> 3 log <sup>2</sup><sup>n</sup> m2δ 2 8 δ<sup>P</sup> , for example,

$$
m_2 = \left(1 + \sqrt{3\log\frac{2n}{\delta}}\right)^2 \frac{8}{\delta_P},
$$

then with a probability 1 − δ, for any t ∈ {T<sup>1</sup> + 1, . . . , T<sup>1</sup> + n/m2} we have

$$
\widetilde{\mathcal{L}}(\mathbf{w}_{t+1}) - \widetilde{\mathcal{L}}(\widetilde{\mathbf{w}}_*) \le \frac{4\delta_P G^2}{\mu}.
$$

Therefore, w<sup>t</sup> ∈ A(4γ1) for any t ∈ {T1+2, . . . , T1+n/m2+1}. Following the similar analysis in Appendix [D,](#page-17-0) we have

$$
\mathrm{E}\left[\mathcal{L}(\mathbf{w}_{T_1+n/m_2+1})-\mathcal{L}(\mathbf{w}_*)\right] \leq \frac{G^2L}{4n\mu_e^2} + \frac{G^2L}{4n\mu_e^2}\log\left(\frac{4n\mu_e^2(\mathcal{L}(\mathbf{w}_1)-\mathcal{L}(\mathbf{w}_*))}{G^2L}\right),
$$

where µ<sup>e</sup> = µ(4γ1).