# Analyzing the Effectiveness of Image Augmentations for Face Recognition from Limited Data

Aleksei Zhuchkov *Innopolis University* Innopolis, Russia a.zhuchkov@innopolis.ru

*Abstract*—This work presents an analysis of the efficiency of image augmentations for the face recognition problem from limited data. We considered basic manipulations, generative methods, and their combinations for augmentations. Our results show that augmentations, in general, can considerably improve the quality of face recognition systems and the combination of generative and basic approaches performs better than the other tested techniques.

*Index Terms*—Data Augmentation, Face Recognition, Neural Nets

#### I. INTRODUCTION

In the past two decades, artificial neural networks(ANNs) have achieved impressive performance in numerous applications, including object recognition [\[1\]](#page-4-0), anomaly detection [\[2\]](#page-4-1), [\[3\]](#page-4-2), accident detection [\[4\]](#page-4-3), [\[5\]](#page-4-4), action recognition [\[6\]](#page-4-5)– [\[8\]](#page-5-0), scene classification [\[9\]](#page-5-1), hyperspectral image classification [\[10\]](#page-5-2), [\[11\]](#page-5-3), medical image analysis [\[12\]](#page-5-4), [\[13\]](#page-5-5), machine translation [\[14\]](#page-5-6), [\[15\]](#page-5-7), and face recognition (FR) [\[16\]](#page-5-8). However, the quality and amount of input data significantly affect the performance of these methods. Collecting and labeling a large amount of face image samples is expensive, labor-intensive, and error-prone. Small companies in the IT industry often lack access to huge corpora of data. For example, the current stateof-art FR technique FaceNet with adaptive threshold [\[17\]](#page-5-9) has high accuracy results. However, it was trained on MS-Celeb-1M dataset [\[18\]](#page-5-10) that is quite large.

Data augmentation is a technique that expands the amount and diversity of training data by applying various transformations to images [\[1\]](#page-4-0), making them a potential solution to limited-data face recognition problem. However, there is a gap in the scientific literature considering research on augmentations' theoretical and methodological aspects for FR. Although some works tried to solve the problem of making decisions related to the number, type, and intensity of augmentations [\[19\]](#page-5-11), only a small number of papers performed a rigorous comparison of the different augmentation techniques. Moreover, there is an overall lack of work on the expansion of the size of small datasets using augmentations. Accordingly, in this work, we aim to fill this gap by comparing various augmentation techniques for FR from limited data.

The rest of the paper is organized as follows. Section [II](#page-0-0) provides an overview of the related work. Section [III](#page-1-0) describes the research methodology we used. Section [IV](#page-2-0) summarizes the findings of the study and explains how they could be interpreted. Finally, Section [V](#page-4-6) makes the conclusion and proposes interesting directions for future research.

#### II. RELATED WORK

<span id="page-0-0"></span>Image augmentations can be divided into two broad categories: traditional augmentation techniques and generative. Traditional augmentation techniques (also referred to as basic transformations) include a variety of techniques e.g. geometric transformations, random crop, and color space transformations. Most common geometric transformations include flipping, cropping, rotation, translation, and noise injection [\[20\]](#page-5-12), [\[21\]](#page-5-13). These methods are easy to understand, implement, and reproduce. However, despite the popularity of geometric data augmentations, they can be applied in a limited scope and they are still the subject of research. Recently, [\[22\]](#page-5-14) showed that random erasing can make ANNs more robust to different kinds of defects on images. Random erasing is easy to implement, parameter learning free, and can be combined with different kinds of data augmentations. Color space transformations are related to the manipulation of color values of an image [\[23\]](#page-5-15). Their limitations include increased training time and higher memory utilization.

Generative approaches include such methods as Neural Style Transfer (NST) and Generative adversarial networks (GAN). NST [\[24\]](#page-5-16) synthesizes a new image using style from a style-image and content from content-image. It facilitates the transfer of textures, color temperatures, and lighting conditions, but it can lead to bias in the dataset. In addition, original NST algorithms [\[25\]](#page-5-17) are slow. GANs [\[26\]](#page-5-18) create samples similar to images from the source dataset. GAN data augmentation approach can boost the performance of the model even if generated samples do not look hyperrealistic [\[27\]](#page-5-19), making them suitable for medical problems [\[28\]](#page-5-20). The main problem of GANs is their unstable training and consumption of a considerable amount of training data, making them impractical for limited-data problems.

Nowadays, deep learning approaches are mainstream methods for face recognition tasks. The most famous neural nets used for face recognition tasks which achieved promising results on the LFW dataset are DeepFace [\[29\]](#page-5-21), DeepID [\[30\]](#page-5-22) and FaceNet [\[31\]](#page-5-23). FaceNet's authors suggested a new loss known as the Triplet Loss. It reduces the distance between an anchor image and a positive example of the same identity and increases the distance between the anchor and a negative example with a different identity. The current state-of-theart method is also based on the CNN [\[17\]](#page-5-9) and has two operations in the system: registration and recognition. In the operation of registration, an embedding is extracted from an input face image by using a FaceNet model. The threshold is assigned to the registered face during each registration (stored in the system), and the thresholds of the other stored faces are modified accordingly in the system. Recognition part is described in details in Section [IV-A.](#page-2-1)

#### III. METHODOLOGY

<span id="page-1-0"></span>As stated earlier, this work explores the performance of different data augmentation techniques (basic, generative, and their combination) for limited-data face recognition. Basic manipulations are simple and easy to perform, but they cannot show realistic face variations. On the other hand, the generative approach can produce great realistic face variations but requires extra resources. To answer our research questions we decided to split our research into five main steps to investigate this problem and find the best approach for augmentation.

- 1) Determine an approach for the realistic generation of faces with different attributes such as different hair colors or glasses.
- 2) Determine the strategy for image augmentations using basic approaches e.g. geometric, random occlusion, or color jitter.
- 3) Generate datasets using only basic manipulations, only generative approach, and the combination of these two methods.
- 4) Choose a state-of-the-art face recognition model architecture.
- 5) Train the chosen recognition model using generated datasets and measure its performance.

## *A. Dataset*

We chose the Labeled Faces in the Wild (LFW) dataset [\[32\]](#page-5-24) for training and testing in our research. It is a collection of face images created to study the issue of unconstrained face recognition. After the LFW dataset cleaning, it consists of 13156 images of faces found on the internet. Overall, there are 5718 different people in LFW. Among them, most are presented by less than 10 distinct images. Thus, this dataset can be considered small and fits our research needs. We decided to split the dataset into two different ways to produce two different training datasets. In the first split, all images of one person are either in the train or the test dataset. We called it the Unique split. It is used to test the generalization abilities of models. In the second split, if there is more than one image of a person, then one of the pictures is assigned to test and the rest belongs to the train split. We called it the Both split. If there is only one picture of a person, this image is placed into the training dataset. Each split has a train/test proportion close to 0.9/0.1.

## *B. Realistic face attributes generation*

As a realistic approach, we used a PA-GAN [\[16\]](#page-5-8) to generate attributes on faces. It is a generative approach and shows good performance in generating different face attributes such as glasses, hair of a different color, or smiles. PA-GAN model requires alignment of a dataset based on predefined landmarks. We used Dlib library [1](#page-1-1) to detect landmarks on LFW dataset. Then we used an affine transformation to transform face image from original to predefined landmarks.

For facial attribute editing this approach uses a progressive attention GAN that edits the attributes in an attention-based manner as they progress from high to low feature stages. The attribute editing is carried out in a coarse-to-fine way, which is stable and accurate, due to the progressive focus process. The authors of PA-GAN conducted experiments on the CelebA dataset and their method achieved state-of-the-art results.

## *C. Image generation using basic transformations*

We used several basic transformations to simulate changes that can happen with a face (accessories, beard, hair color, skin tone, and more). For example, random black occlusion can simulate wearing a beard or glasses. Grid distortion can change the width or height of the face which can happen to a person if he or she loses or gains some weight. Moreover, changing image parameters such as brightness, saturation or contrast can simulate skin and hair tone changing.

## *D. Generating datasets*

We generated 24 augmented images for each original image in the training datasets in each of the six produced datasets (generative, combined, and basic approaches for each of 2 training splits).

*1) Generative:* We used the official implementation [2](#page-1-2) for attributes generation on the LFW dataset. It already has a pretrained model that can generate the following features:

- age: old, young
- bangs: yes, no
- eyebrows: usual, bushy
- eyeglasses: yes, no
- gender: male, female
- facial hair: beard, mustache, no hair
- hair: bald, blond, black, brown
- mouth: open, closed
- skin: pale, usual

We generated all combinations of hair (4 options), eyeglasses (2 options) and hair face (3 options) attributes that resulted in 24 images. All other attributes were added to combinations randomly. Fig. [1](#page-2-2) shows the examples of images from aligned LFW dataset; the images were pre-processed by PA-GAN.

<span id="page-1-1"></span><sup>1</sup>http://dlib.net/

<span id="page-1-2"></span><sup>2</sup><https://github.com/LynnHo/PA-GAN-Tensorflow>

![](_page_2_Picture_0.jpeg)

**Caption:** Figure 1 illustrates the application of PA-GAN for generating diverse facial attributes on aligned images from the LFW dataset. The generated images showcase variations in attributes such as hair color, eyeglasses, and facial expressions, demonstrating the model's capability to enhance dataset diversity for face recognition tasks.

Fig. 1. Example of changing attributes on aligned image from LFW dataset using PA-GAN.

<span id="page-2-2"></span>![](_page_2_Picture_2.jpeg)

**Caption:** Figure 2 presents examples of basic image manipulations applied to aligned images from the LFW dataset using the Albumentations library. Techniques such as random occlusion, brightness adjustment, and distortion are employed to augment the dataset, aiming to improve the robustness of face recognition models under varied conditions.

Fig. 2. Example of basic manipulations application on aligned images from LFW dataset

<span id="page-2-4"></span>*2) Basic manipulations:* We used Albumentations library [3](#page-2-3) for applying basic manipulations on train images. Fig. [2](#page-2-4) shows the example of application of basic manipulations: random black occlusion, changing brightness, contrast, and saturation parameters, adaptive histogram equalization, blur and downscale of the image, horizontal flip, random distortion, Gauss noise, and grid distortion.

*3) Combining by consecutive applying:* The central idea of this method is to combine strategies. We applied basic manipulations on images produces by the generative approach. The number of augmented images did not change: it was still 24 images per face. Fig. [3](#page-2-5) shows the examples of augmented images using a combined technique.

#### *E. Choice of face recognition model*

As the face recognition approach, we chose the FaceNet model with adaptive threshold. It shows near to state-of-the-art performance on our reference LFW dataset [\[17\]](#page-5-9). We used the official implementations of FaceNet [4](#page-2-6) and Adaptive threshold [5](#page-2-7) to construct our experiments. However, we did not use a pre-trained FaceNet model because our goal is to simulate a case when only a small amount of data is available.

#### *F. Training and testing of the models*

After generating datasets we trained the FaceNet model on different datasets resulting in eight models. Two of them are trained on the Unique and Both train splits. We trained six models on generated datasets that were produced using the methods described above. Then, using the Adaptive threshold

<span id="page-2-3"></span><sup>3</sup>https://albumentations.ai/

<span id="page-2-5"></span><sup>5</sup><https://github.com/ivclab/Online-Face-Recognition-and-Authentication>

![](_page_2_Picture_13.jpeg)

**Caption:** Figure 3 displays augmented images resulting from the combination of basic manipulations applied to images generated by the generative approach. This combined technique aims to enhance the diversity of training data, potentially leading to improved performance in face recognition tasks by simulating realistic variations.

Fig. 3. Examples of images after application of basic manipulations. Source images are produced by the generative approach from aligned images from the LFW dataset.

technique we measured the performance of trained models in the face recognition task on test splits of the LFW dataset.

## IV. EVALUATION AND EXPERIMENT

#### <span id="page-2-1"></span><span id="page-2-0"></span>*A. Evaluation metrics*

Before going into details about the models' results, it is necessary to describe the metrics to compare models. The most intuitive efficiency metric is accuracy, which is in general the number of correct observations to all observations. We chose a slightly modified accuracy metric that was introduced in the work of Chou, Lee, Chan, *et al.* [\[17\]](#page-5-9) and will be described below. An experiment starts with a test dataset of size N that consists of images I = {I1, I2, ..., I<sup>N</sup> } and labels P = {P1, P2, ..., P<sup>N</sup> }. The system has an empty database D, which is used to store information about people known to the system, feature vectors obtained from their images, and thresholds T. For each image I<sup>i</sup> , feature vector Fi is calculated using FaceNet. Then, the system calculates similarity scores S(i, j) for each F<sup>i</sup> to each vector F<sup>j</sup> ∈ D where 0 < j < |D|. Afterwards, the system determines the maximum score S(i, m) out of calculated similarity scores, where m is the order of embedding F<sup>m</sup> with the highest score in the database. Next, the system compares the score S(i, m) to threshold T<sup>m</sup> associated with F<sup>m</sup> from the database and predicts label P ∗ i for F<sup>i</sup> . After that, predicted label P ∗ h is compared to the true label P<sup>i</sup> . Tab. [I](#page-3-0) shows the protocol which is used for evaluation of predictions. Fig. [4](#page-2-8) describes the whole recognition process for image I<sup>i</sup> . Finally, the image vector F<sup>i</sup> and its label P<sup>i</sup> are saved in the database after the comparison stage. The whole process repeats for the next image Ii+1. The final accuracy is defined as the average correctness of predictions for all N images as shown in Eq. [1,](#page-3-1) where TA is true accept and TR is true reject.

![](_page_2_Figure_19.jpeg)

**Caption:** Figure 4 outlines the recognition pipeline utilized in the face recognition model. It details the process of computing similarity scores between input embeddings and stored database embeddings, followed by threshold comparison to determine acceptance or rejection of input images, crucial for evaluating model performance.

<span id="page-2-8"></span>Fig. 4. Recognition pipeline. F<sup>i</sup> is an embedding of I<sup>i</sup> image from the test dataset. (1) Compute similarity scores S(i, j) of each of embeddings F<sup>j</sup> from database with input Fi. (2) Get the maximum similarity score S(i, m), where m is the order of embedding with the highest similarity score in the database. As an example, S(i, 1) has been taken as maximum on the diagram. (3) Compare maximum score S(i, m) with the threshold Tm stored in the database for embedding Fm. If the score is greater than the threshold, then the input image is accepted and assigned embedding Fm's label. P ∗ i is assigned label for Fi. Otherwise, the input image is rejected as an unknown person.

<span id="page-2-7"></span><span id="page-2-6"></span><sup>4</sup>[https://github.com/davidsandberg/facenet]( https://github.com/davidsandberg/facenet)

TABLE I EVALUATION PROTOCOL OF PERSON PREDICTIONS

<span id="page-3-0"></span>

|                                           | S(i, m) ≥ Tm              | S(i, m) < Tm      |
|-------------------------------------------|---------------------------|-------------------|
| ∗<br>P<br>i = Pi                          | True accept (TA)          | False reject (FR) |
| ∗<br>∗<br>6= Pi and P<br>i ∈ D<br>P<br>i  | Identification error (IE) | False reject (FR) |
| ∗<br>∗<br>6= Pi and P<br>i ∈/ D<br>P<br>i | False accept (FA)         | True reject (TR)  |

Note: F<sup>i</sup> is an embedding of I<sup>i</sup> image from the test dataset. P<sup>i</sup> is the true label of Fi. P ∗ i is assigned label for F<sup>i</sup> by recognition pipeline. S(i, m) represents maximum similarity that is between F<sup>i</sup> and Fm. T<sup>m</sup> is the threshold for F<sup>m</sup> stored in the database.

<span id="page-3-1"></span>
$$
Accuracy = \frac{\sum_{i=1}^{N} |TA(i)| + |TR(i)|}{N}
$$
 (1)

As we have already mentioned, experiment starts with a test dataset of size N that consists of images I = {I1, I2, ..., I<sup>N</sup> } and labels P = {P1, P2, ..., P<sup>N</sup> }. Fig. [4](#page-2-8) shows that based on a comparison of similarity score and threshold, the input image can be accepted or rejected. Thus, we have several accepted (ACP) and rejected (REJ) images when all images are passed through the system. A few metrics can be calculated based on ACP/REJ numbers and the evaluation protocol that are shown in Table [I.](#page-3-0) These metrics can give better insights into the actual performance of face recognition models and show the strengths and weaknesses of the models.

• True Accept Rate (TAR) is the ratio of correctly accepted images to the total number of accepted pictures.

$$
TAR = \frac{\sum_{i=1}^{N} |TA(i)|}{ACP}
$$
 (2)

• True Reject Rate (TRR) is the ratio of correctly rejected images to the total number of rejected pictures.

$$
TRR = \frac{\sum_{i=1}^{N} |TR(i)|}{REJ}
$$
 (3)

• False Accept Rate (FAR) is the ratio of wrongly accepted images to the total number of accepted pictures.

$$
FAR = \frac{\sum_{i=1}^{N} |FA(i)|}{ACP}
$$
 (4)

• False Reject Rate (FRR) is the ratio of wrongly rejected images to the total number of rejected pictures.

$$
FRR = \frac{\sum_{i=1}^{N} |FR(i)|}{REJ}
$$
 (5)

• Wrong Identification Rate (WAR) is the ratio of wrongly predicted labels for accepted images to the total number of accepted pictures.

$$
WAR = \frac{\sum_{i=1}^{N} |IE(i)|}{ACP}
$$
 (6)

#### *B. Results*

As mentioned in Section [III,](#page-1-0) we had two test datasets that were produced by different splits of the source LFW dataset. Additionally, we had eight models of the same architecture but trained on different datasets. We conducted 10 runs of each model on a test set that corresponds to its split and took their average accuracy. We restricted calculations of adaptive threshold only to 100 entries in the database to speed up the testing phase.

Table [II](#page-3-2) contains the metrics obtained from the test set with Unique split. The findings on Unique split show the increase in accuracy when a combined augmentation technique is used compared to baseline results. This method shows the lowest false acceptance rate among all tested methods. The basic approach demonstrated accuracy results close to the combined approach. It shows the highest true acceptance and true rejection rates. Moreover, the basic approach has the lowest false rejection and wrong identification rates. The generative augmentation method shows results considerably worse than the basic and combined approaches and they are close to the baseline.

It is worth taking a closer look at the classification results. Examples of misclassified images of identities on test dataset with Unique split are shown in Table [III.](#page-4-7) It can be noticed that in general, all models misidentified people who have similar facial features or photos that have a similar color scheme. Moreover, models show false rejection results on the same identities when photos from the database and input photo have different accessories, lightning conditions, or contrast background. However, models trained on datasets augmented by generative, combined, and basic methods are less prone to this problem.

Both split is constructed so that if there is more than one image of a person in the dataset, then one of the pictures is assigned to test and the rest belongs to the train split. Thus, the test dataset with Both split contains only one image per identity and contains identities that are in the training dataset. Based on the identity recognition scheme shown in Fig. [4,](#page-2-8) we can say that model needs to reject all input pictures to get accuracy equal to 1.0. Table [IV](#page-4-8) shows metrics derived from testing models on the test dataset with Both split. Based on received information, models trained on baseline dataset and datasets generated using generative and combined approaches show similar accuracy results. However, the basic manipulations method has degraded results compared to baseline.

<span id="page-3-2"></span>TABLE II THE AVERAGE METRICS OF THE MODELS WHICH WERE RUN 10 TIMES ON TEST DATASET WITH UNIQUE SPLIT

| Method | Baseline | Generative | Basic  | Combined | µunique |
|--------|----------|------------|--------|----------|---------|
| ACC    | 0.3574   | 0.3668     | 0.4628 | 0.4637   | 0.4127  |
| TAR    | 0.1137   | 0.0687     | 0.3562 | 0.2876   | 0.2065  |
| TRR    | 0.5550   | 0.5474     | 0.7058 | 0.6071   | 0.6038  |
| FAR    | 0.4721   | 0.4698     | 0.4356 | 0.4087   | 0.4465  |
| FRR    | 0.4449   | 0.4525     | 0.2941 | 0.3928   | 0.3960  |
| WAR    | 0.4140   | 0.4614     | 0.2081 | 0.3035   | 0.3467  |

Note: µunique is a mean of the metric for all methods.

## *C. Discussion*

This work introduces novel research on measuring the performance of augmentations for face recognition problems. Experiments were conducted to assess the performance of

<span id="page-4-7"></span>TABLE III EXAMPLES OF MISCLASSIFICATION OF IDENTITIES ON TEST DATASET WITH UNIQUE SPLIT

| Method     | Wrong identification |           | False reject |          |  |
|------------|----------------------|-----------|--------------|----------|--|
|            | Database             | Predicted | Database     | Rejected |  |
| Baseline   |                      |           |              |          |  |
| Generative |                      |           |              |          |  |
| Basic      |                      |           |              |          |  |
| Combined   |                      |           |              |          |  |

<span id="page-4-8"></span>TABLE IV THE AVERAGE METRICS OF THE MODELS PASSED 10 TIMES ON TEST DATASET WITH BOTH SPLIT

| Method | Baseline | Generative | Basic  | Combined | µboth  |
|--------|----------|------------|--------|----------|--------|
| ACC    | 0.6062   | 0.6095     | 0.3632 | 0.6169   | 0.5489 |
| TAR    | 0.0000   | 0.0000     | 0.0000 | 0.0000   | 0.0000 |
| TRR    | 1.0000   | 1.0000     | 1.0000 | 1.0000   | 1.0000 |
| FAR    | 1.0000   | 1.0000     | 1.0000 | 1.0000   | 1.0000 |
| FFR    | 0.0000   | 0.0000     | 0.0000 | 0.0000   | 0.0000 |
| WAR    | 0.0000   | 0.0000     | 0.0000 | 0.0000   | 0.0000 |

models trained on different datasets. In general, augmentations increased the performance of face recognition models. Results showed a significant increase in accuracy when the combined augmentation technique was used in the Unique split and a slight increase in Both split cases. Nevertheless, the basic approach demonstrated results close to the combined approach for the Unique split case. It means that the generative approach did not make a big contribution to the increase of accuracy of the combined approach. However, the basic approach caused a great accuracy degradation in Both split case. Meanwhile, the generative approach showed a little improvement compared to the baseline. Experiments show that the basic approach contributes to the increase of the true acceptance rate and decrease of misidentifications of identities. At the same time, the generative approach works well with the increase of the true rejection rate. A combination of approaches probably contributes to their strengths to deliver better results. Results suggest that using a combined approach can help to overcome the problem of a small training dataset. However, the basic approach can be preferable since it showed quite good results in an unconstrained unique split experiment and since it requires fewer resources than the generative one.

#### V. CONCLUSION

<span id="page-4-6"></span>The main problem for face recognition systems is diversity in the intra-subject face's images. This diversity can be caused by absence of structuring elements or the presence of components such as a beard and/or a mustache, a cap, sunglasses, etc., or occlusions of the face by background or foreground objects. The goal of the current research was to measure the effectiveness of two big classes of augmentations - basic manipulations and generative approaches on small datasets in face recognition problems and compare their performance. Thus, we generated several datasets using the two approaches mentioned above and their combination. Then we trained several face recognition models and achieved accuracy results on the test datasets. The experiments showed that augmentations can significantly boost the performance of face recognition systems. Moreover, although the combinations of augmentation approaches demonstrated a good accuracy increase, the basic approach is not far behind in performance but demands less time and hardware resources.

Several enhancements can be done for further development of the researched topic. Firstly, another dataset with more various examples can be taken. Diverse datasets for training lead to a better generalized model. Another way of research is to try more complex augmentations approaches, e.g that change face rotation angle, add more accessories, or change the background.

## REFERENCES

- <span id="page-4-0"></span>[1] A. Khan and K. Fraz, "Post-training iterative hierarchical data augmentation for deep networks," *Advances in Neural Information Processing Systems*, vol. 33, 2020.
- <span id="page-4-1"></span>[2] A. R. Rivera, A. Khan, I. E. I. Bekkouch, and T. S. Sheikh, "Anomaly detection based on zero-shot outlier synthesis and hierarchical feature distillation," *IEEE Transactions on Neural Networks and Learning Systems*, 2020.
- <span id="page-4-2"></span>[3] K. Yakovlev, I. E. I. Bekkouch, A. M. Khan, and A. M. Khattak, "Abstraction-based outlier detection for image data," in *Proceedings of SAI Intelligent Systems Conference*, Springer, 2020, pp. 540–552.
- <span id="page-4-3"></span>[4] E. Batanina, I. E. I. Bekkouch, Y. Youssry, A. Khan, A. M. Khattak, and M. Bortnikov, "Domain adaptation for car accident detection in videos," in *2019 Ninth International Conference on Image Processing Theory, Tools and Applications (IPTA)*, IEEE, 2019, pp. 1–6.
- <span id="page-4-4"></span>[5] M. Bortnikov, A. Khan, A. M. Khattak, and M. Ahmad, "Accident recognition via 3d cnns for automated traffic monitoring in smart cities," in *Science and Information Conference*, Springer, 2019, pp. 256–264.
- <span id="page-4-5"></span>[6] Y. Gavrilin and A. Khan, "Across-sensor feature learning for energy-efficient activity recognition on mobile devices," in *2019 International Joint Conference on Neural Networks (IJCNN)*, IEEE, 2019, pp. 1–7.
- [7] K. Sozykin, S. Protasov, A. Khan, R. Hussain, and J. Lee, "Multi-label class-imbalanced action recognition in hockey videos via 3d convolutional neural networks," in *2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)*, IEEE, 2018, pp. 146–151.
- <span id="page-5-0"></span>[8] A. M. Khan, Y.-K. Lee, S. Lee, and T.-S. Kim, "Accelerometer's position independent physical activity recognition system for long-term activity monitoring in the elderly," *Medical & biological engineering & computing*, vol. 48, no. 12, pp. 1271–1279, 2010.
- <span id="page-5-1"></span>[9] S. Protasov, A. M. Khan, K. Sozykin, and M. Ahmad, "Using deep features for video scene detection and annotation," *Signal, Image and Video Processing*, vol. 12, no. 5, pp. 991–999, 2018.
- <span id="page-5-2"></span>[10] M. Ahmad, A. M. Khan, M. Mazzara, S. Distefano, M. Ali, and M. S. Sarfraz, "A fast and compact 3-d cnn for hyperspectral image classification," *IEEE Geoscience and Remote Sensing Letters*, 2020.
- <span id="page-5-3"></span>[11] M. Ahmad, A. M. Khan, M. Mazzara, and S. Distefano, "Multi-layer extreme learning machine-based autoencoder for hyperspectral image classification.," in *VISIGRAPP (4: VISAPP)*, 2019, pp. 75–82.
- <span id="page-5-4"></span>[12] M. Gusarev, R. Kuleev, A. Khan, A. R. Rivera, and A. M. Khattak, "Deep learning models for bone suppression in chest radiographs," in *2017 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)*, IEEE, 2017, pp. 1–7.
- <span id="page-5-5"></span>[13] A. Dobrenkii, R. Kuleev, A. Khan, A. R. Rivera, and A. M. Khattak, "Large residual multiple view 3d cnn for false positive reduction in pulmonary nodule detection," in *2017 IEEE conference on computational intelligence in bioinformatics and computational biology (CIBCB)*, IEEE, 2017, pp. 1–6.
- <span id="page-5-6"></span>[14] A. Khusainova, A. Khan, and A. R. Rivera, "Sartsimilarity, analogies, and relatedness for tatar language: New benchmark datasets for word embeddings evaluation," *arXiv preprint arXiv:1904.00365*, 2019.
- <span id="page-5-7"></span>[15] A. Valeev, I. Gibadullin, A. Khusainova, and A. Khan, "Application of low-resource machine translation techniques to russian-tatar language pair," *arXiv preprint arXiv:1910.00368*, 2019.
- <span id="page-5-8"></span>[16] Z. He, M. Kan, J. Zhang, and S. Shan, "Pa-gan: Progressive attention generative adversarial network for facial attribute editing," *arXiv preprint arXiv:2007.05892*, 2020.
- <span id="page-5-9"></span>[17] H.-R. Chou, J.-H. Lee, Y.-M. Chan, and C.-S. Chen, "Data-specific adaptive threshold for face recognition and authentication," in *2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)*, IEEE, 2019, pp. 153–156.
- <span id="page-5-10"></span>[18] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao, "Msceleb-1m: A dataset and benchmark for large-scale face recognition," in *European conference on computer vision*, Springer, 2016, pp. 87–102.
- <span id="page-5-11"></span>[19] I. Sato, H. Nishimura, and K. Yokoi, "Apac: Augmented pattern classification with neural networks," *arXiv preprint arXiv:1505.03229*, 2015.
- <span id="page-5-12"></span>[20] H. Noh, T. You, J. Mun, and B. Han, "Regularizing deep neural networks by noise: Its interpretation and

optimization," in *Advances in Neural Information Processing Systems*, 2017, pp. 5109–5118.

- <span id="page-5-13"></span>[21] A. Kwasigroch, A. Mikołajczyk, and M. Grochowski, "Deep neural networks approach to skin lesions classification—a comparative analysis," in *2017 22nd International Conference on Methods and Models in Automation and Robotics (MMAR)*, IEEE, 2017, pp. 1069– 1074.
- <span id="page-5-14"></span>[22] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, "Random erasing data augmentation.," in *AAAI*, 2020, pp. 13 001–13 008.
- <span id="page-5-15"></span>[23] A. Galdran, A. Alvarez-Gila, M. I. Meyer, C. L. Saratxaga, T. Araujo, E. Garrote, G. Aresta, P. Costa, ´ A. M. Mendonc¸a, and A. Campilho, "Data-driven color augmentation techniques for deep skin image analysis," *arXiv preprint arXiv:1703.03702*, 2017.
- <span id="page-5-16"></span>[24] X. Zheng, T. Chalasani, K. Ghosal, S. Lutz, and A. Smolic, "Stada: Style transfer as data augmentation," *arXiv preprint arXiv:1909.01056*, 2019.
- <span id="page-5-17"></span>[25] L. A. Gatys, A. S. Ecker, and M. Bethge, "Image style transfer using convolutional neural networks," in *Proceedings of the IEEE conference on computer vision and pattern recognition*, 2016, pp. 2414–2423.
- <span id="page-5-18"></span>[26] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial nets," in *Advances in neural information processing systems*, 2014, pp. 2672–2680.
- <span id="page-5-19"></span>[27] K. Chaitanya, N. Karani, C. F. Baumgartner, A. Becker, O. Donati, and E. Konukoglu, "Semi-supervised and task-driven data augmentation," in *International conference on information processing in medical imaging*, Springer, 2019, pp. 29–41.
- <span id="page-5-20"></span>[28] Y. Xue, Q. Zhou, J. Ye, L. R. Long, S. Antani, C. Cornwell, Z. Xue, and X. Huang, "Synthetic augmentation and feature-based filtering for improved cervical histopathology image classification," in *International conference on medical image computing and computerassisted intervention*, Springer, 2019, pp. 387–396.
- <span id="page-5-21"></span>[29] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, "Deepface: Closing the gap to human-level performance in face verification," in *Proceedings of the IEEE conference on computer vision and pattern recognition*, 2014, pp. 1701–1708.
- <span id="page-5-22"></span>[30] Y. Sun, X. Wang, and X. Tang, "Deep learning face representation from predicting 10,000 classes," in *Proceedings of the IEEE conference on computer vision and pattern recognition*, 2014, pp. 1891–1898.
- <span id="page-5-23"></span>[31] F. Schroff, D. Kalenichenko, and J. Philbin, "Facenet: A unified embedding for face recognition and clustering," in *Proceedings of the IEEE conference on computer vision and pattern recognition*, 2015, pp. 815–823.
- <span id="page-5-24"></span>[32] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, "Labeled faces in the wild: A database for studying face recognition in unconstrained environments," University of Massachusetts, Amherst, Tech. Rep. 07- 49, Oct. 2007.