Jason Wei Chengyu Huang Shiqi Xu Soroush Vosoughi

ProtagoLabs International Monetary Fund Dartmouth College jason@protagolabs.com

# Abstract

Traditional data augmentation aims to increase the coverage of the input distribution by generating augmented examples that strongly resemble original samples in an online fashion where augmented examples dominate training.

In this paper, we propose an alternative perspective—a *multi-task view* (MTV) of data augmentation—in which the primary task trains on original examples and the auxiliary task trains on augmented examples. In MTV data augmentation, both original and augmented samples are weighted substantively during training, relaxing the constraint that augmented examples must resemble original data and thereby allowing us to apply stronger levels of augmentation.

In empirical experiments using four common data augmentation techniques on three benchmark text classification datasets, we find that the MTV leads to higher and more robust performance improvements than traditional augmentation.

# 1 Introduction

Most data augmentation techniques aim to generate augmented examples for training that are similar to original data. In computer vision, operations such as flipping, cropping, and color jittering are both widely used and highly effective—it is selfevident that augmented examples closely resemble original data, and so we generate augmented data in an online fashion during each minibatch such that no original, unmodified examples are seen during training [\(Krizhevsky et al.,](#page-5-0) [2012;](#page-5-0) [Zagoruyko and](#page-5-1) [Komodakis,](#page-5-1) [2016;](#page-5-1) [Huang et al.,](#page-4-0) [2017\)](#page-4-0).

In language, on the other hand, even slight modifications can cause significant semantic changes, and so it is not always clear whether augmented examples resemble original data. Despite this uncertainty, many augmentation techniques in NLP still

#### Traditional Data Augmentation

- Intuition: Increase coverage of input distribution by using augmented examples for training.
- Guideline: Augmented examples should be similar to original data.
- Training: Dominated by augmented examples that are generated stochastically.

#### Multi-Task View (MTV) of Data Augmentation

- Intuition: Auxiliary task of classifying augmented examples acts as regularization for the primary task of classifying original examples.
- Guideline: It might be a good idea for augmented samples to resemble original data, but they can be anything that boosts performance.
- Training: Both original and augmented data receive substantive weighting during training.

Table 1: Summary of traditional data augmentation versus MTV data augmentation.

generate examples stochastically and ignore original data [\(Zhang et al.,](#page-5-2) [2015;](#page-5-2) [Sennrich et al.,](#page-5-3) [2016a;](#page-5-3) [Xie et al.,](#page-5-4) [2017;](#page-5-4) [Li et al.,](#page-5-5) [2017;](#page-5-5) [Kobayashi,](#page-4-1) [2018;](#page-4-1) [Wang et al.,](#page-5-6) [2018\)](#page-5-6). When it is unclear whether augmented examples resemble original data—as is often the case—is it wise to neglect the original training data?

Our paper questions this practice by proposing to include original data during training. Specifically, we make two contributions:

- 1. We propose a *multi-task view of data augmentation* (MTV data augmentation), which trains on both original and augmented examples and therefore allows us to relax the constraint that augmented examples must resemble original data. The MTV facilitates augmentation using a higher strength parameter.
- 2. We show empirically that four common data augmentation techniques provide higher and more robust performance gains using the MTV compared with traditional augmentation.

# <span id="page-1-1"></span>2 Traditional Data Augmentation[1](#page-1-0)

Situation. During regular training, the canonical maximum likelihood objective minimizes the cost of the original training set JO:

$$
J_{\mathbf{O}}(\theta) = \mathbb{E}_{x,y \sim \hat{\mathbf{p}}(X,Y)} \left[ -\log \mathbf{p}_{\theta}(y|x) \right],
$$

where ˆp(X, Y ) is the empirical distribution of training pairs x, y and pθ(y∣x) is the parameterized model that we aim to learn (e.g., a neural network). As ˆp(X, Y ) is typically the observed data, it will likely have some mismatch with the true data distribution p(X, Y ). When the mismatch is dramatic for instance, when ˆp(X, Y ) does not sufficiently cover the training space—model performance will likely suffer.

Remedy. In practice, we often use data augmentation to mitigate the inadequacy of ˆp(X, Y ) by providing additional training data. We generate an augmented distribution q(X, ˆ Yˆ ) and now minimize the cost of this augmented training set Jaug:

$$
J_{\text{aug}}(\theta) = \mathbb{E}_{x, y \sim \mathbf{q}(\hat{X}, \hat{Y})} \left[ -\log \mathbf{p}_{\theta}(y|x) \right].
$$

As we now optimize solely on q(X, ˆ Yˆ ), our goal is to find (x, ˆ yˆ) pairs that are likely to fall in the true distribution p. Assuming the smoothness of p, similar (x, y) pairs will have similar probabilities, and therefore if an augmented example is more similar to an observed example, it is more likely to be sampled under the true distribution. In other words, good augmented examples resemble the observed data, and we aim to find them. Conversely, if an augmented example diverges too far from any observed data, it is likely invalid and thus harmful for training; we don't want to train on these examples.

The majority of prior work follows this framework of augmented examples resembling real data. As popular techniques, semantic noising substitutes tokens with synonyms [\(Wang and Yang,](#page-5-7) [2015;](#page-5-7) [Zhang et al.,](#page-5-2) [2015;](#page-5-2) [Li et al.,](#page-5-5) [2017\)](#page-5-5); Pervasive Dropout randomly removes words from the input sequence [\(Sennrich et al.,](#page-5-3) [2016a\)](#page-5-3); and SwitchOut (for machine translation) replaces some words in both source and target sentences with other words from their corresponding vocabularies [\(Wang et al.,](#page-5-6) [2018\)](#page-5-6).

Moreover, most of these techniques perform augmentation on every training example in an online fashion, implicitly assuming that augmented examples so closely resemble original data that directly

training on original examples is not even worth considering. As we shall see in the next section, adding in these original examples during training might actually be a worthwhile idea.

# 3 MTV Data Augmentation

Multi-task optimization jointly trains on a primary task and one or more auxiliary tasks—the intuition is that requiring an algorithm to also learn an auxiliary task can act as better regularization than penalizing all complexity uniformly. Prior work has found that multi-task models work particularly well when the tasks are similar, but can also improve performance even on unrelated tasks [\(Paredes et al.,](#page-5-8) [2012;](#page-5-8) [Hajiramezanali et al.,](#page-4-2) [2018\)](#page-4-2).

We propose a multi-task view of data augmentation that has a primary task that optimizes regular training on original examples and an auxiliary task that optimizes training on augmented data. This MTV jointly optimizes the primary and auxiliary task(s) using a weighted cost function so that both original and augmented data receive substantial weight during training:

$$
J(\theta) = \gamma_{\rm O} \cdot J_{\rm O}(\theta) + \gamma_{\rm aug} \cdot J_{\rm aug}(\theta) ,
$$

where γ<sup>O</sup> is the weight of original data and γaug is the weight of augmented data, and γ<sup>O</sup> + γaug = 1. In this context, observe that vanilla training uses γ<sup>O</sup> = 1 and γaug = 0, and traditional data augmentation uses γ<sup>O</sup> = 0 and γaug = 1.

The MTV gives us an important freedom that is not offered by the traditional data augmentation framework. Since traditional data augmentation only trains on augmented examples, performance suffers detrimentally when augmented data differs too much from the true distribution therefore, most studies aim to generate augmented examples that resemble original data. MTV data augmentation, however, jointly trains on both original and augmented data, thereby allowing us to relax the constraint that original and augmented examples come from the same distribution. In fact, accepting that the original and augmented distributions might differ or could even be unrelated—as work in multi-task learning has done [\(Paredes et al.,](#page-5-8) [2012;](#page-5-8) [Hajiramezanali et al.,](#page-4-2) [2018;](#page-4-2) [Rai and Daume´,](#page-5-9) [2010\)](#page-5-9)—liberates us to apply stronger levels of data augmentation, which, as we will demonstrate in the next section, leads to higher and more robust performance.

<span id="page-1-0"></span><sup>1</sup>We closely follow the intuition and notation of Wang et al. [\(2018\)](#page-5-6)

# 4 Experiments

This section compares multi-task view augmentation to traditional augmentation for various datasets and augmentation techniques.

## 4.1 Experimental Setup

Datasets. We conduct experiments on three text classification tasks often used as benchmarks [\(Kim,](#page-4-3) [2014\)](#page-4-3): (1) Stanford Sentiment Treebank (SST2) [\(Socher et al.,](#page-5-10) [2013\)](#page-5-10) of movie reviews classified as positive/negative, (2) subjectivity/objectivity dataset (SUBJ) [\(Pang and Lee,](#page-5-11) [2004\)](#page-5-11), where sentences are classified as either subjective or objective, and (3) question type dataset (TREC) [\(Li and](#page-5-12) [Roth,](#page-5-12) [2002\)](#page-5-12), in which questions ask for either a description, entity, abbreviation, human, location, or number.

Models and Experimental Procedures. For text classification, we use BERT [\(Devlin et al.,](#page-4-4) [2019\)](#page-4-4) (bert-base-uncased from HuggingFace) to extract features by averaging the last hidden states of the input tokens. To reduce the number of model hyperparameters and save computation time, we classify these features using a linear SVM trained for 1000 epochs.[2](#page-2-0) Since training data size depends on the amount of augmented data, we adjust the number of training epochs so that all models receive the same number of updates. All experiments are run for five random seeds. Our baseline models without data augmentation achieved 84.5%, 93.1%, and 83.9% accuracy respectively on the SST2, SUBJ, and TREC tasks.

Augmentation Techniques. In this paper, we experiment with four simple and common data augmentation techniques studied in [Wei and Zou](#page-5-13) [\(2019\)](#page-5-13): (1) Token Substitution [\(Zhang et al.,](#page-5-2) [2015\)](#page-5-2) replaces words with WordNet [\(Miller,](#page-5-14) [1995\)](#page-5-14) synonyms; (2) Pervasive Dropout [\(Sennrich et al.,](#page-5-3) [2016a\)](#page-5-3) applies word-level dropout; (3) Token Injection [\(Wei and Zou,](#page-5-13) [2019\)](#page-5-13) insert a synonym of a random token in the sequence into a random position in that sequence; (4) Positional Shuffling [\(Wei and Zou,](#page-5-13) [2019\)](#page-5-13) randomly chooses two tokens and swaps their positions. For all four techniques, a parameter α indicates *augmentation strength* by dictating how many perturbations are performed. For a given α, we perform n =α l perturbations, where l is the sequence length.

<span id="page-2-1"></span>Aug. Technique MTV Best α Avg. Boost (∆MTV)

| Token Substitution   | ✗ | 0.05 | 1.3% | -       |
|----------------------|---|------|------|---------|
|                      | ✓ | 0.3  | 2.1% | (+0.8%) |
| Pervasive Dropout    | ✗ | 0.1  | 1.8% | -       |
|                      | ✓ | 0.4  | 2.5% | (+0.7%) |
| Token Injection      | ✗ | 0.05 | 0.7% | -       |
|                      | ✓ | 0.5  | 2.2% | (+1.5%) |
| Positional Shuffling | ✗ | 0.05 | 1.4% | -       |
|                      | ✓ | 0.4  | 2.5% | (+1.1%) |

Table 2: Average performance boost on three text classification tasks for four augmentation techniques using the best-performing augmentation strength from α ∈ {0.05, 0.1, 0.2, 0.3, 0.4, 0.5}. Traditional data augmentation works best at low α, whereas MTV data augmentation provides the strongest performance for high α. ∆MTV indicates additional boost from using the MTV compared with traditional augmentation.

#### 4.2 Stronger augmentation for more gains

Table [2](#page-2-1) summarizes results for data augmentation in the MTV using γ<sup>O</sup> = γaug = 0.5 compared with traditional augmentation for the bestperforming augmentation strength from α ∈ {0.05, 0.1, 0.2, 0.3, 0.4, 0.5}. In the traditional framework, pervasive dropout had the strongest performance boost of 1.8% using α = 0.1. The MTV, however, allowed for stronger augmentation (i.e., α ≥ 0.3) that resulted in all four techniques to achieving boosts of more than 2.0%.

Perhaps strikingly, token injection and positional shuffling, which are less intuitive and not as commonly used as token substitution and pervasive dropout, achieve the strongest gains (> 1.0%) from using the MTV. One potential reason for this is that, compared with token substitution and pervasive dropout, token injection and positional shuffling are non-destructive in that they do not remove any of the original words, and so the nature of examples augmented at high α could be more conducive for the MTV.

## 4.3 More-robust gains at high α

When using data augmentation with high α, high levels of noising are employed and augmented data are therefore more likely to diverge from their original examples. Figure [1](#page-3-0) takes a closer look at how performance is affected by varying α. Whereas traditional augmentation often negatively affected performance at high α, the multi-task view, which jointly optimizes the original distribution, had robust performance gains at high augmentation strengths.

<span id="page-2-0"></span><sup>2</sup> This setup is not state-of-the-art but allows for experiments to be performed on CPU.

<span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)

**Caption:** Figure 1 illustrates the performance boosts across three text classification tasks using traditional and multi-task view (MTV) frameworks with four augmentation techniques. Traditional methods show diminishing returns at high augmentation strengths (α), while MTV maintains robust performance gains, highlighting its effectiveness in leveraging stronger augmentations for improved outcomes.

Figure 1: Performance boosts on three tasks using the traditional and multi-task view (MTV) frameworks for four data augmentation techniques: Token Substitution [\(Zhang et al.,](#page-5-2) [2015\)](#page-5-2) (A), Pervasive Dropout [\(Sennrich](#page-5-3) [et al.,](#page-5-3) [2016a\)](#page-5-3) (B), Token Injection [\(Wei and Zou,](#page-5-13) [2019\)](#page-5-13) (C), Positional Shuffling [\(Wei and Zou,](#page-5-13) [2019\)](#page-5-13) (D). In the traditional framework, improvements are largest when augmentation strength α is small, with performance deteriorating for large α. The MTV, on the other hand, jointly optimizes for both original and augmented data, leveraging higher α to provide higher and more robust performance gains.

<span id="page-3-1"></span>![](_page_3_Figure_2.jpeg)

**Caption:** Figure 2 depicts the performance boost (%) with varying augmentation strengths (α) and original data weights (γO) during training. Traditional augmentation achieves modest gains with low α and no original data, while MTV allows for stronger augmentations and improved performance with balanced weighting of original and augmented data, demonstrating the benefits of joint optimization.

Figure 2: Performance boost (%) with varying augmentation strengths α and weights of original data γ<sup>O</sup> during training. Traditional data augmentation (yellow solid box in lower left) uses modest augmentation strength (α = 0.05, 1) with no original examples for training (γ<sup>O</sup> = 0). The MTV data augmentation approach (green dashed box) suggests substantive weighting of original examples (e.g., γ<sup>O</sup> = 0.5) which allows for much stronger augmentation (e.g., α ≥ 0.3).

# 4.4 Choosing γ<sup>O</sup> and γaug weighting

As our experiments so far have used the MTV with balanced weighting of original and augmented data (γ<sup>O</sup> = γaug = 0.5), in this section we explore different weightings of γ<sup>O</sup> and γaug. Figure [2](#page-3-1) shows these results averaged over all three datasets and

all four augmentation techniques. Traditional data augmentation, which uses modest augmentation strength (e.g., α ∈ {0.05, 0.1}) and does not train on original data (γ<sup>O</sup> = 0.0), achieves reasonable performance gains. As expected, when stronger augmentations were applied (e.g., α ≥ 0.4), training with only augmented data hurts performance. When training on both augmented and original data, however, performance improved with stronger augmentation and remained robust for varying augmentation strengths 0.2 ≤ α ≤ 0.5 and original data weights 0.3 ≤ γ<sup>O</sup> ≤ 0.7.

# 5 Further Related Work

Prior work on data augmentation, to our knowledge, generally follows the traditional data augmentation framework. In addition to the methods mentioned in §[2](#page-1-1), Xie et al. [\(2017\)](#page-5-4) replaced words with samples from the unigram frequency distribution; Yu et al. [\(2018\)](#page-5-15) translated English sentences to French and back to English (backtranslation); and Kobayashi [\(2018\)](#page-4-1) replaced words with other words based on a language model. All these methods could potentially be formulated in the MTV.

Some prior work has also drawn connections between seeing data augmentation as multiple tasks. Similar to how we optimize augmented data as a separate task, [Meyerson and Miikkulainen](#page-5-16) [\(2018\)](#page-5-16)

created fake tasks by using multiple distinct decoders to train a shared structure to solve the same problem in different ways. In machine translation, [Sennrich et al.](#page-5-17) [\(2016b\)](#page-5-17) used monolingual training examples as parallel examples with an empty source side, noting that their setup could be seen as multi-task learning with the tasks as translation with known sources and language modeling with unknown sources. Compared with these papers that create multiple tasks in very specialized scenarios, the multi-task view that we have presented here can be used for any type of text data augmentation.

To be clear, our study is not the first to mix original and augmented data in training. For instance, [Wang and Yang](#page-5-7) [\(2015\)](#page-5-7) use a ratio of 1:5 original to augmented examples, but this weight of original data is much smaller than the 0.3 ≤ γ<sup>O</sup> ≤ 0.7 that we advocate for. [Sennrich et al.](#page-5-17) [\(2016b\)](#page-5-17) also include original data when training with backtranslation augmentation, but the given ratios of original and augmented data they use appear to dictated by the speed of their back-translation models rather than an intentionally-motivated design choice. We see our work as the first to explicitly formulate the MTV, advocate for a joint optimization function, and comprehensively explore its implications on common text augmentation techniques.

As a limitation, our study has focused on labelpreserving augmentation techniques, and our line of reasoning may not apply when augmentation techniques intentionally change the label. Moreover, we have only studied text classification with simple models using task-agnostic augmentation techniques. Future work in this direction could experiment with larger-scale models or study taskspecific augmentation.

# 6 Conclusions

We have proposed a multi-task view that gives both original and augmented examples substantial weight during training, contrasting prior work that performs stochastic data augmentation and ignores original training data. For four common augmentation techniques, we found experimentally that this alternative view allows for stronger levels of augmentation, which in turn leads to better and more robust performance than traditional augmentation. We hope our paper inspires future work using text data augmentation to think more explicitly about how much augmented examples resemble original data and consider substantive weighting of original data when using data augmentation to improve model performance.

To close, we leave the enthusiastic reader with one last thought. Most existing text data augmentation techniques have obediently followed the paradigm from computer vision of generating augmented examples that are similar to the original data. Who's to say that's how data augmentation ought to work in NLP? In this paper, we've shown how to search for relative freedom from this constraint, simply by taking a different view of the underlying assumptions. Now, a bigger question arises on the horizon—what new text augmentation techniques are unlocked when augmented data are not forced to resemble the original?

# References

- <span id="page-4-4"></span>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. [BERT: Pre-training of](https://doi.org/10.18653/v1/N19-1423) [deep bidirectional transformers for language under](https://doi.org/10.18653/v1/N19-1423)[standing.](https://doi.org/10.18653/v1/N19-1423) In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
- <span id="page-4-2"></span>Ehsan Hajiramezanali, Siamak Zamani Dadaneh, Alireza Karbalayghareh, Mingyuan Zhou, and Xiaoning Qian. 2018. [Bayesian multi-domain learning](https://papers.nips.cc/paper/8125-bayesian-multi-domain-learning-for-cancer-subtype-discovery-from-next-generation-sequencing-count-data.pdf) [for cancer subtype discovery from next-generation](https://papers.nips.cc/paper/8125-bayesian-multi-domain-learning-for-cancer-subtype-discovery-from-next-generation-sequencing-count-data.pdf) [sequencing count data.](https://papers.nips.cc/paper/8125-bayesian-multi-domain-learning-for-cancer-subtype-discovery-from-next-generation-sequencing-count-data.pdf) In *Proceedings of the 32nd International Conference on Neural Information Processing Systems*, NIPS'18, page 9133–9142, Red Hook, NY, USA. Curran Associates Inc.
- <span id="page-4-0"></span>Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. 2017. [Densely connected con](https://arxiv.org/abs/1608.06993)[volutional networks.](https://arxiv.org/abs/1608.06993) In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 4700–4708.
- <span id="page-4-3"></span>Yoon Kim. 2014. [Convolutional neural networks](https://doi.org/10.3115/v1/D14-1181) [for sentence classification.](https://doi.org/10.3115/v1/D14-1181) In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 1746–1751, Doha, Qatar. Association for Computational Linguistics.
- <span id="page-4-1"></span>Sosuke Kobayashi. 2018. [Contextual augmentation:](https://doi.org/10.18653/v1/N18-2072) [Data augmentation by words with paradigmatic re](https://doi.org/10.18653/v1/N18-2072)[lations.](https://doi.org/10.18653/v1/N18-2072) In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)*, pages 452–457, New Orleans, Louisiana. Association for Computational Linguistics.
- <span id="page-5-0"></span>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. [Imagenet classification with deep con](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)[volutional neural networks.](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) In *Advances in neural information processing systems*, pages 1097–1105.
- <span id="page-5-12"></span>Xin Li and Dan Roth. 2002. [Learning question clas](https://doi.org/10.3115/1072228.1072378)[sifiers.](https://doi.org/10.3115/1072228.1072378) In *Proceedings of the 19th International Conference on Computational Linguistics - Volume 1*, COLING '02, pages 1–7, Stroudsburg, PA, USA. Association for Computational Linguistics.
- <span id="page-5-5"></span>Yitong Li, Trevor Cohn, and Timothy Baldwin. 2017. [Robust training under linguistic adversity.](https://www.aclweb.org/anthology/E17-2004) In *Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers*, pages 21–27, Valencia, Spain. Association for Computational Linguistics.
- <span id="page-5-16"></span>Elliot Meyerson and Risto Miikkulainen. 2018. [Pseudo-task augmentation: From deep multitask](http://proceedings.mlr.press/v80/meyerson18a.html) [learning to intratask sharing-and back.](http://proceedings.mlr.press/v80/meyerson18a.html) In *Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, ¨ Stockholm, Sweden, July 10-15, 2018*, volume 80 of *Proceedings of Machine Learning Research*, pages 3508–3517. PMLR.
- <span id="page-5-14"></span>George A Miller. 1995. Wordnet: a lexical database for english. *Communications of the ACM*, 38(11):39– 41.
- <span id="page-5-11"></span>Bo Pang and Lillian Lee. 2004. [A sentimental educa](https://doi.org/10.3115/1218955.1218990)[tion: Sentiment analysis using subjectivity summa](https://doi.org/10.3115/1218955.1218990)[rization based on minimum cuts.](https://doi.org/10.3115/1218955.1218990) In *Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics*, ACL '04, Stroudsburg, PA, USA. Association for Computational Linguistics.
- <span id="page-5-8"></span>Bernardino Romera Paredes, Andreas Argyriou, Nadia Berthouze, and Massimiliano Pontil. 2012. [Exploit](http://proceedings.mlr.press/v22/romera12.html)[ing unrelated tasks in multi-task learning.](http://proceedings.mlr.press/v22/romera12.html) In *Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics*, volume 22 of *Proceedings of Machine Learning Research*, pages 951–959, La Palma, Canary Islands. PMLR.
- <span id="page-5-9"></span>Piyush Rai and Hal Daume. 2010. ´ [Infinite predic](http://proceedings.mlr.press/v9/rai10a.html)[tor subspace models for multitask learning.](http://proceedings.mlr.press/v9/rai10a.html) In *Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics*, volume 9 of *Proceedings of Machine Learning Research*, pages 613–620, Chia Laguna Resort, Sardinia, Italy. PMLR.
- <span id="page-5-3"></span>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. [Edinburgh neural machine translation sys](https://doi.org/10.18653/v1/W16-2323)[tems for WMT 16.](https://doi.org/10.18653/v1/W16-2323) In *Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers*, pages 371–376, Berlin, Germany. Association for Computational Linguistics.
- <span id="page-5-17"></span>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016b. [Improving neural machine translation mod](https://doi.org/10.18653/v1/P16-1009)[els with monolingual data.](https://doi.org/10.18653/v1/P16-1009) In *Proceedings of the*

*54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 86–96, Berlin, Germany. Association for Computational Linguistics.

- <span id="page-5-10"></span>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. [Parsing with compo](https://www.aclweb.org/anthology/P13-1045)[sitional vector grammars.](https://www.aclweb.org/anthology/P13-1045) In *Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 455–465, Sofia, Bulgaria. Association for Computational Linguistics.
- <span id="page-5-7"></span>William Yang Wang and Diyi Yang. 2015. [That's so an](https://doi.org/10.18653/v1/D15-1306)[noying!!!: A lexical and frame-semantic embedding](https://doi.org/10.18653/v1/D15-1306) [based data augmentation approach to automatic cat](https://doi.org/10.18653/v1/D15-1306)[egorization of annoying behaviors using #petpeeve](https://doi.org/10.18653/v1/D15-1306) [tweets.](https://doi.org/10.18653/v1/D15-1306) In *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, pages 2557–2563, Lisbon, Portugal. Association for Computational Linguistics.
- <span id="page-5-6"></span>Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. [SwitchOut: an efficient data aug](https://doi.org/10.18653/v1/D18-1100)[mentation algorithm for neural machine translation.](https://doi.org/10.18653/v1/D18-1100) In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 856–861, Brussels, Belgium. Association for Computational Linguistics.
- <span id="page-5-13"></span>Jason Wei and Kai Zou. 2019. [EDA: Easy data aug](https://doi.org/10.18653/v1/D19-1670)[mentation techniques for boosting performance on](https://doi.org/10.18653/v1/D19-1670) [text classification tasks.](https://doi.org/10.18653/v1/D19-1670) In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 6382–6388, Hong Kong, China. Association for Computational Linguistics.
- <span id="page-5-4"></span>Ziang Xie, Sida I. Wang, Jiwei Li, Daniel Levy, Aim- ´ ing Nie, Dan Jurafsky, and Andrew Y. Ng. 2017. [Data noising as smoothing in neural network lan](https://openreview.net/forum?id=H1VyHY9gg)[guage models.](https://openreview.net/forum?id=H1VyHY9gg) In *5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*. OpenReview.net.
- <span id="page-5-15"></span>Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. 2018. [Qanet: Combining local convolution with](https://openreview.net/forum?id=B14TlG-RW) [global self-attention for reading comprehension.](https://openreview.net/forum?id=B14TlG-RW) In *6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*. OpenReview.net.
- <span id="page-5-1"></span>Sergey Zagoruyko and Nikos Komodakis. 2016. [Wide](https://doi.org/10.5244/C.30.87) [residual networks.](https://doi.org/10.5244/C.30.87) In *Proceedings of the British Machine Vision Conference (BMVC)*, pages 87.1–87.12. BMVA Press.
- <span id="page-5-2"></span>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. [Character-level convolutional networks for text clas](https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf)[sification.](https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf) In *Proceedings of the 28th International*

*Conference on Neural Information Processing Systems - Volume 1*, NIPS'15, page 649–657, Cambridge, MA, USA. MIT Press.