# <span id="page-0-2"></span>TeachAugment: Data Augmentation Optimization Using Teacher Knowledge

Teppei Suzuki

Denso IT Laboratory, Inc.

## Abstract

*Optimization of image transformation functions for the purpose of data augmentation has been intensively studied. In particular, adversarial data augmentation strategies, which search augmentation maximizing task loss, show significant improvement in the model generalization for many tasks. However, the existing methods require careful parameter tuning to avoid excessively strong deformations that take away image features critical for acquiring generalization. In this paper, we propose a data augmentation optimization method based on the adversarial strategy called TeachAugment, which can produce informative transformed images to the model without requiring careful tuning by leveraging a teacher model. Specifically, the augmentation is searched so that augmented images are adversarial for the target model and recognizable for the teacher model. We also propose data augmentation using neural networks, which simplifies the search space design and allows for updating of the data augmentation using the gradient method. We show that TeachAugment outperforms existing methods in experiments of image classification, semantic segmentation, and unsupervised representation learning tasks[.](#page-0-0)*

# 1. Introduction

Data augmentation is an important technique used to improve model generalization. To automatically search efficient augmentation strategies for model generalization, AutoAugment [\[9\]](#page-11-0) has been proposed. Searched data augmentation policies lead to significant generalization improvements. However, AutoAugment requires thousands of GPU hours to search for efficient data augmentation.

Recent studies [\[17,](#page-11-1) [22,](#page-12-0) [30\]](#page-12-1) have demonstrated methods leading to dramatic reductions in search costs with AutoAugment, meaning that computational costs are no longer a problem. In particular, online data augmentation optimization frameworks [\[18,](#page-11-2) [28,](#page-12-2) [51,](#page-15-0) [61\]](#page-16-0) that alternately update augmentation policies and a target network have not only reduced the computational costs but also simplified the data

<span id="page-0-1"></span>![](_page_0_Figure_9.jpeg)

**Caption:** Figure 1 illustrates the concept of TeachAugment, highlighting the difference between traditional adversarial data augmentation and the proposed method. TeachAugment generates augmented images that are adversarial for the target model while remaining recognizable for the teacher model, enhancing the informativeness of the augmented data.

Figure 1. Concept of TeachAugment. Adversarial data augmentation, a baseline method, transforms data to increase loss for the target model fθ. The augmented data are often meaningless (e.g., black-out and noise images) or difficult to recognize without any constraints. TeachAugment, the proposed method, transforms data so that they are adversarial for the target model but they are recognizable for the teacher model fθ<sup>ˆ</sup>. As a result, the augmented images will be more informative than the adversarial data augmentation.

augmentation search pipeline by unifying the search and training processes.

Many online optimization methods are based on an adversarial strategy that searches augmentation maximizing task loss for the target model, which is empirically known to improve model generalization [\[28,](#page-12-2) [38,](#page-13-0) [43,](#page-14-0) [50,](#page-15-1) [61\]](#page-16-0). However, the adversarial data augmentation is unstable without any constraint because maximizing the loss can be achieved by collapsing the inherent meanings of images, as shown in Fig. [1.](#page-0-1) To avoid the collapse, previous methods regularize augmentation based on prior knowledge and/or restrict the search range of the magnitude parameters of functions in the search space, but there are many tuned parameters that will annoy practitioners.

To alleviate the parameter tuning problem, we propose an online data augmentation optimization method using teacher knowledge, called TeachAugment. TeachAugment is also based on the adversarial data augmentation strategy, but it searches augmentation in the range where the transformed image can be recognizable for a teacher model, as

<span id="page-0-0"></span>Code: [https : / / github . com / DensoITLab /](https://github.com/DensoITLab/TeachAugment) [TeachAugment](https://github.com/DensoITLab/TeachAugment)

<span id="page-1-0"></span>shown in Fig. [1.](#page-0-1) Unlike previous adversarial data augmentation methods [\[38,](#page-13-0) [50,](#page-15-1) [51,](#page-15-0) [61\]](#page-16-0), thanks to the teacher model, TeachAugment does not require priors and hyperparameters to avoid the excessively strong augmentation that collapses the inherent meanings of images. As a result, TeachAugment does not require parameter tuning to ensure that the transformed images are recognizable.

Moreover, we propose data augmentation using neural networks that represent two functions, geometric augmentation and color augmentation. Our augmentation model applies only two transformations to data but they can represent most functions included in the search space of AutoAugment and their composite functions. The use of the neural networks has two advantages compared to conventional augmentation functions: (i) we can update the augmentation parameters using the gradient method with backpropagation, and (ii) we can reduce the number of functions in the search space from tens of functions to two functions. In particular, because of the latter advantage, practitioners only need to consider the range of the magnitude parameters for the two functions when they adjust the size of the search space for better convergence.

Contribution. Our contribution is summarized as follows: (1) We propose an online data augmentation optimization framework based on the adversarial strategy using teacher knowledge called TeachAugment. TeachAugment makes the adversarial augmentation more informative without careful parameter tuning by leveraging the teacher model to avoid collapsing the inherent meaning of images. (2) We also propose data augmentation using neural networks. The proposed augmentation simplifies the search space design and enables updating of its parameters by the gradient method in TeachAugment. (3) We show that TeachAugment outperforms previous methods, including online data augmentation [\[28,](#page-12-2) [61\]](#page-16-0) and state-of-the-art augmentation strategies [\[10,](#page-11-3) [40\]](#page-13-1) in classification, semantic segmentation, and unsupervised representation learning tasks without adjusting the hyperparameters and size of the search space for each task.

## 2. Related work

As conventional data augmentation for image data, geometric and color transformations are widely used in deep learning. Besides, advanced data augmentations [\[12,](#page-11-4)[21,](#page-11-5)[23,](#page-12-3) [54,](#page-15-2) [58,](#page-16-1) [60,](#page-16-2) [63\]](#page-16-3) have been recently developed and they have improved accuracy on image recognition tasks. Data augmentation not only enhances the image recognition accuracy, but also plays an important role in recent unsupervised representation learning [\[5–](#page-11-6)[7,](#page-11-7) [15,](#page-11-8) [19\]](#page-11-9) and semi-supervised learning [\[38,](#page-13-0) [47,](#page-14-1) [50\]](#page-15-1). While the data augmentation usually improves model generalization, it sometimes hurts performance or induces unexpected biases. Thus, one needs to manually find the effective augmentation policies based on domain knowledge to enhance model generalization.

Cubuk et al. [\[9\]](#page-11-0) proposed a method to automatically search for effective data augmentation, called AutoAugment. AutoAugment outperforms hand-designed augmentation strategies and shows state-of-the-art performances on various benchmarks. Data augmentation search has become a research trend, and many methods have been proposed [\[10,](#page-11-3) [17,](#page-11-1) [18,](#page-11-2) [28](#page-12-2)[–31,](#page-12-4) [33,](#page-13-2) [40,](#page-13-1) [45,](#page-14-2) [51,](#page-15-0) [53,](#page-15-3) [55,](#page-15-4) [61\]](#page-16-0).

We roughly categorize them into two types: a *proxy task based* method and a *proxy task free* method. The proxy task based methods [\[17,](#page-11-1)[29,](#page-12-5)[30,](#page-12-1)[53,](#page-15-3)[61\]](#page-16-0) search data augmentation strategies on *proxy tasks* that use subsets of the training data and/or small models to reduce computational costs. Thus, policy searches using the proxy task based method might be sub-optimal. The proxy task free methods [\[10,](#page-11-3)[28,](#page-12-2)[33,](#page-13-2)[40,](#page-13-1)[61\]](#page-16-0) directly search data augmentation strategies on the target network with all training data. Thus, policies obtained with this method are potentially optimal.

In proxy task free methods, several approaches, such as RandAugment [\[10\]](#page-11-3) and TrivialAugment [\[40\]](#page-13-1), randomize the parameters searched and reduce the size of the search space. Other methods, such as Adversarial AutoAugment [\[61\]](#page-16-0) and PointAugment [\[28\]](#page-12-2), update augmentation policies in an online manner, meaning that they alternately update a target network and augmentation policies. The online optimization methods simplify data augmentation optimization frameworks by unifying the search and training processes. However, these methods are fraught with minor problems. For example, PointAugment [\[28\]](#page-12-2) unjustly binds the difficulty of augmented images, Adversarial AutoAugment [\[61\]](#page-16-0) manually restricts the search space to guarantee convergence, and OnlineAugment [\[51\]](#page-15-0) has many hyperparamters for regularization. These problems stem from reliance on the adversarial strategy that searches augmentation maximizing task loss. In other words, these problems are induced to ensure that the transformed images are recognizable.

In this work, we focus on proxy task free methods updating the policies in an online manner for two reasons: (i) they can directly search data augmentation strategies on the target network with all training data, and (ii) they unify the search and training processes, simplifying the framework.

# 3. Data augmentation optimization using teacher knowledge

#### 3.1. Preliminaries

Let x ∼ X and a<sup>φ</sup> be an image sampled from dataset X and an augmentation function parameterized by φ, respectively. In conventional data augmentation, φ corresponds to the magnitude of augmentation, while in this work, it corresponds to the neural network's parameters. In general training procedures using data augmentation, the mini-

<span id="page-2-4"></span><span id="page-2-1"></span>![](_page_2_Figure_0.jpeg)

**Caption:** Figure 2 presents an overview of the training procedure for TeachAugment. The method alternates between updating the target model and the augmentation model, ensuring that the augmented images maximize the loss for the target model while being recognizable to the teacher model, thus improving model generalization.

Figure 2. Overview of training procedure for TeachAugment. Our method alternately updates the target model f<sup>θ</sup> and the augmentation model a<sup>φ</sup> (i.e., step 1 and step 2 are repeated).

batch samples are transformed by a<sup>φ</sup> and fed into a target network f<sup>θ</sup> (i.e., fθ(aφ(x))). Then, the parameters of the target network are updated to minimize task loss L in the stochastic gradient descent. This training procedure is represented as min<sup>θ</sup> E<sup>x</sup>∼X L(fθ(aφ(x)). Note that we omitted the target label because we consider not only supervised learning but also unsupervised representation learning in this work.

In addition, adversarial data augmentation searches the parameters φ maximizing the loss. The objective is defined as max<sup>φ</sup> min<sup>θ</sup> E<sup>x</sup>∼X L(fθ(aφ(x)). This objective is sometimes solved by alternately updating φ and θ [\[28,](#page-12-2) [61\]](#page-16-0). The adversarial data augmentation is empirically known to improve model generalization [\[43,](#page-14-0) [61\]](#page-16-0). However, it does not work without some regularization or restriction in the size of the search space because the maximization with respect to φ can be achieved by collapsing the inherent meanings of x. Thus, instead of using regularization based on prior knowledge, we utilized a teacher model to avoid the collapse.

#### 3.2. TeachAugment

Let fθ<sup>ˆ</sup> be the teacher model, which allows for any model as long as it is different from the target model fθ. [1](#page-2-0) In this work, we suggest two types of the teacher model, a pretrained teacher and an EMA teacher whose weights are updated as an exponential moving average of the target model's weights. We provide detailed definitions and evaluate the effect of teacher model choices in Sec. [5.2.](#page-4-0)

<span id="page-2-2"></span>The proposed objective is defined as follows:

$$
\max_{\phi} \min_{\theta} \mathbb{E}_{x \sim \mathcal{X}} \left[ L(f_{\theta}(a_{\phi}(x))) - L(f_{\hat{\theta}}(a_{\phi}(x))) \right]. \quad (1)
$$

For the target model, this objective has the same properties as the adversarial data augmentation, but the augmentation function needs to minimize the loss for the teacher model, in addition to maximizing the loss for the target model. The augmentation that is obtained in this manner avoids collapsing the inherent meanings of images because the loss for the teacher model will explode when the transformed images are unrecognizable. In other words, the introduced teacher loss requires the augmentation function to transform images so that they are adversarial for the target model in the range where they are recognizable for the teacher model.

As shown in Fig. [2,](#page-2-1) the objective is solved by alternately updating the augmentation function and the target model in the stochastic gradient descent, similar to previous methods [\[18,](#page-11-2)[28,](#page-12-2)[61\]](#page-16-0). We first update the target network for ninner steps, and then update the augmentation function. A pseudo-code can be found in the appendix. Note that the augmentation function is updated by the gradient method because our proposed augmentation using the neural networks introduced in Sec. [4](#page-3-0) is differentiable with respect to φ. We refer to the augmentation strategy following Eq. [1](#page-2-2) as *TeachAugment*.

Unlike previous methods [\[28,](#page-12-2) [51\]](#page-15-0), TeachAugment does not regularize augmentation functions based on the domain knowledge, such as cycle consistency and smoothness [\[51\]](#page-15-0). It also does not bind the difficulty of the transformed images as in [\[28\]](#page-12-2) to ensure that the transformed images are recognizable.

#### <span id="page-2-3"></span>3.3. Improvement techniques

The training procedure for TeachAugment is similar to that for generative adversarial networks (GANs) [\[14\]](#page-11-10)

<span id="page-2-0"></span><sup>1</sup> In most of our experiments, we define the teacher model as the same model as the target model but with different parameters. Thus, the same symbol f is used for the teacher model and the target model although it may be a little complicated.

<span id="page-3-2"></span>and actor-critic methods in reinforcement learning [\[26,](#page-12-6) [49\]](#page-15-5), which alternately update two networks. Practitioners in both fields have amassed a large number of strategies to mitigate instabilities and improve training [\[44\]](#page-14-3). TeachAugment also benefits from three techniques used in both fields, *experience replay*, *non-saturating loss*, and *label smoothing*. Moreover, we introduced color regularization to mitigate inconsistency between color distributions of images before and after data augmentation. The techniques introduced here are also applicable for other online methods [\[28,](#page-12-2) [51,](#page-15-0) [61\]](#page-16-0).

Non-saturating loss. For classification tasks, the loss function L is usually defined as the cross-entropy loss, L(fθ(aφ(x)) = P<sup>K</sup> <sup>k</sup>=1 −y<sup>k</sup> log fθ(aφ(x))k, where y ∈ {0, 1} <sup>K</sup> and K denote the one-hot ground truth label and the number of classes, respectively. In this case, the gradient of the first term in Eq. [\(1\)](#page-2-2) often saturates in the maximization problem with respect to φ when the target model's predictions are very confident. Thus, we use P<sup>K</sup> <sup>k</sup>=1 y<sup>k</sup> log(1 − fθ(aφ(x))k) when updating the augmentation model rather than P<sup>K</sup> <sup>k</sup>=1 −y<sup>k</sup> log fθ(aφ(x))k. This technique has been used in GANs [\[14\]](#page-11-10).

The non-saturating loss is a key factor for TeachAugment; it improves error rates of WideResNet-28-10 [\[59\]](#page-16-4) on CIFAR-100 [\[27\]](#page-12-7) from 18.7% to 17.4% (a baseline's error rate is 18.4%). Thus, we basically use the non-saturating loss for updating augmentation models in our experiments.

Experience replay. In reinforcement learning, experience replay [\[32,](#page-13-3) [39\]](#page-13-4) stores actions chosen by the actor in the past and reuses them to update the critic. We apply this technique to our method by storing the augmentation models and prioritizing them in a manner similar to prioritized experience replay [\[46\]](#page-14-4). Then, the target network is updated using the augmentation model randomly sampled from the buffer following their priorities.

Let p<sup>i</sup> be a priority of the i-th stored augmentation model. We compute the priority as p<sup>i</sup> = γ S−i , where S denotes the number of augmentation models stored in the buffer, γ denotes a decay rate, and we set γ to 0.99. In our experiments, we stored the augmentation model every nbuffer epochs. For image classification on CIFAR-10, CIFAR-100, and semantic segmentation, nbuffer was set to 10, and for other tasks and datasets, it was set to 1.

Label smoothing. Label smoothing is a technique that replaces the one-hot labels with yˆ<sup>k</sup> = (1 − )y<sup>k</sup> + /K, where ∈ [0, 1) is a smoothing parameter. For our method, the label smoothing prevents gradients from exploding when the target model's predictions are very confident under the non-saturating loss. In particular, such a situation tends to occur for easy tasks or strong target models. To mitigate exploding gradients, we used the smoothed labels for the first term in Eq. [\(1\)](#page-2-2) when updating the augmentation model. Note that for a fair comparison, we did not apply it when updating the target model.

Color regularization. In practice, the color augmentation model tends to transform the pixel colors outside the color distribution of the training data. As a result, the augmented images will be out-of-distribution data, which may hurt the recognition accuracy for the in-distribution samples. To align the color distributions before and after augmentation, we regularized the color augmentation model by introducing sliced Wasserstein distance (SWD) [\[3\]](#page-11-11) between pixel colors before and after the augmentation. SWD is a variant of Wasserstein distance that represents a distance between two distributions.

We define the color regularization term as follows:

$$
L_{\text{color}}(\{x^b\}_b^B, \{\tilde{x}^b\}_b^B) = \sum_i \text{SWD}(\{x_i^b\}_b^B, \{\tilde{x}_i^b\}_b^B), \tag{2}
$$

where {x b i } B b denotes a set of i-th pixels of images in a mini-batch with a batch size of B, and {x˜ b} B b denotes coloraugmented images defined in Eq. [\(4\)](#page-3-1). Because costs of computing SWD at each pixel position depends linearly on the image resolution, we can compute SWD for highresolution images handled in semantic segmentation with low computational resources. Then, in the stochastic gradient descent, the gradient with respect to φ in each iteration is represented as follows:

$$
\frac{\partial}{\partial \phi} \frac{1}{B} \sum_{b}^{B} \left[ L(f_{\theta}(a_{\phi}(x^{b}))) - L(f_{\tilde{\theta}}(a_{\phi}(x^{b}))) \right] - \lambda L_{\text{color}}(\{x^{b}\}_{b}^{B}, \{\tilde{x}^{b}\}_{b}^{B}), \quad (3)
$$

where λ is a hyperparameter controlling the effect of the regularization that was set to 10 in our experiments.

## <span id="page-3-0"></span>4. Data augmentation using neural networks

We propose data augmentation using neural networks parameterized by φ, which consists of two models, a color augmentation model c<sup>φ</sup><sup>c</sup> and a geometric augmentation model g<sup>φ</sup><sup>g</sup> . Thus, a<sup>φ</sup> is defined as the composite function of c<sup>φ</sup><sup>c</sup> and g<sup>φ</sup><sup>g</sup> , a<sup>φ</sup> = g<sup>φ</sup><sup>g</sup> ◦ c<sup>φ</sup><sup>c</sup> , and the parameter φ is a set of φ<sup>c</sup> and φg, φ = {φc, φg}. Our augmentation model enables updating of its parameters by the gradient method and construction of the search space with only two functions.

The augmentation procedure is illustrated in Fig. [3.](#page-4-1) Given an image <sup>x</sup> <sup>∈</sup> <sup>R</sup><sup>M</sup>×<sup>3</sup> , where M denotes the number of pixels and 3 corresponds to the RGB color channels, the color augmentation is applied with probability of p<sup>c</sup> ∈ (0, 1), and then the geometric augmentation is applied with probability of p<sup>g</sup> ∈ (0, 1).

The color augmentation is defined as follows:

<span id="page-3-1"></span>
$$
\tilde{x}_i = t(\alpha_i \odot x_i + \beta_i), \ (\alpha_i, \beta_i) = c_{\phi_c}(x_i, z, c), \tag{4}
$$

<span id="page-4-2"></span><span id="page-4-1"></span>![](_page_4_Figure_0.jpeg)

**Caption:** Figure 3 depicts the data augmentation procedure in TeachAugment, which combines color and geometric augmentations. Each transformation is applied probabilistically to the input image, allowing for a diverse range of augmented outputs while maintaining the integrity of the original data.

Figure 3. Data augmentation procedure. Our data augmentation consists of the color augmentation c<sup>φ</sup><sup>c</sup> and the geometric augmentation g<sup>φ</sup><sup>g</sup> . Each augmentation is applied to an input image x with probabilities p<sup>c</sup> and pg.

where α<sup>i</sup> , β<sup>i</sup> ∈ R <sup>3</sup> denote scale and shift parameters;  denotes the element-wise multiplication between vectors; t(·) denotes the triangle wave, t(x) = arccos(cos(xπ))/π, which ensures x˜<sup>i</sup> ∈ [0, 1]; z ∼ N (0, I<sup>N</sup> ), where N (0, I<sup>N</sup> ) denotes N-dimensional unit Gaussian distribution, and c is an optional context vector. In our experiments, we used a one-hot ground truth label as c for image classification and omitted it for other tasks. The color augmentation model can transform the input image into any image *in principle* when the augmentation model is sufficiently large because of the universal approximation theorem.

The geometric augmentation is defined as follows:

$$
\hat{x} = \text{Affine}(\tilde{x}, A + I), \ A = g_{\phi_g}(z, c), \tag{5}
$$

where Affine(˜x, A + I) denotes an affine transformation of x˜ with a parameter A + I; I ∈ R <sup>2</sup>×<sup>3</sup> denotes a matrix where ∀i, Iii = 1 and 0 otherwise, which makes the affine transformation the identity mapping Affine(x, I) = x; A ∈ R <sup>2</sup>×<sup>3</sup> denotes the residual parameter, and c and z are the same vectors used in Eq. [\(4\)](#page-3-1).

The geometric augmentation model can be defined by transformations other than the affine transformation, similar to [\[24\]](#page-12-8). However, the affine transformation can represent all geometric transformations in the search space of AutoAugment and their composite functions. Thus, we considered only the affine transformation in this work.

In addition to φ<sup>c</sup> and φg, we also learned the probabilities p<sup>c</sup> and p<sup>g</sup> by using the gradient method. However, the decision process of applying the augmentation is nondifferentiable with respect to p<sup>c</sup> and pg. To make it differentiable with respect to the probabilities, we used the same approach as in previous works [\[17,](#page-11-1) [29\]](#page-12-5). A detailed pipeline can be found in the appendix.

## 5. Experiments

We evaluate our method with three tasks. For an ablation study and comparison with existing automatic data augmentation search methods, we evaluate our method on image classification tasks. We train WideResNet-40-2 (WRN-40-2) [\[59\]](#page-16-4), WideResNet-28-10 (WRN-28-10) [\[59\]](#page-16-4), Shake-Shake (26 2×96d) [\[13\]](#page-11-12), and PyramidNet [\[16\]](#page-11-13) with ShakeDrop regularization [\[57\]](#page-16-5) on CIFAR-10 and CIFAR-100 [\[27\]](#page-12-7), and train ResNet-50 [\[20\]](#page-11-14) on ImageNet [\[11\]](#page-11-15). The training and evaluation protocols are the same settings as in previous work [\[30\]](#page-12-1).

In addition to the above experiments, we examine our method with semantic segmentation and unsupervised representation learning. For semantic segmentation, we train FCN-32s [\[34\]](#page-13-5), PSPNet [\[62\]](#page-16-6), and Deeplabv3 [\[4\]](#page-11-16) on Cityscapes [\[8\]](#page-11-17). The training protocol is the same as [\[62\]](#page-16-6). For unsupervised representation learning, we pretrain ResNet-50 [\[20\]](#page-11-14) on ImageNet [\[11\]](#page-11-15) using SimSiam [\[7\]](#page-11-7) with various data augmentation and then evaluate the model following the linear evaluation protocol [\[7\]](#page-11-7).

More details can be found in the appendix.

### 5.1. Implementation details

We construct the geometric augmentation model using a multi-layer perceptron (MLP) and the color augmentation model using two MLPs that receive an RGB vector and a noise vector as inputs and then add up the outputs. We apply the sigmoid function to the outputs of each augmentation model and normalize the outputs in a range of A ∈ (−0.25, <sup>0</sup>.25)<sup>2</sup>×<sup>3</sup> , α ∈ (0.6, 1.4), and β ∈ (−0.5, 0.5). The dimension of the noise vector z is set to 128. For stochasticity, Dropout [\[48\]](#page-15-6) is applied after the linear layers, except to the output layer. We initialize the weights of the output layer as zero to make the augmentation the identity mapping in the initial state. We use AdamW optimizer [\[35\]](#page-13-6) to train the augmentation model. All the hyperparameters of AdamW (e.g., learning rate and weight decay) are set to the PyTorch default parameters [\[42\]](#page-14-5). More details can be found in the appendix.

#### <span id="page-4-0"></span>5.2. Ablation study

Effect of teacher model choices. We first investigate the effect of teacher model choices. We trained WRN-40-2 with two types of teacher models: a teacher that was pretrained with the baseline augmentation, and an EMA teacher that is exponential moving average of the target model (i.e., <sup>ˆ</sup><sup>θ</sup> <sup>←</sup> ξ <sup>ˆ</sup><sup>θ</sup> + (1 <sup>−</sup> <sup>ξ</sup>)θ). We set the decay rate <sup>ξ</sup> to 0.999, following [\[52\]](#page-15-7). For the pretrained teacher, we pretrained WRN-40-2, which is the same model as the target model, and WRN-28- 10, which is a stronger model than the target model.

The results are shown in Tab. [1.](#page-5-0) For both datasets, the EMA teacher achieves lower error rates than the others. The

<span id="page-5-3"></span><span id="page-5-0"></span>

| Teacher   | WRN-40-2 | WRN-28-10 | EMA  |
|-----------|----------|-----------|------|
| CIFAR-10  | 4.4      | 3.7       | 3.7  |
| CIFAR-100 | 21.3     | 21.6      | 20.3 |

Table 1. Effect of teacher models. We report the error rates (%) of WideResNet-40-2 trained with various teacher models. WRN-40- 2 and WRN-28-10 are pretrained with the baseline augmentation. EMA is an exponential moving average of the target model.

<span id="page-5-1"></span>

| Dataset               | CIFAR-10 | CIFAR-100 |
|-----------------------|----------|-----------|
| All                   | 2.5      | 16.8      |
| w/o Color Reg.        | 3.0      | 16.5      |
| w/o Experience Replay | 2.7      | 17.3      |
| w/o Label Smoothing   | 3.0      | 17.3      |
| w/o all techniques    | 3.0      | 17.4      |

Table 2. Error rates (%) of WideResNet-28-10 on CIFAR-10 and CIFAR-100 with various technique choices.

augmentation model may cause overfitting to the teacher model when the pretrained teacher is used because they are not updated during training. The EMA teacher would prevent overfitting by updating parameters at the same time as the target model during training, bringing more improvement than the pretrained teachers.

Interestingly, we found the stronger teacher was not a better teacher. In fact, WRN-28-10 demonstrates error rates comparable to the EMA teacher in CIFAR-10, but in CIFAR-100, it was slightly worse than WRN-40-2.

In the rest of the experiments, we used the EMA teacher for TeachAugment because the EMA teacher not only has lower error rates but also eliminates the architecture choices of the teacher model.

Effect of stabilizing techniques. We next investigate the effect of stabilizing techniques introduced in Sec. [3.3.](#page-2-3) We trained WRN-28-10 on CIFAR-10 and CIFAR-100, with the exception of each technique.

The results are shown in Tab. [2.](#page-5-1) Except for the color regularization on CIFAR-100, all of the techniques contributes to the improvement of error rates.

We visualize the effect of the color regularization in Fig. [4.](#page-5-2) The color distributions are aligned by the color regularization. In particular, the augmented images without regularization lose brightness in many pixels with CIFAR-100. However, alignment of the color distribution improve the error rates only for CIFAR-10. This would be due to the color diversity of CIFAR-10 being narrower than CIFAR-100, which can be seen from the color distributions in Fig. [4.](#page-5-2) In other words, in the case of training without color regularization, the color augmentation produces out-of-distribution colors, and the obtained images hurt the recognition accuracy of the target model for the in-distribution samples. The transformed image from CIFAR-10 tends to be this kind of

<span id="page-5-2"></span>![](_page_5_Picture_10.jpeg)

**Caption:** Figure 4 compares augmented images and their corresponding color distributions for CIFAR-10 and CIFAR-100. It shows the impact of color regularization, demonstrating how it aligns color distributions and improves recognition accuracy by preventing out-of-distribution color transformations.

Figure 4. Augmented images and corresponding color distributions on CIFAR-10 and CIFAR-100. From left to right, original images, augmented images from the model trained without color regularization, and augmented images from the model trained with color regularization. We randomly picked 128 images and plotted their pixel colors (i.e., each point corresponds to a pixel).

an out-of-distribution sample due to the low diversity of colors.

We evaluate label smoothing with various smoothing parameter values, and the results are shown in Fig. [5.](#page-6-0) For the easy tasks of CIFAR-10, label smoothing with the large works well. As already described in Sec. [3.3,](#page-2-3) the gradient of the non-saturating loss tends to be large for easy tasks or strong models, meaning that the model's predictions tend to be very confident. Thus, a larger avoids exploding gradients and stabilizes training for CIFAR-10.

Evaluation of the proposed objective function. We evaluate the effectiveness of the proposed objective function, eq. [\(1\)](#page-2-2). As baseline methods, we replace the objective function in our framework with the loss of adversarial AutoAugment (Adv. AA) [\[61\]](#page-16-0) and PointAugment (PA) [\[28\]](#page-12-2). For a fair comparison, all of the models were trained with the same protocol and the difference between them was the objective function. The detailed setting is described in the appendix.

<span id="page-6-3"></span><span id="page-6-0"></span>![](_page_6_Figure_0.jpeg)

**Caption:** Figure 5 illustrates the effect of label smoothing on error rates for CIFAR-10 and CIFAR-100. The results indicate that label smoothing helps stabilize training, particularly for easier tasks, by preventing gradients from exploding when the target model's predictions are overly confident.

Figure 5. Effect of on label smoothing. The upper figure shows error rates on CIFAR-100, and the lower figure shows the error rates on CIFAR-10. Stars indicate achieving minimum error rates.

<span id="page-6-1"></span>

| Objective | Baseline | Adv. AA [61] | PA [28] | Ours |
|-----------|----------|--------------|---------|------|
| CIFAR-10  | 3.1      | 3.6          | 2.9     | 2.5  |
| CIFAR-100 | 18.4     | 19.4         | 17.5    | 16.8 |

Table 3. Error rates (%) of WideResNet-28-10 trained with various online optimization frameworks. All results are obtained under the same augmentation models and training protocol, except for the objective function.

The results are shown in Tab. [3.](#page-6-1) The error rates of Adv. AA degrades from the baseline because the augmentation model produces the unrecognizable images that confuse the target model. For Adv. AA, one needs to carefully tune the size of the search space to guarantee the convergence [\[61\]](#page-16-0) but we do not adjust it. In addition, for such instability, the proposed augmentation model would be unsuitable because of its property, i.e., the color augmentation model can transform the input image into any image. In other words, the proposed augmentation would work only for the methods that guarantee transformed images to be recognizable. Our method achieves better error rates than PointAugment. PointAugment controls the upper bound of the loss for the augmented data with a dynamic parameter, but our method has no such restriction. As a result, our method provides more diverse transformations and better improvements in the error rates than PointAugment. The augmented images are visualized in the appendix.

Evaluation of the proposed augmentation model. We compare the proposed augmentation model to a differentiable data augmentation pipeline (DDA) proposed in [\[17\]](#page-11-1) and augmentation models used in OnlineAugment (OA) [\[51\]](#page-15-0). The results are shown in Tab. [4.](#page-6-2) The improvement of DDA from baseline is less than that of ours. DDA uses some techniques (e.g., relaxation [\[25,](#page-12-9)[37\]](#page-13-7) and straight-through estimator [\[1\]](#page-11-18)) to make some data augmentation functions differentiable, but these techniques induce inaccurate gradi-

<span id="page-6-2"></span>

| Augmentation | Baseline | DDA  | OA   | Ours |
|--------------|----------|------|------|------|
| CIFAR-10     | 3.1      | 2.8  | 16.7 | 2.5  |
| CIFAR-100    | 18.4     | 18.0 | 26.2 | 16.8 |

Table 4. Error rates (%) of WideResNet-28-10 with DDA [\[17\]](#page-11-1), the augmentation models of OnlineAugment [\[51\]](#page-15-0) (OA) and the proposed augmentation on CIFAR-10 and CIFAR-100. Each augmentation is optimized by the TeachAugment strategy.

ents and the vanishing gradient problem that would make it difficult to learn effective augmentation in the TeachAugment framework. OA is much worse than baseline. OnlineAugment regularizes augmentation models based on prior knowledge, such as cycle consistency and smoothness, instead of bounding the search space. However, TeachAugment does not impose strong regularization. The lack of regularization would be unsuitable for augmentation with the unbounded search space. We note that the proposed augmentation is better than DDA and OA for TeachAugment, but as seen in Tab. [3,](#page-6-1) it does not work in all cases.

#### 5.3. Image classification

We compare our method to several previous methods: AutoAugment (AA) [\[9\]](#page-11-0), PBA [\[22\]](#page-12-0), Fast AA [\[30\]](#page-12-1), Faster AA [\[17\]](#page-11-1), DADA [\[29\]](#page-12-5), RandAugment (RandAug) [\[10\]](#page-11-3), OnlineAugment (OnlineAug) [\[51\]](#page-15-0), and Adv. AA [\[61\]](#page-16-0).

For CIFAR-10 and CIFAR-100, we used the smoothing parameter achieving the best error rate in the previous section. We set to 0.1 for training on ImageNet, following the tendency in Fig. [5](#page-6-0) (i.e., the smaller tends to be better for difficult tasks).

We show the comparison results in Tab. [5.](#page-7-0) Our method achieves error rates comparable to other method except for Adv. AA, which uses multiple augmented samples per mini-batch. In particular, our method achieves the lowest top-1 error rate on ImageNet among the methods without a multiple augmentation strategy.

#### 5.4. Semantic segmentation

We also evaluate our method on Cityscapes [\[8\]](#page-11-17). We implement widely used models with a ResNet-101 backbone, FCN-32s [\[34\]](#page-13-5), PSPNet [\[62\]](#page-16-6), and Deeplabv3 [\[4\]](#page-11-16). As baseline methods, we adopted RandAugment [\[10\]](#page-11-3) and TrivialAugment [\[40\]](#page-13-1). These methods also does not need careful parameter tuning because they have few or no hyperparameters.

We show the results in Tab. [6.](#page-7-1) Our method achieves the best mIoU for each model. RandAugment hurts the mIoU for FCN-32s, and TrivialAugment does not improve the mIoU for any of the models. In fact, it has been reported that TrivialAugment does not work well for tasks other than image classification [\[40\]](#page-13-1). We suspect that the search spaces of RandAugment and TrivialAugment are not suitable for

<span id="page-7-3"></span><span id="page-7-0"></span>

|                  |           | CIFAR-10    |            |           | CIFAR-100   |            | ImageNet   |
|------------------|-----------|-------------|------------|-----------|-------------|------------|------------|
|                  | WRN-28-10 | Shake-Shake | PyramidNet | WRN-28-10 | Shake-Shake | PyramidNet | ResNet-50  |
| Baseline         | 3.9       | 2.9         | 2.7        | 18.8      | 17.1        | 14.0       | 23.7/6.9   |
| Cutout [12]      | 3.1       | 2.6         | 2.3        | 18.4      | 16.0        | 12.2       | -          |
| AA [9]           | 2.6       | 2.0         | 1.5        | 17.1      | 14.3        | 10.7       | 22.4/6.4   |
| PBA [22]         | 2.6       | 2.0         | 1.5        | 16.7      | 15.3        | 10.9       | -          |
| RandAug [10]     | 2.7       | 2.0         | 1.5        | 16.7      | -           | -          | 22.4/6.2   |
| Fast AA [30]     | 2.7       | 2.0         | 1.7        | 17.3      | 14.6        | 11.7       | 22.4/6.3   |
| Faster AA [17]   | 2.6       | 2.0         | -          | 17.3      | 15.0        | -          | 23.5/6.8   |
| DADA [29]        | 2.7       | 2.0         | 1.7        | 17.5      | 15.3        | 11.2       | 22.5/6.5   |
| OnlineAug [51]   | 2.4       | -           | -          | 16.6      | -           | -          | 22.4/-     |
| Adv. AA†<br>[61] | (1.9)     | (1.9)       | (1.4)      | (15.5)    | (14.1)      | (10.4)     | (20.6/5.5) |
| Ours             | 2.5       | 2.0         | 1.5        | 16.8      | 14.5        | 11.8       | 22.2/6.3   |

Table 5. Test set top-1 error rates (%) on CIFAR-10 and CIFAR-100 and validation set top-1/top-5 error rates on ImageNet. Please note that Adv. AA† uses multiple augmented samples per mini-batch.

<span id="page-7-1"></span>

| Model     | Baseline | RA [10] | TA [40] | Ours |
|-----------|----------|---------|---------|------|
| FCN-32    | 71.7     | 71.0    | 71.3    | 72.1 |
| PSPNet    | 77.7     | 78.8    | 77.5    | 78.8 |
| Deeplabv3 | 78.4     | 79.3    | 78.9    | 79.4 |

<span id="page-7-2"></span>Table 6. Validation set mIoU (%) on Cityscapes. RA and TA denote RandAugment [\[10\]](#page-11-3) and TrivialAugment [\[40\]](#page-13-1), respectively.

| #Epochs             | 100  | 200  | 400  |
|---------------------|------|------|------|
| Baseline [7]        | 68.1 | 70.0 | 70.8 |
| RandAugment [10]    | 68.0 | 70.0 | 70.7 |
| TrivialAugment [40] | 62.7 | 68.7 | 71.3 |
| Ours                | 68.2 | 70.2 | 71.0 |

Table 7. ImageNet linear evaluation accuracy (%) for various pretraining epochs. We set batch size to 256 for pretraining.

semantic segmentation tasks and model capacities. However, our method improves mIoU for all conditions without adjusting parameters from the classification tasks.

#### 5.5. Unsupervised representation learning

Finally, we evaluate our method with unsupervised representation learning tasks using SimSiam [\[7\]](#page-11-7). To generate two different views of the same image, we used two augmentation models, a<sup>φ</sup><sup>1</sup> and a<sup>φ</sup><sup>2</sup> . The detailed settings can be found in the appendix.

The results are shown in Tab. [7.](#page-7-2) TrivialAugment causes underfitting for 100 and 200 epoch training due to the diversity of augmentation induced by the strong randomness. RandAugment does not contribute to the improvement of accuracy. Our proposed method consistently improves the accuracy for all training schedules, although the improvements are slight. This would be because the online optimization frameworks have aspects similar to curriculum learning [\[2\]](#page-11-19). In other words, our method adjusts the augmentation magnitude according to the learning progress of the target model.

## 6. Conclusion

We proposed an online data augmentation optimization method called TeachAugment that introduces a teacher model into the adversarial data augmentation and makes it more informative without the need for careful parameter tuning. We also proposed neural network based augmentation that simplified the search space design and enabled updating of the data augmentation with the gradient method. In experiments, our method outperformed existing data augmentation search frameworks, including stateof-the-art methods, on image classification, semantic segmentation, and unsupervised representation learning tasks without adjusting the hyperparameters for each task.

Limitation. The proposed color augmentation cannot represent transformation using global information of a target image, such as Equalize and AutoContrast, because of the lack of global information in the input. Such transformations may be realized using the color histogram as the context vector, although at the expense of computational cost, especially for high-resolution images. Moreover, we only focused on geometric and color augmentation, but there are many advanced augmentation that are not categorized with them, for example, cutout [\[12\]](#page-11-4) and mixup [\[60\]](#page-16-2). Considering such augmentations will be future work.

The training time for TeachAugment is approximately three times longer than the conventional training procedure because updating the augmentation model requires the forward and backward computation of the target model and the teacher model, in addition to updating the target network. However, the training time of TeachAugment is almost the same as other online methods [\[28,](#page-12-2)[51\]](#page-15-0) and is a realistic time in comparison to AutoAugment [\[9\]](#page-11-0). We believe that it is a negligible problem under the recent advances of computational power.

## <span id="page-8-3"></span>A. Training setups

#### A.1. Image classification

We summarize the hyperparamters for training in Tab. [8.](#page-9-0) Each model was trained with Nesterov's accelerated gradient method [\[41\]](#page-14-6) in the stochastic gradient descent. The cross entropy loss between the model prediction and the ground truth label was used as the loss function. We gradually warmed up the learning rate for five epochs until it reached the predefined learning rate shown in Tab. [8.](#page-9-0) As baseline augmentation, we used random horizontal flipping and random cropping with a crop size of 32 for CIFAR-10 and CIFAR-100 and 224 for ImageNet. In addition, we also used Cutout with a crop size of 16 for CIFAR-10 and CIFAR-100, and random color distortion for ImageNet.[2](#page-8-0)

#### A.2. Semantic segmentation

We trained all models using the stochastic gradient descent with momentum of 0.9 for 300 epochs. The cross entropy loss between the model prediction and the ground truth label was used as the loss function. We set the initial learning rate to 5e-3 and decayed it with poly learning rate decay where the initial learning rate was multiplied by 1 − iter max iter <sup>0</sup>.<sup>9</sup> . The coefficient of the auxiliary loss used in [\[62\]](#page-16-6) was set to 0.4. As baseline augmentation, we used random horizontal flipping and random cropping with a crop size of 1024. For TeachAugment, the smoothing parameter in label smoothing was set to 0.1. For RandAugment, the number of transformations n and the magnitude m, were set to 1 and 5, which were tuned for FCN-32s using grid search.

### A.3. Unsupervised representation learning

We evaluated each method following the linear evaluation setting [\[7\]](#page-11-7).[3](#page-8-1) We modified the objective of our method for SimSiam as follows:

$$
\max_{\phi_1, \phi_2} \min_{\theta} \mathbb{E}_{x \sim \mathcal{X}} [L(f_{\theta}(a_{\phi_1}(x)), f_{\theta}(a_{\phi_2}(x))) - L(f_{\theta}(a_{\phi_1}(x)), f_{\theta}(a_{\phi_2}(x)))], \quad (6)
$$

where L denotes the cosine distance. Because the nonsaturating loss and label smoothing cannot be applied to the cosine distance, we omitted them in this experiment.

Note that our method with the EMA teacher cannot be simply applied to other methods, such as BYOL [\[15\]](#page-11-8) and MoCo [\[6,](#page-11-20) [19\]](#page-11-9), because they already integrate the EMA teacher into their training frameworks. It will be future work to investigate combinations of such methods.

<span id="page-8-2"></span>Algorithm 1 Training procedure for TeachAugment

- Input: A target model fθ, a teacher model fθˆ, dataset X , the number of inner iterations ninner, learning rate η<sup>θ</sup> and ηφ, and decay rate for the EMA teacher ξ
- 1: while θ has not converged do
- 2: for i = 0, · · · , ninner do
- 3: if fθ<sup>ˆ</sup> is the EMA teacher then
- 4: Update teacher weights, <sup>ˆ</sup><sup>θ</sup> <sup>←</sup> <sup>ξ</sup> <sup>ˆ</sup><sup>θ</sup> + (1 <sup>−</sup> <sup>ξ</sup>)<sup>θ</sup>
- 5: end if
- 6: Randomly sample a mini-batch, {x b} B b ∼ X
- 7: Compute loss for the target model, )))

$$
L_{\theta} = \sum_{b} L(f_{\theta}(a_{\phi}(x^{b}
$$

- 8: Update θ by the gradient descent, θ ← θ − ηθ∂Lθ/∂θ
- 9: end for
- 10: Randomly sample a mini-batch, {x¯ b} B b ∼ X
- 11: Compute loss for the augmentation model,
- L<sup>φ</sup> = P b (L(fθ(aφ(¯x b ))) − L(fθ<sup>ˆ</sup>(aφ(¯x b ))))
- 12: Update φ by the gradient ascent, φ ← φ+ηφ∂Lφ/∂φ 13: end while

As pretraining, we trained ResNet-50 for 100, 200, and 400 epochs with a batch size of 256. The momentum SGD was employed as the optimizer. The learning rate and the momentum were set to 0.05 and 0.9, respectively. After pretraining, we trained a linear classifier for 90 epochs with a batch size of 4,096. We set the hyperparameters for each of the methods to the same as the parameters for ImageNet classification. As baseline augmentation, we used the same augmentation as in SimSiam [\[7\]](#page-11-7), namely, random cropping, random horizontal flipping, color jittering, and Gaussian blur. For more details and SimSiam's hyperparameters, please refer to [\[7\]](#page-11-7).

# B. Pseudo-Code of TeachAugment

We show the pseudo-code of TeachAugment in Algorithm [1.](#page-8-2)

# C. Details of augmentation model

For the geometric augmentation, we used a three-layer perceptron. The dimension of the noise vector was 128 and the number of units in hidden layers was 512. As a nonlinear activation function, we used leaky ReLU [\[36\]](#page-13-8) with the negative slope of 0.2. The output, Aunnorm, was normalized through the sigmoid function:

$$
A = \lambda_{g_{\text{scale}}}(\text{sigmoid}(A^{\text{unnorm}}) - 0.5), \tag{7}
$$

where λ<sup>g</sup>scale controls the search range of A, and we set it to 0.5 (i.e., <sup>A</sup> <sup>∈</sup> (−0.25, <sup>0</sup>.25)<sup>2</sup>×<sup>3</sup> ).

For the color augmentation, we used two three-layer perceptrons that receive an RGB vector and a noise vector as

<span id="page-8-0"></span><sup>2</sup>The implementation of the baseline augmentation and models are based on [https : / / github . com / kakaobrain / fast](https://github.com/kakaobrain/fast-autoaugment)  [autoaugment](https://github.com/kakaobrain/fast-autoaugment).

<span id="page-8-1"></span><sup>3</sup>The experimental code is based on [https://github.com/](https://github.com/facebookresearch/simsiam) [facebookresearch/simsiam](https://github.com/facebookresearch/simsiam).

<span id="page-9-4"></span><span id="page-9-0"></span>

|                     | WideResNet-40-2 | WideResNet-28-10 | Shake-Shake (26 2×96d) | PyramidNet | ResNet-50 |
|---------------------|-----------------|------------------|------------------------|------------|-----------|
| Learning rate       | 0.1             | 0.1              | 0.01                   | 0.05       | 0.05      |
| Weight decay        | 2e-4            | 5e-4             | 1e-3                   | 5e-5       | 1e-4      |
| Epochs              | 200             | 200              | 1,800                  | 1,800      | 270       |
| Batch size          | 128             | 128              | 128                    | 64         | 128       |
| Learning rate decay | cosine          | cosine           | cosine                 | cosine     | step      |

Table 8. Hyperparameters for classification tasks. We set parameters following [\[30\]](#page-12-1). Note that we show the batch size per GPU and the learning rate was multiplied by the number of GPUs (e.g., if two GPUs are used for training, the learning rate is doubled). We used a single GPU for WideResNet-40-2 and WideResNet-28-10, and four GPUs for the other models. We decayed the learning rate for ResNet-50 by 10-fold at epochs 90, 180, and 240.

<span id="page-9-1"></span>
$$
x_i \in \mathbb{R}^3
$$
\n
$$
\longleftrightarrow (\alpha_i^{\text{RGB}}, \beta_i^{\text{RGB}}) \in \mathbb{R}^{3 \times 2}
$$
\n
$$
\longleftrightarrow (\alpha_i, \beta_i) \in \mathbb{R}^{3 \times 2}
$$
\n
$$
z \sim \mathcal{N}(0, I_N)
$$
\n
$$
\longleftrightarrow (\alpha_i^{\text{Noise}}, \beta_i^{\text{Noise}}) \in \mathbb{R}^{1 \times 2}
$$

Figure 6. Illustration of the color augmentation model. σ denotes the sigmoid function.

inputs and add up their outputs. The number of units in hidden layers was 128 and 512, respectively. As the non-linear activation, leaky ReLU was used. We illustrate the computational scheme of the color augmentation model in Fig. [6.](#page-9-1) The former model outputs 3-dimensional scale and shift parameters:

$$
(\alpha_i^{\text{RGB}}, \beta_i^{\text{RGB}}) \in \mathbb{R}^{3 \times 2},\tag{8}
$$

and the latter outputs scalar scale and shift parameters:

$$
(\alpha_i^{\text{Noise}}, \beta_i^{\text{Noise}}) \in \mathbb{R}^{1 \times 2}.
$$
 (9)

The scale and shift parameters from the noise vector control the global brightness of images. Then, we add up these scale and shift parameters:

$$
(\alpha_i^{\text{unnorm}})_j = (\alpha_i^{\text{RGB}})_j + \alpha_i^{\text{Noise}}, \tag{10}
$$

$$
(\beta_i^{\text{unnorm}})_j = (\beta_i^{\text{RGB}})_j + \beta_i^{\text{Noise}}, \tag{11}
$$

where (αi)<sup>j</sup> denotes the j-th element of α<sup>i</sup> ∈ R 3 . We normalized the scale and shift parameters, (α unnorm i , βunnorm i ), using the sigmoid function:

$$
\alpha_i = \lambda_{c_{\text{scale}}}(\text{sigmoid}(\alpha_i^{\text{unnorm}}) - 0.5) + 1,\tag{12}
$$

$$
\beta_i = \lambda_{c_{\text{scale}}}(\text{sigmoid}(\beta_i^{\text{unnorm}}) - 0.5), \tag{13}
$$

where λ<sup>c</sup>scale controls the search range of α<sup>i</sup> and β<sup>i</sup> , and we set it to 0.8, namely, α<sup>i</sup> ∈ (0.6, 1.4) and β<sup>i</sup> ∈ (−0.4, 0.4).

We adopted AdamW [\[35\]](#page-13-6) as the optimizer for the augmentation model. The learning rate and the weight decay were set to 1e-3 and 1e-2, which are the default parameters in PyTorch [\[42\]](#page-14-5). Dropout [\[48\]](#page-15-6) was applied after the linear layers except for the output layer with the drop ratio of 0.8.

# D. Learning pipeline of probabilities p<sup>c</sup> and p<sup>g</sup>

We made the decision process of applying the augmentation differentiable using weights sampled from the relaxed Bernoulli distribution defined in [\[37\]](#page-13-7).[4](#page-9-2) A sample from the relaxed Bernoulli, w ∼ ReBern(p, τ ), is obtained as follows:

$$
w = \text{sigmoid}((L + \log p)/\tau), \ L \sim \text{Logistic}(0, 1), \tag{14}
$$

where τ and p denote the temperature parameter and a probability that corresponds to p<sup>c</sup> and p<sup>g</sup> in our case and L is a sample from the Logistic distribution, which is obtained by L = log(U)−log(1−U), U ∼ Uniform(0, 1). We set τ to 0.05 following [\[17\]](#page-11-1). We note that the sampling procedure, Eq. [14,](#page-9-3) is differentiable with respect to the probability p.

We compute weighted sum of the parameters generated by augmentation models and parameters that make the augmentation the identity mapping:

<span id="page-9-3"></span>
$$
\hat{\alpha}_i = w_c \alpha_i + (1 - w_c) \cdot 1,\tag{15}
$$

$$
\hat{\beta}_i = w_c \beta_i + (1 - w_c) \cdot 0,\tag{16}
$$

$$
\hat{A} = w_g A + (1 - w_g)I,\tag{17}
$$

where w<sup>c</sup> ∼ ReBern(pc, τ ) and w<sup>g</sup> ∼ ReBern(pg, τ ). We use ( ˆα<sup>i</sup> , βˆ <sup>i</sup>) and Aˆ to transform images, instead of (α<sup>i</sup> , βi) and A:

$$
\tilde{x}_i = t(\hat{\alpha}_i \odot x_i + \hat{\beta}_i),\tag{18}
$$

$$
\hat{x} = \text{Affine}(\tilde{x}, \hat{A} + I). \tag{19}
$$

Because the sampling procedure from ReBern(p, τ ) is differentiable with respect to p, we can also update the probabilities using the gradient method.

# E. Additional results

### E.1. Relation between consistency regularization and TeachAugment

TeachAugment can be viewed as a method that minimizes the distance between predictions of the target model

<span id="page-9-2"></span><sup>4</sup>The relaxed Bernoulli distribution is referred to as the BinConcrete distribution in [\[37\]](#page-13-7).

<span id="page-10-3"></span><span id="page-10-0"></span>![](_page_10_Figure_0.jpeg)

**Caption:** Figure 7 visualizes the training process in TeachAugment, showing how data augmentation transforms input data points to maximize the loss for the target model while minimizing it for the teacher model. This iterative process refines the decision boundary of the target model.

Figure 7. Illustration of the training process in TeachAugment. The data augmentation transforms a data point to the point so that L(fθ(aφ(x))) > L(fθ<sup>ˆ</sup>(aφ(x))). Then, the target model is updated to minimize loss for the augmented point. By repeating this process, the decision boundary of the target model is close to that of the teacher model.

and the teacher model. We show it qualitatively.

We assume that data augmentation a<sup>φ</sup> can transform an input data point to any point in the input space, and the teacher model is fixed during training. Then, solving max<sup>φ</sup> L(fθ(aφ(x)))−L(fθ<sup>ˆ</sup>(aφ(x))) corresponds to searching the data augmentation maximizing the distance between the predictions under the constraint of L(fθ(aφ(x))) > L(fθ<sup>ˆ</sup>(aφ(x))). Then, the target model updates its decision boundary for minimizing the loss for the augmented data. This procedure is illustrated in Fig. [7.](#page-10-0) If the decision boundary of the target model completely corresponds to that of the teacher model, the data augmentation reaches the stationary point because the objective L(fθ(aφ(x))) − L(fθ<sup>ˆ</sup>(aφ(x))) is 0 for all data points.

However, the target model would not match the teacher model in practice, because above analysis lacks certain points: (1) The data augmentation cannot transform an input data point to any point in the input. (2) We used the nonsaturating loss for the image classification tasks instead of cross entropy. Thus, L(fθ(aφ(x))) − L(fθ<sup>ˆ</sup>(aφ(x))) 6= 0 if the decision boundary of the target model is the same as that of the teacher model. (3) TeachAugment does not explicitly minimize the distance between the predictions. In particular, (3) is a more critical point compared to (1) and (2) because they may be solved by adopting a model large enough to satisfy the requirement as the augmentation model, and using an original loss function instead of the non-saturating loss. Because of the lack of explicit costs for the consistency, TeachAugment does not ensure that the target model converges with the teacher model.

However, we believe that the above analysis gives some insights, and comparing TeachAugment to consistency regularization is important. Thus, we trained model to minimize the following costs and compared the results to TeachAugment:

$$
\min_{\theta} L(f_{\theta}(x)) + D(f_{\theta}(x), f_{\hat{\theta}}(x)), \tag{20}
$$

<span id="page-10-2"></span>

| Dataset         | CIFAR-10 | CIFAR-100 |
|-----------------|----------|-----------|
| Baseline        | 3.1      | 18.4      |
| KLD consistency | 3.1      | 18.2      |
| MSE consistency | 3.2      | 18.5      |
| TeachAugment    | 2.5      | 16.8      |

Table 9. Comparison with the consistency regularization. We report the error rates of WideResNet-28-10. For training with KLD and MSE consistency, we used random horizontal flipping, random cropping, and cutout [\[12\]](#page-11-4) as data augmentation.

where D(·, ·) denotes the distance function. We used the mean squared error and Kullback–Leibler divergence as D(·, ·). These functions are widely used in the consistency regularization and the knowledge distillation. We refer to the former as MSE Consistency and the later as KLD Consistency. Note that we use random horizontal flipping, random cropping, and cutout [\[12\]](#page-11-4) as the data augmentation but omitted them in Eq. [\(20\)](#page-10-1) because they were not optimized. As the teacher model, we used the same EMA teacher in TeachAugment.

The results are shown in Tab. [9.](#page-10-2) The consistency methods do not improve the error rates from the baseline. Thus, TeachAugment has different properties than the consistency regularization, and it works well in supervised learning.

#### E.2. Qualitative analysis

We show augmented images obtained by the augmentation model trained with various objective functions: TeachAugment, Adversarial AutoAugment (Adv. AA) [\[61\]](#page-16-0), and PointAugment [\[28\]](#page-12-2). All methods used the same proposed augmentation model as data augmentation.

The objective of Adv. AA is as follows:

$$
\max_{\phi} \min_{\theta} L(f_{\theta}(a_{\phi}(x))). \tag{21}
$$

Also, the objective of PointAugment is as follows:

$$
\min_{\theta} L(f_{\theta}(a_{\phi}(x))), \qquad (22)
$$

$$
\min_{\phi} L(f_{\theta}(a_{\phi}(x))) + |1 - \exp(L(f_{\theta}(a_{\phi}(x))) - \rho L(f_{\theta}(x)))|, \quad (23)
$$

where ρ is a dynamic parameter defined as ρ = exp(y T · fθ(aφ(x))). Note that Adv. AA updates augmentation functions using the REINFORCE algorithm [\[56\]](#page-16-7) because many functions in the search space of AutoAugment are nondifferentiable. However, in our experiments, because our proposed augmentation is differentiable, we updated augmentation with the gradient descent rather than the REIN-FORCE algorithm.

<span id="page-10-1"></span>The augmented images of CIFAR-10 and CIFAR-100 are shown in Figs. [8](#page-12-10) and [9.](#page-13-9) The images augmented by Adv. AA obviously collapse, so the meaningful information is lost. The augmented images obtained by PointAugment and the proposed method are recognizable, but the proposed method transforms images more strongly than PointAugment so that the proposed method distorts the aspect ratio for the geometric augmentation. PointAugment binds the difficulty of the augmented images through a dynamic parameter ρ ≤ exp(1), but the proposed method requires only that the augmented images are recognizable for the teacher model. As a result, the proposed method allows stronger augmentation than PointAugment.

#### E.3. Example of augmented images

We show example results of augmentation for ImageNet and Cityscapes in Figs. [10](#page-14-7) and [11.](#page-15-8) As can see from Fig. [11,](#page-15-8) the augmentation obtained for FCN-32s is obviously different from the others. Because the output stride of FCN-32s (i.e., the ratio of input image spatial resolution to final output resolution) is lower than that of the others, FCN-32s will have different properties than PSPNet and Deeplav3. We believe that the difference leads the different augmentation of these models. Moreover, it also leads the degradation of mIoU for FCN-32s in RandAugment and TrivialAugment.

## References

- <span id="page-11-18"></span>[1] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. ´ Estimating or propagating gradients through stochastic neurons for conditional computation. *arXiv preprint arXiv:1308.3432*, 2013. [7](#page-6-3)
- <span id="page-11-19"></span>[2] Yoshua Bengio, Jer´ ome Louradour, Ronan Collobert, and Ja- ˆ son Weston. Curriculum learning. In *Proceedings of the 26th annual international conference on machine learning*, pages 41–48, 2009. [8](#page-7-3)
- <span id="page-11-11"></span>[3] Nicolas Bonneel, Julien Rabin, Gabriel Peyre, and Hanspeter ´ Pfister. Sliced and radon wasserstein barycenters of measures. *Journal of Mathematical Imaging and Vision*, 51(1):22–45, 2015. [4](#page-3-2)
- <span id="page-11-16"></span>[4] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. *arXiv preprint arXiv:1706.05587*, 2017. [5,](#page-4-2) [7](#page-6-3)
- <span id="page-11-6"></span>[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In *International conference on machine learning*, pages 1597–1607. PMLR, 2020. [2](#page-1-0)
- <span id="page-11-20"></span>[6] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. *arXiv preprint arXiv:2003.04297*, 2020. [2,](#page-1-0) [9](#page-8-3)
- <span id="page-11-7"></span>[7] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 15750–15758, 2021. [2,](#page-1-0) [5,](#page-4-2) [8,](#page-7-3) [9](#page-8-3)
- <span id="page-11-17"></span>[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes

dataset for semantic urban scene understanding. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 3213–3223, 2016. [5,](#page-4-2) [7](#page-6-3)

- <span id="page-11-0"></span>[9] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 113–123, 2019. [1,](#page-0-2) [2,](#page-1-0) [7,](#page-6-3) [8](#page-7-3)
- <span id="page-11-3"></span>[10] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops*, pages 702–703, 2020. [2,](#page-1-0) [7,](#page-6-3) [8](#page-7-3)
- <span id="page-11-15"></span>[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In *2009 IEEE conference on computer vision and pattern recognition*, pages 248–255. Ieee, 2009. [5](#page-4-2)
- <span id="page-11-4"></span>[12] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. *arXiv preprint arXiv:1708.04552*, 2017. [2,](#page-1-0) [8,](#page-7-3) [11](#page-10-3)
- <span id="page-11-12"></span>[13] Xavier Gastaldi. Shake-shake regularization. *arXiv preprint arXiv:1705.07485*, 2017. [5](#page-4-2)
- <span id="page-11-10"></span>[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. *Advances in neural information processing systems*, 27, 2014. [3,](#page-2-4) [4](#page-3-2)
- <span id="page-11-8"></span>[15] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin ´ Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. *arXiv preprint arXiv:2006.07733*, 2020. [2,](#page-1-0) [9](#page-8-3)
- <span id="page-11-13"></span>[16] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 5927–5935, 2017. [5](#page-4-2)
- <span id="page-11-1"></span>[17] Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster autoaugment: Learning augmentation strategies using backpropagation. In *European Conference on Computer Vision*, pages 1–16. Springer, 2020. [1,](#page-0-2) [2,](#page-1-0) [5,](#page-4-2) [7,](#page-6-3) [8,](#page-7-3) [10](#page-9-4)
- <span id="page-11-2"></span>[18] Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Meta approach to data augmentation optimization. *arXiv preprint arXiv:2006.07965*, 2020. [1,](#page-0-2) [2,](#page-1-0) [3](#page-2-4)
- <span id="page-11-9"></span>[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 9729–9738, 2020. [2,](#page-1-0) [9](#page-8-3)
- <span id="page-11-14"></span>[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 770–778, 2016. [5](#page-4-2)
- <span id="page-11-5"></span>[21] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. *arXiv preprint arXiv:1912.02781*, 2019. [2](#page-1-0)

<span id="page-12-10"></span>![](_page_12_Picture_0.jpeg)

**Caption:** Figure 8 displays augmented images generated from CIFAR-10 using various methods. It highlights the differences in augmentation quality, with TeachAugment producing more recognizable images compared to other methods, which may lead to loss of meaningful information.

Figure 8. Augmented images obtained with CIFAR-10 using various methods..

- <span id="page-12-0"></span>[22] Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efficient learning of augmentation policy schedules. In *International Conference on Machine Learning*, pages 2731–2741. PMLR, 2019. [1,](#page-0-2) [7,](#page-6-3) [8](#page-7-3)
- <span id="page-12-3"></span>[23] Hiroshi Inoue. Data augmentation by pairing samples for images classification. *arXiv preprint arXiv:1801.02929*, 2018. [2](#page-1-0)
- <span id="page-12-8"></span>[24] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. *Advances in neural information processing systems*, 28:2017–2025, 2015. [5](#page-4-2)
- <span id="page-12-9"></span>[25] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. *arXiv preprint arXiv:1611.01144*, 2016. [7](#page-6-3)
- <span id="page-12-6"></span>[26] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In *Advances in neural information processing systems*, pages 1008–1014, 2000. [4](#page-3-2)
- <span id="page-12-7"></span>[27] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [4,](#page-3-2) [5](#page-4-2)
- <span id="page-12-2"></span>[28] Ruihui Li, Xianzhi Li, Pheng-Ann Heng, and Chi-Wing Fu. Pointaugment: an auto-augmentation framework for point cloud classification. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 6378–6387, 2020. [1,](#page-0-2) [2,](#page-1-0) [3,](#page-2-4) [4,](#page-3-2) [6,](#page-5-3) [7,](#page-6-3) [8,](#page-7-3) [11](#page-10-3)
- <span id="page-12-5"></span>[29] Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Dada: Differentiable automatic data augmentation. *arXiv preprint arXiv:2003.03780*, 2020. [2,](#page-1-0) [5,](#page-4-2) [7,](#page-6-3) [8](#page-7-3)
- <span id="page-12-1"></span>[30] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. *Advances in Neural Information Processing Systems*, 32:6665–6675, 2019. [1,](#page-0-2) [2,](#page-1-0) [5,](#page-4-2) [7,](#page-6-3) [8,](#page-7-3) [10](#page-9-4)
- <span id="page-12-4"></span>[31] Chen Lin, Minghao Guo, Chuming Li, Xin Yuan, Wei Wu, Junjie Yan, Dahua Lin, and Wanli Ouyang. Online

<span id="page-13-9"></span>![](_page_13_Picture_0.jpeg)

**Caption:** Figure 9 shows augmented images obtained from CIFAR-100 using different augmentation strategies. The comparison emphasizes the effectiveness of TeachAugment in maintaining image quality while enhancing diversity in the augmented dataset.

Figure 9. Augmented images obtained with CIFAR-100 using various methods..

hyper-parameter learning for auto-augmentation strategy. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 6579–6588, 2019. [2](#page-1-0)

- <span id="page-13-3"></span>[32] Long-Ji Lin. *Reinforcement learning for robots using neural networks*. Carnegie Mellon University, 1992. [4](#page-3-2)
- <span id="page-13-2"></span>[33] Tom Ching LingChen, Ava Khonsari, Amirreza Lashkari, Mina Rafi Nazari, Jaspreet Singh Sambee, and Mario A Nascimento. Uniformaugment: A search-free probabilistic data augmentation approach. *arXiv preprint arXiv:2003.14348*, 2020. [2](#page-1-0)
- <span id="page-13-5"></span>[34] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 3431–3440, 2015. [5,](#page-4-2) [7](#page-6-3)
- <span id="page-13-6"></span>[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. *arXiv preprint arXiv:1711.05101*, 2017. [5,](#page-4-2) [10](#page-9-4)
- <span id="page-13-8"></span>[36] Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rectifier nonlinearities improve neural network acoustic models. In *Proc. icml*, volume 30, page 3. Citeseer, 2013. [9](#page-8-3)
- <span id="page-13-7"></span>[37] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. *arXiv preprint arXiv:1611.00712*, 2016. [7,](#page-6-3) [10](#page-9-4)
- <span id="page-13-0"></span>[38] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. *IEEE transactions on pattern analysis and machine intelligence*, 41(8):1979–1993, 2018. [1,](#page-0-2) [2](#page-1-0)
- <span id="page-13-4"></span>[39] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013. [4](#page-3-2)
- <span id="page-13-1"></span>[40] Samuel G Muller and Frank Hutter. Trivialaugment: Tuning- ¨

<span id="page-14-7"></span>![](_page_14_Picture_0.jpeg)

**Caption:** Figure 10 illustrates augmentations obtained from ImageNet, showcasing the variations in image quality and transformation strength across different methods. TeachAugment demonstrates a balance between strong augmentation and recognizability.

(a) Original images (b) Augmented images

Figure 10. Augmentations obtained with ImageNet.

free yet state-of-the-art data augmentation. *arXiv preprint arXiv:2103.10158*, 2021. [2,](#page-1-0) [7,](#page-6-3) [8](#page-7-3)

- <span id="page-14-6"></span>[41] Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o (1/kˆ 2). In *Dokl. akad. nauk Sssr*, volume 269, pages 543–547, 1983. [9](#page-8-3)
- <span id="page-14-5"></span>[42] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In *Advances in Neural Information Processing Systems 32*, pages 8024–8035. Curran Associates, Inc., 2019. [5,](#page-4-2) [10](#page-9-4)
- <span id="page-14-0"></span>[43] Xi Peng, Zhiqiang Tang, Fei Yang, Rogerio S Feris, and Dimitris Metaxas. Jointly optimize data augmentation and network training: Adversarial data augmentation in human

pose estimation. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 2226– 2234, 2018. [1,](#page-0-2) [3](#page-2-4)

- <span id="page-14-3"></span>[44] David Pfau and Oriol Vinyals. Connecting generative adversarial networks and actor-critic methods. *arXiv preprint arXiv:1610.01945*, 2016. [4](#page-3-2)
- <span id="page-14-2"></span>[45] Colorado J Reed, Sean Metzger, Aravind Srinivas, Trevor Darrell, and Kurt Keutzer. Selfaugment: Automatic augmentation policies for self-supervised learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 2674–2683, 2021. [2](#page-1-0)
- <span id="page-14-4"></span>[46] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. *arXiv preprint arXiv:1511.05952*, 2015. [4](#page-3-2)
- <span id="page-14-1"></span>[47] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-

<span id="page-15-8"></span>![](_page_15_Picture_0.jpeg)

**Caption:** Figure 11 presents augmentations from the Cityscapes dataset, highlighting the effectiveness of TeachAugment in semantic segmentation tasks. The augmented images maintain essential features while introducing variability, crucial for improving model performance.

Figure 11. Augmentations obtained with Cityscapes.

supervised learning with consistency and confidence. *arXiv preprint arXiv:2001.07685*, 2020. [2](#page-1-0)

- <span id="page-15-6"></span>[48] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. *The journal of machine learning research*, 15(1):1929–1958, 2014. [5,](#page-4-2) [10](#page-9-4)
- <span id="page-15-5"></span>[49] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In *Advances in neural information processing systems*, pages 1057–1063, 2000. [4](#page-3-2)
- <span id="page-15-1"></span>[50] Teppei Suzuki and Ikuro Sato. Adversarial transformations for semi-supervised learning. *arXiv preprint arXiv:1911.06181*, 2019. [1,](#page-0-2) [2](#page-1-0)
- <span id="page-15-0"></span>[51] Zhiqiang Tang, Yunhe Gao, Leonid Karlinsky, Prasanna Sattigeri, Rogerio Feris, and Dimitris Metaxas. Onlineaugment: Online data augmentation with less domain knowledge. In *Computer Vision–ECCV 2020: 16th European Conference,*

*Glasgow, UK, August 23–28, 2020, Proceedings, Part VII 16*, pages 313–329. Springer, 2020. [1,](#page-0-2) [2,](#page-1-0) [3,](#page-2-4) [4,](#page-3-2) [7,](#page-6-3) [8](#page-7-3)

- <span id="page-15-7"></span>[52] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. *arXiv preprint arXiv:1703.01780*, 2017. [5](#page-4-2)
- <span id="page-15-3"></span>[53] Keyu Tian, Chen Lin, Ming Sun, Luping Zhou, Junjie Yan, and Wanli Ouyang. Improving auto-augment via augmentation-wise weight sharing. *arXiv preprint arXiv:2009.14737*, 2020. [2](#page-1-0)
- <span id="page-15-2"></span>[54] Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Between-class learning for image classification. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 5486–5494, 2018. [2](#page-1-0)
- <span id="page-15-4"></span>[55] Longhui Wei, An Xiao, Lingxi Xie, Xiaopeng Zhang, Xin Chen, and Qi Tian. Circumventing outliers of autoaugment

with knowledge distillation. In *European Conference on Computer Vision*, pages 608–625. Springer, 2020. [2](#page-1-0)

- <span id="page-16-7"></span>[56] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. *Machine learning*, 8(3):229–256, 1992. [11](#page-10-3)
- <span id="page-16-5"></span>[57] Yoshihiro Yamada, Masakazu Iwamura, Takuya Akiba, and Koichi Kise. Shakedrop regularization for deep residual learning. *IEEE Access*, 7:186126–186136, 2019. [5](#page-4-2)
- <span id="page-16-1"></span>[58] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 6023–6032, 2019. [2](#page-1-0)
- <span id="page-16-4"></span>[59] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. *arXiv preprint arXiv:1605.07146*, 2016. [4,](#page-3-2) [5](#page-4-2)
- <span id="page-16-2"></span>[60] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. *arXiv preprint arXiv:1710.09412*, 2017. [2,](#page-1-0) [8](#page-7-3)
- <span id="page-16-0"></span>[61] Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial autoaugment. *arXiv preprint arXiv:1912.11188*, 2019. [1,](#page-0-2) [2,](#page-1-0) [3,](#page-2-4) [4,](#page-3-2) [6,](#page-5-3) [7,](#page-6-3) [8,](#page-7-3) [11](#page-10-3)
- <span id="page-16-6"></span>[62] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 2881–2890, 2017. [5,](#page-4-2) [7,](#page-6-3) [9](#page-8-3)
- <span id="page-16-3"></span>[63] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34, pages 13001–13008, 2020. [2](#page-1-0)