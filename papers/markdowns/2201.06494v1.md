# <span id="page-0-0"></span>AugLy: Data Augmentations for Robustness

Zoe Papakipos and Joanna Bitton ¨ Meta AI {zoep, jbitton}@fb.com

![](_page_0_Picture_2.jpeg)

**Caption:** Figure 1 illustrates various image augmentations provided by AugLy, including overlay text, brightness adjustments, and cropping. These augmentations mimic real-world data manipulations, enhancing model robustness against adversarial attacks and improving generalization in computer vision tasks.

### Abstract

*We introduce AugLy, a data augmentation library with a focus on adversarial robustness. AugLy provides a wide array of augmentations for multiple modalities (audio, image, text, & video). These augmentations were inspired by those that real users perform on social media platforms, some of which were not already supported by existing data augmentation libraries. AugLy can be used for any purpose where data augmentations are useful, but it is particularly wellsuited for evaluating robustness and systematically generating adversarial attacks. In this paper we present how AugLy works, benchmark it compared against existing libraries, and use it to evaluate the robustness of various state-of-the-art models to showcase AugLy's utility. The AugLy repository can be found at* [https://github.](https://github.com/facebookresearch/AugLy) [com/facebookresearch/AugLy](https://github.com/facebookresearch/AugLy)

## 1. Introduction

Data augmentations are a key component in the computer vision model development life cycle[\[24\]](#page-7-0), and are also becoming increasingly prevalent in other domains[\[7\]](#page-6-0). They are commonly used to increase the size of datasets and prevent overfitting by performing perturbations on the input data. In addition to the classical use cases, data augmentations can also be used to evaluate the robustness of trained models to perturbations not seen at train time[\[11\]](#page-6-1)[\[10\]](#page-6-2).

For instance, to preserve a sense of data provenance, being robust to data manipulations is critical. Content online is often manipulated and reshared, for example when users screenshot & share a post, or overlay text or images on top of an image to make a meme. It is therefore non-trivial to be able to detect that two pieces of media are near-duplicates [\[17\]](#page-7-1). Additionally, adversaries may try to intentionally pass in obfuscated data to a model to evade detection.

|                    | <b>SCREENSHOTS</b><br>$\alpha$<br>retweek<br><b>D</b> Management and                                                                                             | <b>OVERLAY TEXT</b><br>LOL. | <b>SATURATION</b> |
|--------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------|-------------------|
| <b>INPUT IMAGE</b> | El horizo - O horizo - O horizonto<br>tons<br>1.11<br>ateine<br>ø.                                                                                               | CROP                        | ROTATE            |
|                    | $\begin{array}{cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc$ |                             |                   |
|                    | <b>BRIGHTNESS</b>                                                                                                                                                | BLUR                        |                   |

Figure 1. Examples of a few AugLy image augmentations

The classical set of data augmentations used during model development does not completely mimic the way individuals online organically perturb data. Most classical augmentation libraries focus on simple transformations such as mirroring, rotating, cropping, brightness changes, etc. While these kinds of augmentations do naturally occur online, others such as overlaying text and emojis, social media screenshots, etc. are also prevalent. In addition, multimodal data processing and learning is becoming increasingly important as many real-world use cases involve multiple types of data, such as text & images or audio & video, <span id="page-1-0"></span>and it can be useful to augment data of multiple modalities under one unified library & API.

AugLy is built with robustness and the vast landscape of organic data augmentations seen online in mind, and to our knowledge is the first multimodal data augmentation library. AugLy can be used to synthetically create realistic data augmentations seen online, as a tool for evaluating and increasing robustness and to augment multiple modalities at a time, and thus stands out in comparison to existing libraries. In this paper we introduce AugLy, explain how it works, its architecture, and how it compares in terms of functionality & efficiency to existing data augmentation libraries. We also conduct a robustness evaluation on state-of-the-art image classification models throughout the years to demonstrate how AugLy can be used to identify robustness gaps in pretrained models.

### 2. Related Work

Most commonly-used augmentation libraries focus on one modality and provide a fairly limited set of augmentations. A majority of libraries focus on images [\[1,](#page-6-3) [19,](#page-7-2) [14\]](#page-7-3) and text [\[18,](#page-7-4) [8,](#page-6-4) [21\]](#page-7-5), however audio [\[20,](#page-7-6) [29,](#page-7-7) [15\]](#page-7-8) and video [\[32,](#page-7-9) [6,](#page-6-5) [16\]](#page-7-10) augmentation libraries do exist as well with more limited augmentations (see Section [4](#page-2-0) for in-depth comparisons between AugLy and existing libraries for each modality). Meanwhile, AugLy provides augmentations for audio, images, text and video under a unified API, and is one of few libraries[\[8\]](#page-6-4) that focus on evaluating robustness rather than augmenting a dataset at train time.

Other works have conducted experiments to find sets of augmentations that when trained on improve robustness at test time, such as AugMix[\[12\]](#page-6-6). Strategies like AutoAugment, on the other hand, find an "optimal" set of augmentations to train on in a more automated way[\[2\]](#page-6-7).

In AI Fairness, studies assessing the robustness of models to various protected categories are common. In NLP, there are studies that augment text to assess a model's biases towards gender [\[31,](#page-7-11) [3\]](#page-6-8) and ethnicity[\[26\]](#page-7-12). AugLy provides "fairness augmentations" since being robust to perturbations in protected classes is an important aspect of robustness that we must evaluate to ensure that models are not amplifying biases.

# 3. AugLy

AugLy is a novel open-source data augmentation library which provides over 100 data augmentations across four modalities: audio, image, text, and video. The augmentations provided in AugLy are informed by the perturbations that real people on the Internet perform on data daily. This includes augmentations such as overlaying text, emojis, and screenshot transforms for image & video and inserting punctuation or similar characters for text.

![](_page_1_Figure_8.jpeg)

**Caption:** Figure 2 showcases examples of text augmentations from AugLy, such as meme formatting and overlaying text. These augmentations are designed to reflect common online practices, enabling models to better handle adversarial text inputs and improve robustness in natural language processing applications.

Figure 2. Examples of some AugLy text augmentations

impor t a u gl y . image a s ima u g s

```
a u g im g = ima u g s . meme fo rmat (
      i n p ut i m g ,
      c a p t i o n h e i g h t =75 ,
      m em e b g c ol o r = ( 0 , 0 , 0 ) ,
      t e x t c o l o r = ( 2 5 5 , 2 5 5 , 2 5 5 ) ,
)
```
![](_page_1_Figure_12.jpeg)

**Caption:** Figure 4 depicts the structure of AugLy's library, highlighting its four sub-libraries for audio, image, text, and video. Each sub-library offers a unified interface for applying various augmentations, facilitating multimodal data processing and enhancing the evaluation of model robustness.

#### 3.1. Library Structure

AugLy has four sub-libraries (audio, image, text, & video), each corresponding to a different modality. All sublibraries follow the same interface: we provide transforms in both function-based and class-based formats, and we provide intensity functions that compute a notion of how strong a transformation is based on the given parameters. AugLy can optionally generate metadata that provides additional context as to how the data was transformed, which is useful to perform comparisons of model performance based on the augmentation type & intensity.

AugLy also provides operators for composing multiple augmentations together, applying augmentations with a given probability, and applying multimodal augmentations (for example augmenting both the audio & frames in a video).

We provide many basic augmentations that are already supported in existing libraries, as well as some new transformation types that are directly informed by data perturbations observed online. For example, one of our augmentations takes an image or video and overlays it onto a social media interface to make it seem as if the image or video was screenshotted by a user on a social network. This augmentation is beneficial because individuals on the internet commonly reshare content this way, and it is important for systems to be able to identify that the content is still the same despite the added interface elements.

```
impor t a u gl y . vi d e o a s vi d a u g s
impor t a u gl y . a u di o a s a u d a u g s
a u d a u g s = a u d a u g s . Compose (
       [
             a u d a u g s . AddBackg roundNoise ( ) ,
             a u d a u g s . Tempo ( f a c t o r = 2 . 0 ) ,
       ] ,
)
v i d a u g s = vi d a u g s . Compose (
       [
             vi d a u g s . R o t a t e ( p = 0 . 5 ) ,
             vi d a u g s . TimeCrop (
                     o f f s e t f a c t o r = 0 . 2 ,
                    d u r a t i o n f a c t o r = 0 . 4 ,
             ) ,
             vi d a u g s . AugmentAudio (
                    a u d i o a u g f u n c t i o n = a u d a u g s ,
             ) ,
       ] ,
)
v i d a u g s ( v i d e o p a t h , o u t p a t h )
```
Figure 4. Composing audio & video augmentations

#### 3.2. Existing Use Cases

AugLy has already been used by several projects. SimSearchNet[\[27\]](#page-7-13), an image copy detection model, was trained using AugLy augmentations. AugLy was used to evaluate the robustness of deepfake detection models in the 2019 Deepfake Detection Challenge[\[4\]](#page-6-9), ultimately influencing who were the top five winners. The dataset (DISC21) for the Image Similarity Challenge[\[5\]](#page-6-10), a NeurIPS 2021 competition on image copy detection, was built using AugLy as well.

# <span id="page-2-0"></span>4. Benchmarking

In order to show how AugLy fits into the existing ecosystem of data augmentation libraries, we compare each modality's sub-library within AugLy to a few of the most popular augmentations libraries in that respective modality. Specifically, we compare the overall focus and functionality of each library, and perform runtime benchmarking to evaluate how efficient AugLy's augmentations are. Note: the augmentations were benchmarked using AugLy v0.2.1, available on Pypi and GitHub. To see the full list of augmentations benchmarked, please review the Appendix.

#### 4.1. Audio

We chose to compare AugLy's audio augmentations to three existing and popular libraries: pydub[\[23\]](#page-7-14), torchaudio[\[29\]](#page-7-7), and audiomentations[\[13\]](#page-6-11). See Figure [5](#page-2-1) to

| Library         | # augmentations |
|-----------------|-----------------|
| pydub           | 10              |
| AugLy           | 20              |
| audiomentations | 25              |
| torchaudio      | 58              |

<span id="page-2-1"></span>Figure 5. The audio augmentation libraries we chose to compare and their corresponding number of augmentations at the time of writing.

compare the number of distinct augmentations provided.

Each library has a slightly different focus: torchaudio and audiomentations integrate easily with pytorch (torchaudio's can also be GPU-accelerated) and are clearly intended to be used at train time to improve generalization of audio machine learning models. Pydub provides more generalpurpose audio processing functionality without much emphasis on either integrating with ML training or evaluation pipelines; the number of transformation functions in Pydub is also much lower than the other three.

We benchmark each audio augmentation in AugLy, as well as some analogues that exist in the other libraries. See Figure [6](#page-2-2) for the runtime in seconds of each augmentation in (1) AugLy, (2) pydub, (3) torchaudio, & (4) audiomentations.

| Augmentation       | (1)   | (2)  | (3)   | (4)   |
|--------------------|-------|------|-------|-------|
| PitchShift         | 1.238 |      | 0.372 | 0.651 |
| TimeStretch        | 0.415 |      | 0.053 | 0.121 |
| Reverb             | 0.271 |      | 0.267 |       |
| AddBackgroundNoise | 0.048 |      |       | 0.019 |
| ChangeVolume       | 0.035 | 3e-5 | 0.034 | 0.004 |
| HighPassFilter     | 0.017 | 3e-4 | 0.017 | 0.413 |
| ToMono             | 0.016 |      | 0.022 |       |
| Normalize          | 0.015 | 4e-5 | 0.043 | 0.004 |
| LowPassFilter      | 0.014 | 5e-4 | 0.013 | 0.163 |
| Clip               | 0.002 |      |       | 0.003 |
| Speed              | 0.002 | 6e-5 |       |       |

<span id="page-2-2"></span>Figure 6. The runtime (in seconds) of audio augmentations in (1) AugLy, (2) pydub, (3) torchaudio, & (4) audiomentations.

### 4.2. Image

We compare AugLy's image augmentations to three well-established libraries: imgaug[\[14\]](#page-7-3), torchvision[\[19\]](#page-7-2), and Albumentations[\[1\]](#page-6-3). See Figure [7](#page-3-0) for a comparison of the four libraries in terms of the number of distinct augmentations provided.

Whereas imgaug, torchvision, and Albumentations are all geared toward providing general image augmentations to be used in computer vision training pipelines for regular-

<span id="page-3-4"></span>

| Library        | # augmentations |
|----------------|-----------------|
| torchvision    | 28              |
| AugLy          | 34              |
| Albumentations | 54              |
| imgaug         | 179             |

<span id="page-3-0"></span>Figure 7. The image augmentation libraries we chose to compare and their corresponding number of augmentations at the time of writing.

ization purposes, AugLy is more focused on replicating image transformations that users perform online. For example none of the other three libraries contain overlay augmentations (e.g. "OverlayText", "OverlayEmoji", or "Overlay-OntoScreenshot"), although these are extremely common image manipulations.

This indicates a gap in existing image augmentation libraries: models are not being trained to be invariant to data manipulations that they will see in the real world. For instance, a model that detects violent or harmful content in images on any online platform needs to be invariant to the augmentations provided in AugLy; otherwise a user can bypass that model by overlaying an emoji onto the harmful image or overlaying the image onto a background.

We benchmark each AugLy image augmentation, as well as any analogues that exist in the other libraries. See Figure [8](#page-3-1) for the runtime in seconds of each augmentation in (1) AugLy, (2) imgaug, (3) torchvision, & (4) Albumentations.

| Augmentation         | (1)   | (2)   | (3)   | (4)   |
|----------------------|-------|-------|-------|-------|
| PerspectiveTransform | 0.333 | 0.032 | 0.076 | 0.013 |
| Sharpen              | 0.159 | 0.021 | 0.141 | 0.005 |
| ColorJitter          | 0.108 | 0.038 | 0.107 | 0.015 |
| Blur                 | 0.097 | 0.013 | 0.143 | 0.005 |
| Saturation           | 0.091 | 1.301 | 0.057 | 0.015 |
| Pixelization         | 0.081 | 0.034 |       |       |
| Brightness           | 0.078 |       | 0.056 | 0.005 |
| Resize               | 0.056 | 0.014 | 0.050 | 0.006 |
| EncodingQuality      | 0.041 | 0.050 |       | 0.002 |
| Contrast             | 0.031 | 0.007 | 0.074 |       |
| Rotate               | 0.024 | 0.019 | 0.011 | 0.028 |
| Pad                  | 0.010 | 0.018 | 0.005 | 0.008 |
| ApplyLambda          | 0.008 |       |       | 2e-5  |
| Grayscale            | 0.005 | 0.030 | 0.002 | 0.001 |
| HFlip                | 0.005 | 0.002 | 0.003 | 0.001 |
| VFlip                | 0.003 | 0.001 | 0.002 | 0.001 |
| Crop                 | 0.001 | 0.008 | 6e-4  | 2e-5  |

<span id="page-3-1"></span>Figure 8. The runtime (in seconds) of image augmentations in (1) AugLy, (2) imgaug, (3) torchvision, & (4) Albumentations. Albumentations consistently outperforms any other library, likely due to the fact that it uses NumPy arrays as opposed to PIL. We continue to use PIL because it allows for (a) an easy integration with torchvision's Compose() and (b) better code readability.

| Library    | # augmentations |
|------------|-----------------|
| TextAttack | 13              |
| AugLy      | 16              |
| nlpaug     | 16              |
| textflint  | 55              |

<span id="page-3-2"></span>Figure 9. The text augmentation libraries we chose to compare and their corresponding number of augmentations at the time of writing.

#### 4.3. Text

We compare AugLy's text augmentations to three existing text libraries: nlpaug[\[18\]](#page-7-4), TextAttack[\[21\]](#page-7-5), & textflint[\[8\]](#page-6-4). See Figure [9](#page-3-2) for a comparison of the five libraries in terms of the number of distinct augmentations provided.

One significant difference between AugLy and the other text augmentation libraries is the prevalence of syntactic versus semantic (i.e. character-level vs word-level) augmentations. Most augmentations in nlpaug and TextAttack are semantic (e.g. words being swapped for synonyms or antonyms), or a few simple syntactic ones (e.g. deleting/adding characters, replacing characters with nearby ones on the keyboard). AugLy provides many syntactic augmentations that are often used online in an attempt to evade detection, such as inserting punctuation, zero-width, or bidirectional characters and changing fonts.

We benchmark each AugLy text augmentation, as well as any analogues that exist in the other libraries. See Figure [10](#page-3-3) for the runtime in seconds of each augmentation in (1) AugLy, (2) nlpaug, (3) TextAttack, & (4) textflint.

| Augmentation  | (1)   | (2)   | (3)   | (4)   |
|---------------|-------|-------|-------|-------|
| SimulateTypos | 0.276 | 0.101 | 0.006 | 4e-4  |
| SwapGendered  |       |       |       |       |
| Words         | 0.102 |       |       | 0.003 |
| Replace       |       |       |       |       |
| SimilarChars  | 0.102 | 0.101 | 0.006 | 0.001 |
| SplitWords    | 0.101 | 0.101 |       |       |
| Contractions  | 0.001 |       | 1e-4  | 2e-4  |
| ChangeCase    | 4e-4  |       |       | 3e-4  |
| Insert        |       |       |       |       |
| Punctuation   |       |       |       |       |
| Chars         | 1e-4  |       | 0.002 | 6e-4  |

<span id="page-3-3"></span>Figure 10. The runtime (in seconds) of text augmentations in (1) AugLy, (2) nlpaug, (3) TextAttack, & (4) textflint.

#### 4.4. Video

We compare AugLy's video augmentations to three existing libraries: moviepy[\[32\]](#page-7-9), pytorchvideo[\[6\]](#page-6-5), and

<span id="page-4-2"></span>

| Library      | # augmentations |
|--------------|-----------------|
| pytorchvideo | 19              |
| moviepy      | 30              |
| vidaug       | 40              |
| AugLy        | 43              |

<span id="page-4-0"></span>Figure 11. The video augmentation libraries we chose to compare and their corresponding number of augmentations at the time of writing.

vidaug[\[16\]](#page-7-10). See Figure [11](#page-4-0) for a comparison of the four libraries in terms of the number of distinct augmentations provided.

Most existing video augmentations either focus on manipulating the spatial dimension *or* the temporal dimension, as opposed to both. For instance, many individuals apply spatial image augmentations frame by frame onto videos; pytorchvideo provides one such API to do this using the torchvision transforms. Although spatial augmentations are effective, applying temporal augmentations in tandem has been shown to improve performance[\[30\]](#page-7-15).

Moviepy is more of a general video processing and editing library, but it provides both spatial and temporal manipulations such as changing the speed of the video, trimming, and spatial cropping. vidaug provides similar spatial and temporal augmentations. However, none of these existing libraries provide the option to augment the audio or to perform overlay augmentations which AugLy does provide. AugLy provides a wide array of spatiotemporal augmentations which are common online such as temporally splicing one video into another, simulating a screenshot reshare, and overlaying one video onto another. AugLy is also unique in its multimodal integration, meaning a video's audio can be transformed then recombined with the video in conjunction with other augmentations).

We benchmark each AugLy video augmentation, as well as the analogues that exist in the other libraries. See Figure [12](#page-4-1) for the runtime in seconds of each augmentation in (1) AugLy, (2) moviepy, (3) pytorchvideo, & (4) vidaug.

### 5. Robustness Evaluation

To demonstrate how AugLy can be used to evaluate robustness, we evaluated a few ImageNet models throughout the years on AugLy augmentations. We were interested to see how robustness has evolved as models' accuracy has improved, as well as understanding which augmentations the models were particularly vulnerable to. We chose three models to evaluate: VGG16[\[25\]](#page-7-16), Resnet152[\[9\]](#page-6-12), and Efficientnet-L2 (Noisy Student)[\[28\]](#page-7-17).

We evaluated the aforementioned models on the ImageNet validation set, which is commonly used since the test

| Augmentation | (1)   | (2)   | (3)   | (4)   |
|--------------|-------|-------|-------|-------|
| Loop         | 2.015 | 2e-4  |       |       |
| Shift        | 0.773 |       |       | 0.016 |
| Pixelization | 0.662 |       |       | 1.996 |
| AugmentAudio | 0.625 | 0.001 |       |       |
| Pad          | 0.400 | 0.018 |       |       |
| TimeCrop     | 0.395 |       |       | 1e-5  |
| Crop         | 0.352 | 9e-5  |       | 2e-5  |
| Rotate       | 0.336 | 1e-4  | 0.202 | 0.275 |
| Blur         | 0.307 |       | 0.140 | 0.179 |
| VFlip        | 0.297 | 9e-5  | 0.151 | 2e-5  |
| AddNoise     | 0.297 |       |       | 0.036 |
| Resize       | 0.289 | 0.015 |       |       |
| ChangeVideo  |       |       |       |       |
| Speed        | 0.269 | 1e-4  |       | 1e-4  |
| HFlip        | 0.269 | 1e-4  | 0.152 | 2e-5  |
| Grayscale    | 0.266 | 0.047 | 0.081 |       |
| ColorJitter  | 0.262 | 0.035 | 0.077 |       |
| Brightness   | 0.258 |       | 0.050 |       |

<span id="page-4-1"></span>Figure 12. The runtime (in seconds) of video augmentations in (1) AugLy, (2) moviepy, (3) pytorchvideo, & (4) vidaug. Although other libraries are faster, we continue to use the FFMPEG CLI because we want to be able to process large videos effectively and conserve memory, instead of storing and passing videos in memory as (3) and (4) do.

set is not available for download. However, to avoid any potential bias due to overfitting, we evaluated on an additional dataset, ImageNet V2. "ImageNet V2"[\[22\]](#page-7-18) was put together by researchers with the intention to be a held-out test set for ImageNet that can be evaluated on with no risk of overfitting.

We evaluated the robustness of each model across many different AugLy image augmentations by sampling 250 images from each dataset, computing the top-5 accuracy on those images, and computing the top-5 accuracy when the images are augmented using each augmentation. The change in top-5 accuracy from the baseline (i.e. when the images are not augmented) to the augmented images gives us a measure of how vulnerable the model is to that augmentation. We chose a diverse set of augmentations and set the parameters such that the augmentations were very noticeable but the content of the image was still recognizable to the human eye. See examples of some of the augmentations in Figure [14.](#page-5-0) The notebook used to perform this robustness evaluation can be found in the AugLy repo at [https://github.com/facebookresearch/](https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/pwc_imagenet_v1_vs_v2_metrics.ipynb) [AugLy/blob/main/examples/imagenet/pwc\\_](https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/pwc_imagenet_v1_vs_v2_metrics.ipynb) [imagenet\\_v1\\_vs\\_v2\\_metrics.ipynb](https://github.com/facebookresearch/AugLy/blob/main/examples/imagenet/pwc_imagenet_v1_vs_v2_metrics.ipynb).

In Figure [13,](#page-5-1) VGG and ResNet are pretty vulnerable to AugLy augmentations across the board. Efficient-Net, on the other hand, is much more robust to most

<span id="page-5-2"></span>![](_page_5_Figure_0.jpeg)

**Caption:** Figure 13 presents the change in top-5 accuracy for three models (VGG16, ResNet152, EfficientNet-L2) when subjected to AugLy augmentations. The results indicate that VGG and ResNet are more vulnerable to augmentations, while EfficientNet-L2 shows greater resilience, reflecting differences in training augmentation strategies.

<span id="page-5-1"></span>Figure 13. The change in top-5 accuracy caused by each augmentation on each model, computed on a sample of 250 images from the ImageNet validation set.

augmentations except for blur and random noise which cause a larger drop in accuracy. This makes sense considering the augmentations each model was trained on: VGG was trained on augmentations equivalent to AugLy's crop, hflip, & color jitter; ResNet was trained on crop, hflip, scale, & color changes similar to color jitter. EfficientNet was trained using AutoAugment[\[2\]](#page-6-7), which includes a much wider range of augmentations such as shear x/y, translate x/y, rotate, contrast, invert, solarize, posterize, color, brightness, sharpness, and cutout.

Whereas VGG & ResNet were trained on a very limited set of spatial and color-based augmentations, EfficientNet was trained on a larger number of both spatial and colorbased augmentations, as well as cutout which is similar to the overlay augmentations in AugLy (but instead of overlaying content over the image, black rectangles are overlaid). However, none of the three models were trained on pixel-level augmentations such as blur, random noise, or pixelization, which likely explains why all three models are vulnerable to those augmentations. Figure [14](#page-5-0) illustrates a few examples from AugLy of the four categories: spatial, color, overlay, and pixel-level augmentations.

We validated that these results are comparable on the ImageNet V2 dataset, shown in Figure [15.](#page-6-13) Similar to evaluation on the ImageNet validation dataset, VGG and ResNet are quite vulnerable to all augmentations at varying degrees, and EfficientNet is significantly less so with the exception of blur & random noise.

Figure [16](#page-6-14) shows the drop in accuracy on EfficientNet for each augmentation with respect to the original ImageNet validation set and ImageNet V2. The drop in accuracy is close on both datasets for all augmentations, so there is no indication of overfitting on the validation set.

![](_page_5_Figure_6.jpeg)

**Caption:** Figure 14 illustrates examples of different categories of image augmentations applied to a sample image from the ImageNet validation set. The augmentations include spatial, color, overlay, and pixel-level transformations, demonstrating AugLy's capability to simulate realistic data perturbations encountered online.

<span id="page-5-0"></span>Figure 14. Examples of each different category of image augmentation, as shown on an image from the ImageNet validation set of class 259 (Pomeranian).

## 6. Conclusion

We presented AugLy, a new multimodal augmentation library with a focus on robustness. We compared each sublibrary (audio, image, text, and video) to other similar aug-

![](_page_6_Figure_0.jpeg)

**Caption:** Figure 15 shows the change in top-5 accuracy for the EfficientNet-L2 model across various augmentations, evaluated on the ImageNet V2 dataset. The results reveal consistent performance trends, indicating the model's robustness to certain augmentations while highlighting areas for potential improvement.

<span id="page-6-13"></span>Figure 15. The change in top-5 accuracy caused by each augmentation on each model, computed on a sample of 250 images from the ImageNet V2 "matched frequency" dataset.

![](_page_6_Figure_2.jpeg)

**Caption:** Figure 16 compares the drop in accuracy for EfficientNet-L2 across different augmentations on both the ImageNet validation set and ImageNet V2. The findings suggest that while the model is generally robust, it remains susceptible to specific augmentations like blur and random noise.

<span id="page-6-14"></span>Figure 16. The change in top-5 accuracy caused by each augmentation on the EfficientNet-L2 (Noisy Student) model, computed on both the ImageNet validation set & the ImageNet V2 set.

mentation libraries, assessing the amount of augmentations offered, the kinds of augmentations available, and benchmarking analogous functions to observe their performance. While other libraries may be more performant time-wise, AugLy provides a wide range of unique augmentation that replicate real modifications seen online. Additionally, we evaluated our augmentations on three state-of-the-art image classification models over time, showing that retraining on augmented data is an effective method for building defenses against various attack types.

## Acknowledgements

We would like to thank A.K.M Adib, Erik Chou, Aditya Prasad, and Guillermo Sanchez for their contributions to this paper by benchmarking and improving the efficiency of AugLy's augmentations!

# References

- <span id="page-6-3"></span>[1] Alexander Buslaev, Alex Parinov, Eugene Khvedchenya, Vladimir I. Iglovikov, and Alexandr A. Kalinin. Albumentations: fast and flexible image augmentations, 2020. Information. [2,](#page-1-0) [3](#page-2-3)
- <span id="page-6-7"></span>[2] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation strategies from data, 2019. CVPR. [2,](#page-1-0) [6](#page-5-2)
- <span id="page-6-8"></span>[3] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston. Queens are powerful too: Mitigating gender bias in dialogue generation, 2020. EMNLP. [2](#page-1-0)
- <span id="page-6-9"></span>[4] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) dataset, 2020. [3](#page-2-3)
- <span id="page-6-10"></span>[5] Matthijs Douze, Giorgos Tolias, Ed Pizzi, Zoe Papakipos, Lowik Chanussot, Filip Radenovic, Tomas Jenicek, Maxim Maximov, Laura Leal-Taixe, Ismail Elezi, Ond ´ ˇrej Chum, and Cristian Canton Ferrer. The 2021 image similarity challenge and dataset, 2021. [3](#page-2-3)
- <span id="page-6-5"></span>[6] Haoqi Fan, Tullie Murrell, Heng Wang, Kalyan Vasudev Alwala, Yanghao Li, Yilei Li, Bo Xiong, Nikhila Ravi, Meng Li, Haichuan Yang, Jitendra Malik, Ross Girshick, Matt Feiszli, Aaron Adcock, Wan-Yen Lo, and Christoph Feichtenhofer. PyTorchVideo: A deep learning library for video understanding. In *Proceedings of the 29th ACM International Conference on Multimedia*, 2021. [https://](https://pytorchvideo.org/) [pytorchvideo.org/](https://pytorchvideo.org/). [2,](#page-1-0) [4](#page-3-4)
- <span id="page-6-0"></span>[7] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. A survey of data augmentation approaches for nlp, 2021. [1](#page-0-0)
- <span id="page-6-4"></span>[8] Tao Gui, Xiao Wang, Qi Zhang, Qin Liu, Yicheng Zou, Xin Zhou, Rui Zheng, Chong Zhang, Jiacheng Ye Qinzhuo Wu, Zexiong Pang, Yongxin Zhang, Zhengyan Li, Ruotian Ma, Zichu Fei, Ruijian Cai, Xingwu Hu Jun Zhao, Zhiheng Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Bolin Zhu, Shan Qin, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Zhongyu Wei Yaqian Zhou, Xipeng Qiu, and Xuanjing Huang. Textflint: Unified multilingual robustness evaluation toolkit for natural language processing, 2021. arXiv preprint arXiv:2103.11441. [2,](#page-1-0) [4](#page-3-4)
- <span id="page-6-12"></span>[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015. [5](#page-4-2)
- <span id="page-6-2"></span>[10] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization, 2021. ICCV. [1](#page-0-0)
- <span id="page-6-1"></span>[11] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations, 2019. ICLR. [1](#page-0-0)
- <span id="page-6-6"></span>[12] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty, 2020. ICLR. [2](#page-1-0)
- <span id="page-6-11"></span>[13] Iver Jordal. Audiomentations, Aug. 2021. [3](#page-2-3)
- <span id="page-7-3"></span>[14] Alexander B. Jung, Kentaro Wada, Jon Crall, Satoshi Tanaka, Jake Graving, Christoph Reinders, Sarthak Yadav, Joy Banerjee, Gabor Vecsei, Adam Kraft, Zheng Rui, ´ Jirka Borovec, Christian Vallentin, Semen Zhydenko, Kilian Pfeiffer, Ben Cook, Ismael Fernandez, Franc¸ois-Michel ´ De Rainville, Chi-Hung Weng, Abner Ayala-Acevedo, Raphael Meudec, Matias Laporte, et al. imgaug. [https:](https://github.com/aleju/imgaug) [//github.com/aleju/imgaug](https://github.com/aleju/imgaug), 2020. Online; accessed 01-Feb-2020. [2,](#page-1-0) [3](#page-2-3)
- <span id="page-7-8"></span>[15] Eugene Kharitonov, Morgane Riviere, Gabriel Synnaeve, ` Lior Wolf, Pierre-Emmanuel Mazare, Matthijs Douze, and ´ Emmanuel Dupoux. Data augmenting contrastive learning of speech representations in the time domain, 2020. CoRR, vol. abs/2007.00991. [2](#page-1-0)
- <span id="page-7-10"></span>[16] Okan Kopuklu. vidaug. [https://github.com/](https://github.com/okankop/vidaug) [okankop/vidaug](https://github.com/okankop/vidaug), 2018. [2,](#page-1-0) [5](#page-4-2)
- <span id="page-7-1"></span>[17] Xiaolong Liu, Jinchao Liang, Zi-Yi Wang, Yi-Te Tsai, Chia-Chen Lin, and Chih-Cheng Chen. Content-based image copy detection using convolutional neural network, 2020. [1](#page-0-0)
- <span id="page-7-4"></span>[18] Edward Ma. nlpaug. [https : / / github . com /](https://github.com/makcedward/nlpaug) [makcedward/nlpaug](https://github.com/makcedward/nlpaug), 2019. [2,](#page-1-0) [4](#page-3-4)
- <span id="page-7-2"></span>[19] Francisco Massa, Vasilis Vryniotis, and Nicolas Hug. torchvision. [https : / / github . com / pytorch /](https://github.com/pytorch/vision) [vision](https://github.com/pytorch/vision), 2017. [2,](#page-1-0) [3](#page-2-3)
- <span id="page-7-6"></span>[20] Brian McFee, Colin Raffel, Dawen Liang, Daniel P.W. Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python, 2015. SCIPY. [2](#page-1-0)
- <span id="page-7-5"></span>[21] John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pages 119–126, 2020. [2,](#page-1-0) [4](#page-3-4)
- <span id="page-7-18"></span>[22] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet?, 2019. [5](#page-4-2)
- <span id="page-7-14"></span>[23] James Robert. pydub. [https : / / github . com /](https://github.com/jiaaro/pydub) [jiaaro/pydub](https://github.com/jiaaro/pydub), 2011. [3](#page-2-3)
- <span id="page-7-0"></span>[24] C. Shorten and T.M. Khoshgoftaar. A survey on image data augmentation for deep learning, 2019. J Big Data 6, 60. [1](#page-0-0)
- <span id="page-7-16"></span>[25] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition, 2015. [5](#page-4-2)
- <span id="page-7-12"></span>[26] Eric Michael Smith and Adina Williams. Hi, my name is martha: Using names to measure and mitigate bias in generative dialogue models, 2021. [2](#page-1-0)
- <span id="page-7-13"></span>[27] Roshan Sumbaly, Mahalia Miller, Hardik Shah, Yang Xie, Sean Chang Culatana, Tim Khatkevich, Enming Luo, Emanuel Strauss, Gergely Szilvasy, Manika Puri, Pratyusa Manadhata, Benjamin Graham, Matthijs Douze, Zeki Yalniz, and Herve Jegou. Using ai to de- ´ tect covid-19 misinformation and exploitative content. [https://ai.facebook.com/blog/using- ai](https://ai.facebook.com/blog/using-ai-to-detect-covid-19-misinformation-and-exploitative-content/)[to- detect- covid- 19- misinformation- and](https://ai.facebook.com/blog/using-ai-to-detect-covid-19-misinformation-and-exploitative-content/)[exploitative-content/](https://ai.facebook.com/blog/using-ai-to-detect-covid-19-misinformation-and-exploitative-content/), 2020. [3](#page-2-3)
- <span id="page-7-17"></span>[28] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks, 2020. [5](#page-4-2)
- <span id="page-7-7"></span>[29] Yao-Yuan Yang, Moto Hira, Zhaoheng Ni, Anjali Chourdia, Artyom Astafurov, Caroline Chen, Ching-Feng Yeh, Christian Puhrsch, David Pollack, Dmitriy Genzel, Donny Greenberg, Edward Z. Yang, Jason Lian, Jay Mahadeokar, Jeff Hwang, Ji Chen, Peter Goldsborough, Prabhat Roy, Sean Narenthiran, Shinji Watanabe, Soumith Chintala, Vincent Quenneville-Belair, and Yangyang Shi. Torchaudio: Build- ´ ing blocks for audio and speech processing. *arXiv preprint arXiv:2110.15018*, 2021. [2,](#page-1-0) [3](#page-2-3)
- <span id="page-7-15"></span>[30] Sangdoo Yun, Seong Oh Joon, Byeongho Heo, Dongyoon Han, and Jinhyung Kim. Videomix: Rethinking data augmentation for video classification, 2020. [5](#page-4-2)
- <span id="page-7-11"></span>[31] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15–20, New Orleans, Louisiana. Association for Computational Linguistics. [2](#page-1-0)
- <span id="page-7-9"></span>[32] Zulko. moviepy. [https://github.com/Zulko/](https://github.com/Zulko/moviepy) [moviepy](https://github.com/Zulko/moviepy), 2018. [2,](#page-1-0) [4](#page-3-4)

# Appendix

| Augmentation       | (1)   | (2)  | (3)   | (4)   |
|--------------------|-------|------|-------|-------|
| Harmonic           | 2.897 |      |       |       |
| Percussive         | 2.897 |      |       |       |
| PitchShift         | 1.238 |      | 0.372 | 0.651 |
| TimeStretch        | 0.415 |      | 0.053 | 0.121 |
| Reverb             | 0.271 |      | 0.267 |       |
| Tempo              | 0.195 |      |       |       |
| AddBackgroundNoise | 0.048 |      |       | 0.019 |
| PeakingEqualizer   | 0.048 |      |       |       |
| InsertInBackground | 0.044 |      |       |       |
| ChangeVolume       | 0.035 | 3e-5 | 0.034 | 0.004 |
| HighPassFilter     | 0.017 | 3e-4 | 0.017 | 0.413 |
| ToMono             | 0.016 |      | 0.022 |       |
| Normalize          | 0.015 | 4e-5 | 0.043 | 0.004 |
| LowPassFilter      | 0.014 | 5e-4 | 0.013 | 0.163 |
| Clicks             | 0.009 |      |       |       |
| Loop               | 0.006 |      |       |       |
| InvertChannels     | 0.003 |      |       |       |
| ApplyLambda        | 0.003 |      |       |       |
| Speed              | 0.002 | 6e-5 |       |       |
| Clip               | 0.002 |      |       | 0.003 |

Figure 17. The runtime (in seconds) of audio augmentations in (1) AugLy, (2) pydub, (3) torchaudio, & (4) audiomentations.

| Augmentation         | (1)   | (2)   | (3)   | (4)   |
|----------------------|-------|-------|-------|-------|
| ShufflePixels        | 1.600 |       |       |       |
| PerspectiveTransform | 0.333 | 0.032 | 0.076 | 0.013 |
| Sharpen              | 0.159 | 0.021 | 0.141 | 0.005 |
| ApplyPILFilter       | 0.117 |       |       |       |
| ColorJitter          | 0.108 | 0.038 | 0.107 | 0.015 |
| Blur                 | 0.097 | 0.013 | 0.143 | 0.005 |
| Saturation           | 0.091 | 1.301 | 0.057 | 0.015 |
| ChangeAspectRatio    | 0.091 |       |       |       |
| Skew                 | 0.084 |       |       |       |
| OverlayStripes       | 0.083 |       |       |       |
| Pixelization         | 0.081 | 0.034 |       |       |
| Brightness           | 0.078 |       | 0.056 | 0.005 |
| OverlayOnto          |       |       |       |       |
| Screenshot           | 0.064 |       |       |       |
| Scale                | 0.059 |       |       |       |
| Resize               | 0.056 | 0.014 | 0.050 | 0.006 |
| OverlayOnto          |       |       |       |       |
| BackgroundImage      | 0.049 |       |       |       |
| EncodingQuality      | 0.041 | 0.050 |       | 0.002 |
| ConvertColor         | 0.039 |       |       |       |
| MaskedComposite      | 0.038 |       |       |       |
| Contrast             | 0.031 | 0.007 | 0.074 |       |
| Opacity              | 0.029 |       |       |       |
| OverlayText          | 0.027 |       |       |       |
| MemeFormat           | 0.025 |       |       |       |
| Rotate               | 0.024 | 0.019 | 0.011 | 0.028 |
| OverlayImage         | 0.023 |       |       |       |
| Pad                  | 0.010 | 0.018 | 0.005 | 0.008 |
| ApplyLambda          | 0.008 |       |       | 2e-5  |
| PadSquare            | 0.008 |       |       |       |
| OverlayEmoji         | 0.006 |       |       |       |
| Grayscale            | 0.005 | 0.030 | 0.002 | 0.001 |
| HFlip                | 0.005 | 0.002 | 0.003 | 0.001 |
| VFlip                | 0.003 | 0.001 | 0.002 | 0.001 |
| ClipImageSize        | 0.002 |       |       |       |
| Crop                 | 0.001 | 0.008 | 6e-4  | 2e-5  |

| Augmentation   | (1)   | (2)   | (3)   | (4)   |
|----------------|-------|-------|-------|-------|
| SimulateTypos  | 0.276 | 0.101 | 0.006 | 4e-4  |
| SwapGendered   |       |       |       |       |
| Words          | 0.102 |       |       | 0.003 |
| Replace        |       |       |       |       |
| FunFonts       | 0.102 |       |       |       |
| ReplaceSimilar |       |       |       |       |
| UnicodeChars   | 0.102 |       |       |       |
| Replace        |       |       |       |       |
| UpsideDown     | 0.102 |       |       |       |
| MergeWords     | 0.102 |       |       |       |
| Replace        |       |       |       |       |
| SimilarChars   | 0.102 | 0.101 | 0.006 | 0.001 |
| SplitWords     | 0.101 | 0.101 |       |       |
| ReplaceWords   | 0.101 |       |       |       |
| GetBaseline    | 0.101 |       |       |       |
| Contractions   | 0.001 |       | 1e-4  | 2e-4  |
| ChangeCase     | 4e-4  |       |       | 3e-4  |
| Insert         |       |       |       |       |
| Punctuation    |       |       |       |       |
| Chars          | 1e-4  |       | 0.002 | 6e-4  |
| Insert         |       |       |       |       |
| Whitespace     |       |       |       |       |
| Chars          | 7e-5  |       |       |       |
| Replace        |       |       |       |       |
| Bidirectional  | 7e-5  |       |       |       |
| InsertZero     |       |       |       |       |
| WidthChars     | 6e-5  |       |       |       |
| ApplyLambda    | 6e-5  |       |       |       |

Figure 19. The runtime (in seconds) of text augmentations in (1) AugLy, (2) nlpaug, (3) TextAttack, & (4) textflint.

Figure 18. The runtime (in seconds) of image augmentations in (1) AugLy, (2) imgaug, (3) torchvision, & (4) Albumentations.

| Augmentation    | (1)   | (2)   | (3)   | (4)   |
|-----------------|-------|-------|-------|-------|
| ReplaceWith     |       |       |       |       |
| ColorFrames     | 2.241 |       |       |       |
| Loop            | 2.015 | 2e-4  |       |       |
| Perspective     |       |       |       |       |
| Transform       |       |       |       |       |
| AndShake        | 2.015 |       |       |       |
| BlendVideos     | 1.905 |       |       |       |
| ReplaceWith     |       |       |       |       |
| Background      | 1.746 |       |       |       |
| OverlayText     | 1.677 |       |       |       |
| OverlayShapes   | 1.659 |       |       |       |
| TimeDecimate    | 1.636 |       |       |       |
| OverlayOnto     |       |       |       |       |
| Screenshot      | 1.632 |       |       |       |
| OverlayDots     | 1.631 |       |       |       |
| MemeFormat      | 1.610 |       |       |       |
| InsertIn        |       |       |       |       |
| Background      | 1.605 |       |       |       |
| OverlayOnto     |       |       |       |       |
| BackgroundVideo | 0.781 |       |       |       |
| Shift           | 0.773 |       |       | 0.016 |
| Pixelization    | 0.662 |       |       | 1.996 |
| AugmentAudio    | 0.625 | 0.001 |       |       |
| OverlayEmoji    | 0.501 |       |       |       |
| Concat          | 0.467 |       |       |       |
| AudioSwap       | 0.445 |       |       |       |
| VStack          | 0.435 |       |       |       |
|                 |       |       |       |       |
| Overlay         | 0.406 |       |       |       |
| HStack          | 0.400 |       |       |       |
| Pad             | 0.400 | 0.018 |       |       |
| TimeCrop        | 0.395 |       |       | 1e-5  |
| Trim            | 0.386 |       |       |       |
| Change          |       |       |       |       |
| AspectRatio     | 0.368 |       |       |       |
| Crop            | 0.352 | 9e-5  |       | 2e-5  |
| Rotate          | 0.336 | 1e-4  | 0.202 | 0.275 |
| Blur            | 0.307 |       | 0.140 | 0.179 |
| VFlip           | 0.297 | 9e-5  | 0.151 | 2e-5  |
| AddNoise        | 0.297 |       |       | 0.036 |
| Resize          | 0.289 | 0.015 |       |       |
| Scale           | 0.284 |       |       |       |
| FPS             | 0.271 |       |       |       |
| ChangeVideo     |       |       |       |       |
| Speed           | 0.269 | 1e-4  |       | 1e-4  |
| HFlip           | 0.269 | 1e-4  | 0.152 | 2e-5  |
| Grayscale       | 0.266 | 0.047 | 0.081 |       |
| Contrast        | 0.264 |       |       |       |
| Encoding        |       |       |       |       |
| Quality         | 0.262 |       |       |       |
| ColorJitter     | 0.262 | 0.035 | 0.077 |       |
| Brightness      | 0.258 |       | 0.050 |       |
| RemoveAudio     | 0.255 |       |       |       |
| ApplyLambda     | 5e-4  |       |       |       |

Figure 20. The runtime (in seconds) of video augmentations in (1) AugLy, (2) moviepy, (3) pytorchvideo, & (4) vidaug. 10