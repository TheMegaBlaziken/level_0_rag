# LABEL-INVARIANT AUGMENTATION FOR SEMI-SUPERVISED GRAPH CLASSIFICATION

Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu Brandeis University {hanyue, chunhuizhang, chuxuzhang, hongfuliu}@brandeis.edu

#### ABSTRACT

Recently, contrastiveness-based augmentation surges a new climax in the computer vision domain, where some operations, including rotation, crop, and flip, combined with dedicated algorithms, dramatically increase the model generalization and robustness. Following this trend, some pioneering attempts employ the similar idea to graph data. Nevertheless, unlike images, it is much more difficult to design reasonable augmentations without changing the nature of graphs. Although exciting, the current graph contrastive learning does not achieve as promising performance as visual contrastive learning. We conjecture the current performance of graph contrastive learning might be limited by the violation of the label-invariant augmentation assumption. In light of this, we propose a label-invariant augmentation for graph-structured data to address this challenge. Different from the node/edge modification and subgraph extraction, we conduct the augmentation in the representation space and generate the augmented samples in the most difficult direction while keeping the label of augmented data the same as the original samples. In the semi-supervised scenario, we demonstrate our proposed method outperforms the classical graph neural network based methods and recent graph contrastive learning on eight benchmark graph-structured data, followed by several in-depth experiments to further explore the label-invariant augmentation in several aspects.

*K*eywords Graph Contrastive Learning Â· Semi-supervised Classification

#### 1 Introduction

Contrastive augmentation aims to expand training data in both volume and diversity in a self-supervised fashion to increase model robustness and generalization. Common sense and domain knowledge are employed to design the contrastive augmentation operations. Denoising auto-encoder [\[1,](#page-8-0) [2\]](#page-8-1) is one of the pioneering studies to apply perturbations to generate contrastive samples for tablet data, which takes a corrupted input and recovers the original undistorted input. For visual data, some operations, including rotation, crop, and flip, combined with dedicated algorithms, significantly improve the learning performance in diverse tasks [\[3,](#page-8-2) [4,](#page-8-3) [5,](#page-8-4) [6,](#page-8-5) [7\]](#page-8-6). Treating the augmented and original samples as positive pairs and the augmented samples from different source samples as negative pairs, contrastive learning aims to learn the augment-invariant representations by increasing the similarity of positive pairs and the dissimilarity of negative pairs [\[8\]](#page-8-7). These positive pairs increase the model robustness due to the assumption that the augmented operations preserve the nature of images and make the augmented samples have consistent labels with the original ones. The negative pairs work as the instance-level discrimination, which is expected to enhance the model generalization, but might deteriorate the downstream task since the negative pairs contain the augmented samples from different source samples but with the same category. The recent BYOL [\[9\]](#page-8-8) and SimSiam [\[10\]](#page-8-9) demonstrate the negative effect of the negative pairs and conclude that the current performance of contrastive learning can be further boosted even without negative pairs.

Following this trend, some pioneering attempts employ contrastive augmentation to graph data. GraphCL [\[11\]](#page-8-10) is the first work to address the graph contrastive learning problem with four types of augmentations, including node dropping, edge perturbation, attribute masking, and subgraph extraction. Later, JOAO [\[12\]](#page-8-11) extends GraphCL by automatically selecting one type of graph augmentation from the above four types plus non-augmentation. GRACE [\[13\]](#page-8-12) treats the original graph data and the novel-level augmented data as two views and learns the graph representation by maximizing

the agreement between the two views. Similarly, MVGRL [\[14\]](#page-8-13) conducts the contrastive multi-view representation learning on both node and graph levels. Beyond the above studies to augment graphs, simGRACE [\[15\]](#page-8-14) perturbs the model parameters for contrastive learning, which can be regarded as an ensemble of model perturbation or a robust regularization. Although exciting, the above studies point out that the effectiveness of graph contrastive learning heavily hinges on ad-hoc data augmentations, which need to be carefully designed or selected per dataset and request more domain knowledge.

Contributions. We conjecture these hand-crafted graph augmentations might change the nature of the original graph and violate the label-invariant assumption in the downstream tasks. Different from treating graph contrastive learning in a pre-trained perspective, we aim to incorporate the downstream classification task into the representation learning, where the label information is fully used for both decision boundary learning and graph augmentation. Specifically, we propose Graph Label-invariant Augmentation (GLA), which conducts augmentation in the representation space and augments the most difficult sample while keeping the label of the augmented sample the same as the original sample. Our major contributions are summarized as follows:

- We propose a label-invariant augmentation strategy for graph contrastive learning, which involves labels in the downstream task to guide the contrastive augmentation. It is worthy to note that we do not generate any graph data. Instead, we directly generate the label-consistent representations as the augmented graphs during the training phase.
- In the rich representation space, we aim to generate the most difficult sample for the model and increase the model generalization. Rather than formulating it as an expensive bi-level optimization problem, we choose a lightweight technique by randomly generating a set of qualified candidates and selecting the most difficult one, i.e., minimizing the maximum loss or worst case loss over the augmented candidates.
- We conduct a series of semi-supervised experiments on eight graph benchmark datasets in a fair setting and compare our label-invariant augmentation with classical graph neural network based methods and recent graph contrastive methods by running the codes provided by the original authors. Extensive results demonstrate our label-invariant augmentation can achieve better performance in general cases without generating real augmented graphs and any specific domain knowledge. Besides algorithmic performance, we also provide rich and in-depth experiments to explore label-invariant augmentation in several aspects.

# 2 Related Work

Here we introduce the related work of graph neural networks and graph contrastive learning for graph classification. Node classification, although related, is not covered here due to its different setting.

Graph Neural Network. Graph Neural Networks (GNNs) have been employed on various graph learning tasks and achieved promising performance [\[16\]](#page-8-15). To extract the representation of each node, GNNs pass node embeddings from its connected neighbor nodes and apply feedforward neural networks to transform the aggregated features. As a pioneer study in GNNs, graph convolutional network (GCN) firstly aims to generalize the convolution mechanism from image to graph [\[16,](#page-8-15) [17,](#page-8-16) [18\]](#page-8-17). Based on GCN, instead of simply summing and averaging connected neighboring node's embedding, graph attention networks [\[19,](#page-8-18) [20,](#page-8-19) [21\]](#page-8-20) adopt an attention mechanism that builds self-attention to score each connected neighboring nodes' embedding to identify the more important nodes and enhance the effectiveness of message passing. Then in order to break prior GNN's limitations on message passing over long distances on large graphs, graph recurrent neural networks [\[22,](#page-8-21) [23\]](#page-8-22) apply the gating mechanism from RNNs to propagation on graph topology. Simultaneously, for dealing with the noise introduced from more than 3 layers of graph convolution, DeepGCN [\[24,](#page-9-0) [25\]](#page-9-1) uses skip connections and enables GCN to achieve better results with deeper layers. Recently, GAE [\[26\]](#page-9-2) and Infomax [\[27\]](#page-9-3) achieve state-of-the-art performance on several benchmark datasets. GAE extends the variational auto-encoder to graph neural networks for unsupervised learning, while Infomax learns the unsupervised representation on graphs to enlarge mutual information between local (node-level) and global (graph-level) representations in one graph.

Graph Contrastive Learning. Recently, many studies has been devoted to the graph contrastive learning area in diverse angles, including graph augmentation, negative sample selection, and view fusion. GraphCL [\[11\]](#page-8-10) summarizes four types of graph augmentations to learn the invariant representation across different augmented views. Built on GraphCL, JOAO [\[12\]](#page-8-11) proposes a learnable module to automatically select augmentation for different datasets to alleviate the human labor in combinations of these augmentations. Differently, MVGRL [\[14\]](#page-8-13) contrasts node and graph encodings across views which enriches more negative samples for contrastive learning. Later, InfoGCL [\[28\]](#page-9-4) diminishes the mutual information between contrastive parts among views while preserving the task-relevant representation. Beyond augmenting graphs, SimGRACE [\[15\]](#page-8-14) disturbs the model weights and then learns the invariant high-level representation at the output end to alleviate the design of graph augmentation.

Different from the above methods that separate the pre-train and fine-tuning phases, we aim to employ the label information in downstream tasks to guide the augmentation process. Specifically, in this study, we propose a labelinvariant augmentation strategy for graph-level representation learning.

### 3 Methodology

A graph can be represented by G = (V, X, A), where V = {v1, v2, ..., vn} is the set of vertexes, X â R <sup>n</sup>Ã<sup>d</sup> denotes the features of each vertex, and A â {0, 1} <sup>n</sup>Ã<sup>n</sup> represents the adjacency matrix. Given a set of labeled graphs S = {(G1, y1),(G2, y2), ...,(GM, yM)} where M is the number of labeled graphs, and y<sup>i</sup> â Y is the corresponding categorical label of graph G<sup>i</sup> â G (1â¤iâ¤M), and another set of unlabeled graphs T = {GM+1, ..., G<sup>N</sup> }, where N is the number of all graphs, M<N, the semi-supervised graph classification problem can be defined as learning a mapping function from graphs to categorical labels f : G â Y to predict the labels of T . In this section, we first illustrate our motivation supported by empirical evidence, then we elaborate on our Graph Label-invariant Augmentation (GLA) method for semi-supervised graph classification.

#### 3.1 Motivation

Augmentation plays an important role in neural network training. It not only improves the robustness of learned representation but also introduces rich data for training. For graph-structured data, GraphCL [\[11\]](#page-8-10) proposes four types of augmentations: node dropping, edge perturbation, attribute masking, and subgraph sampling. However, it is widely noticed that the effectiveness of graph contrastive learning highly depends on the chosen types of augmentations for specific graph data [\[12,](#page-8-11) [12\]](#page-8-11). To illustrate the difference between various augmentation combinations, we conduct experiments on *MUTAG* [\[29\]](#page-9-5) in a semi-supervised graph classification task, where the label rate is set to 50%. Figure [1](#page-2-0) shows the performance gains (classification accuracy %) of different augmenta-

![](_page_2_Figure_6.jpeg)

**Caption:** Figure 1 illustrates the performance gains (%) of GraphCL and JOAOv2 under various augmentation settings on the MUTAG dataset, highlighting that different combinations yield varying results. Notably, some augmentations negatively impact model training, emphasizing the need for careful selection in graph contrastive learning.

<span id="page-2-0"></span>Figure 1: Performance gains (%) of GraphCL and JOAOv2 under different augmentation settings on *MUTAG* [\[29\]](#page-9-5) dataset compared to none augmentation setting.

tion combinations compared to none augmentation. We can see that different augmentation combinations result in different performances, and JOAOv2 automatically selects data augmentations but cannot guarantee to outperform GraphCL with all augmentation combinations. Moreover, some augmentation combinations work worse than none augmentation, which demonstrates that some augmentations hurt the model training. We further fine-tune a model with 100% labeled graphs from *MUTAG*, and then feed the augmented graphs (randomly selected from the four augmentation types with an augmentation ratio of 0.2) to this model, finding that about 80% augmented graphs have the same labels with their corresponding original graphs. It indicates that most augmentations are reasonable, which is one of the reasons that GraphCL works well. While on the other hand, there are still about 20% augmented graphs getting different labels from the original ones. Motivated by this, we design a label-invariant augmentation strategy for graph contrastive learning.

#### 3.2 Label-invariant Augmentation

Framework Overview. Figure [2](#page-3-0) illustrates the framework of our proposed Graph Label-invariant Augmentation (GLA) for semi-supervised graph classification, which mainly consists of four components: Graph Neural Network Encoder, Classifier, Label-invariant Augmentation, and Projection Head. We first use GNN Encoder to get graph-level original representation for the input graph. Then Label-invariant Augmentation, together with Classifier, is utilized to generate augmented representation from original representation under a label-invariant constraint. For an unlabeled graph, we expect that the labels represented by original prediction and augmented prediction are the same. For a labeled graph, we expect that the label represented by augmented prediction is the same as the ground truth label. A cross-entropy loss is used to keep refining the classifier with labeled graphs. Finally, a Projection Head is adopted to generate projections for contrastive loss. We use Î = {Î¸G, Î¸<sup>C</sup> , Î¸<sup>P</sup> } to denote the trainable parameters set, where Î¸G, Î¸<sup>C</sup> , and Î¸<sup>P</sup> denote the parameters of GNN Encoder, Classifier and Projection Head, respectively. Details of each component are as follows.

Graph Neural Network Encoder. Graph Neural Network Encoder aims to get graph-level representations for graphstructured data. It is flexible to adopt various GNNs for this part. We follow GraphCL [\[11\]](#page-8-10) and utilize ResGCN [\[30\]](#page-9-6), which takes Graph Convolutional Network (GCN) [\[31\]](#page-9-7) as the backbone, to extract node-level representations from the input graph, and then a global sum pooling layer is used to obtain its graph-level representation. The computation of the

![](_page_3_Figure_1.jpeg)

**Caption:** Figure 2 presents the framework of Graph Label-invariant Augmentation (GLA) for semi-supervised graph classification. It includes components such as a GNN Encoder, Classifier, and Label-invariant Augmentation, which generates augmented representations while ensuring label consistency, enhancing model robustness.

<span id="page-3-0"></span>Figure 2: Framework of our Graph Label-invariant Augmentation (GLA) for semi-supervised graph classification. Given an input graph, a Graph Neural Network (GNN) Encoder is employed to encode the input graph into a graphlevel representation (original representation). Then we perturb the original representation to get multiple augmented representations. A classifier is adopted to verify whether the augmentations are label-invariant or not. We select the "hardest" augmented representation, i.e., the one that has the least probability of belonging to the same class as original representation, from all augmented representations that satisfy the label-invariant constraint. On top of GNN Encoder, we build a projection head to get projections for both original representation and label-invariant augmented representation. We maximize the agreement between projections via a contrastive loss for all graphs and refine the classifier via a cross-entropy loss with labeled graphs.

GCN layer with the parameter Î¸<sup>G</sup> is described as follows:

$$
G^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}G^{(l)}\theta_G^{(l)}),\tag{1}
$$

where AË = A + I<sup>n</sup> is the adjacency matrix A with added self-connections, I<sup>n</sup> â R <sup>n</sup>Ã<sup>n</sup> is the identity matrix, DË is the degree matrix of AË, and Î¸ (l) <sup>G</sup> is a layer-specific trainable weight matrix. G(l) denotes the matrix in the l-th layer, and G(0) = X. We employ Ï(Â·) = ReLU(Â·) as the activation function.

Then on top of the ResGCN, we use a global sum pooling layer to get graph-level representations from node-level representations as follows:

$$
H = \text{Pooling}(G). \tag{2}
$$

Here we use H<sup>O</sup> to denote the original representation of the input graph and H<sup>A</sup> to denote the augmented representation of the augmented graph. The augmentation method will be described in the Label-invariant Augmentation part.

Classifier. Based on the graph-level representations, we employ fully-connected layers with the parameter Î¸<sup>C</sup> for prediction:

$$
C^{(l+1)} = \text{Softmax}(\sigma(C^{(l)} \cdot \theta_C^{(l)})),\tag{3}
$$

where C (l) denotes the embeddings in the l-th layer, and the input layer C (0) = H<sup>O</sup> or C (0) = H<sup>A</sup> for the original representation and augmented representation, respectively. In our experiments, we adopt a 2-layer multilayer perceptron and obtain predictions C <sup>O</sup> and C <sup>A</sup> for original representation H<sup>O</sup> and augmented representation H<sup>A</sup>.

Label-invariant Augmentation. Instead of augmenting graph data by node dropping, edge perturbation, attribute masking, or subgraph sampling as recent graph contrastive learning methods [\[11,](#page-8-10) [12\]](#page-8-11), we conduct the augmentation in the representation space by adding a perturbation to the original representation H<sup>O</sup> so that we do not need to generate any graph data. In our experiment, we first calculate the centroid of original representations for all graphs and get the average value of euclidean distances between each original representation and the centroid as d, that is:

$$
d = \frac{1}{N} \sum_{i=1}^{N} \| H_i^O - \frac{1}{N} \sum_{j=1}^{N} H_j^O \|.
$$
 (4)

Then the augmented representation H<sup>A</sup> is calculated by:

<span id="page-3-1"></span>
$$
H^A = H^O + \eta d\Delta,\tag{5}
$$

where Î· scales the magnitude of the perturbation, and â is a random unit vector.

To achieve the label-invariant augmentation, each time, we randomly generate multiple perturbations and select the qualified augmentation candidates that obey the label-invariant property. Among these qualified candidates, we choose the most difficult one, i.e., the one closest to the decision boundary of the classifier, to increase the model generalization ability.

| Datasets | Category              |   | #Graph | Avg. #Node | Avg. #Edge |
|----------|-----------------------|---|--------|------------|------------|
| MUTAG    | Biochemical Molecules | 2 | 188    | 17.93      | 19.79      |
| PROTEINS | Biochemical Molecules | 2 | 1113   | 39.06      | 72.82      |
| DD       | Biochemical Molecules | 2 | 1178   | 284.32     | 715.66     |
| NCI1     | Biochemical Molecules | 2 | 4110   | 29.87      | 32.30      |
| COLLAB   | Social Networks       | 3 | 5000   | 74.49      | 2457.78    |
| RDT-B    | Social Networks       | 2 | 2000   | 429.63     | 497.75     |
| RDT-M5K  | Social Networks       | 5 | 4999   | 508.52     | 594.87     |
| GITHUB   | Social Networks       | 2 | 12725  | 113.79     | 234.64     |

<span id="page-4-2"></span>Table 1: Statistics of datasets for semi-supervised graph classification

Projection Head. We employ fully-connected layers with the parameter Î¸<sup>P</sup> to get projections for contrastive learning from graph-level representations, which is shown as:

$$
P^{(l+1)} = \sigma(P^{(l)} \cdot \theta_P^{(l)}).
$$
\n(6)

We adopt a 2-layer multilayer perceptron and get projections P <sup>O</sup> and P <sup>A</sup> from original representation H<sup>O</sup> and augmented representation HA.

<span id="page-4-0"></span>Objective Function. Our objective function consists of contrastive loss and classification loss. For contrastive loss, we utilize the normalized temperature-scaled cross-entropy loss (NT-Xent) [\[11\]](#page-8-10) but only keep the positive-pair part as follows:

<span id="page-4-1"></span>
$$
\mathcal{L}_P = \frac{-(P^O)^\top P^A}{\|P^O\| \|P^A\|}.\tag{7}
$$

Maximizing the agreement between original projection and augmented projection would increase the robustness of the model.

For classification loss, we adopt cross-entropy, which is defined as:

$$
\mathcal{L}_C = -\sum_{i=1}^{c} (Y_i^O \log P_i^O + Y_i^O \log P_i^A),\tag{8}
$$

where Y <sup>O</sup> is the label of the input graph, and c is the number of graph categories. We only calculate L<sup>C</sup> for labeled graphs. The improvement of the classifier would help with the label-invariant augmentation, which in turn benefits the training of the classifier.

Combining Eq. [\(7\)](#page-4-0) and [\(8\)](#page-4-1), our overall objective function can be written as follows:

<span id="page-4-3"></span>
$$
\min_{\Theta} \mathcal{L}_P + \alpha \mathcal{L}_C,\tag{9}
$$

where Î± is a trade-off hyperparameter to balance the contrastive loss and classification loss.

Discussion. From the perspective of information usage for model training, our proposed method is the same as the semi-supervised learning task by recent graph contrastive learning methods [\[11,](#page-8-10) [12,](#page-8-11) [14,](#page-8-13) [15\]](#page-8-14), which use structure information of all graphs and label information of a subset of all graphs for model training. From the perspective of training strategy, the previous methods first pre-train a model via a contrastive loss and then fine-tune the model for downstream tasks. While our proposed method focuses on semi-supervised classification, we merge the pre-train and fine-tuning phases into one integrated phase. During our training phase, the augmented samples increase the model robustness and generalization ability, and the classifier helps to generate better augmented samples, which in turn benefits classification performance.

## 4 Experiments

In this section, we first describe our semi-supervised settings of experiments and baseline methods for comparison. Then we show the algorithmic performance of these methods on eight graph benchmark datasets in a fair setting. Finally, we provide some insightful experiments to demonstrate the effectiveness of the proposed Graph Label-invariant Augmentation (GLA) method.

#### 4.1 Experiment Settings

Datasets. We select eight public graph classification benchmark datasets from TUDataset [\[32\]](#page-9-8) for evaluation, including *MUTAG* [\[29\]](#page-9-5), *PROTEINS* [\[33\]](#page-9-9), *DD* [\[34\]](#page-9-10), *NCI1* [\[35\]](#page-9-11), *COLLAB* [\[36\]](#page-9-12), *RDT-B* [\[36\]](#page-9-12), *RDT-M5K* [\[36\]](#page-9-12), and *GITHUB* [\[37\]](#page-9-13).

| Label | Methods    | MUTAG        | PROTEINS     | DD           | NCI1         | COLLAB       | RDT-B        | RDT-M5K      | GITHUB       | Avg.  | Rank |
|-------|------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|-------|------|
| 30%   | GAE        | 83.63 Â± 0.81 | 74.31 Â± 0.33 | 77.33 Â± 0.36 | 77.20 Â± 0.22 | 77.46 Â± 0.11 | 90.75 Â± 0.17 | 54.81 Â± 0.18 | 65.22 Â± 0.11 | 75.09 | 5.00 |
|       | Infomax    | 84.68 Â± 1.12 | 74.84 Â± 0.28 | 77.07 Â± 0.45 | 79.49 Â± 0.17 | 77.30 Â± 0.19 | 90.65 Â± 0.17 | 55.37 Â± 0.20 | 66.45 Â± 0.06 | 75.73 | 4.25 |
|       | MVGRL      | 83.16 Â± 0.98 | 75.56 Â± 0.44 | 77.08 Â± 0.56 | 72.41 Â± 0.18 | 75.28 Â± 0.12 | 88.20 Â± 0.16 | 53.16 Â± 0.06 | 64.71 Â± 0.04 | 73.70 | 6.12 |
|       | SimGRACE   | 83.68 Â± 0.84 | 74.38 Â± 0.30 | 76.27 Â± 0.38 | 78.52 Â± 0.17 | 78.66 Â± 0.24 | 90.60 Â± 0.17 | 55.54 Â± 0.16 | 66.81 Â± 0.14 | 75.56 | 4.50 |
|       | GraphCL    | 85.20 Â± 0.98 | 74.12 Â± 0.30 | 78.60 Â± 0.37 | 79.22 Â± 0.09 | 77.90 Â± 0.20 | 90.35 Â± 0.18 | 56.07 Â± 0.15 | 67.63 Â± 0.13 | 76.14 | 3.38 |
|       | JOAOv2     | 85.67 Â± 0.91 | 75.02 Â± 0.30 | 77.16 Â± 0.30 | 78.69 Â± 0.18 | 79.88 Â± 0.17 | 91.65 Â± 0.15 | 55.23 Â± 0.14 | 67.96 Â± 0.10 | 76.41 | 2.62 |
|       | GLA (Ours) | 86.32 Â± 1.25 | 75.65 Â± 0.37 | 77.49 Â± 0.40 | 79.71 Â± 0.13 | 78.78 Â± 0.12 | 91.05 Â± 0.25 | 55.85 Â± 0.22 | 65.16 Â± 0.19 | 76.25 | 2.12 |
| 50%   | GAE        | 84.12 Â± 0.90 | 74.75 Â± 0.38 | 78.35 Â± 0.31 | 79.56 Â± 0.16 | 80.47 Â± 0.14 | 90.95 Â± 0.19 | 55.69 Â± 0.16 | 67.09 Â± 0.13 | 76.37 | 5.88 |
|       | Infomax    | 87.37 Â± 1.11 | 75.38 Â± 0.38 | 78.26 Â± 0.38 | 80.80 Â± 0.13 | 79.70 Â± 0.11 | 91.50 Â± 0.26 | 56.51 Â± 0.18 | 67.70 Â± 0.09 | 77.15 | 3.75 |
|       | MVGRL      | 85.79 Â± 0.23 | 76.72 Â± 0.34 | 78.60 Â± 0.46 | 74.09 Â± 0.10 | 76.08 Â± 0.05 | 88.55 Â± 0.06 | 54.04 Â± 0.06 | 64.89 Â± 0.05 | 74.84 | 5.62 |
|       | SimGRACE   | 86.32 Â± 0.88 | 75.09 Â± 0.35 | 78.39 Â± 0.35 | 79.78 Â± 0.24 | 80.48 Â± 0.15 | 91.45 Â± 0.16 | 56.50 Â± 0.20 | 67.71 Â± 0.16 | 76.97 | 4.50 |
|       | GraphCL    | 87.28 Â± 0.71 | 75.29 Â± 0.29 | 78.73 Â± 0.46 | 80.17 Â± 0.19 | 80.40 Â± 0.16 | 91.45 Â± 0.25 | 56.83 Â± 0.19 | 68.71 Â± 0.09 | 77.36 | 3.25 |
|       | JOAOv2     | 86.78 Â± 0.79 | 75.74 Â± 0.29 | 78.52 Â± 0.45 | 80.10 Â± 0.17 | 81.50 Â± 0.18 | 92.10 Â± 0.18 | 56.51 Â± 0.17 | 68.97 Â± 0.11 | 77.53 | 2.75 |
|       | GLA (Ours) | 90.00 Â± 0.94 | 76.19 Â± 0.28 | 80.22 Â± 0.37 | 80.66 Â± 0.28 | 80.84 Â± 0.12 | 91.65 Â± 0.22 | 56.63 Â± 0.13 | 66.59 Â± 0.14 | 77.85 | 2.25 |
| 70%   | GAE        | 87.31 Â± 0.66 | 75.47 Â± 0.38 | 79.37 Â± 0.36 | 79.78 Â± 0.17 | 80.78 Â± 0.12 | 91.50 Â± 0.19 | 56.25 Â± 0.16 | 68.42 Â± 0.14 | 77.36 | 5.62 |
|       | Infomax    | 88.33 Â± 0.73 | 75.92 Â± 0.38 | 79.28 Â± 0.33 | 82.85 Â± 0.16 | 81.04 Â± 0.12 | 92.15 Â± 0.13 | 56.63 Â± 0.18 | 68.88 Â± 0.14 | 78.14 | 3.62 |
|       | MVGRL      | 87.95 Â± 0.35 | 77.81 Â± 0.35 | 79.51 Â± 0.34 | 74.43 Â± 0.08 | 76.42 Â± 0.08 | 88.65 Â± 0.23 | 54.40 Â± 0.11 | 65.00 Â± 0.08 | 75.52 | 5.25 |
|       | SimGRACE   | 87.37 Â± 0.71 | 76.52 Â± 0.36 | 78.90 Â± 0.29 | 81.80 Â± 0.15 | 81.88 Â± 0.23 | 92.45 Â± 0.13 | 56.58 Â± 0.09 | 68.19 Â± 0.15 | 77.96 | 4.12 |
|       | GraphCL    | 88.33 Â± 0.86 | 76.36 Â± 0.25 | 79.03 Â± 0.29 | 82.50 Â± 0.13 | 81.08 Â± 0.17 | 91.85 Â± 0.14 | 56.91 Â± 0.17 | 69.19 Â± 0.08 | 78.16 | 3.62 |
|       | JOAOv2     | 87.78 Â± 0.76 | 76.46 Â± 0.27 | 79.11 Â± 0.38 | 81.70 Â± 0.26 | 82.16 Â± 0.17 | 92.20 Â± 0.19 | 56.67 Â± 0.16 | 69.96 Â± 0.11 | 78.26 | 3.25 |
|       | GLA (Ours) | 91.05 Â± 0.86 | 77.45 Â± 0.38 | 80.71 Â± 0.29 | 83.24 Â± 0.14 | 81.54 Â± 0.14 | 91.70 Â± 0.17 | 57.01 Â± 0.14 | 67.11 Â± 0.18 | 78.73 | 2.50 |

<span id="page-5-0"></span>Table 2: Semi-supervised graph classification results (Accuracy % Â± Standard Deviation %) on eight benchmark datasets. The best and second-best results are highlighted in red and blue, respectively.

Table [1](#page-4-2) shows the statistics of these datasets. The first four datasets include biochemical molecules and proteins, and the last four datasets are about social networks. The numbers of graphs in these datasets range from 188 to 12725, the average node numbers range from 17.93 to 508.52, and the average edge numbers are from 19.79 to 2457.78, indicating the diversity of these datasets.

Compared Methods and Implementation. We choose two heuristic self-supervised methods, GAE [\[26\]](#page-9-2) and Infomax [\[27\]](#page-9-3), and four recent graph contrastive learning methods, MVGRL [\[14\]](#page-8-13), GraphCL [\[11\]](#page-8-10), JOAOv2 [\[12\]](#page-8-11), and SimGRACE [\[15\]](#page-8-14), for comparison on semi-supervised graph classification task. GAE performs adjacency matrix reconstruction by using a graph convolutional network (GCN) [\[31\]](#page-9-7) encoder and a simple inner product decoder. Infomax is based on global-local representation consistency enforcement, which maximizes the mutual information between global and local representation. MVGRL proposes to learn node and graph level representations by node diffusion and contrasting encodings. GraphCL presents four types of graph augmentations. Based on GraphCL, JOAOv2 is designed as a unified bi-level optimization framework to automatically select graph augmentations. SimGRACE perturbs parameters of graph encoder for contrastive learning, which does not require data augmentations.

For GAE, Infomax, and GraphCL, we adopt the implementations and default hyperparameter settings provided by the source codes of GraphCL [\[11\]](#page-8-10). For other compared methods, we follow the implementations and hyperparameter settings in their corresponding source codes. The compared methods are pre-trained first and then are fine-tuned for the semi-supervised graph classification task. For our proposed Graph Label-invariant Augmentation (GLA) method, we perform contrastive learning and graph classifier learning synchronously. The implementation details of GLA are as follows. We implement the networks based on GraphCL [\[11\]](#page-8-10) by PyTorch, set the magnitude of perturbation Î· to 1.0, and the weight of classification loss Î± to 1.0, which is the same with GraphCL. We adopt Adam optimizer [\[38\]](#page-9-14) to minimize the objective function in Eq. [\(9\)](#page-4-3).

Evaluation Protocol. We evaluate the models with 10-fold cross-validation. We randomly shuffle a dataset and then evenly split it into 10 parts. Each fold corresponds to one part of data as the test set and another part as the validation set to select the best epoch, where the rest folds are used for training. We select 30%, 50%, 70% graphs from the training set as labeled graphs for each fold, then conduct semi-supervised learning. For a fair comparison, we use the same training/validation/test splits for all compared methods on each dataset. Semi-supervised graph classification results are reported by the average accuracy across 10 folds and their standard deviations.

# 4.2 Algorithmic Performance

Table [2](#page-5-0) shows the prediction results of two self-supervised and five graph contrastive learning methods under the semi-supervised graph classification setting with 30%, 50%, and 70% label ratios on eight benchmark datasets, where the best and second-best results are highlighted in red and blue, respectively, and the last column is the average rank score across all datasets. Although different algorithms achieve their best performances on different datasets, the contrastiveness-based methods perform better than the non-contrastiveness-based methods in general, which indicates the effectiveness of the graph augmentation. Our proposed GLA achieves the best ranking scores under all 30%, 50%, and 70% label ratios in experiments, the second-best average performance under 30% label ratio, and the best average

<span id="page-6-0"></span>![](_page_6_Figure_1.jpeg)

**Caption:** Figure 3 shows performance gains and label-invariant rates across eight datasets. Panel (a) highlights average performance improvements with increased labeled samples for GraphCL, JOAOv2, and GLA. Panel (b) depicts label-invariant rate distributions for different methods, while panel (c) illustrates GLA's rates across varying semi-supervised settings.

<span id="page-6-2"></span><span id="page-6-1"></span>Figure 3: Performance gain and label-invariant rates. (a) demonstrates the average performance gains on eight datasets with more labeled samples produced by GraphCL, JOAOv2, and GLA. (b) shows the label-invariant rate distributions of different augmentation methods over eight datasets. (c) shows the label-invariant rates of our GLA over different semi-supervised settings.

performance under 50% and 70% label ratios. In our algorithmic design, we employ the decision boundary learned from the labeled samples to verify the label-invariant augmentation. It is worthy to note that the quality of the decision boundary depends on the number of labeled samples. We conjecture that a 30% label ratio is not sufficient enough to learn a high-quality decision boundary, resulting in our GLA performing slightly worse than JOAOv2 on average. With more labeled samples, our GLA delivers the best average performance over other competitive methods. Different from other graph contrastive learning methods, our augmentation method aims to generate label-invariant augmentations, which decreases the possibility of getting "bad" augmentations, thus resulting in better performance.

Besides the general comparison in Table [2,](#page-5-0) we dive into details and discover several interesting findings. Figure [3\(a\)](#page-6-0) demonstrates the average performance gains on eight datasets with more labeled samples produced by GraphCL, JOAOv2, and our GLA, the top three methods in our experiments. In addition to seeing that the increased performance of all three methods well aligns with more labeled samples, our GLA receives more performance gains than GraphCL and JOAOv2. By such comparisons, we can roughly eliminate the effect of more labeled samples and attribute the extra gains to the label-invariant augmentation. It also verifies our aforementioned conjecture that the high-quality decision boundary is beneficial to the label-invariant augmentation, further bringing in the performance boost. Moreover, we further verify our motivation by checking the label-invariant property of different contrastive methods. While we do not have a ground truth classifier, we use fine-tuned classifiers in the representation spaces learned by these contrastive methods with a 100% label ratio as the surrogates of the ground truth classifier. Then we use these classifiers to assess how many of the augmented representations belong to the same class as their corresponding original representations. Figure [3\(b\)](#page-6-1) presents the distributions of label-invariant rates across eight baseline datasets for all graph contrastive methods. As our GLA trained under different label ratios would generate different augmentations, we put the results of GLA's label-invariant rates under 30%, 50%, and 70% label ratios together for plotting. We can see that GLA has the highest label-invariant rates on average compared to other methods. It is also noticed that the label-invariant rates of different contrastive methods keep the same ranking with the performance in Table [2](#page-5-0) (the last column), which verifies our motivation for designing a label-invariant augmentation strategy. Moreover, we further demonstrate our GLA's label-invariant rates along with different label ratios in Figure [3\(c\),](#page-6-2) which accords with our expectation that more labeled samples lead to a high-quality decision boundary and further promote the label-invariant rate in GLA.

#### 4.3 In-depth Exploration

Here we further demonstrate several in-depth explorations of our GLA in terms of negative pairs, augmentation space, and strategy.

*Negative Pairs*. The existing graph contrastive learning methods treat the augmented graphs from different source samples as negative pairs and employ the instance-level discrimination on these negative pairs. Since these methods separate the pre-train and fine-tuning phases, the negative pairs contain the augmented samples from different source samples but with the same category in the downstream tasks. Here we explore the effect of negative pairs on our GLA. Figure [4\(a\)](#page-7-0) shows the performance of our GLA with and without negative pairs on four datasets. We can see the performance with negative pairs significantly drops compared with our default setting without negative pairs, which behaves consistently on all four datasets. Different from the existing graph contrastive methods, our GLA integrates the pre-train and fine-tuning phases, where the negative pairs designed in a self-supervised fashion are not beneficial to the downstream tasks. This finding is also in accord with the recent studies [\[10,](#page-8-9) [9\]](#page-8-8) in the visual contrastive learning area.

<span id="page-7-2"></span><span id="page-7-0"></span>![](_page_7_Figure_1.jpeg)

**Caption:** Figure 4 explores GLA's performance with and without negative pairs, revealing that excluding negative pairs significantly enhances results. It also compares different augmentation strategies, demonstrating that selecting the most challenging augmentations improves model generalization and performance across datasets.

<span id="page-7-1"></span>Figure 4: In-depth exploration of GLA. (a) contrastive loss with/without negative pairs, (d) performance of different label-invariant augmentation strategies, (b,c,e,f) performance of magnitude of perturbation Î· on different datasets under 50% label ratio.

*Augmentation Space*. Different from the most graph contrastive learning methods that directly augment raw graphs, our GLA conducts the augmentation in the representation space, as we believe the raw graphs can be mapped into the representation space, and this space is much easier to augment than the original graph space. In Eq. [\(5\)](#page-3-1), we design our representation augmentation with a random unit vector scaled by the magnitude of the perturbation Î·. Figure [4\(](#page-7-1)b,c,e,f) show the performance of our GLA with different values of Î· on four datasets, where we provide GraphCL and GraphCL+Label-Invariant as references. GraphCL+Label-Invariant takes the augmented graph from GraphCL and filters the augmented samples that violate the label-invariant property by the downstream classifier. Comparing the two references, we can see that the label-invariant property benefits not only our GLA but also other contrastive methods in most cases. For our GLA, although the Î· values corresponding to the best performance vary on different datasets, the default setting with Î· = 1 delivers satisfying performance in general, which outperforms GraphCL+Label-Invariant and indicates the superior of the representation augmentation over the raw graph augmentation. *Augmentation Strategy*. In the representation space, there might exist multiple qualified candidates that obey the label-invariant property. Our GLA chooses the most difficult augmentation for the model. Here we demonstrate the performance of different augmentation strategies among qualified candidates, including the most difficult augmentation, random augmentation, and the easiest augmentation in Figure [4\(d\),](#page-7-2) where the random augmentation can be regarded as GraphCL+Label-Invariant. We can see that the most difficult augmentation increases the model generalization and indeed brings in significant improvements over the other two ways. This also provides good support for our representation augmentation, where we can find the most difficult augmentation in the representation space, but it is difficult to directly generate the raw graphs that are challenging to the downstream classifier.

## 5 Conclusion

In this paper, we consider the graph contrastive learning problem. Different from the existing methods from the pre-train perspective, we propose a novel Graph Label-invariant Augmentation (GLA) algorithm which integrates the pre-train and fine-tuning phases to conduct the label-invariant augmentation in the representation space by perturbations. Specifically, GLA first checks whether the augmented representation obeys the label-invariant property and chooses the most difficult sample from the qualified samples. By this means, GLA achieves the contrastive augmentation without generating any raw graphs and also increases the model generalization. Extensive experiments in the semi-supervised setting on eight benchmark graph datasets demonstrate the effectiveness of our GLA. Moreover, we also provide extra experiments to verify our motivation and explore the in-depth factors of GLA in the effect of negative pairs, augmentation space, and strategy.

#### References

- <span id="page-8-0"></span>[1] Minmin Chen, Kilian Weinberger, Fei Sha, and Yoshua Bengio. Marginalized denoising auto-encoders for nonlinear representations. In *International Conference on Machine Learning*, 2014.
- <span id="page-8-1"></span>[2] Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei Sha. Marginalized denoising autoencoders for domain adaptation. In *International Conference on Machine Learning*, 2012.
- <span id="page-8-2"></span>[3] Taesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. In *European Conference on Computer Vision*, 2020.
- <span id="page-8-3"></span>[4] Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. *Advances in Neural Information Processing Systems*, 33:8765â8775, 2020.
- <span id="page-8-4"></span>[5] Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In *AAAI Conference on Artificial Intelligence*, 2021.
- <span id="page-8-5"></span>[6] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. *Advances in Neural Information Processing Systems*, 33:18661â18673, 2020.
- <span id="page-8-6"></span>[7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. *arXiv preprint arXiv:2003.04297*, 2020.
- <span id="page-8-7"></span>[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In *International Conference on Machine Learning*, 2020.
- <span id="page-8-8"></span>[9] Jean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. *Advances in Neural Information Processing Systems*, 33:21271â21284, 2020.
- <span id="page-8-9"></span>[10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021.
- <span id="page-8-10"></span>[11] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. *Advances in Neural Information Processing Systems*, 33:5812â5823, 2020.
- <span id="page-8-11"></span>[12] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In *International Conference on Machine Learning*, 2021.
- <span id="page-8-12"></span>[13] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep Graph Contrastive Representation Learning. In *ICML Workshop on Graph Representation Learning and Beyond*, 2020.
- <span id="page-8-13"></span>[14] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In *International Conference on Machine Learning*, 2020.
- <span id="page-8-14"></span>[15] Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li. Simgrace: A simple framework for graph contrastive learning without data augmentation. In *ACM Web Conference*, 2022.
- <span id="page-8-15"></span>[16] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In *International Conference on Learning Representations*, 2017.
- <span id="page-8-16"></span>[17] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In *International Conference on Machine Learning*, 2019.
- <span id="page-8-17"></span>[18] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In *ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, 2018.
- <span id="page-8-18"></span>[19] Petar Velickovi Ë c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua Bengio. Graph Â´ attention networks. In *International Conference on Learning Representations*, 2018.
- <span id="page-8-19"></span>[20] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph attention network. In *The World Wide Web Conference*, 2019.
- <span id="page-8-20"></span>[21] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. Kgat: Knowledge graph attention network for recommendation. In *ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, 2019.
- <span id="page-8-21"></span>[22] Ehsan Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan, Nick Duffield, Mingyuan Zhou, and Xiaoning Qian. Variational graph recurrent neural networks. *Advances in Neural Information Processing Systems*, 32, 2019.
- <span id="page-8-22"></span>[23] Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang. Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting. *IEEE Transactions on Intelligent Transportation Systems*, 21(11):4883â4894, 2019.
- <span id="page-9-0"></span>[24] Guohao Li, Matthias MÃ¼ller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In *The IEEE International Conference on Computer Vision*, 2019.
- <span id="page-9-1"></span>[25] Guohao Li, Matthias MÃ¼ller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In *International Conference on Machine Learning*, 2021.
- <span id="page-9-2"></span>[26] Thomas N Kipf and Max Welling. Variational graph auto-encoders. *arXiv preprint arXiv:1611.07308*, 2016.
- <span id="page-9-3"></span>[27] Petar Velickovic, William Fedus, William L Hamilton, Pietro LiÃ², Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. *International Conference on Learning Representations*, 2(3):4, 2019.
- <span id="page-9-4"></span>[28] Dongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl: Information-aware graph contrastive learning. 34:30414â30425, 2021.
- <span id="page-9-5"></span>[29] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. *Journal of Medicinal Chemistry*, 34(2):786â797, 1991.
- <span id="page-9-6"></span>[30] Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on graph classification. *arXiv preprint arXiv:1905.04579*, 2019.
- <span id="page-9-7"></span>[31] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. *arXiv preprint arXiv:1609.02907*, 2016.
- <span id="page-9-8"></span>[32] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In *ICML 2020 Workshop on Graph Representation Learning and Beyond*, 2020.
- <span id="page-9-9"></span>[33] Karsten M Borgwardt, Cheng Soon Ong, Stefan SchÃ¶nauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. *Bioinformatics*, 21(suppl\_1):i47âi56, 2005.
- <span id="page-9-10"></span>[34] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. *Journal of Molecular Biology*, 330(4):771â783, 2003.
- <span id="page-9-11"></span>[35] Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. *Knowledge and Information Systems*, 14(3):347â375, 2008.
- <span id="page-9-12"></span>[36] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In *ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 2015.
- <span id="page-9-13"></span>[37] Benedek Rozemberczki, Oliver Kiss, and Rik Sarkar. Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs. In *ACM International Conference on Information and Knowledge Management*, 2020.
- <span id="page-9-14"></span>[38] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*, 2014.