# LuoJiaHOG: A Hierarchy Oriented Geo-aware Image Caption Dataset for Remote Sensing Image-Text Retrival

Yuanxin Zhao, Mi Zhang † *Member, IEEE,* Bingnan Yang, Zhan Zhang, Jiaju Kang, Jianya Gong.

**Abstract**—Image-text retrieval (ITR) plays a significant role in making informed decisions for various remote sensing (RS) applications, such as urban development and disaster prevention. Nonetheless, creating ITR datasets containing vision and language modalities not only requires significant geo-spatial sampling area but also varing categories and detailed descriptions. To this end, we introduce an image caption dataset LuojiaHOG, which is geospatial-aware, label-extension-friendly and comprehensive-captioned. LuojiaHOG involves the hierarchical spatial sampling, extensible classification system to Open Geospatial Consortium (OGC) standards, and detailed caption generation. In addition, we propose a CLIP-based Image Semantic Enhancement Network (CISEN) to promote sophisticated ITR. CISEN consists of two components, namely dual-path knowledge transfer and progressive cross-modal feature fusion. The former transfers the multi-modal knowledge from the large pretrained CLIP-like model, whereas the latter leverages a visual-to-text alignment and fine-grained cross-modal feature enhancement. Comprehensive statistics on LuojiaHOG reveal the richness in sampling diversity, labels quantity and descriptions granularity. The evaluation on LuojiaHOG is conducted across various state-of-the-art ITR models, including ALBEF, ALIGN, CLIP, FILIP, Wukong, GeoRSCLIP and CISEN. We use second- and third-level labels to evaluate these vision-language models through adapter-tuning and CISEN demonstrates superior performance. For instance, it achieves the highest scores with WMAP@5 of 88.47% and 87.28% on third-level ITR tasks, respectively. In particular, CISEN exhibits an improvement of approximately 1.3% and 0.9% in terms of WMAP@5 compared to its baseline. These findings highlight CISEN advancements accurately retrieving pertinent information across image and text. LuojiaHOG and CISEN can serve as a foundational resource for future RS image-text alignment research, facilitating a wide range of vision-language applications.

✦

**Index Terms**—RS image caption dataset, image-text retrieval, fine-grained recognition, deep learning, multi-modal.

# **1 INTRODUCTION**

Image-text retrieval (ITR) is a critical area of interest that supports various remote sensing challenges such as geo-localization [\[1\]](#page-15-0), [\[2\]](#page-15-1), [\[3\]](#page-15-2), disaster rescue [\[4\]](#page-15-3), [\[5\]](#page-15-4), [\[6\]](#page-15-5), economic assessment [\[7\]](#page-15-6), [\[8\]](#page-15-7), [\[9\]](#page-15-8), and ecology prediction [\[10\]](#page-15-9). It is essential for automated decision-making and intelligent recommendations, enhancing the capability to access geospatial information swiftly and accurately.

Current works in ITR primarily relies on datasets like UCM-captions [\[11\]](#page-15-10), RSICD [\[12\]](#page-15-11), and NWPU-Captions [\[13\]](#page-15-12). which lack geographic diversity, offer only brief descriptions, and are confined to fixed or mixed classes (Tab. [1\)](#page-1-0). This limitation hinders the development of more sophisticated and advanced ITR models due to insufficient data variety and a lack of intra-modal and inter-modal semantic similarity. Recognizing the critical role of high-quality datasets in ITR, there is an urgent need for a dataset that incorporates geographic awareness, provides detailed captions, and is adaptable for extensions. Such a dataset would not only advance ITR algorithm development but also enhance related image-text tasks, including image text generation and visual question answering.

<span id="page-0-0"></span>![](_page_0_Picture_9.jpeg)

Fig. 1: Overview of ITR dataset LuojiaHOG.

In this study, we introduce a novel image caption dataset, named LuojiaHOG (Fig. [1\)](#page-0-0), which is geospatialaware, label-extension-friendly and comprehensivecaptioned, to address the aforementioned issues. Unlike the majority of existing datasets, such as UCM-Captions and RSICD, all images are collected from regions around the world with varing levels of development and topography through geo-spatial analysis. Besides, LuoJiaHOG classification system adopts the OGC standards [\[14\]](#page-15-13) and thus compatible with various new data under different task requirements. It comprises 94,856 images,

<sup>•</sup> *Yuanxin Zhao, Mi Zhang, Bingnan Yang, Jianya Gong are with the School of Remore Sensing and Information Engineering, Wuhan University, No.129, Luoyu Road, Wuhan 430079, China.*

<sup>•</sup> † *Corresponding Author: Mi Zhang is also with Hubei Luojia Laboratory, No.129, Luoyu Road, Wuhan 430079, China. E-mail: mizhang@whu.edu.cn.*

| TABLE 1: Comparison of current datasets. |  |  |  |
|------------------------------------------|--|--|--|
|------------------------------------------|--|--|--|

<span id="page-1-0"></span>

| Dataset              | Classes/Images | Geographic area | Classification system |
|----------------------|----------------|-----------------|-----------------------|
| Sydney-Captions [11] | 7/613          | Sydney          | fixed                 |
| UCM-Captions [11]    | 21/2,100       | UC Merced       | fixed                 |
| RSICD [12]           | 30/10,921      | -               | fixed                 |
| RSITMD [15]          | 32/4,743       | -               | mixed                 |
| NWPU-Captions [13]   | 45/31,500      | global          | mixed                 |
| RS5M [16]            | -/5 million    | global          | -                     |
| RSGPT [17]           | -/2,585        | multi-cities    | -                     |
| LuojiaHOG(Ours)      | 131/94856      | global sample   | extensible            |

<sup>1</sup> **Fixed** classification system (CS) is usually constructed according to expert experience.

**Mixed** CS adds some new labels based on fixed CS.

**Extensible** represents a complete CS standard which can be expanded according to diffenrent task requirements.

categorized into 131 third-level categories that fall into 21 second-level classes, including residential, farmland, cemetery, and playground, etc. In addition to rich categories, we have diligently conducted extensive data cleaning and professional annotations, leveraging Vision-Language Models (VLMs) to generate and augment the textual captions automatically. Moreover, prompt engineering is adopted to improve the quality of generated text. LuojiaHOG supports two basic retrieval tasks: textto-image (T2I) and image-to-text (I2T). By evaluating performance across different granularities using tailored metrics for multi-label retrieval, we establish baseline for state-of-the-art models on ITR. We anticipate it as a finegrained ITR benchmark, thus facilitating the development of RS vision-language learning.

The primary contributions of this study can be summarized as follows:

• A hierarchical sampling method and automatic are employed to collect RS images. Both manual and automatic annotation methods are utilized to generate detailed descriptions.

• We establish an extensible classification system, which is aligned with the Open Geospatial Consortium (OGC) standards. It supports dynamic expansion of database for new samples and enables the mapping and conversion of different classification systems.

• Extensive ITR baselines on LuojiaHOG are provided across two levels of granularity.

The rest of this paper is organized as follows: In Section [2,](#page-1-1) we review the related work of image caption datasets, image caption and image-text retrieval. The construction procedure of our dataset are described in Section [3.](#page-2-0) Then, we provide the details of our dataset in Section [4.](#page-7-0) In Section [5,](#page-9-0) the evaluation of baseline image retrieval methods under different experimental settings are given. Finally, we draw some conclusions with several ways for further improving LuojiaHOG in Section [6.](#page-14-0)

# <span id="page-1-1"></span>**2 RELATED WORK**

**Image Caption Datasets.** Considerable efforts have been directed towards advancing benchmark datasets and novel caption techniques in the remote sensing domain. For instance, Qu et al. [\[11\]](#page-15-10)introduced a pioneering deep multimodal neural network model alongside two benchmark datasets, Sydney-Captions [\[11\]](#page-15-10) and UCM-Captions [\[11\]](#page-15-10). Their model ingeniously combined different CNNs with

RNN/LSTMs to enhance performance. UCM-Captions includes 2,100 images of 21 categories, each of which is 256×256 pixels. The data, based on UC Merced Land Use Dataset [\[18\]](#page-15-17), were extracted from urban area images of the National Map of the United States Geological Survey. Whereas Sydney-Captions, contains 613 images of 7 categories, which were collected from Sydney, Australia. Both datasets offer 5 descriptions for each image. Building upon this work, Lu et al. [\[12\]](#page-15-11) and Cheng et al. [\[13\]](#page-15-12) conducted a comprehensive analysis of the challenges associated with RS image captioning. They further contributed to the field by creating a larger benchmark dataset separately known as RSICD and NWPU-Captions, aimed at generating more precise and adaptable descriptions. NWPU-Captions, based on NWPU-RESISC Dataset [\[19\]](#page-15-18), encompasses 31,500 images along with 157,500 captions of 45 categories. RSICD comprises 10,921 images and 54,605 captions, with 24,333 of these being unique captions. Subsequently, numerous enhanced approaches have emerged, each carefully tailored to the unique characteristics of RS images. Yuan et al. [\[15\]](#page-15-14) used manual annotation to construct a fine-grained and more challenging Remote Sensing Image-Text Match dataset (RSITMD) to address the problem of excessive repetition of text descriptions in traditional RS image-text dataset. RSITMD selects 4,743 images from RSICD and provide 23,715 captions. One particularly effective strategy involves the incorporation of diverse attention mechanisms into the standard encoder-decoder architecture. Notably, some of these methods [\[20\]](#page-15-19) [\[21\]](#page-15-20) [\[22\]](#page-15-21) have demonstrated promising performance improvements in image caption. The RS5M dataset, a recent creation by Zhang et al. [\[16\]](#page-15-15), stands out as the most extensive RS image-text pairing dataset available to date. It was meticulously curated by filtering existing publicly available image-text paired datasets and leveraging a pre-trained VLM specifically fine-tuned for RS datasets, utilizing only subtitle labels. RS5M collects 5 million data from 11 publicly available image-text paired datasets [\[23\]](#page-15-22) [\[24\]](#page-15-23) [\[25\]](#page-15-24) [\[26\]](#page-15-25) [\[27\]](#page-15-26) [\[28\]](#page-15-27) [\[29\]](#page-15-28) [\[30\]](#page-15-29) [\[31\]](#page-15-30) and 3 largescale RS image classification dataset [\[32\]](#page-15-31) [\[33\]](#page-15-32) [\[34\]](#page-15-33). Motivated by the impressive image and text comprehension capabilities of VLMs, Hu et al. [\[17\]](#page-15-16) embarked on the creation of the Remote Sensing Image Captioning dataset (RSICap). This dataset collected 2585 image-text pairs that have been carefully annotated by professionals. Each image corresponds to a sentence that describes in detail the attributes of the features in the image. They also provided an evaluation dataset (RSIEval) dataset that can be used for the evaluation of domain-specific or general VLMs. RSIEval consists of 100 human-annotated captions and 936 visual question-answer pairs with rich information and open-ended questions and answers. There work serves as a valuable resource, designed to support the development of robust vision language models within the remote sensing domain. In Tab[.1,](#page-1-0) we give statistics of existing image caption datasets together with LuojiaHOG.

**Image Caption.** Although the access to remote sensing images is getting easier, how to quickly obtain detailed and accurate text descriptions of remote sensing images is still a problem. For this reason, a large research effort has been devoted to image captioning, i.e. the task of describing images with syntactically and semantically meaningful sentences.

For sentence generation, the studies has developed from traditional template-based and retrieved-based methods to Recurrent Neural Network (RNN) and LLM. Template-based methods generate descriptive sentences for a given image through fixing templates with a number of blank slots. In these approaches, different objects, attributes, actions are detected first and then the blank spaces in the templates are filled. Farhadi et al. [\[35\]](#page-15-34)use a triplet of scene elements to fill the template slots for generating image captions. A Conditional Random Field (CRF) is adopted by Kulkarni et al. [\[36\]](#page-15-35) to infer the objects, attributes, and prepositions before filling in the gaps. Retrieval-based approaches first extracted a candidate caption set from a set of caption pool with a basic retrieval (pre-retrieval) model. The final bestmatching captions for the input image are then chosen from the captions pool by the re-ranking method. For example, Hodosh et al. [\[37\]](#page-16-0) treated the image captioning as a ranking or retrieval task, and introduced a ranking-based method to extract image description. Gong et al. [\[38\]](#page-16-1)associated the query image with a textual description by projecting them into a shared latent space. Although retrieval-based methods can produce syntactically correct captions, the retrieved captions are not tailored for the query images and limited by the size of the pre-constructed image-caption repository. Motivated by the remarkable success of deep neural networks in CV and NLP, the seq2seq paradigm has become the mainstream in image captioning. Attention mechanisms play an essential role in enhancing the performance of the seq2seq models. For example, an attentive seq2seq model was introduced in [\[39\]](#page-16-2), which learned to dynamically attend to different locations of the query image at different decoding step. Mun et al. [\[40\]](#page-16-3) used associated captions that were retrieved from training data to learn visual attention for image captioning. Besides, Yang et al. [\[41\]](#page-16-4)focused on the improvement of both retrieval- or generation-based model by using a dual generator generative adversarial network with two generators and one discriminator. With the rapid development of LLMs in recent years, VLM that combines vision and language, has been recently introduced and demonstrated several impressive capabilities of visionlanguage understanding and generation. Flamingo [\[42\]](#page-16-5), for instance, integrates visual adaptation layers into an LLM and is trained on a large-scale interleaved image text dataset. ML-MFSL [\[43\]](#page-16-6) is similar to Flamingo, where a visual prefix is introduced as a learnable feature to extract information related to text from the image. After enhancing the visual prefix with the meta mapper network and concatenating it with textual features, LLM is employed to predict the responses. BLIP-2 [\[20\]](#page-15-19)utilizes multiple visionlanguage losses to align visual features with text via the Q-Former model, and tunes a simple fully connected layer to feed the queried embedding to a frozen language model. Based on BLIP-2 , MiniGPT4 [\[21\]](#page-15-20)and InstructBLIP [\[44\]](#page-16-7) retain the Q-Former model, replace the language model with a larger one, and fine-tune on meticulously collected instruction data. In addition, simpler and more direct methods, such as LLaVA [\[45\]](#page-16-8), directly feed visual features to the LLM using only a learnable fully connected layer. RSGPT utilizes high-quality RS image and text pairs and fine-tunes them on the basis of minigpt4 to obtain a RS image caption model. These image caption models can obtain corresponding text descriptions for images, but the quality of text generation will be limited by the LLM model.

**Image-Text Retrieval** Image-Text retrieval from RS big data refers to finding RS images/descriptions that satisfies a text description/ remote sensing image from large RS image collections. Thanks to the prosperity of deep models for language and vision, we have witnessed the great success of image-text retrieval over the past few years. Frome et al. [\[46\]](#page-16-9)firstly encoded image and text features independently for image-text retrieval. Afterwards, a stream of works [\[47\]](#page-16-10), [\[48\]](#page-16-11) tries to excavate the high-order data information for learning powerful features. Wang et al. [\[49\]](#page-16-12)proposed a maximum-margin ranking loss with the neighborhood constraints for better extracting features. Lee et al. [\[50\]](#page-16-13)made the first attempt to consider the dense pairwise cross-modal interaction and yielded tremendous accuracy improvements at the time. Jia et al. [\[51\]](#page-16-14) tended to learn image-text representation by scaling up the dataset with some noise. As a milestone, OpenAi [\[52\]](#page-16-15) proposed a large vision language model CLIP, which achieved amazing results in retrieval tasks. Yao et al. [\[53\]](#page-16-16) conducted more fine-grained image-text matching research based on CLIP. On the basis of fine-grained image-text matching, Gu et al. [\[54\]](#page-16-17) introduced a token reduction layer to further improve the retrieval capabilities of this type of method. Li et al. [\[55\]](#page-16-18) and Li et al. [\[56\]](#page-16-19) explored the fusion of visual and textual features and add a classification head to determine whether the image-text pairs match. In remote sensing, Yuan et al. [\[15\]](#page-15-14) introduced an asymmetric multimodal feature match network to extract multi-scale features. Yuan et al. [\[57\]](#page-16-20) fused multi-level image features and added a multivariate rerank algorithm to improve the retrieval performance. In view of the great success of CLIP, Zhang et al. [\[16\]](#page-15-15) integrated large-scale remote sensing (RS) and computer vision (CV) datasets, specifically screening remote sensing images for pre-training, and developed a RS CLIP model named GeoRSCLIP. These models predominantly adopted dual-encoders to enhance retrieval capabilities, emphasizing dataset scale, fine-grained image-text matching, and fusion of image-text features.

# <span id="page-2-0"></span>**3 LUOJIAHOG DATASET**

### **3.1 Dataset Construction**

Four example descriptions of a sample scene are depicted in Fig. [1.](#page-0-0) This dataset is sourced from Google Maps and OpenStreetMap (OSM). Google Maps contributes an extensive collection of remote sensing images, while OSM offers a wealth of comprehensive geographical information. As shown in Fig [2,](#page-3-0) we firstly acquired global sampling points through spatial analysis and the evaluation of landscape indices in subsection [3.1.1.](#page-3-1) It allows us to obtain remote sensing images of countries and regions with various topography and different economic levels. Next, we built an extensible classification system and integrated the obtained OSM labels into this classification system in subsection [3.1.2.](#page-4-0) Finally, we adopted a variety of annotation strategies and dataset enhancement methods to generate text descriptions and construct final image caption dataset from the collected images and labels in subsection [3.1.3.](#page-5-0)

#### <span id="page-3-1"></span><span id="page-3-0"></span>*3.1.1 Hierarchical sampling method.*

![](_page_3_Figure_1.jpeg)

Fig. 2: Flowchart of sampling method.

*Spatial auto-correlation for sampling area.* The Moran's I and Getis-Ord Gi\* Index are widely-used methodologies for spatial auto-correlation analysis, which can help select globally representative regions to optimize the subsequent sampling procedure. In our approach, Moran's I is used to distinguish global regional development patterns, while G\* further focuses precisely on regions where hotspots and coldspots exist. The calculation of the Moran's I and Getis-Ord Gi\* Index contains two parts, analysis data and spatial weights. For analysis data, we employed global nighttime data due to its capacity to depict urbanization levels. According to Tobler's First Law of Geography, everything is related, but similar things are more closely related. It explains that spatial locations are involved in the spread of objects or actions. Thus for spatial weight W, we adapt the weights originally proposed by Moran (1950) and specify the neighborhood as follows. Regions i and j are viewed as' neighbours' if they share the boundary or node, which is represented by ⊙. Thus, when i ̸= j, weight wi,j indicates whether i and j are neighbors in space. The spatial weight wi,j is formulated as follows:

$$
w_{ij} = \begin{cases} ||i - j||^{-\gamma}, & \text{if } i \odot j \\ 0, & \text{otherwise} \end{cases}, \tag{1}
$$

which reflects the degree of connection between region i and j.

*Moran's I* contains global Moran's I and local Moran's I (Anselin Local Moran's I). The positive Moran's I denotes a positive spatial correlation, with a larger value signifying a more pronounced spatial correlation. Conversely, the negative Moran's I signifies a negative spatial correlation, with a smaller value indicating greater spatial dissimilarity. The Global Moran's I assesses the pattern of a dataset spatially and determines if it is dispersed, clustered, or random based on the locations and values of the analysis data. It is calculated using the below formula,

$$
I_{global} = \frac{n}{S_0} \frac{\sum_{i=1}^{n} \sum_{j=1}^{n} w_{i,j} z_i z_j}{\sum_{i=1}^{n} z_i^2},
$$
 (2)

where z<sup>i</sup> is the deviation of the nighttime light value x<sup>i</sup> of region i from its average value. S<sup>0</sup> is the aggregation of all spatial weights,

$$
S_0 = \sum_{i=1}^n \sum_{j=1}^n w_{i,j}.
$$
 (3)

Getis-Ord Gi\* Index Hotspot Evaluation The range of the Global Moran's I is between 1 and - 1. When Iglobal approaches 1, it suggests that the pattern observed is clustered spatially, while the opposite indicates dispersion. When Iglobal is close or equal to zero, it signifies the absence of auto-correlation. The final conclusions about the observed pattern are drawn only after looking at the z-score and the p-value of the Index. Only when there is clustering or dispersion in the study region, the Local Moran's I I i local in region i is calculated to further determine regional spatial clustering patterns of all regions around the world,

<span id="page-3-2"></span>
$$
I_{local}^{i} = \frac{z_i}{S_i^2} \sum_{j=1, j \neq i}^{n} w_{i,j} z_j,
$$
 (4)

Image Collection where n is the total number of regions and the function of Si is as follow,

$$
S_i^2 = \frac{\sum_{j=1, j \neq i}^n (x_j - \bar{X})^2}{n - 1}.
$$
 (5)

In Eq. [4,](#page-3-2) z<sup>i</sup> reflects the level of economic development P of the region i and the average level of the entire region. n <sup>j</sup>=1,j̸=<sup>i</sup> wi,j z<sup>j</sup> is referred to Local indicators of spatial association (LISA), reflecting the level between the surrounding regions of the region i and the level of the entire region.

*Getis-Ord Gi\* Index* is used to identify clusters of high or lowvalue elements in space and determine whether they possess significant statistical significance. By examining each region within its neighborhood, it helps establish whether high-value features have statistical significance. The comparison involves evaluating the local against the overall value, and if a substantial disparity exists, it signifies the presence of a hotspot. The model is formulated as follows,

$$
G_i^* = \frac{\sum_{j=1}^n w_{i,j} x_j - \bar{X} \sum_{j=1}^n w_{i,j}}{S \sqrt{\frac{\left[n \sum_{j=1}^n w_{i,j}^2 - \left(\sum_{j=1}^n w_{i,j}\right)^2\right]}{n-1}}},\tag{6}
$$

where x<sup>j</sup> is the nighttime light value of region j, X¯ is the average nighttime light value of the whole region, the function of S is as follows,

$$
S = \sqrt{\frac{\sum_{j=1}^{n} x_j^2}{n} - (\bar{X})^2}.
$$
 (7)

If G<sup>∗</sup> i is greater than 0 and the higher G<sup>∗</sup> i is, the high values of the target object are clustered more tightly (hotspots). Oppositely, the low values of the target object are clustered more tightly (cold-spots).

Sampling area procedure is as follows,

$$
r = (M(D)^+ \cap G(D)) \cup M(D)^-, \tag{8}
$$

where M(·) <sup>+</sup> represents high-high and low-low region calculated based on Local Moran'I and M(·) <sup>−</sup> is the highlow and low-high region. G(·) represents the hot-spots and cold-spots calculated based on Getis-Ord Gi\* Index, D is the global nighttime light data, r is the selected significant regions.

*Spatial heterogeneity sampling points.* Spatial sampling design is the key steps in building a dataset, and many traditional sampling methods may not achieve credible sampling due to the high spatial heterogeneity of land cover. Landscape index (LSI) is the ratio of landscape perimeter to region within a certain range, which quantitatively represents the landscape heterogeneity of the region. For raster data,

$$
LSI = \frac{1}{4} \sum \frac{b_p}{\sqrt{q}} (p = 1, 2, \dots, q),
$$
 (9)

where q is the number of pixels, b<sup>p</sup> represents the number of four neighborhood pixels belonging to different classes than pixel p. Guided by Chen et al. [\[58\]](#page-16-21), for a given region i, we use LSI as three levels to characterize the spatial heterogeneity: rLSI for regional sampling points, cLSI for land cover classes under such region and uLSI for each geographic sampling unit. Following their method, corresponding sample sizes and their spatial distributions according to landscapes classes can be determined.

In order to enable more heterogeneous regions with higher sample density and larger sample size, the number of regional sampling points N<sup>i</sup> in region i is determined by rLSI.

$$
N_i = \frac{rLSI_i \times A_i}{\sum_{j=1}^{j=n} rLSI_j \times A_j} \times N, (i = 1, 2, \dots, n), \quad (10)
$$

where A<sup>i</sup> and A<sup>j</sup> represent the areas of region i and j respectively, N is the total sample size, and n is the total number of regions.

Subsequently, the cLSI represents the spatial variability in land cover classes. A class with a lager cLSI has a more complex spatial distribution and higher spatial heterogeneity; thus, more samples are allocated. For the number of samples of class k in region i, the sample number cNi,k is as follows,

$$
cN_{i,k} = \frac{cLSI_{i,k} \times W_{i,k}}{\sum_{k=1}^{k=m} cLSI_{i,k} \times W_{i,k}} \times N_i, (k = 1, 2, ..., m),
$$
\n(11)

where N<sup>i</sup> is the number of regional sampling points, Wi,k is the proportion of category k in region i, and m is the total number of categories.

Lastly, uLSI adaptively selects the sample point location. Suppose that region i can be divided into R × L geographical units. In each geographical unit, the uLSI of class k in row r and column l, i <sup>k</sup>uLSIa,b, is calculated.

$$
uLSI = {}^{i}_{k}uLSI_{a,b}.
$$
 (12)

A distribution curve C i <sup>k</sup> depicts the heterogeneity of each unit ranked from large to small. The x-axis is the geographical unit coordinate, and the y-axis is <sup>i</sup> <sup>k</sup>uLSIa,b. Firstly, remove the part of C i <sup>k</sup> where the values are equal to zero. Then divide cNi,j equal parts on the x-axis. Finally, sampling points are randomly selected in each interval.

#### <span id="page-4-0"></span>*3.1.2 Extensible classification system construction.*

Firstly, we adopt the OGC-based classification system to solve the issues of different existing classification systems in terms of category naming, category hierarchy, category semantics and compatibility. In OGC-based classification system T [\[59\]](#page-16-22), all categories are hierarchically organized in a three-level tree: third-level labels T<sup>3</sup> fall into secondlevel labels T<sup>2</sup> and then grouped into first-level labels T1, which is the highest level. We utilize ≺ to represent the lowlevel label belongs to high-level label. For OSM labels set

L, we select labels that cannot directly correspond to the classification system L <sup>−</sup> for processing.

$$
L^- = \{l | l \in L \quad and \quad l \notin T\},\tag{13}
$$

$$
T = T_1 \cup T_2 \cup T_3,\tag{14}
$$

$$
T_2 = \bigcup_i \left\{ t_2 \prec t_1^i \mid t_1^i \in T_1, t_2 \in T \right\},\tag{15}
$$

$$
T_3 = \bigcup_i \{ t_3 \prec t_2^i \mid t_2^i \in T_2, t_3 \in T \}.
$$
 (16)

Through the establishment of principles for the inclusion of novel labels, the consolidation of duplicated labels, and the execution of label mapping, we construct the final classification system denoted as T ∗ in Fig. [6.](#page-6-0) Unlike fixed and mixed classification systems, the extensible classification system can be updated through novel labels inclusion, duplicated labels consolidation and label mapping.

*Novel labels inclusion.* For l ∈ L <sup>−</sup>, a top-down strategy is adopted to add it into the T. We analyze the category l belongs to from the OSM classification system, and perform a semantic search starting from the first-level label T<sup>1</sup> by using LLMs. As we confirm that t ∈ T<sup>n</sup> is the best matching label, we compare the function (OSM descriptions) of l with the the candidate label set N{n ≺ t|n ∈ T} to judge whether there is a relationship with the label in N. If not exists, l is added to Tn+1; otherwise, continue searching until no relationship exists. For instance, the description of 'farmyard' in OSM is: '*buildings* for keeping animals, or crop supplies would typically be part of a farmyard tagged landuse=farmyard.' Therefore second-level label 'building' is the best matching label. According to the judgment of experts and LLMs, 'farmyard' has no relationships with the existing candidate labels of 'building', so it is determined that 'farmyard' can be added as the third-level label belongs to 'building'. Another example is 'restaurant', which belongs to 'amenity' according to OSM. Through semantic comparison, it is found that 'amenity' and 'infrastucture' have the same meaning, and 'restaurant' can be added as the third-level label belongs to 'infrastucture' following above rules. If possible, forth-level labels can be added in our classification system.

*Duplicated labels consolidation.* For some common synonyms found in OSM labels, such as "cemetery" and "graveyard" or words with different spellings like "reservior" and "reservoir", we perform the first merging step by asking LLMs about the synonyms for each geographical objects. To avoid potential omissions in the first step, we further merge similar words based on the function of geographical objects. By consulting the descriptions of labels on OSM and then querying LLMs to compare the function with the labels in the existing classification system, we select possible duplicate labels. Finally, human inspection of the label merging results is conducted to prevent errors in merging and potential omissions.

*Label mapping.* Ultimately, we perform statistics of the labels associated with all the images within the dataset. Subsequently, we curate a sub-classification system labeled as T ∗ and determine the categories for the final dataset by selecting those labels with a frequency exceeding zero.

#### <span id="page-5-0"></span>*3.1.3 Detailed caption generation.*

With the collected images for a specific interpretation task, annotation is performed to assign specific semantic labels to the content of interest in the images. In this step, we adopt both professionally manual and automatic annotation to generate corresponding text descriptions for each image.

<span id="page-5-1"></span>![](_page_5_Figure_2.jpeg)

Fig. 3: An example illustrates the scope problems in RS images. As only a small corner of cemetery is captured in the sampled image, it should be excluded from the labels.

*Manual Annotation.* In practice, constructing a largescale image dataset by manual scheme is laborious and time-consuming. To relieve this problem, crowd-sourcing annotation becomes an alternative solution that can be employed to create a large-scale image dataset [\[33\]](#page-15-32), [\[60\]](#page-16-23) while paying efforts to its challenge with quality control. Therefore, annotators with rich experience in remote sensing annotation manually correct the OSM labels corresponding to all images in dataset. In addition, inspired by previous effort [\[12\]](#page-15-11), [\[17\]](#page-15-16), we acquire accurate descriptions of dataset through the fully supervised annotation process.

The rectification of image labels primarily deals with two prevalent issues. The first arises due to the scope problems in RS images, while the second emanates from inaccuracies present within the crowdsourced data. RS images only contain a small part of objects, causing the labels to be discarded. As illustrated in the Fig. [3,](#page-5-1) there is a cemetery in this area according to OSM labels. However, the collected image (in red box) contains a small corner of the cemetery, so the 'cemetery' needs to be removed from the image labels. To solve this type of problem, professional annotators are required to refer to the original map image to determine whether there are features that have been mistakenly added to the label because a small part of it is included in the image. Furthermore, as OSM is crowd-sourced data, the lack of professional annotations may lead to potential errors or outdated labels. Annotators need to visually inspect the images, remove clearly erroneous labels, and fill in any obviously missing labels.

Remote sensing images contain numerous geographical objects, each with distinct attributes and interrelationships with other objects. Therefore, it is imperative to establish specific guidelines for standardizing text descriptions. In the course of formulating guidelines, we take the previous work as a reference [\[12\]](#page-15-11), [\[17\]](#page-15-16), [\[61\]](#page-16-24). The final annotation procedure follows the principles of: (1) describing object attributes, including color, shape, size, relative position between objects and special symbols (such as character 'H' for Helipad). (2) reducing vague words, such as using specific numbers to replace words like many, some, etc. for countable objects. (3) using words like 'near' and 'next to' to replace direction, such as up, down, left, right, since the remote sensing images are aerial view. (4) generally, the annotation process involves first describing the main objects (occupying most of the image), followed by describing detailed objects (5) adding some synonym substitutions to reduce duplication

Additionally, unlike previous work that described each image in five sentences, we do not impose any restrictions on the number of sentences and only require that the image can be fully described. The manual annotation can be formulated as follows,

$$
Desc = annot(I, rect(I, L)), \t(17)
$$

where I is the image, L is the corresponding labels and Desc is the image descriptions. rect(·) represents the annotator's correction of OSM labels based on image. annot(·) represents the procedure annotator following to describe the image.

*Automatic Annotation* Although relatively high-quality datasets can be obtained using manual annotation, its timeconsuming and labor-intensive characteristics are not suitable for large-scale image text generation in today's era of remote sensing big data. In this step, we use image-based text generation methods to automatically get the description of the image separately with carefully designed prompts to boost performance.

With the emergence of various powerful VLMs, it has become feasible to automatically generate a large number of accurate image descriptions. Referenced to [\[17\]](#page-15-16), Minigpt4 [\[21\]](#page-15-20) has very powerful capabilities in remote sensing image caption tasks. In view of the problems existing in the VLMs in terms of details, position and hallucination, we designed different prompts to improve the generated results. Finally, we randomly sampled 10 percent of the generated texts to evaluate the generation quality to ensure the quality of the generated text.

Furthermore we adopt prompt engireering to improve caption quality. We design prompts from the following aspects: direct task specification, task demonstration, memetic proxy, constraining behavior. A direct specification consists in constructing a signifier for the task, which is a pattern for the intended behavior. We designed some templates to constructing the signifier. For example, we set the task to provide a text description of the features contained in a RS image. In task demonstration, formulating guidelines mentioned in manual annotation is adopted. Since Few-shot examples are effective for task specification, some description examples are added to help LLMs better understand the task. Specification by memetic proxy is mechanistically similar to direct specification, which specifies intended tasks from memespace/cultural consciousness. LLMs' ability to create simulations of well-known figures and to draw on cultural information far exceeds the ability of most humans. Therefore, we allow LLMs to play the role of professional annotators, experienced remote sensing scientists, etc., so

<span id="page-6-1"></span>![](_page_6_Figure_0.jpeg)

<span id="page-6-3"></span>Fig. 4: Distribution of sampling points around the world.

<span id="page-6-2"></span>![](_page_6_Figure_2.jpeg)

Fig. 5: The probability density function (PDF) visualization on LuojiaHOG

that LLMs can more accurately understand the task targets. Lastly, in order to make the generated text more suitable for remote sensing images and reduce unreasonable descriptions, we impose constraints in terms of word count, content elimination, etc.

<span id="page-6-0"></span>![](_page_6_Figure_5.jpeg)

Fig. 6: Classification system of dataset: there are 7 first-level labels, 21 second-level labels for each first-level labels, and 131 third-level labels to describe more detailed type. The figure shows part of the classification system.

<span id="page-6-4"></span>TABLE 2: Statistical indicators of the LuojiaHOG dataset.

| Indicators                              | Count    |
|-----------------------------------------|----------|
|                                         |          |
| Number of vocabularies                  | 10044775 |
| Number of distinctive vocabularies      | 14128    |
| Number of sentences                     | 565231   |
| Average length of captions              | 123.56   |
| Average number of sentences per caption | 6.95     |
| Number of images                        | 94856    |

## **3.2 Dataset Statistics and Analysis**

In this section, we perform a thorough analysis of data statistics and visual examination, focusing on sampling diversity, labels quantity, and descriptions granularity.

*Sampling diversity.* We collected images from Google Earth with different resolutions from all over the world. The size of images is 1280 × 1280 and the total number is 94856. Images in dataset are actually multisource, as Google Earth images are from different remote imaging sensors. Fig. [4](#page-6-1) shows the distribution of sampling points in a global level.

*Labels quantity.* Fig. [6](#page-6-0) illustrates the classification system of dataset: there are 7 first-level labels (like "Building area" and "Arable land"), 21 second-level labels for each firstlevel labels (like "Building" and "Infrastucture" in "Building area"), and 131 third-level labels to describe more detailed type (like "Church" and "Cemetery" in "Infrastucture"). As presented in Tab. [1,](#page-1-0) the number of labels in our dataset surpasses that of existing image caption datasets.

*Descriptions granularity.* Fig. [5\(a\)](#page-6-2) displays the probability density function (PDF) of caption length, which (takes on a shape similar to a normal distribution). The longest caption length contains 188 vocabularies, with an average length of 123.562 vocabularies per caption. Fig. [5\(b\)](#page-6-3) illustrates the

<span id="page-7-1"></span>![](_page_7_Figure_0.jpeg)

Fig. 7: The similarity visualization results of the six datasets, where the similarity scores are weighted by commonly used metrics in the field of natural language processing, BLEU and METEOR. (a): RSICD (b): RSITMD (c): NWPU (d): Sydney (e): UCM (f): LuojiaHOG (ours).

PDF of the sentence length, with the longest containing 35 sentences and an average of 6.953 sentences per caption. Tab. [2](#page-6-4) shows several statistical indicators of dataset, such as the total number of vocabularies in captions being 10,044,775, the number of distinct vocabularies being 14,128, and the total number of sentences being 565,231. The datasets with high inter-text similarity inadequately support retrieval models within the domain of remote sensing. We adopted BLEU and METEOR weighted scores as evaluation metrics to assess the quality of existed datasets as well as our LuojiaHOG. For better comparison, the captions in each dataset are clustered according to the text feature, and then randomly selected from each cluster for evaluation. The visualizaiotn is shown in Fig. [7.](#page-7-1) Compared with other datasets, most of our captions have the similar templates at the beginning or end, such as 'This is an image of...' or 'In conclusion, the image. ..'etc., so there are some light blue (representing very weak correlation) in our results. RSTIMD showed the best results in this evaluation due to its carefully processed and relatively short captions. Overall, our result has the second-least severe chunking effect, only lay behind RSTIMD with carefully processed short captions, which reflects the uniqueness of our captions. Datasets like Sydney and UCM, there is a considerable amount of 'noise' indicative of the high language similarity present within these datasets.

# <span id="page-7-0"></span>**4 OUR METHOD**

Motivated by [\[62\]](#page-16-25), we present the proposed CLIP-based Image Semantic Enhancement Network (CISEN) in Fig. [8.](#page-8-0) CISEN mainly consists of dual-path knowledge transferring (in subsection [4.1\)](#page-7-2) and progressive feature fusion (in subsection [4.2\)](#page-7-3). The former mainly transfers the multi-modal knowledge from the large pre-trained visionlanguage model. The progressive feature fusion consists of two stages, visual to text feature mapping (V2TMap) and hierarchical feature enhancement (HFE), to fuse semantic information from textual features to visual features. The V2TMap utilizes an image adapter to transfer global visual features to textual-like features. The HFE adopts feature pyramids network to incorporate textual-like features into local visual features. It enhances the local visual feature representation. Note that global text features are used to obtain text-like features and guide the learning of fused features.

# <span id="page-7-2"></span>**4.1 Dual-path transfer learning**

Dual encoder models can align two modalities representations in the same embedding space. We adopt pretrained CLIP and GeoRSCLIP as our backbone to extract features since they are effective model to learn strong feature representations.

**Multi-level Vision Transformer** For image encoder, the Modified ResNet used in CLIP provides multi-level visual features, while ViT used in CLIP and GeoRSCLIP only produces single-scale feature maps. To reconcile this discrepancy, we follow the technique introduced in [\[63\]](#page-16-26) to generate multi-scale feature from ViT. We integrate four resolutionmodifying modules at evenly distributed intervals of last four transformer blocks. The initial module upsamples the feature map by a factor of 4 using two stride-two 2× transposed convolutions, group normalization and GeLU activation. The output of the second block is upsampled by 2× using a single stride-two 2×2 transposed convolution. The next block's output is taken as is, and the final ViT block's output is downscaled by a factor of 2 using stridetwo 2×2 max pooling. Each of these modules preserves the ViT's embedding/channel dimension.

**Image Encoder.** For an input image I ∈ R H×W×3 , the global visual feature ovc ∈ R<sup>D</sup> and multi-level features are extracted. We select 2th-5th level visual features for further fusion, which are defined as ov<sup>2</sup> ∈ R H <sup>8</sup> <sup>×</sup> <sup>W</sup> <sup>8</sup> ×D<sup>2</sup> , ov<sup>3</sup> ∈ R H <sup>16</sup> <sup>×</sup> <sup>W</sup> <sup>16</sup> ×D<sup>3</sup> , ov<sup>4</sup> ∈ R H <sup>32</sup> <sup>×</sup> <sup>W</sup> <sup>32</sup> ×D<sup>4</sup> , and ov<sup>5</sup> ∈ R 1×D<sup>5</sup> respectively. The 2th-4th level features are the local representations of the image, with the 5th visual feature serving as the global representation. Note that D and D<sup>i</sup> are the ith level feature dimension, H and W are the height and width of the original image.

**Text Encoder.** For an input caption T ∈ R <sup>L</sup>, textual features o ′ <sup>t</sup> ∈ RL×<sup>D</sup> is extracted by Transformer with the architecture modifications described in [\[52\]](#page-16-15), [\[64\]](#page-16-27). T is bracketed with [SOS] and [EOS] tokens. The activations of the highest layer of the transformer at the [EOS] token are treated as the global textual feature o<sup>t</sup> ∈ R<sup>D</sup>, which is transformed into the multi-modal embedding space. Note that D is the feature dimension, L is the length of the caption.

# <span id="page-7-3"></span>**4.2 Progressive Cross-modal Feature Fusion**

Given the abundance of geographical objects within RS images, relying solely on the aligned global visual feature and textual feature acquired through CLIP may not yield the most optimal results for RS ITR. Consequently, we design two training stage to progressively fuse fine-grained semantic features to enrich the visual representation.

**Visual-to-text feature mapping.** Different from just fusing visual features, the first stage of training aims to learn visual-to-text feature mapping (V2TMap). The global visual feature is firstly transformed through image adapter [\[65\]](#page-16-28). It exclusively integrates a limited number of supplementary

<span id="page-8-0"></span>![](_page_8_Figure_1.jpeg)

Fig. 8: The framework of CISEN. CISEN employs a *dual-path knowledge transfer* approach for extracting multi-modal features. *Progressive cross-modal feature fusion* contains V2TMap and Hierarchical feature enhancement (HFE). Through *V2TMap*, global visual features are transformed into text-like representations. *HFE* module facilitates the fusion of transformed visual features with multi-level visual features, resulting in the production of enhanced visual features.

learnable bottleneck linear layers into the image encoder, maintaining the original backbone in a frozen state throughout the training process. For the extracted global image feature ov5, a learnable feature adapter Fadp transforms ov<sup>5</sup> into o ′ v5 , which contains two layers of linear layers.

$$
\mathbf{o}'_{v5} = \mathcal{F}_{adp}(\mathbf{o}_{v5})\tag{18}
$$

A residual connection is adopted for the feature adapter to avoid forgetting the original knowledge encoded by the pretrained CLIP. The residual ratio α helps adjust the degree of maintaining the original knowledge for better performance. The new transformed feature o ∗ v5 is calculated as follows.

<span id="page-8-1"></span>
$$
\mathbf{o}_v = \alpha \mathbf{o}'_{v5} + (1 - \alpha) \mathbf{o}_{v5} \tag{19}
$$

We project the newly transformed visual feature o<sup>v</sup> and paired text feature o<sup>t</sup> into a shared embedding space, allowing o<sup>v</sup> to acquire semantic information, akin to textual features.

**Hierarchical Feature Enhancement.** Inspired by [\[66\]](#page-16-29), ov2, ov3, ov<sup>4</sup> is fused with o<sup>v</sup> in a top-down pathway in the second training stage, named hierarchical feature enhancement (HFE). We firstly enhance ov<sup>4</sup> with o<sup>v</sup> by element-wise multiplication and then upsample the spatial resolution by a factor of 2 to obtain the multi-modal feature om<sup>4</sup> ∈ R H <sup>16</sup> <sup>×</sup> <sup>W</sup> <sup>16</sup> <sup>×</sup>D:

$$
\mathbf{o}_{m4} = \mathcal{C}_{3\times3}(\mathcal{F}_{up}\left(\sigma\left(\mathcal{F}_{proj}(\mathbf{o}_{v4})\right)\cdot\sigma\left(\mathcal{F}_{proj}(\mathbf{o}_{tc})\right)\right)), (20)
$$

where Fup(·) denotes 2 × upsampling, · denotes the elementwise multiplication, σ demotes RELU, and Fproj (·) denotes a projector with 1 × 1 convolution to transform the visual and textual feature into the same feature dimension. C3×3(·) is a 3 × 3 convolution to reduce the aliasing effect of upsampling. Then, om<sup>4</sup> is merged with ov<sup>3</sup> to generate om3:

$$
\mathbf{o}_{m3} = \mathcal{C}_{3\times3} \left( \mathcal{F}_{\text{concat}} \left( \sigma \left( \mathcal{F}_{\text{proj}} \left( \mathbf{o}_{v3} \right) \right), \sigma \left( \mathcal{F}_{\text{proj}} \left( \mathbf{o}_{m4} \right) \right) \right) \right), \tag{21}
$$

where Fconcat(·) denotes the concatenation operation. Afterwards, ov<sup>2</sup> undergoes a 2 × 2 average pooling with 2 strides [\[62\]](#page-16-25) and then is fused with om3:

$$
\mathbf{o}_{m2} = C_{3\times3} \left( \mathcal{F}_{\text{concat}} \left( \sigma \left( \mathcal{F}_{\text{proj}} \left( \mathbf{o}_{v_2} \right) \right), \sigma \left( \mathcal{F}_{\text{proj}} \left( \mathbf{o}_{m3} \right) \right) \right) \right)
$$
  
\n
$$
\mathbf{o}_{v2}' = \mathcal{M} \left( \mathbf{o}_{v2} \right),
$$
\n(22)

where M denotes a kernel size of 2 × 2 average pooling with 2 strides. Subsequently, we aggregate three multimodal features with a 2D spatial-aware feature ospatial into enhanced visual feature o<sup>e</sup> ∈ RN×D:

$$
\mathbf{o}_m = \mathcal{F}_{fuse}^{1 \times 1}(\mathcal{F}_{concat}(\mathbf{o}_{m2}, \mathbf{o}_{m3}, \mathbf{o}_{m4}))
$$
  
\n
$$
\mathbf{o}_e^{prime} = \mathcal{F}_{flatten}(\mathcal{F}_{fuse}^{3 \times 3}(\mathcal{F}_{concat}(\mathbf{o}_m, \mathbf{o}_{spatial})),
$$
\n(23)

where F 1×1 fuse is a 1 × 1 convolution layer and F 3×3 fuse is a 3 × 3 convolution layer, Ff latten flattens the spatial domain of o<sup>v</sup> into a sequence and N = H <sup>16</sup> × W <sup>16</sup> . Finally, attention pooling(AP) extract the global visual feature o<sup>e</sup> from o ′ e as follows.

$$
o_z = [o_{cls}; o'_e] + E_{pos},
$$
  
\n
$$
o_e = \mathcal{F}_{MHSA}(o_z)[0, :],
$$
\n(24)

where ocls serves as image representation capturing global visual feature, Epos is the positional embedding and FMHSA(·) denotes multi-head self-attention.

# **4.3 Model Training**

Assume a batch of B image-text pairs {(I<sup>i</sup> , Ti)} B <sup>i</sup>=1, where I<sup>i</sup> and T<sup>i</sup> are the image and text inputs of the i-th pair. We first train V2TMap by utilizing contrastive loss following original CLIP. The image and text inputs are encoded into {o i v} B <sup>i</sup>=1 and {o i t} B <sup>i</sup>=1, respectively. The contrastive loss L<sup>θ</sup><sup>1</sup> is adopted to maximize the similarity between the paired o i t and o i v and minimize the similarity with other irrelevant o j v or o j t :

$$
\mathcal{L}_{\theta_1} = \mathcal{L}_{I \to T} + \mathcal{L}_{I \leftarrow T}
$$
\n
$$
\mathcal{L}_{I \to T} = -\frac{1}{B} \sum_{i=1}^{B} \log \frac{\exp (\mathbf{o}_v^i \cdot \mathbf{o}_t^i / \tau)}{\sum_{j=1}^{B} \exp (\mathbf{o}_v^i \cdot \mathbf{o}_t^j / \tau)}
$$
\n
$$
\mathcal{L}_{I \leftarrow T} = -\frac{1}{B} \sum_{i=1}^{B} \log \frac{\exp (\mathbf{o}_t^i \cdot \mathbf{o}_v^i / \tau)}{\sum_{j=1}^{B} \exp (\mathbf{o}_t^i \cdot \mathbf{o}_v^j / \tau)},
$$
\n(25)

where τ is the temperature parameter to scale the logits.

# <span id="page-9-0"></span>**5 EXPERIMENT**

#### **5.1 Significance Testing on Spatial Sampling**

Global nighttime data can reflect the intensity of human activities, social and economic development degree [\[67\]](#page-16-30), etc., which is related to the richness of OSM labels and Google images. Consequently, we opted to utilize the VIIRS Stray Light Corrected Nighttime Day/Night Band [\[68\]](#page-16-31) data in 2022 for spatial analysis (Moran'I), aiming to delineate the sampling area. The basic assumption for the Moran's I statistic is that the data values are independent and randomly distributed in the geographical space. When the pvalue obtained is greater than 0.05, the basic assumption is accepted implying that the data values are randomly spread out spatially. Oppositely, the p-value is less than 0.05 and the z-score is negative, the basic assumption of randomness is rejected, inferring that the high and the low values in the dataset are dispersed spatially. Similarly, when the P-value is less than 0.05 with a positive Z-score, the assumption of randomness is again ruled out and the inference drawn is that the high and/or low data values are spatially clustered in the geographical space. As shown in Fig. [9,](#page-9-1) nighttime data around the world are clustered. The cluster conditions in neighborhood areas were classified into four cluster types: High-High, High-Low, Low-High and Low-Low, according to the positive and negative values of the Z score and the LISA value. The cluster type is shown in Tab. [3,](#page-9-2) where ↑ represents positive value; otherwise, negative value. The High-High cluster type suggests spatial agglomerations of neighboring areas marked by high levels of economics. Conversely, the Low-Low cluster type indicates spatial agglomerations of neighboring areas with limited urbanization. The High-Low and Low-High cluster types imply significant development disparities among neighboring areas.

<span id="page-9-1"></span>![](_page_9_Figure_5.jpeg)

Fig. 9: Significant test of Global Moran's Index on global nighttime data.

<span id="page-9-2"></span>TABLE 3: Cluster type according to Local Moran's I. ↑ denotes value is greater than zero, while ↓ denotes less than zero.

| Zi | LISA | Ilocal | Cluster Type |
|----|------|--------|--------------|
| ↑  | ↑    | ↑      | High-High    |
| ↓  | ↓    | ↑      | Low-Low      |
| ↓  | ↑    | ↓      | Low-High     |
| ↑  | ↓    | ↓      | High-Low     |

# **5.2 Experiments**

# *5.2.1 Experimental preparation*

**Implementation details.** LuojiaHOG is divided into 70%, 10% and 20% images for training, validation and test set, respectively. The proposed CISEN is implemented on the PyTorch platform, and all deep models are trained using CPU with i7-6850K, GPU with 32GB Tesla V100. We select CLIP with ResNet-50, ViT-B and GeoRSCLIP with ViT-B/32 as our backbone. Input images are resized to 224 × 224 pixels, and the input tokens length are set with a maximum sentence length of 328 instead of default 77. Initially, the backbone, except position embedding, is frozen, we train the network for 60 epochs using the AdamW optimizer with the learning rate λ = 0.0001. The learning rate is decreased by a factor of 0.1 at the 40th epoch. Following the same settings, V2TMap is trained with backbone frozen in the first stage. In the second stage, V2TMap is frozen as well and only the HFE module is trainable.

**Evaluation metrics.** We evaluate the image retrieval quality using four widelyused metrics: Average Cumulative Gains (ACG) [\[69\]](#page-16-32), Normalized Discounted Cumulative Gains (NDCG) [\[70\]](#page-16-33), Mean Average Precision (MAP) [\[71\]](#page-16-34) and Weighted Mean Average Precision (WMAP) [\[72\]](#page-16-35). ACG represents the average number of shared labels between the query image and the top n retrieved images. Given a query image Iq, the ACG score of the top n retrieved images is calculated by

$$
ACG@n = \frac{1}{n} \sum_{i}^{n} C(q, i),
$$
 (26)

where n denotes the number of top retrieval images and C(q, i) is the number of shared labels between I<sup>q</sup> and Ii . NDCG is a popular evaluation metric in information retrieval. Given a query image Iq, the DCG score of top n retrieved images is defined as

DCG@
$$
n = \sum_{i}^{n} \frac{2^{C(q,i)} - 1}{\log(1 + i)}
$$
. (27)

Then, the normalized DCG (NDCG) score at the position n can be calculated by NDCG@n = DCG@n Z<sup>n</sup> , where Z<sup>n</sup> is the maximum value of DCG@n, which constrains the value of NDCG in the range [0, 1]. MAP is the mean of average precision for each query, which can be calculated by

$$
MAP = \frac{1}{Q} \sum_{q}^{Q} AP(q),
$$
 (28)

where

$$
AP(q) = \frac{1}{N_{Tr}(q)\mathbb{Q}n} \sum_{i}^{n} \left( \text{Tr}(q, i) \frac{N_{Tr}(q)\mathbb{Q}i}{i} \right), \qquad (29)
$$

and T r(q, i) ∈ 0, 1 is an indicator function that if I<sup>q</sup> and I<sup>i</sup> share some labels, T r(q, i) = 1; otherwise T r(q, i) = 0. Q is the number of query sets and NT r(q)@i indicates the number of the relevant imgaes w.r.t the query image I<sup>q</sup> within the top i images.

The definition of WMAP is similar with MAP. The only difference is that WMAP computes the average ACG scores at each top n retrieved image rather than average precision. WMAP can be calculated by

$$
\text{WMAP} = \frac{1}{Q} \sum_{q}^{Q} \left( \frac{1}{N_{Tr}(q) \mathbb{Q} n} \sum_{i}^{n} (Tr(q, i) \times ACG \mathbb{Q} i) \right)
$$
(30)

**Comparison with state-of-the-art models.** To verify the effectiveness of CISEN, we conducted some experiments on LuojiaHOG. We select current state-of-the-art (SOTA) in image-text retrieval tasks. They are ALBEF [\[56\]](#page-16-19), ALIGN [\[51\]](#page-16-14), CLIP [\[52\]](#page-16-15), FILIP [\[53\]](#page-16-16), Wukong [\[54\]](#page-16-17), BLIP [\[55\]](#page-16-18), GeoRSCLIP [\[16\]](#page-15-15). For fair comparison, we froze the backbone of all models and utilize image adapter [\[73\]](#page-16-36) for finetuning on ITR tasks. We train all the networks with pre-trained weights with a learning rate of 0.0001, and divided by 10 after 40 epoches. All the networks are optimized using the AdamW with a momentum of 0.9, and weight decay of 0.0001. Further, the relevant parameters can be slightly adjusted, making it applicable to the ITR.

# *5.2.2 Quantitative evaluation on LuojiaHOG*

We quantify the ITR retrieval performance by comparing current SOTA vision-language models with our CISEN in terms of MAP, WMAP, NDCG and ACG scores. Tab. [4](#page-12-0) and Tab. [5](#page-13-0) shows the quantitative results from second-level and third-level labels, respectively. By and large, GeoRSCLIP is more suited for remote sensing image retrieval tasks owing to its pretraining on remote sensing datasets. It has demonstrated notable performance in both I2T and T2I retrieval tasks. CISEN (RS), utilizing GeoRSCLIP as its backbone, achieved superior performance across all tasks. In the I2T retrieval task, the results of CLIP with ViT-B as its backbone (CLIP-ViT) exhibit an average increase of 1.9%∼2% MAP, 2.8%∼3% WMAP, 1%∼1.4% NDCG and 3%∼3.4% ACG at second-level, 0.4%∼1.2% MAP, 1.2%∼2.3% WMAP, 0.5%∼1% NDCG and 0.4%∼1.8% ACG at third-level compared to using ResNet-50 (CLIP-RN50) as the backbone. Conversely, the difference in results between the two backbones in text-image retrieval tasks is negligible. However, this phenomenon changes significantly with the introduction of CISEN. CISEN (ViT), leveraging ViT-B as the backbone, yields substantial enhancements in both I2T and T2I retrieval tasks compared with CISEN (RN50). For example, CISEN (ViT) brings increments of 5% WMAP@5 and 3.4% WMAP@5 on T2I and I2T retrieval task at thirdlevel compared to CISEN (RN50). In contrast, Filip, another dual encoder model integrating fine-grained token-wise contrastive learning based on CLIP, does not yield optimal results on datasets like LuojiaHOG which is characterized

<span id="page-10-0"></span>![](_page_10_Figure_8.jpeg)

Fig. 10: UMAP visualization of generated embeddings from models. Paired inputs are fed into the pre-trained models and the embeddings are visualized in 2D using UMAP (lines indicate pairs).

by longer text lengths and more complex scenes. As its improvement, Wukong's performance matches that of the dual encoder combined with fusion module models, like Blip and Albef. It is noteworthy that ALIGN outperforms CISEN (RN50) and slightly trails behind CISEN (ViT-B).

# *5.2.3 Ablation Study on CISEN*

We conduct various ablation studies to understand the effectiveness of our method from different aspects, where CLIP with ViT-B is used as the backbone network.

**Effect of Different Components** We study the effect of different components in our method, including dual encoder backbone, V2TMap and HFE. To evaluate the importance of these modules for ITR retrieval task, we implement the ablation study on LuojiaHOG with thirdlevel labels by using the features extracted from these three

<span id="page-11-1"></span>

| Query                                                                                                                                  |                       |      |      |               |      |      |               | Top  | 10<br>Retrieval | Result |               |                       |                        |                           |
|----------------------------------------------------------------------------------------------------------------------------------------|-----------------------|------|------|---------------|------|------|---------------|------|-----------------|--------|---------------|-----------------------|------------------------|---------------------------|
|                                                                                                                                        |                       | NO.1 | NO.2 | NO.3          | NO.4 | NO.5 | NO.6          | NO.7 | NO.8            | NO.9   | NO.10         |                       |                        |                           |
| The<br>image<br>depicts<br>a<br>marina<br>with<br>a<br>large                                                                           |                       |      |      |               |      |      |               |      |                 |        |               |                       |                        |                           |
| number<br>of<br>boats<br>docked<br>along<br>the                                                                                        | CLIP<br>(RN50)        |      |      |               |      |      |               |      |                 |        |               | #0:<br>residential    | #13:<br>store          | #25:<br>resort            |
| shoreline.<br>The<br>boats<br>are<br>of<br>various<br>sizes<br>and<br>shapes,<br>with<br>some<br>being<br>small                        | +<br>Image<br>Adapter |      |      |               |      |      |               |      |                 |        |               | #1:<br>wetland        | #14:<br>retail         | #26:<br>riverbank         |
| fishing<br>boats<br>and<br>othersbeing<br>larger                                                                                       |                       |      |      | M#36,<br>W#19 |      |      |               |      |                 |        | M#36,<br>W#19 | #2:<br>nature_reserve | #15:<br>commercial     | #27:<br>school            |
| yachts.<br>The<br>water<br>in<br>the<br>image<br>is<br>a<br>deep<br>blue<br>color,<br>and<br>the<br>sky<br>is<br>clear<br>and<br>blue, | CISEN<br>(RN50)       |      |      |               |      |      |               |      |                 |        |               | #3:<br>river          | #16:<br>parking_lot    | #28:<br>reservoir         |
| with<br>a<br>few<br>fluffy<br>cloudsvisible.<br>The                                                                                    |                       |      |      |               |      |      |               | M#36 |                 |        |               | #4:<br>stream         | #17:<br>island         | #29:<br>greenhouse        |
| marina<br>is<br>surrounded<br>by<br>green                                                                                              | CLIP<br>(ViT-B)<br>+  |      |      |               |      |      |               |      |                 |        |               | #5:<br>forest         | #18:<br>building       | #30:<br>urban_residential |
| vegetation,<br>including<br>trees<br>and<br>bushes,<br>which<br>provide<br>shade<br>for<br>the<br>boats<br>and                         | Image<br>Adapter      |      |      |               |      |      | M#36,<br>W#19 |      |                 |        |               | #6:<br>airport        | #19:<br>industrial     | #31:<br>canal             |
| the<br>docked<br>area.<br>The<br>dock<br>itself<br>is<br>made                                                                          |                       |      |      |               |      |      |               |      |                 |        |               | #7:<br>orchard        | #20:<br>pitch          | #32:<br>greengrocer       |
| of<br>wooden<br>planks<br>and<br>appears<br>to<br>be<br>well-<br>maintained.<br>In<br>the<br>background,<br>a                          | CISEN<br>(ViT-B)      |      |      |               |      |      |               |      |                 |        |               | #8:<br>park           |                        | #33:<br>golf_course       |
| mountain<br>range<br>can<br>be<br>seen,<br>with<br>its                                                                                 |                       |      |      |               |      |      |               |      |                 | M#36   |               |                       | #21:<br>playground     |                           |
| peaks<br>covered<br>in<br>snow.<br>The<br>imagealso                                                                                    | GeoRSCLIP<br>+        |      |      |               |      |      |               |      |                 |        |               | #9:<br>farmyard       | #22:<br>natural_meadow | #34:<br>airfield          |
| shows<br>a<br>few<br>boats<br>that<br>are<br>moored<br>in<br>the                                                                       | Image<br>Adapter      |      |      |               |      |      |               |      | M#36,<br>W#19   |        |               | #10:<br>farmland      | #23:<br>theme_park     | #35:<br>water_works       |
| water,<br>with<br>their<br>sails<br>down.                                                                                              |                       |      |      |               |      |      |               |      |                 |        |               | #11:<br>scrub         | #24:<br>train_station  | #36:<br>marina            |
|                                                                                                                                        | CISEN<br>(RS)         |      |      |               |      |      |               |      |                 |        |               | #12:<br>water         |                        |                           |
| True<br>Label:<br>#12;<br>#36                                                                                                          |                       |      |      |               |      |      |               |      |                 |        |               |                       |                        |                           |

Fig. 11: The T2I retrieval results on CLIP, GeoRSCLIP and CISEN. The retrieved images with red box are incorrect, with yellow box are inaccurate and with green box are correct. At the bottom are some third-level labels.

<span id="page-11-0"></span>![](_page_11_Figure_2.jpeg)

Fig. 12: The I2T retrieval results on CLIP, GeoRSCLIP and CISEN. The retrieved texts in red are incorrect, in yellow are inaccurate and in green are correct.

modules. Tab. [6,](#page-14-1) we can see that: (1) The introduction of V2TMap and HFE brings the retrieval performance gain. When incorporated with RN50 as backbone, V2TMap can significantly improve at an average incremental of 3.8% MAP, 5.1% WMAP, 2.7% NDCG and 5.2% ACG on I2T retreival task, 4% MAP, 8.1% WMAP, 3.8% NDCG and 6% ACG on T2I retrieval task. When it comes to ViT as backbone, the retrieval results obtained by incorporating each of the two modules are quite similar. (2) Except utilizing CLIP (RN50) on T2I retrieval task, the combination of V2TMap and HFE gives further performance boost, which shows that our method is effective in learning and enhancing visual features via both V2TMap and HFE. (3) Although GeoRSCLIP-based model outperforming others across all metrics, the inclusion of the two modules narrows the performance gap. For instance, in terms of MAP@5 on I2T retrieval task, when RN50 is used in a zero-shot setting, it achieves only 0.4588, while GeoRSCLIP achieves 0.6597, resulting in a difference of 0.2009. However, after adding the two modules, this difference reduces to 0.0315, which indicating that our approach enables smaller models to achieve significant improvements, approaching the performance of larger models.

**Effect of Residual Ratio in V2TMap** In Eq. [19,](#page-8-1) α balance the raw knowledge from pretrained backbone and new knowledge from V2TMap. To study the impacts of residual ratio α in V2TMap, we select CLIP (ViT-B) as backbone and conduct experiments with the residual ratio ranging from 0.1 to 0.9. Fig. [13](#page-17-0) and Fig. [14](#page-18-0) respectively illustrate the results at second-level and third-level of LuojiaHOG. The training performance of V2TMap (depicted by the blue lines) shows a gradual improvement as the ratio value increases, reaching its peak at 0.9. Afterwards, introducing HFE (depicted by the orange lines) during training yields varied outcomes. In I2T retrieval, the performance remains

<span id="page-12-0"></span>

| TABLE 4: Quantative performance comparison of all models in terms of MAP@n, Weighted MAP@n, NDCG@n and ACG@n |  |  |
|--------------------------------------------------------------------------------------------------------------|--|--|
| (n=5, 10, 20 ,50 , 100) on LuojiaHOG second-level labels. The best is marked in bold.                        |  |  |

|            |                 |              |        |        | Image To Text |        |        | Text To Image |        |        |        |        |  |  |
|------------|-----------------|--------------|--------|--------|---------------|--------|--------|---------------|--------|--------|--------|--------|--|--|
| Methods    | Image Encoder   | Text Encoder | @5     | @10    | @20           | @50    | @100   | @5            | @10    | @20    | @50    | @100   |  |  |
|            |                 |              |        |        |               |        |        | MAP           |        |        |        |        |  |  |
| Albef      | ViT-B/16        | BERT         | 0.7312 | 0.7087 | 0.6881        | 0.6591 | 0.6377 | 0.6257        | 0.6059 | 0.5732 | 0.5387 | 0.5194 |  |  |
| Align      | EfficientNet-B7 | BERT         | 0.7516 | 0.7304 | 0.7140        | 0.6852 | 0.6640 | 0.7339        | 0.7196 | 0.7007 | 0.6752 | 0.6555 |  |  |
|            | RN50            |              | 0.7411 | 0.7228 | 0.7036        | 0.6740 | 0.6519 | 0.7468        | 0.7287 | 0.7046 | 0.6725 | 0.6486 |  |  |
| CLIP       | ViT-B/32        | Transformer  | 0.7606 | 0.7420 | 0.7229        | 0.6941 | 0.6725 | 0.7460        | 0.7309 | 0.7113 | 0.6863 | 0.6661 |  |  |
| Blip       | ViT-B/32        | BERT         | 0.7294 | 0.7066 | 0.6780        | 0.6441 | 0.6204 | 0.6959        | 0.6783 | 0.6553 | 0.6239 | 0.5997 |  |  |
| Filip      | ViT-B/32        | Transformer  | 0.6564 | 0.6294 | 0.6053        | 0.5763 | 0.5603 | 0.6387        | 0.6193 | 0.5945 | 0.5669 | 0.5532 |  |  |
| WuKong     | ViT-B/32        | Transformer  | 0.7295 | 0.7112 | 0.6953        | 0.6659 | 0.6427 | 0.6738        | 0.6585 | 0.6403 | 0.6177 | 0.6031 |  |  |
| GeoRSCLIP  | ViT-B/32        | Transformer  | 0.7667 | 0.7506 | 0.7318        | 0.7041 | 0.6826 | 0.7616        | 0.7461 | 0.7263 | 0.6990 | 0.6781 |  |  |
|            | RN50            |              | 0.7433 | 0.7234 | 0.7049        | 0.6752 | 0.6533 | 0.7306        | 0.7133 | 0.6930 | 0.6658 | 0.6457 |  |  |
| CISEN      | ViT-B/32        | Transformer  | 0.7698 | 0.7502 | 0.7318        | 0.7025 | 0.6795 | 0.7575        | 0.7425 | 0.7221 | 0.6968 | 0.6752 |  |  |
| CISEN (RS) | ViT-B/32        | Transformer  | 0.7748 | 0.7566 | 0.7387        | 0.7091 | 0.6861 | 0.7661        | 0.7516 | 0.7311 | 0.7030 | 0.6806 |  |  |
|            |                 |              |        |        |               |        |        | Weighted MAP  |        |        |        |        |  |  |
| Albef      | ViT-B/16        | BERT         | 0.9118 | 0.8795 | 0.8506        | 0.8103 | 0.7798 | 0.7303        | 0.7112 | 0.6781 | 0.6436 | 0.6239 |  |  |
| Align      | EfficientNet-B7 | BERT         | 0.9421 | 0.9106 | 0.8871        | 0.8470 | 0.8173 | 0.8720        | 0.8561 | 0.8347 | 0.8069 | 0.7853 |  |  |
| CLIP       | RN50            | Transformer  | 0.9089 | 0.8838 | 0.8590        | 0.8210 | 0.7928 | 0.9092        | 0.8870 | 0.8571 | 0.8178 | 0.7879 |  |  |
|            | ViT-B/32        |              | 0.9373 | 0.9135 | 0.8882        | 0.8514 | 0.8229 | 0.8943        | 0.8768 | 0.8543 | 0.8256 | 0.8027 |  |  |
| Blip       | ViT-B/32        | BERT         | 0.9191 | 0.8875 | 0.8489        | 0.8026 | 0.7701 | 0.8473        | 0.8229 | 0.7931 | 0.7543 | 0.7258 |  |  |
| Filip      | ViT-B/32        | Transformer  | 0.7910 | 0.7604 | 0.7341        | 0.7025 | 0.6848 | 0.7597        | 0.7383 | 0.7119 | 0.6834 | 0.6702 |  |  |
| WuKong     | ViT-B/32        | Transformer  | 0.8688 | 0.8483 | 0.8313        | 0.7980 | 0.7706 | 0.7987        | 0.7836 | 0.7643 | 0.7402 | 0.7251 |  |  |
| GeoRSCLIP  | ViT-B/32        | Transformer  | 0.9486 | 0.9272 | 0.9018        | 0.8649 | 0.8359 | 0.9262        | 0.9069 | 0.8823 | 0.8494 | 0.8242 |  |  |
|            | RN50            |              | 0.9136 | 0.8871 | 0.8619        | 0.8249 | 0.7967 | 0.8792        | 0.8573 | 0.8320 | 0.7998 | 0.7764 |  |  |
| CISEN      | ViT-B/32        | Transformer  | 0.9570 | 0.9298 | 0.9038        | 0.8639 | 0.8327 | 0.9246        | 0.9062 | 0.8822 | 0.8515 | 0.8246 |  |  |
| CISEN (RS) | ViT-B/32        | Transformer  | 0.9678 | 0.9407 | 0.9144        | 0.8725 | 0.8409 | 0.9407        | 0.9191 | 0.8925 | 0.8563 | 0.8274 |  |  |
|            |                 |              |        |        |               |        |        | NDCG          |        |        |        |        |  |  |
| Albef      | ViT-B/16        | BERT         | 0.7491 | 0.7418 | 0.7397        | 0.7437 | 0.7536 | 0.6711        | 0.6763 | 0.6721 | 0.6764 | 0.6903 |  |  |
| Align      | EfficientNet-B7 | BERT         | 0.7617 | 0.7528 | 0.7518        | 0.7542 | 0.7640 | 0.7445        | 0.7407 | 0.7375 | 0.7407 | 0.7510 |  |  |
| CLIP       | RN50            | Transformer  | 0.7571 | 0.7494 | 0.7466        | 0.7484 | 0.7583 | 0.7548        | 0.7491 | 0.7439 | 0.7463 | 0.7556 |  |  |
|            | ViT-B/32        |              | 0.7713 | 0.7614 | 0.7575        | 0.7588 | 0.7685 | 0.7582        | 0.7527 | 0.7463 | 0.7496 | 0.7588 |  |  |
| Blip       | ViT-B/32        | BERT         | 0.7471 | 0.7401 | 0.7328        | 0.7348 | 0.7448 | 0.7162        | 0.7134 | 0.7103 | 0.7149 | 0.7264 |  |  |
| Filip      | ViT-B/32        | Transformer  | 0.6914 | 0.6877 | 0.6895        | 0.6955 | 0.7105 | 0.6792        | 0.6819 | 0.6812 | 0.6891 | 0.7041 |  |  |
| WuKong     | ViT-B/32        | Transformer  | 0.7452 | 0.7392 | 0.7395        | 0.7416 | 0.7511 | 0.7005        | 0.7006 | 0.7006 | 0.7078 | 0.7211 |  |  |
| GeoRSCLIP  | ViT-B/32        | Transformer  | 0.7739 | 0.7653 | 0.7612        | 0.7633 | 0.7735 | 0.7683        | 0.7619 | 0.7555 | 0.7577 | 0.7666 |  |  |
| CISEN      | RN50            | Transformer  | 0.7588 | 0.7504 | 0.7469        | 0.7487 | 0.7587 | 0.7480        | 0.7448 | 0.7404 | 0.7429 | 0.7524 |  |  |
|            | ViT-B/32        |              | 0.7771 | 0.7659 | 0.7631        | 0.7640 | 0.7729 | 0.7647        | 0.7577 | 0.7534 | 0.7568 | 0.7659 |  |  |
| CISEN (RS) | ViT-B/32        | Transformer  | 0.7822 | 0.7722 | 0.7684        | 0.7682 | 0.7773 | 0.7719        | 0.7659 | 0.7601 | 0.7608 | 0.7686 |  |  |
|            |                 |              |        |        |               |        |        | ACG           |        |        |        |        |  |  |
| Albef      | ViT-B/16        | BERT         | 0.8021 | 0.7902 | 0.7793        | 0.7496 | 0.7191 | 0.6219        | 0.6098 | 0.6038 | 0.5980 | 0.5869 |  |  |
| Align      | EfficientNet-B7 | BERT         | 0.8375 | 0.8272 | 0.8178        | 0.7880 | 0.7552 | 0.7970        | 0.7913 | 0.7799 | 0.7610 | 0.7352 |  |  |
| CLIP       | RN50            | Transformer  | 0.8146 | 0.8048 | 0.7915        | 0.7630 | 0.7266 | 0.8189        | 0.8085 | 0.7905 | 0.7598 | 0.7267 |  |  |
|            | ViT-B/32        |              | 0.8455 | 0.8358 | 0.8239        | 0.7944 | 0.7563 | 0.8149        | 0.8096 | 0.8017 | 0.7792 | 0.7494 |  |  |
| Blip       | ViT-B/32        | BERT         | 0.8055 | 0.7877 | 0.7691        | 0.7391 | 0.7070 | 0.7450        | 0.7341 | 0.7201 | 0.6936 | 0.6664 |  |  |
| Filip      | ViT-B/32        | Transformer  | 0.6674 | 0.6658 | 0.6624        | 0.6583 | 0.6489 | 0.6462        | 0.6482 | 0.6471 | 0.6467 | 0.6421 |  |  |
| WuKong     | ViT-B/32        | Transformer  | 0.7756 | 0.7721 | 0.7686        | 0.7429 | 0.7079 | 0.7085        | 0.7125 | 0.7126 | 0.7033 | 0.6899 |  |  |
| GeoRSCLIP  | ViT-B/32        | Transformer  | 0.8611 | 0.8523 | 0.8385        | 0.8071 | 0.7664 | 0.8475        | 0.8380 | 0.8247 | 0.7972 | 0.7655 |  |  |
|            | RN50            |              | 0.8162 | 0.8061 | 0.7953        | 0.7672 | 0.7306 | 0.7896        | 0.7817 | 0.7724 | 0.7497 | 0.7228 |  |  |
| CISEN      | ViT-B/32        | Transformer  | 0.8627 | 0.8520 | 0.8369        | 0.8028 | 0.7614 | 0.8474        | 0.8377 | 0.8270 | 0.7980 | 0.7617 |  |  |
| CISEN (RS) | ViT-B/32        | Transformer  | 0.8740 | 0.8609 | 0.8453        | 0.8090 | 0.7669 | 0.8621        | 0.8466 | 0.8312 | 0.7989 | 0.7633 |  |  |

relatively stable across different ratio values. At level 2, fluctuations in MAP@5 and NDCG@5 are within 1%, while those in WMAP@5 and ACG@5 are within 2.5%. At level 3, fluctuations in ACG@5, NDCG@5, and MAP@5 are within 2%, and within 4% for WMAP@5. The best performance is achieved when the residual ratio is equal to 0.9. Overall, the better the feature representation obtained through V2TMap, the better the enhanced visual feature obtained from HFE in the end.

13

<span id="page-13-0"></span>TABLE 5: Quantative performance comparison of all models in terms of MAP@n, Weighted MAP@n, NDCG@n and ACG@n (n=5, 10, 20 ,50 , 100) on LuojiaHOG third-level labels. The best is marked in bold.

|               |                 |              |        |        | Image To Text |        | Text To Image |              |        |        |        |        |  |
|---------------|-----------------|--------------|--------|--------|---------------|--------|---------------|--------------|--------|--------|--------|--------|--|
| Methods       | Image Encoder   | Text Encoder | @5     | @10    | @20           | @50    | @100          | @5           | @10    | @20    | @50    | @100   |  |
|               |                 |              |        |        |               |        |               | MAP          |        |        |        |        |  |
| Albef         | ViT-B/16        | BERT         | 0.6620 | 0.6442 | 0.6237        | 0.5903 | 0.5657        | 0.5481       | 0.5342 | 0.5057 | 0.4719 | 0.4527 |  |
| Align         | EfficientNet-B7 | BERT         | 0.6855 | 0.6656 | 0.6473        | 0.6123 | 0.5873        | 0.6625       | 0.6476 | 0.6258 | 0.5975 | 0.5758 |  |
|               | RN50            |              | 0.6817 | 0.6642 | 0.6432        | 0.6105 | 0.5878        | 0.6873       | 0.6688 | 0.6447 | 0.6125 | 0.5877 |  |
| CLIP          | ViT-B/32        | Transformer  | 0.6934 | 0.6743 | 0.6515        | 0.6175 | 0.5927        | 0.6900       | 0.6737 | 0.6509 | 0.6195 | 0.5952 |  |
| Blip          | ViT-B/32        | BERT         | 0.6697 | 0.6478 | 0.6179        | 0.5799 | 0.5542        | 0.6221       | 0.6053 | 0.5801 | 0.5478 | 0.5248 |  |
| Filip         | ViT-B/32        | Transformer  | 0.5771 | 0.5550 | 0.5309        | 0.5011 | 0.4842        | 0.5672       | 0.5512 | 0.5290 | 0.5004 | 0.4857 |  |
| WuKong        | ViT-B/32        | Transformer  | 0.6531 | 0.6386 | 0.6236        | 0.5921 | 0.5673        | 0.5986       | 0.5846 | 0.5638 | 0.5386 | 0.5220 |  |
| GeoRSCLIP     | ViT-B/32        | Transformer  | 0.6942 | 0.6755 | 0.6547        | 0.6222 | 0.5983        | 0.6989       | 0.6823 | 0.6584 | 0.6263 | 0.6017 |  |
|               | RN50            |              | 0.6854 | 0.6695 | 0.6498        | 0.6167 | 0.5937        | 0.6690       | 0.6572 | 0.6378 | 0.6097 | 0.5869 |  |
| CISEN         | ViT-B/32        | Transformer  | 0.6983 | 0.6798 | 0.6571        | 0.6231 | 0.5977        | 0.6940       | 0.6789 | 0.6578 | 0.6278 | 0.6027 |  |
| CISEN (RS)    | ViT-B/32        | Transformer  | 0.7112 | 0.6906 | 0.6684        | 0.6337 | 0.6083        | 0.7037       | 0.6881 | 0.6661 | 0.6349 | 0.6093 |  |
|               |                 |              |        |        |               |        |               | Weighted MAP |        |        |        |        |  |
| Albef         | ViT-B/16        | BERT         | 0.7959 | 0.7703 | 0.7423        | 0.6982 | 0.6651        | 0.6194       | 0.6065 | 0.5777 | 0.5427 | 0.5222 |  |
| Align         | EfficientNet-B7 | BERT         | 0.8388 | 0.8099 | 0.7831        | 0.7350 | 0.6998        | 0.7794       | 0.7617 | 0.7358 | 0.7019 | 0.6751 |  |
|               | RN50            |              | 0.8194 | 0.7943 | 0.7665        | 0.7242 | 0.6939        | 0.8206       | 0.7965 | 0.7658 | 0.7263 | 0.6947 |  |
| CLIP          | ViT-B/32        | Transformer  | 0.8431 | 0.8161 | 0.7857        | 0.7396 | 0.7054        | 0.8286       | 0.8078 | 0.7783 | 0.7380 | 0.7060 |  |
| Blip          | ViT-B/32        | BERT         | 0.8428 | 0.8081 | 0.7640        | 0.7082 | 0.6700        | 0.7583       | 0.7340 | 0.6994 | 0.6555 | 0.6245 |  |
| Filip         | ViT-B/32        | Transformer  | 0.6630 | 0.6400 | 0.6144        | 0.5822 | 0.5640        | 0.6544       | 0.6361 | 0.6115 | 0.5808 | 0.5654 |  |
| WuKong        | ViT-B/32        | Transformer  | 0.7618 | 0.7449 | 0.7269        | 0.6896 | 0.6592        | 0.6896       | 0.6749 | 0.6522 | 0.6251 | 0.6070 |  |
| GeoRSCLIP     | ViT-B/32        | Transformer  | 0.8592 | 0.8304 | 0.7993        | 0.7530 | 0.7180        | 0.8490       | 0.8258 | 0.7929 | 0.7499 | 0.7163 |  |
|               | RN50            |              | 0.8295 | 0.8057 | 0.7785        | 0.7352 | 0.7040        | 0.7936       | 0.7802 | 0.7572 | 0.7230 | 0.6940 |  |
| CISEN         | ViT-B/32        | Transformer  | 0.8633 | 0.8336 | 0.8003        | 0.7521 | 0.7155        | 0.8440       | 0.8238 | 0.7956 | 0.7555 | 0.7209 |  |
| CISEN (RS)    | ViT-B/32        | Transformer  | 0.8847 | 0.8523 | 0.8196        | 0.7689 | 0.7313        | 0.8728       | 0.8499 | 0.8183 | 0.7727 | 0.7352 |  |
|               |                 |              |        |        |               |        |               | NDCG         |        |        |        |        |  |
| Albef         | ViT-B/16        | BERT         | 0.6874 | 0.6854 | 0.6820        | 0.6781 | 0.6820        | 0.5938       | 0.6064 | 0.6052 | 0.6051 | 0.6128 |  |
| Align         | EfficientNet-B7 | BERT         | 0.7042 | 0.6977 | 0.6938        | 0.6873 | 0.6903        | 0.6819       | 0.6799 | 0.6729 | 0.6701 | 0.6758 |  |
|               | RN50            |              | 0.7019 | 0.6979 | 0.6909        | 0.6844 | 0.6885        | 0.7069       | 0.7014 | 0.6923 | 0.6859 | 0.6862 |  |
| CLIP          | ViT-B/32        | Transformer  | 0.7122 | 0.7051 | 0.6978        | 0.6915 | 0.6943        | 0.7069       | 0.7018 | 0.6927 | 0.6875 | 0.6893 |  |
|               |                 | BERT         | 0.6911 | 0.6850 | 0.6746        | 0.6675 | 0.6718        | 0.6506       | 0.6514 | 0.6449 | 0.6428 | 0.6500 |  |
| Blip<br>Filip | ViT-B/32        | Transformer  | 0.6180 | 0.6225 | 0.6252        | 0.6261 | 0.6341        | 0.6103       | 0.6174 | 0.6186 | 0.6207 | 0.6294 |  |
|               | ViT-B/32        | Transformer  | 0.6773 | 0.6778 | 0.6792        | 0.6760 | 0.6796        | 0.6334       | 0.6383 | 0.6372 | 0.6399 | 0.6466 |  |
| WuKong        | ViT-B/32        | Transformer  | 0.7155 | 0.7073 | 0.6986        | 0.6929 | 0.6972        | 0.7118       | 0.7073 | 0.6968 | 0.6907 | 0.6933 |  |
| GeoRSCLIP     | ViT-B/32        |              |        | 0.6981 | 0.6926        | 0.6866 | 0.6907        | 0.6858       | 0.6850 | 0.6795 | 0.6790 | 0.6818 |  |
| CISEN         | RN50            | Transformer  | 0.7024 |        |               |        |               |              |        |        |        |        |  |
|               | ViT-B/32        |              | 0.7177 | 0.7100 | 0.7013        | 0.6944 | 0.6978        | 0.7083       | 0.7047 | 0.6958 | 0.6924 | 0.6941 |  |
| CISEN (RS)    | ViT-B/32        | Transformer  | 0.7274 | 0.7157 | 0.7073        | 0.6999 | 0.7029        | 0.7157       | 0.7100 | 0.7025 | 0.6969 | 0.6992 |  |
|               |                 |              |        |        |               |        |               | ACG          |        |        |        |        |  |
| Albef         | ViT-B/16        | BERT         | 0.6828 | 0.6697 | 0.6590        | 0.6257 | 0.5948        | 0.5115       | 0.5041 | 0.5010 | 0.4932 | 0.4835 |  |
| Align         | EfficientNet-B7 | BERT         | 0.7221 | 0.7084 | 0.6969        | 0.6588 | 0.6234        | 0.6894       | 0.6808 | 0.6678 | 0.6431 | 0.6127 |  |
| CLIP          | RN50            | Transformer  | 0.7123 | 0.7005 | 0.6858        | 0.6554 | 0.6189        | 0.7174       | 0.7074 | 0.6953 | 0.6616 | 0.6256 |  |
|               | ViT-B/32        |              | 0.7309 | 0.7192 | 0.7015        | 0.6645 | 0.6237        | 0.7321       | 0.7204 | 0.7039 | 0.6716 | 0.6325 |  |
| Blip          | ViT-B/32        | BERT         | 0.7106 | 0.6887 | 0.6648        | 0.6291 | 0.5949        | 0.6410       | 0.6259 | 0.6125 | 0.5849 | 0.5581 |  |
| Filip         | ViT-B/32        | Transformer  | 0.5366 | 0.5331 | 0.5352        | 0.5333 | 0.5264        | 0.5381       | 0.5403 | 0.5397 | 0.5383 | 0.5322 |  |
| WuKong        | ViT-B/32        | Transformer  | 0.6647 | 0.6585 | 0.6525        | 0.6227 | 0.5872        | 0.5895       | 0.5902 | 0.5893 | 0.5806 | 0.5655 |  |
| GeoRSCLIP     | ViT-B/32        | Transformer  | 0.7451 | 0.7323 | 0.7138        | 0.6756 | 0.6324        | 0.7502       | 0.7335 | 0.7147 | 0.6783 | 0.6378 |  |
| CISEN         | RN50            | Transformer  | 0.7257 | 0.7115 | 0.6971        | 0.6644 | 0.6269        | 0.7099       | 0.7049 | 0.6931 | 0.6613 | 0.6268 |  |
|               | ViT-B/32        |              | 0.7495 | 0.7336 | 0.7134        | 0.6728 | 0.6278        | 0.7513       | 0.7415 | 0.7242 | 0.6846 | 0.6393 |  |
| CISEN (RS)    | ViT-B/32        | Transformer  | 0.7691 | 0.7508 | 0.7292        | 0.6862 | 0.6399        | 0.7783       | 0.7612 | 0.7374 | 0.6955 | 0.6491 |  |

## *5.2.4 Visualization Analysis*

**Feature structure** In this section, we study the structure of the image-text features. For clarity, we select CLIP (ViT), CLIP (ViT) + V2TMap and CISEN (ViT) for comparison. First, we sample 1% image-text pairs from LuojiaHOG and extract their features through three models. Then, their structure is displayed in the 2-D space using UMAP visualization. The visual results of the three archives can be found in Fig. [10.](#page-10-0) In CLIP (ViT), the features of images and texts are scattered in the spatial distribution. The paired image-text pairs may not necessarily be the closest in feature space, and similar image or text features are

not orderly clustered together. This leads to errors in ITR retrieval. However, features derived by our method are compact and organized. The image features (in red) are clustered internally, while the corresponding text features (in blue) are distributed externally. These results illustrate that the discrimination of enhanced visual features obtained by CISEN is high, which is beneficial to the ITR task. **Retrieval example** To testify our method in an intuitional way, we visualize the typical ITR retrieval results. Fig [12](#page-11-0) and Fig [11](#page-11-1) illustrate the retrieval performance visualization of CLIP (RN50), CLIP (ViT-B), and GeoRSCLIP fine-tuned with image adapter and corresponding CISEN. CISEN (RS) does

<span id="page-14-1"></span>TABLE 6: Ablation study on LuojiaHOG where Model represents backbone feature extractor, V2TMap represents visual-totext mapping and HFE represents Hierarchie feature enhancement. The best is in bold and the second best is underlined.

|                 | V2TMap | HFE |        |        |         |        |         | Image To Text |        |         |          |        |        |         |
|-----------------|--------|-----|--------|--------|---------|--------|---------|---------------|--------|---------|----------|--------|--------|---------|
| Backbone        |        |     | MAP@5  | MAP@20 | MAP@100 | WMAP@5 | WMAP@20 | WMAP@100      | NDCG@5 | NDCG@20 | NDCG@100 | ACG@5  | ACG@20 | ACG@100 |
|                 | #      | #   | 0.4588 | 0.4202 | 0.3690  | 0.5293 | 0.4841  | 0.4257        | 0.5091 | 0.5440  | 0.5675   | 0.4001 | 0.3948 | 0.3916  |
| CLIP (RN50)     | !      | #   | 0.7411 | 0.7036 | 0.6519  | 0.9089 | 0.8590  | 0.7928        | 0.7571 | 0.7466  | 0.7583   | 0.8146 | 0.7915 | 0.7266  |
|                 | #      | !   | 0.7031 | 0.6624 | 0.6150  | 0.8583 | 0.8043  | 0.7444        | 0.7261 | 0.7207  | 0.7358   | 0.7536 | 0.7346 | 0.6907  |
|                 | !      | !   | 0.7433 | 0.7049 | 0.6533  | 0.9136 | 0.8619  | 0.7967        | 0.7588 | 0.7469  | 0.7587   | 0.8162 | 0.7953 | 0.7306  |
|                 | #      | #   | 0.6546 | 0.6194 | 0.5903  | 0.7869 | 0.7425  | 0.7140        | 0.6994 | 0.6989  | 0.7250   | 0.6644 | 0.6896 | 0.6799  |
| CLIP (ViT)      | !      | #   | 0.7606 | 0.7229 | 0.6725  | 0.9373 | 0.8882  | 0.8229        | 0.7713 | 0.7575  | 0.7685   | 0.8455 | 0.8239 | 0.7563  |
|                 | #      | !   | 0.7630 | 0.7238 | 0.6721  | 0.9351 | 0.8837  | 0.8175        | 0.7740 | 0.7580  | 0.7685   | 0.8385 | 0.8160 | 0.7503  |
|                 | !      | !   | 0.7698 | 0.7318 | 0.6795  | 0.9570 | 0.9038  | 0.8327        | 0.7771 | 0.7631  | 0.7729   | 0.8627 | 0.8369 | 0.7614  |
|                 | #      | #   | 0.6597 | 0.6432 | 0.6116  | 0.8051 | 0.7832  | 0.7444        | 0.6979 | 0.7092  | 0.7351   | 0.7117 | 0.7337 | 0.6996  |
|                 | !      | #   | 0.7667 | 0.7318 | 0.6826  | 0.9486 | 0.9018  | 0.8359        | 0.7739 | 0.7612  | 0.7735   | 0.8611 | 0.8385 | 0.7664  |
| GeoRSCLIP (ViT) | #      | !   | 0.7740 | 0.7342 | 0.6817  | 0.9589 | 0.9050  | 0.8334        | 0.7797 | 0.7638  | 0.7745   | 0.8636 | 0.8356 | 0.7634  |
|                 | !      | !   | 0.7748 | 0.7387 | 0.6861  | 0.9678 | 0.9144  | 0.8409        | 0.7822 | 0.7684  | 0.7773   | 0.8740 | 0.8453 | 0.7669  |
|                 |        |     |        |        |         |        |         | Text To Image |        |         |          |        |        |         |
|                 |        |     | MAP@5  | MAP@20 | MAP@100 | WMAP@5 | WMAP@20 | WMAP@100      | NDCG@5 | NDCG@20 | NDCG@100 | ACG@5  | ACG@20 | ACG@100 |
|                 | #      | #   | 0.4594 | 0.4341 | 0.3922  | 0.5140 | 0.4909  | 0.4499        | 0.5183 | 0.5585  | 0.5839   | 0.4005 | 0.4221 | 0.4163  |
| CLIP (RN50)     | !      | #   | 0.7468 | 0.7046 | 0.6486  | 0.9092 | 0.8571  | 0.7879        | 0.7612 | 0.7505  | 0.7567   | 0.8189 | 0.7905 | 0.7267  |
|                 | #      | !   | 0.6885 | 0.6635 | 0.6280  | 0.7961 | 0.7732  | 0.7415        | 0.7077 | 0.7132  | 0.7326   | 0.7234 | 0.7268 | 0.7038  |
|                 | !      | !   | 0.7306 | 0.6930 | 0.6457  | 0.8792 | 0.8320  | 0.7764        | 0.7480 | 0.7404  | 0.7524   | 0.7896 | 0.7724 | 0.7228  |
|                 | #      | #   | 0.7144 | 0.6774 | 0.6304  | 0.8644 | 0.8230  | 0.7705        | 0.7319 | 0.7266  | 0.7413   | 0.7798 | 0.7680 | 0.7201  |
|                 | !      | #   | 0.7460 | 0.7113 | 0.6661  | 0.8943 | 0.8543  | 0.8027        | 0.7582 | 0.7463  | 0.7588   | 0.8149 | 0.8017 | 0.7494  |
| CLIP (ViT)      | #      | !   | 0.7481 | 0.7131 | 0.6663  | 0.9076 | 0.8668  | 0.8111        | 0.7576 | 0.7485  | 0.7611   | 0.8277 | 0.8099 | 0.7540  |
|                 | !      | !   | 0.7575 | 0.7221 | 0.6752  | 0.9246 | 0.8822  | 0.8246        | 0.7647 | 0.7534  | 0.7659   | 0.8474 | 0.8270 | 0.7617  |
|                 | #      | #   | 0.7335 | 0.6977 | 0.6537  | 0.9011 | 0.8589  | 0.8066        | 0.7454 | 0.7383  | 0.7539   | 0.8158 | 0.8029 | 0.7515  |
|                 | !      | #   | 0.7616 | 0.7263 | 0.6781  | 0.9262 | 0.8823  | 0.8242        | 0.7683 | 0.7555  | 0.7666   | 0.8475 | 0.8247 | 0.7655  |
| GeoRSCLIP (ViT) | #      | !   | 0.7558 | 0.7234 | 0.6789  | 0.9230 | 0.8828  | 0.8296        | 0.7649 | 0.7558  | 0.7684   | 0.8424 | 0.8286 | 0.7700  |
|                 | !      | !   | 0.7661 | 0.7311 | 0.6806  | 0.9407 | 0.8925  | 0.8274        | 0.7719 | 0.7601  | 0.7686   | 0.8621 | 0.8312 | 0.7633  |

not retrieve any incorrect images, while other models had some inaccurate retrieval results. We can also observe that the overall quality of retrieval results is largely dependent on the backbone model used. Specifically, models based on GeoRSCLIP demonstrate the best performance, followed by those based on ViT-B, and finally, those based on RN50 exhibit the poorest performance. This discrepancy is primarily attributed to differences in model architecture and training data. Notably, the GeoRSCLIP backbone, trained on remote sensing image-text data, unsurprisingly showcases superior performance in lateral comparisons under similar conditions. CISEN consistently achieves better retrieval results, primarily due to VTMap and HFE, which enable the integration of global semantic information and multi-scale image features, thereby obtaining superior feature representation. For example, features such as "boats of different sizes and types," "calm and deep water surface," and "broad and neat harbor" should be fully reflected in the retrieved text, while irrelevant words such as "residential area," "houses," and "grassland" should not be retrieved as results, as indicated by the yellow and red text representing these incorrect results. Similarly, in the task of T2I retrieval task, all models perform well in distinguishing scenes corresponding to the label "ship" but sometimes overlook the semantically related label "port". Additionally, scenes labeled with "industrial area," the neat arrangement of containers bears resemblance to containers on port docks, leading to misclassifications. When using RN50 as the backbone, the model also confuses green water surfaces with vegetation. Fig. [15,](#page-19-0) Fig. [16](#page-19-1) and Fig. [17](#page-19-2) are visualizations on T2I retrieval task. In general, CISEN retrieves a relatively small number of inaccurate (in yellow) and wrong results (in red), showcasting its superiority. Fig. [18](#page-20-0) and Fig. [19,](#page-20-1) Fig. [20](#page-21-0) are visualizations of the retrieval performance on I2T retrieval task with three backbone model incorporating V2TMap and HFE modules.

# <span id="page-14-0"></span>**6 CONCLUSIONS**

In this paper, we present LuojiaHOG with geo-awareness, comprehensive-caption and extensible-friendly, which can boost remote sensing image-caption development. Land monitoring and management heavily rely on remote sensing technology. The success of RS intelligent interpretation enables accurate identification and retrieval of interested geographic features in complex RS scenarios. With the surge of large language models and multimodal architectures, using prior knowledge as language to match and integrate with remote sensing images and further enhancing the capability of deep models in remote sensing applications is a promising research area. Existing image caption datasets often overlook geographic characteristics during sampling, and the images are mostly single-labeled, which mismatches the complexity of remote sensing images typically found in diverse scenes. Additionally, the dataset descriptions are often brief and contain a large amount of similar text, further hindering the development of multimodal models for remote sensing. To address these issues, we first explore a novel method to construct image-text datasets and create a multi-labeled image-text dataset called LuojiaHOG. Then, we propose CISEN, a method capable of enhancing features of pretrained models. Experiments conducted on Luojia-HOG dataset for RS ITR tasks, our method outperforms other state-of-the-art models across all metrics. Furthermore, we will release the LuojiaHOG dataset and demo, contributing to the advancement of research in remote sensing imagetext multimodality.

In our forthcoming efforts, we aspire to expand the size of LuojiaHOG while addressing the challenges posed by the illusions inherent in large language models for automatic text generation. Besides, more geospatial prior information will be incorporated, such as specific geographical locations, image capture seasons, climate conditions, and other relevant details. Furthermore, the dataset can be applied to a broader range of RS multi-modal downstream tasks, such as image caption, visual question answering, etc.

# **ACKNOWLEDGMENT**

This work was supported by the Key Research and Development Program of Hubei Province (No. 2023BAB173), the State Key Laboratory of Geo-Information Engineering, NO. SKLGIE2021-M-3-1, funded by Chinese National Natural Science Foundation Projects (No. 41901265), a Major Program of the National Natural Science Foundation of China (No. 92038301), and was supported in part by the Special Fund of Hubei Luojia Laboratory (No. 220100028).

# **REFERENCES**

- <span id="page-15-0"></span>[1] Q. Yu, Y. Shang, X. Liu, Z. Lei, X. Li, X. Zhu, X. Liu, X. Yang, A. Su, X. Zhang *et al.*, "Full-parameter vision navigation based on scene matching for aircrafts," *Science China Information Sciences*, vol. 57, pp. 1–10, 2014.
- <span id="page-15-1"></span>[2] Z. Wang, D. Zhao, and Y. Cao, "Visual navigation algorithm for night landing of fixed-wing unmanned aerial vehicle," *Aerospace*, vol. 9, no. 10, p. 615, 2022.
- <span id="page-15-2"></span>[3] V. Jovanovic, C. Moroney, and D. Nelson, "Multi-angle geometric processing for globally geo-located and co-registered misr image data," *Remote Sensing of Environment*, vol. 107, no. 1-2, pp. 22–32, 2007.
- <span id="page-15-3"></span>[4] T. T. Lê, J.-L. Froger, and D. H. T. Minh, "Multiscale framework for rapid change analysis from sar image time series: Case study of flood monitoring in the central coast regions of vietnam," *Remote Sensing of Environment*, vol. 269, p. 112837, 2022.
- <span id="page-15-4"></span>[5] G. Panteras and G. Cervone, "Enhancing the temporal resolution of satellite-based flood extent generation using crowdsourced data for disaster monitoring," *International journal of remote sensing*, vol. 39, no. 5, pp. 1459–1474, 2018.
- <span id="page-15-5"></span>[6] P. Ge, H. Gokon, and K. Meguro, "A review on synthetic aperture radar-based building damage assessment in disasters," *Remote Sensing of Environment*, vol. 240, p. 111693, 2020.
- <span id="page-15-6"></span>[7] S. Rivest, Y. Bédard, M.-J. Proulx, M. Nadeau, F. Hubert, and J. Pastor, "Solap technology: Merging business intelligence with geospatial technology for interactive spatio-temporal exploration and analysis of data," *ISPRS journal of photogrammetry and remote sensing*, vol. 60, no. 1, pp. 17–33, 2005.
- <span id="page-15-7"></span>[8] J. De Leeuw, A. Vrieling, A. Shee, C. Atzberger, K. M. Hadgu, C. M. Biradar, H. Keah, and C. Turvey, "The potential and uptake of remote sensing in insurance: A review," *Remote Sensing*, vol. 6, no. 11, pp. 10 888–10 912, 2014.
- <span id="page-15-8"></span>[9] C. Milesi, C. D. Elvidge, R. R. Nemani, and S. W. Running, "Assessing the impact of urban land development on net primary productivity in the southeastern united states," *Remote Sensing of Environment*, vol. 86, no. 3, pp. 401–410, 2003.
- <span id="page-15-9"></span>[10] O. J. Reichman, M. B. Jones, and M. P. Schildhauer, "Challenges and opportunities of open data in ecology," *Science*, vol. 331, no. 6018, pp. 703–705, 2011.
- <span id="page-15-10"></span>[11] B. Qu, X. Li, D. Tao, and X. Lu, "Deep semantic understanding of high resolution remote sensing image," in *2016 International conference on computer, information and telecommunication systems (Cits)*. IEEE, 2016, pp. 1–5.
- <span id="page-15-11"></span>[12] X. Lu, B. Wang, X. Zheng, and X. Li, "Exploring models and data for remote sensing image caption generation," *IEEE Transactions on Geoscience and Remote Sensing*, vol. 56, no. 4, pp. 2183–2195, 2017.
- <span id="page-15-12"></span>[13] Q. Cheng, H. Huang, Y. Xu, Y. Zhou, H. Li, and Z. Wang, "Nwpu-captions dataset and mlca-net for remote sensing image captioning," *IEEE Transactions on Geoscience and Remote Sensing*, vol. 60, pp. 1–19, 2022.
- <span id="page-15-13"></span>[14] M. N. Kamel Boulos, B. Resch, D. N. Crowley, J. G. Breslin, G. Sohn, R. Burtner, W. A. Pike, E. Jezierski, and K.-Y. S. Chuang, "Crowdsourcing, citizen sensing and sensor web technologies for public and environmental health surveillance and crisis management: trends, ogc standards and application examples," *International journal of health geographics*, vol. 10, pp. 1–29, 2011.
- <span id="page-15-14"></span>[15] Z. Yuan, W. Zhang, K. Fu, X. Li, C. Deng, H. Wang, and X. Sun, "Exploring a fine-grained multiscale method for cross-modal remote sensing image retrieval," *arXiv preprint arXiv:2204.09868*, 2022.
- <span id="page-15-15"></span>[16] Z. Zhang, T. Zhao, Y. Guo, and J. Yin, "Rs5m: A large scale visionlanguage dataset for remote sensing vision-language foundation model," *arXiv preprint arXiv:2306.11300*, 2023.
- <span id="page-15-16"></span>[17] Y. Hu, J. Yuan, C. Wen, X. Lu, and X. Li, "Rsgpt: A remote sensing vision language model and benchmark," *arXiv preprint arXiv:2307.15266*, 2023.
- <span id="page-15-17"></span>[18] Y. Yang and S. Newsam, "Bag-of-visual-words and spatial extensions for land-use classification," in *Proceedings of the 18th SIGSPA-TIAL international conference on advances in geographic information systems*, 2010, pp. 270–279.
- <span id="page-15-18"></span>[19] G. Cheng, J. Han, and X. Lu, "Remote sensing image scene classification: Benchmark and state of the art," *Proceedings of the IEEE*, vol. 105, no. 10, pp. 1865–1883, 2017.
- <span id="page-15-19"></span>[20] J. Li, D. Li, S. Savarese, and S. Hoi, "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models," *arXiv preprint arXiv:2301.12597*, 2023.
- <span id="page-15-20"></span>[21] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, "Minigpt-4: Enhancing vision-language understanding with advanced large language models," *arXiv preprint arXiv:2304.10592*, 2023.
- <span id="page-15-21"></span>[22] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," *arXiv preprint arXiv:2304.08485*, 2023.
- <span id="page-15-22"></span>[23] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman *et al.*, "Laion-5b: An open large-scale dataset for training next generation image-text models," *Advances in Neural Information Processing Systems*, vol. 35, pp. 25 278–25 294, 2022.
- <span id="page-15-23"></span>[24] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki, "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs," *arXiv preprint arXiv:2111.02114*, 2021.
- <span id="page-15-24"></span>[25] P. Sharma, N. Ding, S. Goodman, and R. Soricut, "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning," in *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, 2018, pp. 2556–2565.
- <span id="page-15-25"></span>[26] K. Srinivasan, K. Raman, J. Chen, M. Bendersky, and M. Najork, "Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning," in *Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval*, 2021, pp. 2443–2449.
- <span id="page-15-26"></span>[27] M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim, "Coyo-700m: Image-text pair dataset," [https://github.com/kakaobrain/](https://github.com/kakaobrain/coyo-dataset) [coyo-dataset,](https://github.com/kakaobrain/coyo-dataset) 2022.
- <span id="page-15-27"></span>[28] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, "Conceptual 12m: Pushing web-scale image-text pre-training to recognize longtail visual concepts," in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 3558–3568.
- <span id="page-15-28"></span>[29] K. Desai, G. Kaul, Z. Aysola, and J. Johnson, "Redcaps: Webcurated image-text data created by the people, for the people," *arXiv preprint arXiv:2111.11431*, 2021.
- <span id="page-15-29"></span>[30] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li, "Yfcc100m: The new data in multimedia research," *Communications of the ACM*, vol. 59, no. 2, pp. 64–73, 2016.
- <span id="page-15-30"></span>[31] V. Ordonez, G. Kulkarni, and T. Berg, "Im2text: Describing images using 1 million captioned photographs," *Advances in neural information processing systems*, vol. 24, 2011.
- <span id="page-15-31"></span>[32] G. Sumbul, M. Charfuelan, B. Demir, and V. Markl, "Bigearthnet: A large-scale benchmark archive for remote sensing image understanding," in *IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium*. IEEE, 2019, pp. 5901–5904.
- <span id="page-15-32"></span>[33] G. Christie, N. Fendley, J. Wilson, and R. Mukherjee, "Functional map of the world," in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 2018, pp. 6172–6180.
- <span id="page-15-33"></span>[34] Y. Long, G.-S. Xia, S. Li, W. Yang, M. Y. Yang, X. X. Zhu, L. Zhang, and D. Li, "On creating benchmark dataset for aerial image interpretation: Reviews, guidances, and million-aid," *IEEE Journal of selected topics in applied earth observations and remote sensing*, vol. 14, pp. 4205–4230, 2021.
- <span id="page-15-34"></span>[35] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth, "Every picture tells a story: Generating sentences from images," in *Computer Vision–ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11*. Springer, 2010, pp. 15–29.
- <span id="page-15-35"></span>[36] S. by Saheel, "Baby talk: Understanding and generating image descriptions."
- <span id="page-16-0"></span>[37] M. Hodosh, P. Young, and J. Hockenmaier, "Framing image description as a ranking task: Data, models and evaluation metrics," *Journal of Artificial Intelligence Research*, vol. 47, pp. 853–899, 2013.
- <span id="page-16-1"></span>[38] Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and S. Lazebnik, "Improving image-sentence embeddings using large weakly annotated photo collections," in *Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV 13*. Springer, 2014, pp. 529–545.
- <span id="page-16-2"></span>[39] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio, "Show, attend and tell: Neural image caption generation with visual attention," in *International conference on machine learning*. PMLR, 2015, pp. 2048–2057.
- <span id="page-16-3"></span>[40] J. Mun, M. Cho, and B. Han, "Text-guided attention model for image captioning," in *Proceedings of the AAAI conference on artificial intelligence*, vol. 31, no. 1, 2017.
- <span id="page-16-4"></span>[41] M. Yang, J. Liu, Y. Shen, Z. Zhao, X. Chen, Q. Wu, and C. Li, "An ensemble of generation-and retrieval-based image captioning with dual generator generative adversarial network," *IEEE Transactions on Image Processing*, vol. 29, pp. 9627–9640, 2020.
- <span id="page-16-5"></span>[42] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds *et al.*, "Flamingo: a visual language model for few-shot learning," *Advances in Neural Information Processing Systems*, vol. 35, pp. 23 716–23 736, 2022.
- <span id="page-16-6"></span>[43] I. Najdenkoska, X. Zhen, and M. Worring, "Meta learning to bridge vision and language models for multimodal few-shot learning," *arXiv preprint arXiv:2302.14794*, 2023.
- <span id="page-16-7"></span>[44] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, "Instructblip: Towards general-purpose vision-language models with instruction tuning," 2023.
- <span id="page-16-8"></span>[45] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," 2023.
- <span id="page-16-9"></span>[46] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov, "Devise: A deep visual-semantic embedding model," *Advances in neural information processing systems*, vol. 26, 2013.
- <span id="page-16-10"></span>[47] Y. Wu, S. Wang, G. Song, and Q. Huang, "Learning fragment selfattention embeddings for image-text matching," in *Proceedings of the 27th ACM international conference on multimedia*, 2019, pp. 2088– 2096.
- <span id="page-16-11"></span>[48] S. Chun, S. J. Oh, R. S. De Rezende, Y. Kalantidis, and D. Larlus, "Probabilistic embeddings for cross-modal retrieval," in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 8415–8424.
- <span id="page-16-12"></span>[49] L. Wang, Y. Li, J. Huang, and S. Lazebnik, "Learning two-branch neural networks for image-text matching tasks," *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 41, no. 2, pp. 394– 407, 2018.
- <span id="page-16-13"></span>[50] K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, "Stacked cross attention for image-text matching," in *Proceedings of the European conference on computer vision (ECCV)*, 2018, pp. 201–216.
- <span id="page-16-14"></span>[51] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig, "Scaling up visual and visionlanguage representation learning with noisy text supervision," in *International conference on machine learning*. PMLR, 2021, pp. 4904– 4916.
- <span id="page-16-15"></span>[52] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark *et al.*, "Learning transferable visual models from natural language supervision," in *International conference on machine learning*. PMLR, 2021, pp. 8748–8763.
- <span id="page-16-16"></span>[53] L. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu, X. Liang, Z. Li, X. Jiang, and C. Xu, "Filip: Fine-grained interactive languageimage pre-training," *arXiv preprint arXiv:2111.07783*, 2021.
- <span id="page-16-17"></span>[54] J. Gu, X. Meng, G. Lu, L. Hou, M. Niu, X. Liang, L. Yao, R. Huang, W. Zhang, X. Jiang, C. Xu, and H. Xu, "Wukong: 100 million large-scale chinese cross-modal pre-training dataset and a foundation framework," 2022. [Online]. Available: <https://arxiv.org/abs/2202.06767>
- <span id="page-16-18"></span>[55] J. Li, D. Li, C. Xiong, and S. Hoi, "Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation," in *International Conference on Machine Learning*. PMLR, 2022, pp. 12 888–12 900.
- <span id="page-16-19"></span>[56] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi, "Align before fuse: Vision and language representation learning with momentum distillation," *Advances in neural information processing systems*, vol. 34, pp. 9694–9705, 2021.
- <span id="page-16-20"></span>[57] Z. Yuan, W. Zhang, C. Tian, X. Rong, Z. Zhang, H. Wang, K. Fu, and X. Sun, "Remote sensing cross-modal text-image retrieval based on global and local information," *IEEE Transactions on Geoscience and Remote Sensing*, vol. 60, pp. 1–16, 2022.
- <span id="page-16-21"></span>[58] F. Chen, J. Chen, H. Wu, D. Hou, W. Zhang, J. Zhang, X. Zhou, and L. Chen, "A landscape shape index-based sampling approach for land cover accuracy assessment," *Science China Earth Sciences*, vol. 59, pp. 2263–2274, 2016.
- <span id="page-16-22"></span>[59] Z. Cao, L. Jiang, P. Yue, J. Gong, X. Hu, S. Liu, H. Tan, C. Liu, B. Shangguan, and D. Yu, "A large scale training sample database system for intelligent interpretation of remote sensing imagery," *Geo-Spatial Information Science*, pp. 1–20, 2023.
- <span id="page-16-23"></span>[60] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, "Microsoft coco: Common objects in context," in *Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13*. Springer, 2014, pp. 740–755.
- <span id="page-16-24"></span>[61] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollár, and C. L. Zitnick, "Microsoft coco captions: Data collection and evaluation server," *arXiv preprint arXiv:1504.00325*, 2015.
- <span id="page-16-25"></span>[62] Z. Wang, Y. Lu, Q. Li, X. Tao, Y. Guo, M. Gong, and T. Liu, "Cris: Clip-driven referring image segmentation," in *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, 2022, pp. 11 686–11 695.
- <span id="page-16-26"></span>[63] A. Ali, H. Touvron, M. Caron, P. Bojanowski, M. Douze, A. Joulin, I. Laptev, N. Neverova, G. Synnaeve, J. Verbeek *et al.*, "Xcit: Crosscovariance image transformers," *Advances in neural information processing systems*, vol. 34, pp. 20 014–20 027, 2021.
- <span id="page-16-27"></span>[64] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *et al.*, "Language models are unsupervised multitask learners," *OpenAI blog*, vol. 1, no. 8, p. 9, 2019.
- <span id="page-16-28"></span>[65] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao, "Clip-adapter: Better vision-language models with feature adapters," *International Journal of Computer Vision*, pp. 1–15, 2023.
- <span id="page-16-29"></span>[66] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, "Feature pyramid networks for object detection," in *Proceedings of the IEEE conference on computer vision and pattern recognition*, 2017, pp. 2117–2125.
- <span id="page-16-30"></span>[67] Q. Zheng, K. C. Seto, Y. Zhou, S. You, and Q. Weng, "Nighttime light remote sensing for urban applications: Progress, challenges, and prospects," *ISPRS Journal of Photogrammetry and Remote Sensing*, vol. 202, pp. 125–141, 2023.
- <span id="page-16-31"></span>[68] S. Mills, S. Weiss, and C. Liang, "Viirs day/night band (dnb) stray light characterization and correction," in *Earth observing systems XVIII*, vol. 8866. SPIE, 2013, pp. 549–566.
- <span id="page-16-32"></span>[69] K. Järvelin and J. Kekäläinen, "Ir evaluation methods for retrieving highly relevant documents," in *ACM SIGIR Forum*, vol. 51, no. 2. ACM New York, NY, USA, 2017, pp. 243–250.
- <span id="page-16-33"></span>[70] ——, "Cumulated gain-based evaluation of ir techniques," *ACM Transactions on Information Systems (TOIS)*, vol. 20, no. 4, pp. 422– 446, 2002.
- <span id="page-16-34"></span>[71] R. Baeza-Yates, "Modern information retrieval," *Addison Wesley google schola*, vol. 2, pp. 127–136, 1999.
- <span id="page-16-35"></span>[72] F. Zhao, Y. Huang, L. Wang, and T. Tan, "Deep semantic ranking based hashing for multi-label image retrieval," in *Proceedings of the IEEE conference on computer vision and pattern recognition*, 2015, pp. 1556–1564.
- <span id="page-16-36"></span>[73] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao, "Clip-adapter: Better vision-language models with feature adapters," *International Journal of Computer Vision*, vol. 132, no. 2, pp. 581–595, 2024.

<span id="page-17-0"></span>![](_page_17_Figure_1.jpeg)

(b) Text To Image Retrieval

Fig. 13: Influence of different residual ratio on the second-level ITR performance based on CISEN(ViT).

<span id="page-18-0"></span>![](_page_18_Figure_0.jpeg)

(b) Text To Image Retrieval

Fig. 14: Influence of different residual ratio on the third-level ITR performance based on CISEN (ViT).

<span id="page-19-0"></span>

| Query                                                                                                                                                                                    |                                     |      |           |           |           |                  | Top  | 10<br>Retrieval | Result    |          |           |                                                             |                                                                  |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------|------|-----------|-----------|-----------|------------------|------|-----------------|-----------|----------|-----------|-------------------------------------------------------------|------------------------------------------------------------------|
| The<br>image<br>depicts<br>a<br>residential                                                                                                                                              |                                     | NO.1 | NO.2      | NO.3      | NO.4      | NO.5             | NO.6 | NO.7            | NO.8      | NO.9     | NO.10     |                                                             |                                                                  |
| area<br>with<br>well-maintained<br>buildings<br>and<br>green<br>meadows<br>in<br>the<br>center.<br>The<br>buildings<br>are<br>of<br>varying<br>heights<br>and                            | CLIP<br>(RN50)<br>+<br>zero<br>shot |      |           | M#16      |           | M#0,#16,#22;W#35 | M#16 | M#16            | M#16      | M#16     |           | #0:<br>residential<br>#1:<br>wetland                        | #19:<br>industrial<br>#20:<br>pitch                              |
| architectural<br>styles,<br>with<br>some<br>having<br>balconies<br>and<br>others<br>without.<br>The<br>green<br>meadows<br>are<br>well-manicured<br>and                                  | CLIP<br>(RN50)<br>+<br>V2TMap       |      |           |           |           |                  |      |                 |           |          | M#16,#22  | #2:<br>nature_reserve<br>#3:<br>river                       | #21:<br>playground<br>#22:<br>natural_meadow                     |
| appear<br>to<br>be<br>used<br>for<br>recreational<br>purposes.<br>There<br>are<br>many<br>cars<br>parked<br>in<br>the<br>parking<br>lot<br>in<br>front<br>of<br>the                      | CLIP<br>(RN50)<br>+<br>HFE          |      |           | M#16,#22  | M#0,#22   |                  |      | M#16            | M#16      | M#16,#22 |           | #4:<br>stream<br>#5:<br>forest<br>#6:<br>airport            | #23:<br>theme_park<br>#24:<br>train_station<br>#25:<br>resort    |
| buildings,<br>indicating<br>a<br>high<br>level<br>of<br>vehicular<br>activity<br>in<br>the<br>area.<br>True<br>Label:<br>#0;<br>#16;<br>#22                                              | CISEN<br>(RN50)                     |      | M#16;W#15 |           | M#16      |                  | M#16 | M#16<br>M#16    | M#16      | M#22     |           | #7:<br>orchard<br>#8:<br>park<br>#9:<br>farmyard            | #26:<br>riverbank<br>#27:<br>school<br>#28:<br>reservoir         |
| The<br>image<br>depicts<br>a<br>large<br>grey<br>airport<br>with<br>a<br>rectangular<br>shape,<br>surrounded<br>by<br>a<br>perimeter<br>fence.<br>The<br>runway                          | CLIP<br>(RN50)<br>+<br>zero<br>shot |      |           | M#34;W#19 | M#34;W#16 | M#34;W#19        |      | M#34;W#16       | M#34;W#16 |          |           | #10:<br>farmland<br>#11:<br>scrub<br>#12:<br>water          | #29:<br>greenhouse<br>#30:<br>urban_residential<br>#31:<br>canal |
| is<br>oriented<br>in<br>a<br>north-south<br>direction<br>and<br>is<br>bordered<br>by<br>taxiways<br>leading<br>to<br>various<br>gates.<br>The<br>airport<br>is<br>situated               | CLIP<br>(RN50)<br>+<br>V2TMap       |      |           |           | M#34;W#16 |                  |      | M#34;W#16       | M#34;W#16 |          | M#34;W#16 | #13:<br>store<br>#14:<br>retail                             | #32:<br>greengrocer<br>#33:<br>golf_course                       |
| on<br>a<br>flat<br>terrain<br>with<br>no<br>visible<br>vegetation<br>in<br>the<br>immediate<br>surroundings.<br>A<br>single<br>airplane<br>is<br>visible<br>on<br>the<br>runway,<br>with | CLIP<br>(RN50)<br>+<br>HFE          |      |           |           | M#34;W#16 |                  |      |                 |           |          |           | #15:<br>commercial<br>#16:<br>parking_lot<br>#17:<br>island | #34:<br>airfield<br>#35:<br>water_works<br>#36:<br>marina        |
| the<br>plane's<br>wingspan<br>and<br>tail<br>visible<br>in<br>the<br>image.<br>True<br>Label:<br>#34                                                                                     | CISEN<br>(RN50)                     |      |           |           |           |                  |      |                 |           |          |           | #18:<br>building                                            |                                                                  |

Fig. 15: The T2I retrieval results of top 10 within LuojiaHOG, leveraging the integration of V2TMap and HFE with CLIP (RN50). The results with red box are incorrect, and with yellow box are inaccurate. At the bottom are some labels of third-level. **Query Top 10 Retrieval Result**

<span id="page-19-1"></span>

| The<br>image<br>depicts<br>a<br>residential<br>area<br>with<br>well-maintained                    |                                      | NO.1 | NO.2 | NO.3     | NO.4     | NO.5 | NO.6     | NO.7 | NO.8 | NO.9      | NO.10     |                       |                           |
|---------------------------------------------------------------------------------------------------|--------------------------------------|------|------|----------|----------|------|----------|------|------|-----------|-----------|-----------------------|---------------------------|
| buildings<br>and<br>green<br>meadows<br>in<br>the<br>center.<br>The<br>buildings<br>are           | (ViT-B)<br>CLIP<br>+                 |      |      |          |          |      |          |      |      |           |           | #0:<br>residential    | #19:<br>industrial        |
| of<br>varying<br>heights<br>and<br>architectural<br>styles,<br>with<br>some                       | zero<br>shot                         |      |      | M#16,#22 |          |      | M#16     |      |      |           |           | #1:<br>wetland<br>M#0 | #20:<br>pitch             |
| having<br>balconies<br>and<br>others                                                              | CLIP<br>(ViT-B)                      |      |      |          |          |      |          |      |      |           |           | #2:<br>nature_reserve | #21:<br>playground        |
| without.<br>The<br>green<br>meadows<br>are<br>well-manicured<br>and                               | +<br>V2TMap                          |      |      |          |          |      |          |      |      |           |           | #3:<br>river          | #22:<br>natural_meadow    |
| appear<br>to<br>be<br>used<br>for                                                                 |                                      |      |      | M#16     | M#16     | M#16 |          | M#16 |      |           |           | #4:<br>stream         | #23:<br>theme_park        |
| recreational<br>purposes.<br>There                                                                | CLIP<br>(ViT-B)                      |      |      |          |          |      |          |      |      |           |           | #5:<br>forest         | #24:<br>train_station     |
| are<br>many<br>cars<br>parked<br>in<br>the<br>parking<br>lot<br>in<br>front<br>of<br>the          | +<br>HFE                             |      |      |          |          |      |          |      |      |           |           | #6:<br>airport        | #25:<br>resort            |
| buildings,<br>indicating<br>a<br>high                                                             |                                      |      | M#16 |          | M#16,#22 |      | M#16,#22 | M#0  |      |           |           | #7:<br>orchard        | #26:<br>riverbank         |
| level<br>of<br>vehicular<br>activity<br>in<br>the<br>area.                                        | CISEN<br>(ViT-B)                     |      |      |          |          |      |          |      |      |           |           | #8:<br>park           | #27:<br>school            |
| True<br>Label:<br>#0;<br>#16;<br>#22                                                              |                                      |      |      |          | M#22     |      |          |      |      |           |           | #9:<br>farmyard       | #28:<br>reservoir         |
| The<br>image<br>depicts<br>a<br>large<br>grey                                                     |                                      |      |      |          |          |      |          |      |      | M#34;W#16 | M#34;W#16 | #10:<br>farmland      | #29:<br>greenhouse        |
| airport<br>with<br>a<br>rectangular<br>shape,<br>surrounded<br>by<br>a                            | CLIP<br>(ViT-B)<br>+<br>zero<br>shot |      |      |          |          |      |          |      |      |           |           | #11:<br>scrub         | #30:<br>urban_residential |
| perimeter<br>fence.<br>The<br>runway                                                              |                                      |      |      |          |          |      |          |      |      |           |           | #12:<br>water         | #31:<br>canal             |
| is<br>oriented<br>in<br>a<br>north-south<br>direction<br>and<br>is<br>bordered<br>by              | CLIP<br>(ViT-B)<br>+                 |      |      |          |          |      |          |      |      |           |           | #13:<br>store         | #32:<br>greengrocer       |
| taxiways<br>leading<br>to<br>various                                                              | V2TMap                               |      |      |          |          |      |          |      |      |           |           | #14:<br>retail        | #33:<br>golf_course       |
| gates.<br>The<br>airport<br>is<br>situated<br>on<br>a<br>flat<br>terrain<br>with<br>no<br>visible |                                      |      |      |          |          |      |          |      |      | M#34;W#6  |           | #15:<br>commercial    | #34:<br>airfield          |
| vegetation<br>in<br>the<br>immediate                                                              | CLIP<br>(ViT-B)<br>+                 |      |      |          |          |      |          |      |      |           |           | #16:<br>parking_lot   | #35:<br>water_works       |
| surroundings.<br>A<br>single<br>airplane<br>is<br>visible<br>on<br>the<br>runway,<br>with         | HFE                                  |      |      |          |          |      |          |      |      |           |           | #17:<br>island        | #36:<br>marina            |
| the<br>plane's<br>wingspan<br>and<br>tail                                                         |                                      |      |      |          |          |      |          |      |      |           | M#34;W#19 | #18:<br>building      |                           |
| visible<br>in<br>the<br>image.                                                                    | CISEN<br>(ViT-B)                     |      |      |          |          |      |          |      |      |           |           |                       |                           |
| True<br>Label:<br>#34                                                                             |                                      |      |      |          |          |      |          |      |      |           |           |                       |                           |

Fig. 16: The T2I retrieval results of top 10 within LuojiaHOG, leveraging the integration of V2TMap and HFE with CLIP (ViT). The results with red box are incorrect, and with yellow box are inaccurate. At the bottom are some labels of thirdlevel.

<span id="page-19-2"></span>

| Query                                                                                                                                   | Top<br>10<br>Retrieval<br>Result |      |          |          |      |           |           |           |      |      |           |                                         |                                                 |
|-----------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|------|----------|----------|------|-----------|-----------|-----------|------|------|-----------|-----------------------------------------|-------------------------------------------------|
| The<br>image<br>depicts<br>a<br>residential                                                                                             |                                  | NO.1 | NO.2     | NO.3     | NO.4 | NO.5      | NO.6      | NO.7      | NO.8 | NO.9 | NO.10     |                                         |                                                 |
| area<br>with<br>well-maintained<br>buildings<br>and<br>green<br>meadows<br>in<br>the<br>center.<br>The<br>buildings<br>are              | GeoRSCLIP<br>+<br>zero<br>shot   |      |          |          |      |           |           |           |      |      |           | #0:<br>residential                      | #19:<br>industrial                              |
| of<br>varying<br>heights<br>and<br>architectural<br>styles,<br>with<br>some<br>having<br>balconies<br>and<br>others                     | GeoRSCLIP                        |      |          | M#16,#22 |      | M#22      | M#16      | M#16      |      | M#22 | M#22      | #1:<br>wetland<br>#2:<br>nature_reserve | #20:<br>pitch<br>#21:<br>playground             |
| without.<br>The<br>green<br>meadows<br>are<br>well-manicured<br>and                                                                     | +<br>V2TMap                      |      |          |          |      |           |           |           |      |      |           | #3:<br>river                            | #22:<br>natural_meadow                          |
| appear<br>to<br>be<br>used<br>for<br>recreational<br>purposes.<br>There<br>are<br>many<br>cars<br>parked<br>in<br>the                   | GeoRSCLIP<br>+                   |      | M#16,#22 | M#16     |      | M#16      |           |           |      |      |           | #4:<br>stream<br>#5:<br>forest          | #23:<br>theme_park<br>#24:<br>train_station     |
| parking<br>lot<br>in<br>front<br>of<br>the<br>buildings,<br>indicating<br>a<br>high                                                     | HFE                              |      |          | M#16     |      |           | M#16      |           | M#16 |      |           | #6:<br>airport<br>#7:<br>orchard        | #25:<br>resort<br>#26:<br>riverbank             |
| level<br>of<br>vehicular<br>activity<br>in<br>the<br>area.                                                                              | CISEN<br>(RS)                    |      |          |          |      |           |           |           |      |      |           | #8:<br>park                             | #27:<br>school                                  |
| True<br>Label:<br>#0;<br>#16;<br>#22                                                                                                    |                                  |      |          | M#16     |      |           |           |           |      | M#16 |           | #9:<br>farmyard                         | #28:<br>reservoir                               |
| The<br>image<br>depicts<br>a<br>large<br>grey<br>airport<br>with<br>a<br>rectangular<br>shape,<br>surrounded<br>by<br>a                 | GeoRSCLIP<br>+<br>zero<br>shot   |      |          |          |      |           |           |           |      |      |           | #10:<br>farmland<br>#11:<br>scrub       | #29:<br>greenhouse<br>#30:<br>urban_residential |
| perimeter<br>fence.<br>The<br>runway<br>is<br>oriented<br>in<br>a<br>north-south<br>direction<br>and<br>is<br>bordered<br>by            | GeoRSCLIP<br>+                   |      |          |          |      | M#34;W#16 | M#34;W#16 | M#34;W#16 |      |      | M#34;W#22 | #12:<br>water<br>#13:<br>store          | #31:<br>canal<br>#32:<br>greengrocer            |
| taxiways<br>leading<br>to<br>various<br>gates.<br>The<br>airport<br>is<br>situated                                                      | V2TMap                           |      |          |          |      |           |           | M#34;W#16 |      |      |           | #14:<br>retail<br>#15:<br>commercial    | #33:<br>golf_course<br>#34:<br>airfield         |
| on<br>a<br>flat<br>terrain<br>with<br>no<br>visible<br>vegetation<br>in<br>the<br>immediate<br>surroundings.<br>A<br>single<br>airplane | GeoRSCLIP<br>+<br>HFE            |      |          |          |      |           |           |           |      |      |           | #16:<br>parking_lot                     | #35:<br>water_works                             |
| is<br>visible<br>on<br>the<br>runway,<br>with<br>the<br>plane's<br>wingspan<br>and<br>tail                                              |                                  |      |          |          |      |           |           |           |      |      | M#34;W#15 | #17:<br>island<br>#18:<br>building      | #36:<br>marina                                  |
| visible<br>in<br>the<br>image.<br>True<br>Label:<br>#34                                                                                 | CISEN<br>(RS)                    |      |          |          |      |           |           |           |      |      |           |                                         |                                                 |

Fig. 17: The T2I retrieval results of top 10 within LuojiaHOG, leveraging the integration of V2TMap and HFE with GeoRSCLIP (ViT). The results with red box are incorrect, and with yellow box are inaccurate. At the bottom are some labels of third-level.

<span id="page-20-0"></span>![](_page_20_Figure_0.jpeg)

Fig. 18: The I2T retrieval results of top 5 within LuojiaHOG, leveraging the integration of V2TMap and HFE with CLIP (RN50). The results in red are incorrect, and in yellow are inaccurate.

<span id="page-20-1"></span>![](_page_20_Figure_2.jpeg)

Fig. 19: The I2T retrieval results of top 5 within LuojiaHOG, leveraging the integration of V2TMap and HFE with CLIP (ViT). The results in red are incorrect, and in yellow are inaccurate.

<span id="page-21-0"></span>![](_page_21_Picture_0.jpeg)

Fig. 20: The I2T retrieval results of top 5 within LuojiaHOG, leveraging the integration of V2TMap and HFE with GeoRSCLIP (ViT). The results in red are incorrect, and in yellow are inaccurate.