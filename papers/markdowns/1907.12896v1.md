# Safe Augmentation: Learning Task-Specific Transformations from Data

Irynei Baran, Orest Kupyn Ukrainian Catholic University Lviv, Ukraine

{i.baran, kupyn}@ucu.edu.ua

# Abstract

*Data augmentation is widely used as a part of the training process applied to deep learning models, especially in the computer vision domain. Currently, common data augmentation techniques are designed manually. Therefore they require expert knowledge and time. Moreover, augmentations are dataset-specific, and the optimal augmentations set on a specific dataset has limited transferability to others. We present a simple and explainable method called Safe Augmentation that can learn task-specific data augmentation techniques that do not change the data distribution and improve the generalization of the model. We propose to use safe augmentation in two ways: for model fine-tuning and along with other augmentation techniques. Our method is model-agnostic, easy to implement, and achieves better accuracy on CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and Cityscapes datasets comparing to baseline augmentation techniques. The code is available at* [https:](https://github.com/Irynei/SafeAugmentation) [//github.com/Irynei/SafeAugmentation](https://github.com/Irynei/SafeAugmentation)*.*

# 1. Introduction

Deep neural networks achieve human-level or even higher performance in many computer vision tasks, such as image classification, image restoration, image or video segmentation, etc. [\[34\]](#page-7-0). For example, the human top-5 image classification error on the ImageNet dataset is 5%, whereas the current state-of-the-art deep neural networks achieve nearly 3% [\[28\]](#page-7-1).

However, deep learning models require a massive amount of training data to be robust. Data augmentation is one of the approaches that can help to handle this issue by expanding training data using transformations that preserve semantic information and class labels. The choice of the data augmentation techniques to use for the specific dataset and task is not a trivial one. While some augmentations increase the performance and generalization of the model, Arseny Kravchenko ods.ai Minsk, Belarus me@arseny.info

![](_page_0_Picture_9.jpeg)

**Caption:** Figure 1 illustrates the application of various safe augmentations on CIFAR-100 images, showcasing four distinct sets of transformations. Each transformation is applied with a probability of 0.5, and crops are sized at 25x25 pixels. This highlights the method's flexibility in enhancing model generalization without altering data distribution.

Figure 1: Example of using random subsets s ⊂ S of safe augmentations on images from CIFAR-100 dataset. Each transformation is applied with the probability p = 0.5. Each crops is of size 25x25 pixels.

Set 1: HorizontalFlip, RandomContrast, RandomSizedCrop. Set 2: RandomCrop, RandomContrast, RandomRotate90.

Set 3: RandomSizedCrop, RandomContrast, RandomCrop.

Set 4: RandomContrast, RandomBrightness, RandomGamma.

others can have a negative impact. For instance, a horizontal flip is proven to be useful augmentation for ImageNet-like datasets, but not for the MNIST dataset [\[20\]](#page-7-2), as it changes the distribution of the data because horizontally flipped digits are often no longer valid digits.

Automated machine learning, in particular, automatic data augmentation is currently am important research topic. AutoAugment uses reinforcement learning to search for optimal augmentation policies along with the magnitudes and probabilities [\[6\]](#page-6-0). While these methods provide superior image classification results, they often have limited explainability and require lots of computational resources. Yet the majority of common data augmentation techniques are designed either empirically or by leveraging expert knowledge. Hence, it decreases the transferability of such methods between different tasks.

This paper aims to make another push on making the process of choosing data augmentations automatic and explainable, namely to learn from data which augmentation techniques lead to model generalization improvement. Our contributions are summarized below.

- We introduce a simple, intuitive model-agnostic method for choosing augmentations that can be safely used during model training. In our implementation, we take a fixed set of the common image processing functions with default magnitudes and select a subset of augmentations which produce images that comes from the same distribution that existing ones while improving the accuracy of the main task(e.g., image classification, image segmentation).
- We propose two ways of using learned augmentations: for model fine-tuning and along with other augmentation techniques. Our experiments on different datasets and tasks show that Safe Augmentation works better than baseline augmentations and comparable with more advanced augmentation techniques while being intuitive, explainable, and straightforward.

# 2. Related Work

### 2.1. Data Augmentation

The recent paper by Hernndez-Garca and Knig has shown that data augmentation alone can achieve the same or even higher performance than explicit regularization techniques (weight decay [\[19\]](#page-7-3), dropout [\[29\]](#page-7-4), etc.), without wasting model capacity [\[12\]](#page-6-1).

Traditional augmentation. The most common type of data augmentations are geometric transformations, such as flipping, cropping, rotating, scaling, etc., and color transformations, such as adjusting color, brightness, resolution, etc. They are often called generic or traditional augmentations. They all fall under the category of data warping and are usually performed in the data space, e.g., Wong *et al*. have shown that it is more efficient to perform data augmentation in data space than in feature space as long as label preserving transforms are known [\[32\]](#page-7-5). This type of transformations is easy to use and efficient to implement. One main disadvantage is that you need to have expert knowledge in the image domain to choose transformations that will not affect the correctness of the image labels. Traditional augmentations are broadly used and have shown excellent results in reducing overfitting and improving model performance [\[26\]](#page-7-6) [\[30\]](#page-7-7).

Generative Adversarial Networks (GANs). In 2014 Goodfellow *et al*. proposed a new class of neural networks that can generate realistic data from scratch using generator and discriminator networks that are trained in the minimax two-player game framework [\[10\]](#page-6-2). GANs can be used as a form of unsupervised data augmentation by generating new data from the source distribution. They have also been used for style transfer, e.g., transferring images from one weather condition to another. These generated images can be used to help the model to work in different conditions, for instance, to train autonomous cars to drive in night or snow, having collected data from sunny weather only. GANs are also shown to be successfully used for data augmentation in the medical imaging domain by synthetically augment mammogram and MRI images [\[3,](#page-6-3) [33\]](#page-7-8).

### 2.2. Automated Data Augmentation

Despite described advantages, common data augmentation methods are usually dataset-specific and designed manually, which require prior expert knowledge and time. Recently, many researches were focused on the automation of the process of data augmentation. We divide them into the following two groups.

Generate augmented data directly. Smart Augmentation proposed by Lemley, Bazrafkan, and Corcoran can automatically generate augmented data by merging two or more samples from the same class, in a way that reduces the loss of the original model [\[21\]](#page-7-9). DeVries and Taylor proposed a domain-independent data augmentation technique by using simple transformations in the learned feature space. They train a sequence autoencoder to construct a learned feature space in which they extrapolate between samples [\[8\]](#page-6-4). Tran *et al*. introduced a novel Bayesian method for generating additional data based on the distribution learned from the training set [\[31\]](#page-7-10). Generative adversarial networks have been extensively used for producing augmented data. For example, Antoniou, Storkey, and Edwards presented DAGAN - An image conditional GAN-based model that learns from one data item how to generate other realistic within-class data items. DAGAN can be applied to unseen classes of data and can also enhance few-shot learning systems. [\[2\]](#page-6-5). Another approach called DADA: Deep Adversarial Data Augmentation was proposed by Zhang *et al*. to train deep learning models in extremely low data regimes. They show that that DADA outperforms both traditional data augmentation and a few GAN-based options [\[35\]](#page-7-11).

Generate data transformations. Ratner *et al*. learned generative sequence model over user-defined transformations using GAN-like framework. Their idea is to compose and parameterize a set of user-specified transformation

<span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)

**Caption:** Figure 2 presents the evaluation metrics for augmentation classification accuracy and false positives on the Tiny ImageNet dataset. The red line indicates the baseline classification accuracy without augmentations. The results demonstrate the effectiveness of safe augmentations in improving model performance while maintaining data integrity.

Figure 2: Image classification vs Augmentation false positives (step 2) and Augmentation classification (step 3). Red line denotes image classification accuracy without augmentations.

functions in ways that are diverse but still preserve class labels. Their approach allows leveraging domain knowledge flexibly and straightforwardly. [\[27\]](#page-7-12).

Cubuk *et al*. proposed a new procedure called AutoAugment[\[6\]](#page-6-0) to learn augmentation policies that lead to the highest accuracy of the image classification model on a given dataset. They created a search space of data augmentation policies and used a search algorithm based on reinforcement learning to find the optimal one. The results are great: they achieved state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Moreover, It is shown that policies learned from one dataset can be transferred to other similar datasets. One of the drawbacks of AutoAugment is the computational complexity and long training time due to the extensive search space of possible policies [\[6\]](#page-6-0). Many interesting works have been published trying to address this issue, including Fast AutoAugment [\[22\]](#page-7-13) and PBA (Population Based Augmentation) [\[13\]](#page-6-6), which achieved similar results to AutoAugment but using much more efficient algorithms.

# 3. Proposed Method

We present an intuitive approach for learning data transformations that can be safely used during the model training. Our learned set of augmentations is called safe augmentations, which can be used either for model fine-tuning or along with other augmentation techniques.

Our method does not require substantial computational resources and can be easily performed along with the main task.

### 3.1. Learning Safe Augmentations

We propose to learn safe augmentations from data using an arbitrary convolutional neural network (CNN). Consider a dataset D and a set of all available augmentation techniques A. The task is to define which transformations from set A do not change the distribution of the D, i.e., to select S ⊂ A, where S is a set of safe augmentation. Our pipeline can be divided into four main steps.

- Step 1. Train the CNN to solve the following multilabel classification problem. Given a set A, for every batch of images, a random subset a ⊂ A is applied. The subset a is of random size from 0 to the defined maximal size. In our experiments, we used maximum size of 5. Each transformation from subset a is applied with the probability p = 1. The model tries to predict which augmentations were applied. As a loss function, we use Laugm - a multi-label one-versus-all loss based on max-entropy, between input x and target y. Laugm is equivalent to applying sigmoid function along with the binary cross-entropy loss.
- Step 2. After the model is being trained, evaluate it on the unseen test data without any augmentations and collect per-label false positives, i.e., how many times the model predicts that the specific augmentation technique was applied when, in fact, it was not.
- Step 3. Evaluate the model on the unseen test data using the same procedure of applying augmentations as in the training phase. Collect per-label classification accuracy for each transformation technique.

• Step 4. Divide all augmentations into two groups: safe and others. If the model fails to distinguish whether a particular transformation was applied and the transformation is never predicted on the clean set, then this transformation does not change the data distribution and can be safely used during the training of the original task. Thus, we consider augmentation as safe if it has relatively low per-label classification accuracy on the test set with augmentations (step 3) and low false positive rate on the clean test set without augmentations (step 2).

Example. Figure [2](#page-2-0) shows the described above metrics for the Tiny ImageNet dataset along with the image classification accuracy of every single augmentation. Blur is an example of non-safe transformation with the low false positive rate on the clean test set and very high augmentation classification accuracy on the augmented test set, so the model can predict when Blur was applied. On the other hand, HorizontalFlip is an example of safe augmentation with both low false positive rate and augmentation classification accuracy. It is clearly shown than HorizontalFlip significantly increases the image classification accuracy on the Tiny ImageNet dataset, whereas Blur decreases it.

Note that we can only evaluate one transformation at a time, i.e., we cannot take into account the impact of different augmentations on each other. So the combination of safe augmentations is not necessarily safe. For example, given a dataset of 32x32 images and image classification task, our method found that *RandomCrop(25, 25)* and *CenterCrop(25, 25)* are safe augmentations. However, when these two functions are applied together, it is likely that such a combination is no longer safe because the augmented image could be too small.

### 3.2. Joint Learning

To learn augmentations that not only do not change the data distribution but also improve original task accuracy(e.g., image classification, image segmentation), we trained the multi-label classification problem (step 1) in a joint learning setup. To do that, we propose to modify the architecture of the original models in the following way.

For the image classification task, the new loss Ltotal is calculated as sum of the augmentation classification loss Laugm and the image classification loss Lclass.

$$
L_{total} = L_{augm} + L_{class} \tag{1}
$$

where Lclass is the cross-entropy loss.

For the semantic image segmentation task, the new loss Ltotal is calculated in a similar way as a sum of the augmentation classification loss Laugm and the semantic segmentation loss Lsegm.

$$
L_{total} = L_{augm} + L_{segm} \tag{2}
$$

where Lsegm is the cross-entropy loss same as Lclass, but here x is a two-dimensional predicted mask, y is the two-dimensional target mask, and the goal is to label every pixel in x with the correct class.

For each augmented batch of images, we calculate defined above Ltotal loss and then perform gradient updates. Joint learning setup helps find a better set of safe augmentations that can be used to improve the performance of the original task. All the results presented were obtained using this approach.

### 3.3. Using Safe Augmentations

Having learned the set of safe augmentations S for a given dataset and task, we propose to use them in the following two ways:

#### • For model fine-tuning

Step 1. Train the original task using a set of all augmentations A. For every batch of images, a random subset a ⊂ A of fixed size is applied. Each transformation is applied with the fixed probability p = 0.5.

Step 2. Fine-tune the already pre-trained model on all augmentations using a subset of safe augmentations S. For every batch of images, a random subset s ⊂ S of fixed size is applied. The subset size and probability of applying transformations are the same as in the previous step.

We believe that using all augmentations, including those that change the data distribution can force the model to learn more general features. Thus, they can be used for the model pre-training. After that, we need to fine-tune the model using safe augmentations for learning dataset-specific features.

#### • Along with other augmentation techniques

Safe augmentations alone cannot provide enough generalization. In our experiments, we show that using safe augmentations along with other augmentation techniques, e.g., baseline augmentations and Cutout [\[9\]](#page-6-7) leads to better results.

# 4. Experimental Evaluation

### 4.1. Implementation Details

We implemented all of our models using PyTorch [\[1\]](#page-6-8) [\[25\]](#page-7-14). All models were trained on a single GTX 1080 GPU. We perform the phase of learning safe augmentations using the joint learning setup.

Augmentations. We use a main set A of 15 common augmentations (see Table [1\)](#page-4-0) from *albumentations* library that provides fast image transforms based on highlyoptimized OpenCV library [\[4\]](#page-6-9).

<span id="page-4-1"></span>![](_page_4_Figure_1.jpeg)

**Caption:** Figure 3 compares the impact of different augmentation subsets on CIFAR-10 classification accuracy. The results indicate that a subset size of three yields optimal performance across various augmentation strategies, emphasizing the importance of carefully selecting augmentation techniques for enhancing model accuracy.

Figure 3: Evaluation of different augmentation subsets on CIFAR-10

Image classification. As the main model, we use DenseNet-121 [\[15\]](#page-6-10). Both learning safe augmentations and image classification tasks are trained from scratch using stochastic gradient descent (SGD) optimizer using the batch of size 256. The initial learning rate is set to 10<sup>−</sup><sup>1</sup> , momentum to 0.9 and weight decay to 0.0005. All models are trained for 500 epochs with reducing learning rate on the plateau by 0.1 with 10 epochs patience and early stopping with 20 epochs patience.

Image segmentation. As the main model, we use Feature Pyramid Network (FPN) [\[23\]](#page-7-15) with the DenseNet-121 [\[15\]](#page-6-10) backbone. Both learning safe augmentations and image classification tasks are trained from scratch using Adam [\[16\]](#page-7-16) optimizer. The initial learning rate is set to 10<sup>−</sup><sup>4</sup> . All models are trained for 200 epochs with reducing learning rate on plateau by 0.5 with 7 epochs patience and early stopping with 15 epochs patience.

### 4.2. Augmentation Subset Size

We investigate impact of different subset sizes on image classification accuracy (see Figure [3\)](#page-4-1). Empirically proven that subset of size 3 leads to the best image classification accuracy for both all augmentations set A and safe augmentations set S. Hence, in all our experiments we are using a ⊂ A and s ⊂ S of size 3.

### 4.3. Quantitative Evaluation on Image Classification

We evaluate our method on 4 popular image classification datasets, namely CIFAR-10 [\[17\]](#page-7-17), CIFAR-100 [\[18\]](#page-7-18), SVHN [\[24\]](#page-7-19) and Tiny ImageNet. For every dataset, we train the image classification task using different augmentation techniques and models. All augmentations in our image classification experiments are applied with the fixed probability p = 0.5 and with default magnitude.

• CIFAR-10 and CIFAR-100. We used all crops de-

<span id="page-4-0"></span>

|                  | C-10 | C-100 | Tiny INet | SVHN |
|------------------|------|-------|-----------|------|
| HorizontalFlip   | X    | X     | X         |      |
| VerticalFlip     |      |       |           |      |
| RandomRotate90   | X    | X     | X         |      |
| Transpose        |      |       |           |      |
| ToGray           |      |       |           |      |
| ShiftScaleRotate |      |       |           |      |
| RandomCrop       | X    | X     | X         |      |
| CenterCrop       | X    | X     | X         |      |
| RandomSizedCrop  | X    | X     | X         |      |
| RandomContrast   |      | X     |           | X    |
| RandomBrightness | X    | X     | X         | X    |
| RandomGamma      | X    | X     | X         | X    |
| CLAHE            |      |       |           |      |
| Blur             |      |       |           |      |
| GaussNoise       |      |       |           | X    |

Table 1: Safe augmentations found using joint learning setup for CIFAR-10, CIFAR-100, Tiny ImageNet and SVHN datasets.

scribed in the Table [1](#page-4-0) with the size (25x25). As a baseline augmentations, we used horizontal flips with 50 % probability, zero-padding and random crops, which are conventional for these datasets [\[11\]](#page-6-11). The training data is also normalized by the respective dataset statistics.

Our method found almost the same set of safe augmentations for CIFAR-10 and CIFAR-100 datasets, which makes sense because these datasets are very similar (see Table [1\)](#page-4-0). We manually defined another set called Safe v2 by removing *RandomCrop* and *CenterCrop*, since our approach can only evaluate transformations independently.

Fine-tuning using set Safe v2 achieves the best accuracy both on CIFAR-10 and CIFAR-100 comparing

<span id="page-5-0"></span>

|                        | CIFAR-10 |          | SVHN     |          | CIFAR-100 | Tiny ImageNet |
|------------------------|----------|----------|----------|----------|-----------|---------------|
|                        | DenseNet | DenseNet | DenseNet | DenseNet | DenseNet  | DenseNet      |
|                        | 121      | 169      | 121      | 169      | 121       | 121           |
| Without                | 79.39    | 79.52    | 95.87    | 96.19    | 53.55     | 49.60         |
| Baseline               | 87.31    | 87.15    | 95.99    | 95.63    | 61.67     | 49.73         |
| Safe                   | 87.85    | 86.84    | 96.14    | 96.67    | 64.17     | 57.61         |
| All                    | 87.79    | 88.15    | 96.19    | 96.43    | 65.82     | 58.66         |
| Fine-tuned on All      | 88.68    | 88.15    | 96.19    | 96.58    | 65.83     | 58.65         |
| Fine-tuned on Safe     | 88.38    | 88.21    | 96.36    | 97.01    | 65.93     | 58.85         |
| Fine-tuned on Safe v2* | 88.59    | 88.46    | -        | -        | 65.99     | 59.00         |

Table 2: Test top-1 accuracy (%). All results are averaged over 3 runs.

All fine-tuned experiments were performed using models pre-trained on all augmentations.

\* Safe v2 is defined manually by removing RandomCrop and CenterCrop.

to other evaluated augmentation approaches, namely without augmentations, baseline, only safe, only all and different fine-tuned setups (see Table [2\)](#page-5-0).

• SVHN. Same as for CIFAR-10 and CIFAR-100 we used all crops described in the Table [1](#page-4-0) with the size (25x25). As a baseline augmentations, we used zeropadding and random crops. The training data is normalized by the respective dataset statistics.

Our method found four safe augmentations (see Table [1\)](#page-4-0), which all are color-based transformations and can be safely used with digits.

Fine-tuning using Safe set produces the best accuracy both for DenseNet-121 and DenseNet-169 models comparing to other evaluated augmentation approaches (see Table [2\)](#page-5-0).

• Tiny ImageNet. We used all crops described in the Table [1](#page-4-0) with the size 50x50. As a baseline augmentations, we used horizontal flips with p = 0.5 and random distortions of colors, which are a standard data augmentation techniques for the ImageNet dataset [\[7,](#page-6-12) [14\]](#page-6-13).

Our method found the same safe augmentations (see Table [1\)](#page-4-0) as for CIFAR-10. We manually defined another set called Safe v2 in the same way as for CIFAR-10 and CIFAR-100.

Fine-tuning using Safe set again achieves the best accuracy comparing to other evaluated augmentation approaches (see Table [2\)](#page-5-0).

Safe Augmentation vs AutoAugment. We also evaluated our second proposed way of using safe augmentations along with other augmentation techniques. We compare our approach with AutoAugment [\[6\]](#page-6-0) and Cutout [\[9\]](#page-6-7) on CIFAR-10, CIFAR-100 and SVHN datasets. For CIFAR-10 and CIFAR-100 we use cutout of size 16x16 and for SVHN of size 20x20 the same way as AutoAugment does. Safe augmentations are applied along with the baseline and Cutout.

The results on Table [5](#page-6-14) show that using safe augmentations in both fine-tuning way and along with other augmentation techniques lead to better results.

### 4.4. Quantitative Evaluation on Image Segmentation

| Augmentations    |
|------------------|
| HorizontalFlip   |
| RandomBrightness |
| RandomGamma      |
| Transpose        |

Table 3: Safe augmentations found using joint learning setup for the Cityscapes [\[5\]](#page-6-15) dataset.

We also evaluate our method on Cityscapes - a popular image segmentation dataset [\[5\]](#page-6-15). We train both augmentation classification task and image classification task using the batch of size 16. We rescale every image to 256x256 pixels due to the limited training resources. We also change

<span id="page-5-1"></span>

|                    | FPN(DenseNet-121) |
|--------------------|-------------------|
| Without            | 45.34             |
| Baseline           | 51.11             |
| Safe               | 51.58             |
| All                | 59.41             |
| Fine-tuned on All  | 60.37             |
| Fine-tuned on Safe | 62.09             |

Table 4: Validation IoU(%) on the Cityscapes [\[5\]](#page-6-15) dataset on single FPN model with DenseNet-121 backbone .

the size of the crops to 512x512. All augmentations in our experiments with the image classification task are applied

<span id="page-6-14"></span>

|                           | CIFAR-10 |          | SVHN     |          | CIFAR-100 |
|---------------------------|----------|----------|----------|----------|-----------|
|                           | DenseNet | DenseNet | DenseNet | DenseNet | DenseNet  |
|                           | 121      | 169      | 121      | 169      | 121       |
| Without                   | 79.39    | 79.52    | 95.87    | 96.19    | 53.55     |
| Best Policy AA            | 84.60    | 85.51    | 96.67    | 96.65    | 59.34     |
| Baseline                  | 87.31    | 87.15    | 95.99    | 95.63    | 61.67     |
| Baseline + Cutout         | 88.10    | 88.84    | 96.31    | 96.39    | 63.26     |
| Finetuned Safe*           | 88.59    | 88.46    | 96.36    | 97.01    | 65.99     |
| Safe* + Baseline + Cutout | 88.16    | 88.32    | 96.73    | 96.39    | 65.39     |
| AutoAugment               | 90.77    | 90.58    | 96.76    | 96.66    | 68.70     |

Table 5: Test top-1 accuracy (%). Comparison with Cutout [\[9\]](#page-6-7) and AutoAugment [\[6\]](#page-6-0). Safe\* means *Safe* for SVHN and *Safe v2* for CIFAR-10 and CIFAR-100 .

with the fixed probability p = 0.5 and with default magnitude.

• CityScapes. As a baseline augmentations, we used horizontal flips with p = 0.5 and rotation with p = 0.5 with angle chosen randomly from 0 to 20 degrees, Our method found four safe augmentations (see Table [1\)](#page-4-0).

Fine-tuning using Safe set again achieves the best accuracy comparing to other evaluated augmentation approaches (see Table [4\)](#page-5-1).

# 5. Conclusion

This paper introduces Safe Augmentation, a simple yet efficient algorithm for automatic selection of data augmentations with promising results on quantitative benchmarks. In addition to the simplicity, explainability and flexibility Safe Augmentation ¯ also introduce two different ways of using learned augmentations along with other augmentations techniques. We plan to extend Safe Augmentation further for automatic parameter selection, as well as for adversarial robustness.

# 6. Acknowledgement

The authors were supported by the WANNABY, Soft-Serve and Ukrainian Catholic University. We thank ods.ai community for the constructive feedback, advice and ideas.

# References

- <span id="page-6-8"></span>[1] PyTorch. <http://pytorch.org>.
- <span id="page-6-5"></span>[2] A. Antoniou, A. Storkey, and H. Edwards. Data Augmentation Generative Adversarial Networks. *ArXiv e-prints*, Nov. 2017.
- <span id="page-6-3"></span>[3] C. Bowles, L. Chen, R. Guerrero, P. Bentley, R. Gunn, A. Hammers, D. A. Dickie, M. Valdes Hern ´ andez, J. Ward- ´ law, and D. Rueckert. GAN Augmentation: Augmenting Training Data using Generative Adversarial Networks. *arXiv e-prints*, page arXiv:1810.10863, Oct. 2018.
- <span id="page-6-9"></span>[4] A. Buslaev, A. Parinov, E. Khvedchenya, V. I. Iglovikov, and A. A. Kalinin. Albumentations: fast and flexible image augmentations. *ArXiv e-prints*, Sept. 2018.
- <span id="page-6-15"></span>[5] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The Cityscapes Dataset for Semantic Urban Scene Understanding. *arXiv e-prints*, page arXiv:1604.01685, Apr. 2016.
- <span id="page-6-0"></span>[6] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. AutoAugment: Learning Augmentation Policies from Data. *ArXiv e-prints*, May 2018.
- <span id="page-6-12"></span>[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In *CVPR09*, 2009.
- <span id="page-6-4"></span>[8] T. DeVries and G. W. Taylor. Dataset Augmentation in Feature Space. *ArXiv e-prints*, Feb. 2017.
- <span id="page-6-7"></span>[9] T. DeVries and G. W. Taylor. Improved Regularization of Convolutional Neural Networks with Cutout. *arXiv e-prints*, page arXiv:1708.04552, Aug. 2017.
- <span id="page-6-2"></span>[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, *Advances in Neural Information Processing Systems 27*, pages 2672–2680. Curran Associates, Inc., 2014.
- <span id="page-6-11"></span>[11] K. He, X. Zhang, S. Ren, and J. Sun. Identity Mappings in Deep Residual Networks. *arXiv e-prints*, page arXiv:1603.05027, Mar 2016.
- <span id="page-6-1"></span>[12] A. Hernandez-Garc ´ ´ıa and P. Konig. Data augmentation in- ¨ stead of explicit regularization. *ArXiv e-prints*, June 2018.
- <span id="page-6-6"></span>[13] D. Ho, E. Liang, I. Stoica, P. Abbeel, and X. Chen. Population based augmentation: Efficient learning of augmentation policy schedules. *arXiv e-prints*, page arXiv:1905.05393, May 2019.
- <span id="page-6-13"></span>[14] A. G. Howard. Some Improvements on Deep Convolutional Neural Network Based Image Classification. *arXiv e-prints*, page arXiv:1312.5402, Dec 2013.
- <span id="page-6-10"></span>[15] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely Connected Convolutional Networks. *arXiv e-prints*, page arXiv:1608.06993, Aug. 2016.
- <span id="page-7-16"></span>[16] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. *arXiv e-prints*, page arXiv:1412.6980, Dec 2014.
- <span id="page-7-17"></span>[17] A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
- <span id="page-7-18"></span>[18] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-100 (canadian institute for advanced research).
- <span id="page-7-3"></span>[19] A. Krogh and J. A. Hertz. A simple weight decay can improve generalization. In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, *Advances in Neural Information Processing Systems 4*, pages 950–957. Morgan-Kaufmann, 1992.
- <span id="page-7-2"></span>[20] Y. LeCun and C. Cortes. MNIST handwritten digit database. 2010.
- <span id="page-7-9"></span>[21] J. Lemley, S. Bazrafkan, and P. Corcoran. Smart Augmentation - Learning an Optimal Data Augmentation Strategy. *ArXiv e-prints*, Mar. 2017.
- <span id="page-7-13"></span>[22] S. Lim, I. Kim, T. Kim, C. Kim, and S. Kim. Fast autoaugment. *arXiv e-prints*, page arXiv:1905.00397, May 2019.
- <span id="page-7-15"></span>[23] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and ´ S. Belongie. Feature Pyramid Networks for Object Detection. *arXiv e-prints*, page arXiv:1612.03144, Dec. 2016.
- <span id="page-7-19"></span>[24] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In *NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011*, 2011.
- <span id="page-7-14"></span>[25] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in PyTorch. In *NIPS Autodiff Workshop*, 2017.
- <span id="page-7-6"></span>[26] L. Perez and J. Wang. The Effectiveness of Data Augmentation in Image Classification using Deep Learning. *ArXiv e-prints*, page arXiv:1712.04621, Dec. 2017.
- <span id="page-7-12"></span>[27] A. J. Ratner, H. R. Ehrenberg, Z. Hussain, J. Dunnmon, and C. Re. Learning to Compose Domain-Specific Transforma- ´ tions for Data Augmentation. *ArXiv e-prints*, Sept. 2017.
- <span id="page-7-1"></span>[28] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. *International Journal of Computer Vision (IJCV)*, 115(3):211–252, 2015.
- <span id="page-7-4"></span>[29] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. *Journal of Machine Learning Research*, 15:1929–1958, 2014.
- <span id="page-7-7"></span>[30] L. Taylor and G. Nitschke. Improving Deep Learning using Generic Data Augmentation. *arXiv e-prints*, page arXiv:1708.06020, Aug. 2017.
- <span id="page-7-10"></span>[31] T. Tran, T. Pham, G. Carneiro, L. Palmer, and I. Reid. A Bayesian Data Augmentation Approach for Learning Deep Models. *ArXiv e-prints*, Oct. 2017.
- <span id="page-7-5"></span>[32] S. C. Wong, A. Gatt, V. Stamatescu, and M. D. McDonnell. Understanding data augmentation for classification: when to warp? *arXiv e-prints*, page arXiv:1609.08764, Sept. 2016.
- <span id="page-7-8"></span>[33] E. Wu, K. Wu, D. Cox, and W. Lotter. Conditional Infilling GANs for Data Augmentation in Mammogram Classification. *arXiv e-prints*, page arXiv:1807.08093, July 2018.
- <span id="page-7-0"></span>[34] M. Zahangir Alom, T. M. Taha, C. Yakopcic, S. Westberg, P. Sidike, M. Shamima Nasrin, B. C. Van Esesn, A. A. S. Awwal, and V. K. Asari. The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches. *arXiv e-prints*, page arXiv:1803.01164, Mar. 2018.
- <span id="page-7-11"></span>[35] X. Zhang, Z. Wang, D. Liu, and Q. Ling. DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification. *arXiv e-prints*, Aug. 2018.