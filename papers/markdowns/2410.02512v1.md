# SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation

Mucong Ding Bang An Yuancheng Xu Anirudh Satheesh Furong Huang

Department of Computer Science, University of Maryland, College Park, MD, USA

{mcding,bangan,ycxu,anirudhs,furongh}@umd.edu

#### Abstract

Data augmentation, a cornerstone technique in deep learning, is crucial in enhancing model performance, especially with scarce labeled data. While traditional methods, such as handcrafted augmentations, are effective but limited in scope, modern, adaptable techniques often come at the cost of computational complexity and are hard to fit into existing processes. In this work, we unveil an efficient approach that universally enhances existing data augmentation techniques by enabling their adaptation and refinement, thereby providing a significant and comprehensive improvement across all existing methods. We present SAflex (Self-Adaptive Augmentation via Feature Label EXtrapolation), an approach that utilizes an efficient bilevel optimization to learn the sample weights and soft labels of augmented samples. This is applicable to augmentations from any source, seamlessly integrating with existing upstream augmentation pipelines. Remarkably, SAflex effectively reduces the noise and label errors of the upstream augmentation pipeline with a marginal computational cost. As a versatile module, SAflex excels across diverse datasets, including natural, medical images, and tabular data, showcasing its prowess in few-shot learning and out-of-distribution generalization. SAflex seamlessly integrates with common augmentation strategies like RandAug and CutMix, as well as augmentations from large pre-trained generative models like stable diffusion. It is also compatible with contrastive learning frameworks, such as fine-tuning CLIP. Our findings highlight the potential to adapt existing augmentation pipelines for new data types and tasks, signaling a move towards more adaptable and resilient training frameworks.

### 1 Introduction

Data augmentation is a cornerstone in improving machine learning models, especially when labeled data is scarce. It enhances model performance by introducing varied training samples. Though traditional methods like rotation and cropping are widely used, they operate under a one-size-fitsall assumption that often falls short in the complexity of real-world data. The key is not just to augment data, but to do it in a way that does not mislead the learning process.

Recent work emphasizes the benefits of learned data augmentation, where techniques such as AutoAugment [\(Cubuk et al., 2019\)](#page-12-0) and RandAugment [\(Cubuk et al., 2020\)](#page-12-1) adapt to specific datasets and tasks. While promising, this area is still nascent and lacks a comprehensive framework to address diverse tasks and data nuances. Furthermore, selecting meaningful transformations remains a challenge, often relying on heuristics or domain expertise, which is especially problematic

<span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)

**Caption:** Figure 1 illustrates the SAflex framework, which optimizes sample weights and soft labels of augmented data through a bilevel optimization approach. This method enhances model performance by refining augmented samples from existing pipelines, demonstrating its integration as a plug-in within standard training frameworks.

Figure 1: SAflex learns to adjust sample weights and soft labels of augmented samples from an upstream pipeline, aiming to maximize the model's performance on the validation set. While formulated as a bilevel optimization problem, it can be efficiently solved by linear programming with a gradient-matching objective. SAflex is a plug-in to the existing training framework.

in specialized fields. Inappropriate transformations can harm model performance, underscoring the need for systematic selection. Amid the rise of image generation methods, such as diffusion models and other generative AI, an abundance of synthetic data is available but requires discerning use. A recent study, LP-A3 [\(Yang et al., 2022a\)](#page-16-0), aims to generate "hard positive examples" for augmentation but risks introducing false positives that could mislead learning. Another recent work, Soft-Augmentation [\(Liu et al., 2023\)](#page-14-0), introduces soft learning targets and loss reweighting to train on augmented samples but is primarily limited to improving image crop augmentation. The overarching need is for smarter, more adaptable data augmentation algorithms.

This paper proposes SAflex (Self-Adaptive Augmentation via Feature Label Extrapolation), which automatically learns the sample weights and soft labels of augmented samples provided by any given upstream augmentation pipeline. Existing learnable augmentation methods that directly learn in the feature space (e.g., image space) often restrict augmentation scope due to differentiability needs and suffer from complicated training in high-dimensional spaces. Contrary to this, we advocate for learning only low-dimensional sample weights and soft labels for each augmented instance sourced from a pre-existing upstream augmentation pipeline like synthetic data generation. While upstream augmentation methods can sometimes alter labels or introduce noise, especially when creating samples outside the data distribution, our approach offers a mechanism to correct them. By calibrating sample weights and labels after augmentation, we considerably alleviate issues stemming from upstream augmentation methods. Without the complexity of learning augmentation transformations from scratch, this strategy ensures that augmentation is both diverse and consistent with the inherent data distribution, thereby fostering better generalization across various tasks. See Fig. [1](#page-1-0) for a demonstration of our proposed SAflex.

We frame learning sample weights and soft labels as a bilevel optimization problem. This captures the interdependent nature of the model and its augmented data: the model's performance depends on the quality of the augmented data, which in turn is guided by the model itself [\(Bard,](#page-12-2) [2013\)](#page-12-2). This new perspective advances our understanding of data augmentation, offering a theoretical framework that underpins its practical applications. Despite the bilevel nature of the problem, direct solutions are computationally infeasible for large-scale real-world applications. To mitigate this, we propose a streamlined, greedy, online, single-level approximation algorithm, which optimizes a gradient-matching objective to accelerate the learning process.

We conducted extensive empirical evaluations to highlight SAflex's superior performance and adaptability. On eight medical images [\(Yang et al., 2023\)](#page-16-1), SAflex elevates popular augmentation techniques like RandAugment [\(Cubuk et al., 2020\)](#page-12-1) and Mixup [\(Zhang et al., 2018\)](#page-16-2), boosting performance by up to 3.6%. On seven tabular datasets, SAflex shows compatibility with categorical data and effectively enhances CutMix [\(Yun et al., 2019\)](#page-16-3). Furthermore, SAflex improves image augmentations from diffusion models, yielding an average improvement of 1.9% in fine-grained classification and out-of-distribution generalization against three diffusion-augmentation methods, harnessing on their pre-trained expertise. We also validate SAflex's integration with contrastive learning through a CLIP fine-tuning experiment. These findings underline its versatility across varied data types and learning tasks.

Our contributions are threefold:

(1) We unveil a novel parametrization for learnable augmentation complemented by an adept bilevel algorithm primed for online optimization.

(2) Our SAflex method is distinguished by its universal compatibility, allowing it to be effortlessly incorporated into a plethora of supervised learning processes and to collaborate seamlessly with an extensive array of upstream augmentation procedures.

(3) The potency of our approach is corroborated by empirical tests on a diverse spectrum of datasets and tasks, all underscoring SAflex's efficiency and versatility, boosting performance by1.2% on average over all experiments.

### <span id="page-2-0"></span>2 Proposed Method: SAflex

Our goal is to refine augmented samples from any upstream pipeline to enhance classifier generalization. The proposed methodology is founded on two pivotal questions: (1) Which aspects of the augmented samples should be refined? (2) What approach should be taken to learn these refined samples? We start from these questions and defer the derivation of the algorithm to Section [3.](#page-4-0)

Limitations of Augmentation Methods. Data augmentation is pivotal in enhancing model generalization. However, its limitations, particularly the unintentional introduction of noise, can sometimes outweigh its benefits. For instance, consider the widespread use of random cropping on natural images. Although largely effective, there are times when this approach inadvertently omits task-relevant information, leading to unintended outcomes like false positives. This inherent noise creates a trade-off: under-augmentation may yield insufficient challenging examples, whereas over-augmentation can flood the dataset with misleading samples. As shown in Fig. [2a,](#page-3-0) reducing the noise in augmentation is the key to resolving the dilemma.

Noise in augmentation primarily arises from two fundamental challenges: (1) the deviation of augmented samples from the original data distribution and (2) the potential mislabeling of augmented samples. We shall envision augmentation as a method to harness prior knowledge in capturing the underlying data distribution. This distribution is encapsulated in the joint distribution, PXY (x, y), where x ∈ X are features and y ∈ {1, . . . , K} represents labels, with K indicating the number of classes. Breaking down this joint distribution: PXY (x, y) = PX(x) · P<sup>Y</sup> <sup>|</sup>X(y|x), we observe that the primary source of noise is associated with the feature distribution PX(x), while the secondary source is tied to the conditional distribution P<sup>Y</sup> <sup>|</sup>X(y|x). Addressing these challenges, our methodology is designed to integrate seamlessly with any upstream augmentation process, amending both types of errors post-augmentation, and considering the initial augmentation process as a separate, unchanged entity.

Feature and Label Extrapolation. A key concern in data augmentation pertains to addressing these two types of errors. Some prior works on learning augmentation (e.g., [\(Yang et al., 2022a\)](#page-16-0)) attempted to reduce noise by fine-tuning augmented features, using them as initializations. Specifically, the aim was to derive a modified feature x ′ that eliminates both error types. Yet, due to the high-dimensionality of feature space X , manipulating x is computationally burdensome.

A more efficient strategy is to handle the errors individually and abstain from modifying x. When encountering erroneous estimation of the feature distribution PX(x), even if augmented samples lie in low-density areas, we can compensate by modulating the sample weights w ∈ [0, 1] in the empirical risk minimization loss. Specifically, rarer augmented features are assigned decreased sample weights. For inaccuracies in estimating the conditional distribution P<sup>Y</sup> <sup>|</sup>X(y|x), it's advantageous to modify the augmented label y directly. We also propose transitioning from a hard class label to a soft one, denoted as y, representing a probability mass across K classes, residing in the K-dimensional simplex y ∈ ∆K. The proposed refinement of augmented samples is depicted in Eq. [\(1\)](#page-3-1). Remarkably, optimizing these sample weights and soft labels effectively mitigates errors resulting from varied augmentation methods across numerous classification challenges.

<span id="page-3-1"></span>
$$
(x, y) \xrightarrow{\text{Upstream Augment}} (x^{\text{aug}}, y^{\text{aug}}) \xrightarrow{\text{SAFLEX}} (w^{\text{aug}}, x^{\text{aug}}, y^{\text{aug}}))
$$
 (1)  
sample weight  $\in [0, 1]$  soft label  $\in \Delta^K$ 

To elucidate, consider a hypothetical example in Fig. [2b.](#page-3-0) Envision a training sample from the green class (represented by a pronounced green dot). Upon applying a noise-prone augmentation, such as Gaussian perturbation in a 2D setting, the augmented sample could either (1) fall into a region with few validation samples regardless of their class, or (2) be overwhelmingly encompassed by validation samples from a different class. In the former case, it is judicious to reduce the sample weights since they might not be pivotal in discerning the conditional distribution. In the latter instance, the label of the augmented sample should be fine-tuned. This can entail a shift to a soft label to rectify or mitigate potential label inconsistencies, informed by patterns in the validation set.

<span id="page-3-0"></span>![](_page_3_Figure_4.jpeg)

**Caption:** Figure 2 presents two scenarios regarding data augmentation: (a) highlights the trade-off between under- and over-augmentation, emphasizing the need to reduce noise for effective learning; (b) shows how adjusting sample weights and soft labels can mitigate errors introduced during augmentation, enhancing model robustness.

Figure 2: (a) Under-augmentation can lead to a scarcity of hard positives, while over-augmentation can introduce an excess of false positives. Reducing the noise in augmentation helps resolve the dilemma. (b) Adjusting sample weights and recalibrating soft labels can address the two types of noises introduced by the augmentation process.

Bilevel Formulation. The remaining question in our design is how to learn the sample weights and soft labels for augmented samples. The overarching goal of augmentation is enhancing model generalization. While the test set remains inaccessible, a prevalent approach is to fine-tune performance using a validation set. This methodology aligns with standard practices in hyperparameter optimization and is evidenced in learnable augmentation methods such as AutoAugment [\(Cubuk](#page-12-0) [et al., 2019\)](#page-12-0) and RandAugment [\(Cubuk et al., 2020\)](#page-12-1). Given a neural network f (·) : X → ∆<sup>K</sup> (where we assume Softmax is already applied and the outputs are L<sup>1</sup> normalized) with parameter θ, let us denote the training set, validation set, and the set of augmented samples as Dtrain, Dval, and Daug, respectively. The ambition is to refine Daug such that a model trained on the amalgamation of Dtrain ∪ Daug optimizes performance on Dval.

<span id="page-4-1"></span>
$$
\min_{\mathcal{D}_{\text{aug}}}, \, \theta \mathcal{L}(\mathcal{D}_{\text{val}}, \theta) \quad \text{s.t.} \quad \theta \in \arg\min_{\theta'} \mathcal{L}(\mathcal{D}_{\text{train}} \cup \mathcal{D}_{\text{aug}}, \theta') \tag{2}
$$

This scenario can be cast as a bilevel optimization problem as in Eq. [\(2\)](#page-4-1), where Daug, the set of augmented samples with parametrized by sample weights and soft labels, and the model parameters θ are learnable. The conventional model training constitutes the inner level, while the quest to identify optimal augmented samples Daug, which minimize the validation loss post-inner level training, establishes the outer problem. Such a paradigm inherently transforms learnable augmentation into bilevel optimization. Intriguingly, much of the existing literature on learnable augmentation eschews this representation. The primary reservations stem from concerns related to efficiency and differentiability. Notably, works such as [\(Mounsaveng et al., 2021,](#page-15-0) [2023\)](#page-15-1) are among the sparse few to apply bilevel optimization for augmentation learning, yet their focus remains constricted to affine transformations. In contrast, our approach sidesteps the modification and modeling of feature augmentation, obviating the challenge of differentiability. The low-dimensional nature of sample weights and soft labels potentially simplifies the learning process. In subsequent sections, we demonstrate that, under benign approximations, we can adeptly navigate the bilevel problem, determining the apt sample weights and soft labels within a singular step for each training iteration.

### <span id="page-4-0"></span>3 Algorithm

We now develop an algorithm for the bilevel problem described in Eq. [\(2\)](#page-4-1).

The Greedy Approach. Bilevel optimization is notoriously challenging, often necessitating nested loops, which introduces significant computational overhead. Upon inspecting Eq. [\(2\)](#page-4-1), it becomes evident that an essential characteristic of the problem — the training dynamics of the model — has been understated. In standard practice, augmented samples are typically generated during model training for each minibatch across all iterations. Therefore, the actual problem deviates from Eq. [\(2\)](#page-4-1) in two ways: (1) different batches of augmentation may influence the learned parameters differently, and the model is not trained on a cumulative set of augmented samples, and conversely, (2) the learned parameters are affected differently by the refined augmented samples across batches, implying that augmentation should be optimized with respect to the corresponding model parameters.

To incorporate model optimization dynamics, we should reformulate the problem on a finer scale: Given the model parameter θt−<sup>1</sup> at an intermediate training step, how can we determine the batch of refined augmented samples, Dbatch aug ? Through a greedy approach, we posit that the granular objective is to minimize the validation loss after a single update, denoted as L(Dval, θt), where θ<sup>t</sup> is the model parameter updated from θt−1.

<span id="page-4-2"></span>
$$
\min_{\mathcal{D}_{\text{aug}}^{\text{batch}}, \theta_t} \mathcal{L}(\mathcal{D}_{\text{val}}, \theta_t) \quad \text{s.t.} \quad \theta_t = \theta_{t-1} - \alpha \cdot \nabla_{\theta} \mathcal{L}(\mathcal{D}_{\text{train}}^{\text{batch}} \cup \mathcal{D}_{\text{aug}}^{\text{batch}}, \theta_{t-1}) \tag{3}
$$

This micro-perspective of Eq. [\(2\)](#page-4-1) is represented in Eq. [\(3\)](#page-4-2), where the batch of augmented samples Dbatch aug = {(w aug 1 , x aug 1 , y aug 1 ), . . . ,(w aug B , x aug B , y aug B )} is parametrized by the set of sample weights (w aug 1 , . . . , w aug B ) and soft labels (y aug 1 , . . . , y aug B ).

As a direct consequence, if the inner loop uses a first-order optimizer like SGD (as assumed), this significantly eases the optimization task. The emergent problem is no longer bilevel. With the analytical solution of the "inner problem" at our disposal, we can integrate the formula for θ<sup>t</sup> into the outer objective, L(Dval, θt), converting it into a single-level problem.

Efficient Solution. We next derive an algorithm for efficiently addressing Eq. [\(3\)](#page-4-2). Crucially, due to the linearity of the loss function L(·, θt−1) with respect to datasets and the inherent linearity of gradient computation, the gradient vector for the combined training and augmentation batch linearly relates to the sample weights and soft labels, assuming the sample-wise loss function, such as the cross-entropy loss, behaves linearly with respect to sample weights and soft labels. Validating this, the cross-entropy loss is indeed linear concerning these variables, a typical characteristic based on their definitions.

By approximating the validation loss L(Dval, θt) up to the first order around parameter θ<sup>t</sup> , we can recast Eq. [\(3\)](#page-4-2) as a linear programming problem. The objective seeks to maximize the inner product of the gradient vectors on the combined train and augmented batch and the validation batch, effectively yielding a gradient-matching loss [\(Zhao et al., 2020\)](#page-16-4). Here, both the objective and normalization constraints linearly correspond to our learnable variables: sample weights and soft labels. The derivation is provided in Appendix [A,](#page-17-0) and we summarize the solution in the subsequent notations and theorem.

Notation 1. Let the Jacobian matrix of logits with respect to the model parameter be ∇θf (x aug) |θ=θt−1∈ R <sup>K</sup>×<sup>m</sup> and the gradient vector on the validation set be ∇θL(Dval, θt−1) ∈ R <sup>m</sup>, where m is the parameter count and K is the class count. The Jacobian-vector product is denoted as Π = ∇θf (x aug) |θ=θt−<sup>1</sup> ∇θL(Dval, θt−1) ∈ R <sup>K</sup>, which can be computed efficiently.

<span id="page-5-0"></span>Theorem 1 (Solution of Eq. [\(3\)](#page-4-2)). The approximated soft label solution is y = OneHot(arg maxk[Π]k), where OneHot(·) denotes one-hot encoding, and the sample weight solution is w = 1 if P<sup>K</sup> <sup>k</sup>=1[Π]<sup>k</sup> ≥ 0; otherwise, w = 0.

Theorem [1](#page-5-0) illustrates that an effective approximation of Eq. [\(3\)](#page-4-2) is computationally efficient. The gradient inner product, Π, a Jacobian-vector product, is readily computed alongside standard back-propagation on the combined training and augmentation batch. While determining the validation set gradient vector mandates an additional back-propagation step, we can approximate the gradient vector for the complete validation set, ∇θL(Dval, θt−1), using a minibatch gradient, ∇θL(Dbatch val , θt−1). Despite necessitating a solution for Eq. [\(3\)](#page-4-2) at every iteration, our efficient SAflex algorithm incurs minimal computational overhead.

A notable takeaway from Theorem [1](#page-5-0) is that while we aim to learn continuous sample weights (in [0, 1]) and soft labels (in ∆K), the derived solutions consistently yield discrete values: either 0 or 1 and one-hot vectors. This consistency does not signify a coarse approximation, especially considering we resolve Eq. [\(3\)](#page-4-2) with a O(α) tolerance, where α is typically small. Nonetheless, this characteristic could potentially impact model generalization in under-parameterized scenarios.

Generalization Aspects. Let's interpret and examine the solution provided by Theorem [1](#page-5-0) from a generalization standpoint, which is our primary objective. The loss function's linearity helps understand Theorem [1.](#page-5-0). Given that Eq. [\(3\)](#page-4-2) is a linear program with straightforward normalization constraints, we effectively form a linear combination of KB gradient vectors (each pertaining to a logit of the augmented sample), with B representing the augmented batch size, to approximate the m-dimensional validation gradient vector. The total constraints sum up to B + 1. If these KB gradient vectors are linearly independent, we can always align the combined gradient vector with the validation gradient vector when the degree of freedom, B + KB − (B + 1), is greater than or equal to the gradient vector dimension, m. This is represented by the condition KB > m. Such a scenario, exceedingly under-parametrized, is rare in deep learning. If the combined gradient vector consistently aligns with the validation gradient vector, training with SAflex will approximate training on the combined training and validation sets, potentially limiting the generalization improvements.

To enhance generalization, it is essential to circumvent the challenges of the under-parametrized paradigm, even if we are not closely approaching it. Here, we suggest two modifications to the solution given by Theorem [1:](#page-5-0)

1. Encouraging Retention of the Original Label. We can introduce a minor constant penalty term to the gradient inner product to incentivize retaining the augmented sample's original label. Thus, we substitute Π with Π + βey aug, where ey aug is a one-hot vector with a value of 1 at the y aug-th position. If no other entry in Π exceeds [Π]<sup>y</sup> aug by a margin of at least β, the learned label remains unaltered. This approach proves especially valuable when the validation set is of limited size.

2. Substituting arg max with Gumbel-SoftMax. Our current solution invariably yields hard labels. This can sometimes manifest as an excessive degree of confidence, particularly when Π contains multiple significant entries. To alleviate this, we can employ the Gumbel-SoftMax function to introduce a "softening" effect to the learned labels, adding a measure of stochasticity. Hence, we have y = softmaxΠ + βe<sup>y</sup> aug + g /τ , where g consists of i.i.d. random variables sourced from Gumbel(0, 1). Typically, unless specified otherwise, we opt for a relatively low fixed temperature value, τ = 0.01.

The pseudo-code of SAflex for cross-entropy loss is shown as Algorithm [1.](#page-6-0)

Algorithm 1: SAflex (Cross-Entropy Loss, Single batch).

- <span id="page-6-0"></span>Input: Neural network f (·) : X → ∆<sup>K</sup> (softmax applied on outputs) with parameters θ, upstream augmented batch {(x aug 1 , y aug 1 ), . . . ,(x aug <sup>B</sup> , y aug <sup>B</sup> )}, validation batch Dbatch val = {(x val 1 , yval 1 ), . . . ,(x val <sup>B</sup>′ , yval <sup>B</sup>′ )}, penalty coefficient β, temperature τ .
- <sup>1</sup> Compute the gradient vector for the validation batch ∇θL(Dbatch val , θ).
- <sup>2</sup> for i = 1, . . . , B do // The actual implementation is vectorized.
- <sup>3</sup> Determine the gradient inner product Π<sup>i</sup> = ∇θf (x aug i ) ∇θL(Dbatch val , θ) via Jacobian-vector product.
- <sup>4</sup> Apply Gumbel-SoftMax to get y<sup>i</sup> = softmaxΠ<sup>i</sup> + βe<sup>y</sup> aug i + g /τ , where e<sup>y</sup> aug i ∈ R <sup>K</sup> is one-hot at y aug i , and g consists of i.i.d. random variables taken from Gumbel(0, 1).
- <sup>5</sup> Set w<sup>i</sup> = 1 if Π<sup>i</sup> · y<sup>i</sup> ≥ 0, otherwise set w<sup>i</sup> = 0.
- <sup>6</sup> Renormalize the sample weights w1, . . . , w<sup>B</sup> to sum to 1.
- <sup>7</sup> return Sample weights w aug 1 , . . . , w aug <sup>B</sup> , and soft labels y aug 1 , . . . , y aug <sup>B</sup> .

SAflex for Contrastive Learning. We conclude this section by discussing to encompass the generalization of the proposed method for certain contrastive learning losses, as illustrated in Eq. [\(12\)](#page-18-0) and Eq. [\(13\)](#page-18-1). Notably, the latter is utilized for CLIP training. In the realm of contrastive learning, labels are not conventionally defined. Yet, one can perceive the contrastive training objectives in Eq. [\(12\)](#page-18-0) and Eq. [\(13\)](#page-18-1) as proxy classification tasks. Here, we posit that the batch of size B can be construed as containing B classes: one positive example coupled with B − 1 negative examples. This interpretation paves the way to introduce the notion of (soft) labels over this surrogate classification task with its B distinct classes.

Under this paradigm, the loss function remains linear concerning the soft labels and sample weights, making the methodology in Theorem [1](#page-5-0) applicable. The sole requisite modification pertains to the gradient inner product's definition. Rather than employing gradients from the cross-entropy logits, ∇θf (x aug), we substitute them with gradients corresponding to the contrastive learning logits.

### 4 Related Works

Traditional data augmentation techniques such as random flipping and cropping [\(Krizhevsky et al.,](#page-14-1) [2017;](#page-14-1) [Simard et al., 2003;](#page-15-2) [Shorten & Khoshgoftaar, 2019\)](#page-15-3) are hand-crafted and static, unlike our adaptive SAflex method that tunes sample weights based on validation performance. Autonomous approaches like AutoAugment [\(Cubuk et al., 2019;](#page-12-0) [Lim et al., 2019;](#page-14-2) [Ho et al., 2019;](#page-13-0) [Mounsaveng](#page-15-0) [et al., 2021,](#page-15-0) [2023\)](#page-15-1) learn transformations but are restricted in scope, primarily focusing on affine transformations. Generative methods employing GANs or diffusion models [\(Odena et al., 2017;](#page-15-4) [Sankaranarayanan et al., 2018;](#page-15-5) [Huang et al., 2018;](#page-13-1) [He et al., 2022;](#page-13-2) [Shipard et al., 2023;](#page-15-6) [Dunlap](#page-13-3) [et al., 2023;](#page-13-3) [Trabucco et al., 2023\)](#page-16-5) can inadvertently alter class-relevant features, which our method avoids by adaptively adjusting sample weights. Research on adversarial perturbations [\(Goodfellow](#page-13-4) [et al., 2015;](#page-13-4) [Yang et al., 2022a](#page-16-0)[,b;](#page-16-6) [Ho & Nvasconcelos, 2020\)](#page-13-5) and noise-robust learning [\(Han et al.,](#page-13-6) [2018;](#page-13-6) [Lang et al., 2022;](#page-14-3) [Thulasidasan et al., 2019;](#page-15-7) [Konstantinov & Lampert, 2019;](#page-14-4) [Gao et al., 2022;](#page-13-7) [Ma et al., 2018;](#page-14-5) [Kremer et al., 2018\)](#page-14-6) address similar problems but often suffer from complexity and stability issues, which we mitigate by our principled approach to weight adjustment. Recently, Soft-Augmentation [\(Liu et al., 2023\)](#page-14-0) also proposes to use soft labels and sample weights to train on augmented samples. However, it implements a specific formula to generate them based on the strength parameter of upstream augmentations. This limits the applicability of Soft-Augmentation mostly to crop augmentation on images. [Wang et al.](#page-16-7) [\(2023\)](#page-16-7) introduce self-adaptive augmentation within the meta-learning framework, MetaMix, which improves the corruption robustness of continual learning models. [Bhattarai et al.](#page-12-3) [\(2020\)](#page-12-3) propose a progressive sampling strategy for GAN synthetic data, while [Caramalau et al.](#page-12-4) [\(2021\)](#page-12-4) introduce a sequential graph convolutional network for active learning. Our work extends these findings by developing a novel sampling and purifying method for augmented data that is specifically designed to improve the performance of downstream tasks.

For a more detailed discussion of related works, please refer to Appendix [B.](#page-18-2)

### 5 Experiments

We validate the effectiveness of SAflex under four very different learning scenarios: (1) adapting augmentations to medical images, (2) refining augmentations for tabular data, (3) purifying diffusion-model-generated augments, and (4) applying to contrastive fine-tuning. Experimental setups and implementation details are provided in Appendix [C.](#page-22-0)

Adapting Augmentations to Medical Images. Unlike natural images, medical images often carry quantitative information (e.g., encoded as color) and objects without a canonical orientation. While we usually lack the domain knowledge to design effective heuristic augmentation transformations for these images, applying augmentation pipelines designed for natural images, such as RandAugment [\(Cubuk et al., 2020\)](#page-12-1), can sometimes degrade performance in the medical context [\(Yang et al., 2022a\)](#page-16-0). Consequently, we investigate whether SAflex can adapt these augmentation pipelines for medical images.

We assess multi-class classification across eight medical image datasets from MedMNIST [\(Yang](#page-16-1) [et al., 2023\)](#page-16-1), with each dataset comprising 10K to 236K 28×28 images and 4 to 11 classes. In line with [\(Yang et al., 2021\)](#page-16-8), we train a ResNet-18 model [\(He et al., 2016\)](#page-13-8) using the Adam optimizer [\(Kingma & Ba, 2014\)](#page-14-7) for 100 epochs. For upstream augmentation, we utilize the widelyadopted RandAugment [\(Cubuk et al., 2020\)](#page-12-1) and Mixup [\(Zhang et al., 2018\)](#page-16-2) methods. Test accuracies are presented in Table [1,](#page-8-0) highlighting that SAflex significantly enhances the performance of both RandAugment and Mixup. It's noteworthy that SAflex, when combined with basic upstream augmentations as shown in Table [1,](#page-8-0) achieves better performance than Soft-Augmentation [\(Liu et al.,](#page-14-0) [2023\)](#page-14-0), and comparable or superior performance than the adversarial-perturbation-based augmentation, LP-A3 [\(Yang et al., 2022a\)](#page-16-0). The latter not only takes significantly longer to train but also demands careful hyperparameter tuning. For a comprehensive view, Soft-Augmentation's performance and LP-A3's performance on the MedMNIST datasets can be found in Appendix [C.](#page-22-0)

<span id="page-8-0"></span>

| Method                 | Path   | Derma  | Tissue | Blood  | OCT    | OrganA | OrganC                                                                                                  | OrganS |
|------------------------|--------|--------|--------|--------|--------|--------|---------------------------------------------------------------------------------------------------------|--------|
| No Aug                 | 94.34  | 76.14  | 68.28  | 96.81  | 78.67  | 94.21  | 91.81                                                                                                   | 81.57  |
|                        | ± 0.18 | ± 0.09 | ± 0.17 | ± 0.19 | ± 0.26 | ± 0.09 | ± 0.12                                                                                                  | ± 0.07 |
| RandAug                | 93.52  | 73.71  | 62.03  | 95.00  | 76.00  | 94.18  | 91.38                                                                                                   | 80.52  |
|                        | ± 0.09 | ± 0.33 | ± 0.14 | ± 0.21 | ± 0.24 | ± 0.20 | ± 0.14                                                                                                  | ± 0.32 |
| SAflex<br>(w/ RandAug) |        |        |        |        |        |        | 95.11 ± 0.14 76.69 ± 0.33 64.32 ± 0.18 96.91 ± 0.15 79.63 ± 0.28 95.32 ± 0.29 92.10 ± 0.21 82.85 ± 0.42 |        |
| Mixup                  | 92.98  | 75.22  | 66.62  | 96.28  | 77.93  | 94.12  | 90.76                                                                                                   | 80.99  |
|                        | ± 0.19 | ± 0.45 | ± 0.31 | ± 0.23 | ± 0.41 | ± 0.35 | ± 0.28                                                                                                  | ± 0.21 |
| SAflex<br>(w/ Mixup)   |        |        |        |        |        |        | 93.71 ± 0.37 76.94 ± 0.51 68.31 ± 0.43 97.21 ± 0.35 79.54 ± 0.44 95.06 ± 0.31 92.73 ± 0.53 82.14 ± 0.27 |        |

Table 1: On medical images, SAflex significantly enhances the performance of RandAugment and Mixup across eight medical image datasets from MedMNIST.

In terms of efficiency, SAflex, designed as an augmentation plug-in, requires only a single-step update per iteration. It only extends the average wall-clock time of a training epoch by roughly 42% in this experiment; see Appendix [C](#page-22-0) for details.

Refining Augmentations for Tabular Data. Tabular data typically encompasses heterogeneous features that include a blend of continuous, categorical, and ordinal values. The presence of discrete features constrains the space of potential transformations. Furthermore, the domain knowledge to design invariant, label-preserving transformations is often absent. One of the few traditional augmentation techniques directly applicable to tabular data is CutMix [\(Yun et al., 2019\)](#page-16-3), which substitutes a portion of continuous or discrete features with values from other randomly chosen samples (see Appendix [C](#page-22-0) for implementation details). However, studies suggest that CutMix, with a relatively small augmentation probability like 0.1, struggles to bolster tabular classification performance [\(Onishi & Meguro, 2023\)](#page-15-8). Conversely, a higher augmentation probability can introduce excessive noise, potentially downgrading the performance. This leads us to explore whether SAflex can mitigate the noise from CutMix and enhance classification performance.

Our experiments span seven tabular datasets varying in size (from 452 to 494K) and feature types (from exclusively continuous features to predominantly discrete ones); detailed dataset information and statistics are available in Appendix [C.](#page-22-0) Except for the Volkert dataset, which involves 10-way classification, all other datasets focus on binary classification. Notably, some datasets, like Credit, exhibit a significantly skewed class distribution (e.g., only 0.17% positive). We consider backbone models such as the sample Multilayer Perceptron (MLP) with two hidden layers and 256 neurons each and tranformer-based models like SAINT [\(Somepalli et al., 2022\)](#page-15-9) (without contrastive pretraining). These models undergo training with dropout [\(Srivastava et al., 2014\)](#page-15-10) and, in certain cases, batch normalization, for 200 epochs.

Table [2](#page-9-0) shows that SAflex almost consistently enhances the performance of CutMix across all datasets, regardless of whether the MLP or SAINT model is used. This improvement is especially

<span id="page-9-0"></span>

| Method                | Model | Appetency          | Arrhythmia      | Click           | Credit          | QASR            | Shrutime                                                         | Volkert         |
|-----------------------|-------|--------------------|-----------------|-----------------|-----------------|-----------------|------------------------------------------------------------------|-----------------|
| No Aug                | MLP   | 49.03<br>± 0.01    | 81.53<br>± 0.03 | 52.54<br>± 0.04 | 66.91<br>± 0.03 | 91.84<br>± 0.02 | 86.27<br>± 0.04                                                  | 61.14<br>± 0.05 |
| CutMix                | MLP   | 48.98<br>± 0.03    | 81.57<br>± 0.05 | 52.59<br>± 0.09 | 73.68<br>± 0.08 | 91.87<br>± 0.02 | 86.39<br>± 0.05                                                  | 61.20<br>± 0.02 |
| SAflex<br>(w/ CutMix) | MLP   | 51.04 ± 0.09       | 83.02 ± 0.06    |                 |                 |                 | 52.81 ± 0.06 74.61 ± 0.15 92.69 ± 0.13 86.90 ± 0.10 61.51 ± 0.05 |                 |
| No Aug                | SAINT | 78.90<br>± 0.03    | 83.90<br>± 0.01 | 65.72<br>± 0.06 | 79.49<br>± 0.05 | 98.18<br>± 0.04 | 87.53<br>± 0.04                                                  | 66.82<br>± 0.05 |
| CutMix                | SAINT | 81.05<br>± 0.07    | 85.32<br>± 0.09 | 65.77<br>± 0.04 | 79.71<br>± 0.08 | 98.61<br>± 0.06 | 87.61<br>± 0.07                                                  | 68.23<br>± 0.10 |
| SAflex<br>(w/ CutMix) |       | SAINT 81.33 ± 0.14 | 85.27 ± 0.14    |                 |                 |                 | 66.12 ± 0.09 79.93 ± 0.17 98.59 ± 0.21 87.93 ± 0.13 68.91 ± 0.17 |                 |

Table 2: On tabular data, SAflex outperforms the upstream augmentation method, CutMix, across diverse tabular datasets using either MLP or SAINT as the backbone models.

noticeable with the MLP backbone, which is typically more intricate to train, and on datasets abundant in discrete features, such as Click and Shrutime, where CutMix tends to inject more noise. Notably, the Volkert dataset demonstrates a considerable performance impovement, potentially attributed to the fact that it has 10 classes where soft labels might be more useful.

Purifying Diffusion-Model-Generated Augments. Recent research [\(Dunlap et al., 2023;](#page-13-3) [Trabucco et al., 2023\)](#page-16-5) has advocated the application of diffusion models for image editing via text prompts. Compared to traditional augmentation techniques, images produced by pretrained diffusion models maintain task-specific details while offering enhanced domain diversity, as dictated by the prompts. Diffusion-model-generated augmentations have been found particularly efficacious in fine-grained classification and out-of-distribution (OOD) generalization tasks [\(Dunlap et al.,](#page-13-3) [2023\)](#page-13-3). However, these diffusion models occasionally generate subtle image alterations, potentially corrupting class-essential information, thus underscoring the necessity for noise reduction [\(Dunlap](#page-13-3) [et al., 2023\)](#page-13-3). In this context, we probe the capability of SAflex to enhance the purity of diffusionmodel-generated augmentations, aiming for improved classification outcomes.

In our experimentation, we adhere to the setups in [\(Dunlap et al., 2023\)](#page-13-3). We assess SAflex using diffusion-model-generated images derived from two distinct approaches: (1) The Img2Img approach involves an image encoder that first converts a given image into a latent representation. Subsequently, employing a diffusion model (specifically, Stable Diffusion v1.5 [\(Rombach et al.,](#page-15-11) [2022\)](#page-15-11) for this experiment), this latent representation undergoes a series of prompt-conditioned transformations. Ultimately, the altered representation is decoded, yielding an augmented image reflecting the modifications stipulated in the prompt. Notably, the diffusion model may or may not undergo fine-tuning (w/ and w/o finetune) on the dataset in question. (2) The InstructPix2Pix approach [\(Brooks et al., 2023\)](#page-12-5) accepts an image and an edit instruction sampled (e.g., "position the animals within the forest") and outputs a correspondingly modified image. InstructPix2Pix is a conditional diffusion model pretrained on a dataset containing paired images and their associated edit instructions.

Our evaluation encompasses two tasks: (1) Fine-grained classification on a CUB dataset subset [\(Wah et al., 2011\)](#page-16-9) (featuring 25 images per category). (2) OOD generalization on an iWildCam subset from the Wilds dataset [\(Koh et al., 2021\)](#page-14-8) (consisting of over 6,000 images and simplified to 7-way classification). We use a ResNet-50 model [\(He et al., 2016\)](#page-13-8) pretrained on ImageNet [\(Deng](#page-12-6) [et al., 2009\)](#page-12-6). For comparison, we also consider data generated solely from text (Text2Img) and the RandAugment method as baselines.

Results, as depicted in Table [3,](#page-10-0) affirm that SAflex consistently elevates the performance of

<span id="page-10-0"></span>

| Task           | No<br>Aug          | RandAug            | Text2Img           | InstructPix2Pix    |                    | Img2Img<br>(w/o<br>finetune) |                    | Img2Img<br>(w/<br>finetune) |                    |
|----------------|--------------------|--------------------|--------------------|--------------------|--------------------|------------------------------|--------------------|-----------------------------|--------------------|
|                | —                  | —                  | —                  | SAflex<br>w/o      | SAflex<br>w/       | SAflex<br>w/o                | SAflex<br>w/       | SAflex<br>w/o               | SAflex<br>w/       |
| Fine-Grained   | ±<br>68.60<br>0.16 | ±<br>71.26<br>0.52 | ±<br>69.68<br>0.97 | ±<br>71.38<br>0.91 | ±<br>72.34<br>0.59 | ±<br>71.25<br>0.86           | ±<br>73.22<br>0.63 | ±<br>72.01<br>1.24          | ±<br>73.61<br>0.78 |
| Classification |                    |                    |                    |                    |                    |                              |                    |                             |                    |
| OOD            |                    |                    |                    |                    |                    |                              |                    |                             |                    |
| Generalization | 57.19<br>±<br>1.13 | 61.34<br>±<br>2.72 | 64.53<br>±<br>3.01 | 67.29<br>±<br>1.96 | 69.92<br>±<br>0.88 | 70.65<br>±<br>1.50           | 72.61<br>±<br>1.44 | 70.49<br>±<br>1.21          | 72.83<br>±<br>0.92 |

Table 3: For diffusion-model-generated augmentations, SAflex enhances the fine-grained classification and OOD generalization performance for various diffusion-generation methods.

all three diffusion-model-generated augmentation techniques, across both fine-grained classification and OOD generalization tasks. Notably, the performance boost is more prominent within the OOD generalization task, where feature and label distortions are particularly detrimental. We confirm that SAflex is useful to refine diffusion-model-generated augmentations, leading to enhanced classification accuracy.

Applying to Contrastive Fine-Tuning. We next shift our focus from the empirical risk minimization (ERM) framework utilizing cross-entropy loss, as demonstrated in the prior scenarios. To test the adaptability and compatibility of SAflex with contrastive loss, we turn to a contrastive fine-tuning paradigm termed "Finetune Like You Pretrain" (FLYP)[\(Goyal et al., 2023\)](#page-13-9). This methodology offers a straightforward yet potent means to fine-tune pretrained image-text models, including notable ones like CLIP [\(Radford et al., 2021\)](#page-15-12). Remarkably, by simply fine-tuning classifiers through the initial pretraining contrastive loss (refer to Eq. [\(13\)](#page-18-1)), FLYP achieves uniformly better classification performance. This entails constructing prompts from class labels and subsequently minimizing the contrastive loss between these prompts and the image embeddings within the fine-tuning set.

Our experimentation adopts the framework presented in [\(Goyal et al., 2023\)](#page-13-9). Specifically, we fine-tune a CLIP model equipped with a ViT-B/16 encoder on the full iWildCam dataset from Wilds [\(Koh et al., 2021\)](#page-14-8). Post fine-tuning, we adopt a strategy from [\(Goyal et al., 2023\)](#page-13-9) that linearly interpolates weights between the pretrained and the fine-tuned checkpoints to optimize in-distribution (ID) performance. As our upstream augmentation technique, we employ RandAugment, following hyperparameter setups as described in [\(Koh et al., 2021\)](#page-14-8). For handling the CLIP contrastive loss, we apply our tailored algorithm, detailed in Section [3.](#page-4-0) For an in-depth understanding, please refer to Appendix [A](#page-17-0) and Appendix [C.](#page-22-0)

<span id="page-10-1"></span>

| Task |                   | Zero-Shot        | LP-FT            | FLYP             | FLYP+RandAug     | FLYP+SAflex<br>(w/<br>RandAug) |
|------|-------------------|------------------|------------------|------------------|------------------|--------------------------------|
| ID   | w/o<br>Ensembling | 8.7<br>±<br>0.0  | 49.7<br>±<br>0.5 | 52.2<br>±<br>0.6 | 52.4<br>±<br>0.8 | 52.7<br>±<br>0.7               |
| OOD  |                   | 11.0<br>±<br>0.0 | 34.7<br>±<br>0.4 | 35.6<br>±<br>1.2 | 36.3<br>±<br>1.4 | 36.9<br>±<br>1.5               |
| ID   | w/<br>Ensembling  | ±<br>8.7<br>0.0  | ±<br>50.2<br>0.5 | ±<br>52.5<br>0.6 | ±<br>52.6<br>1.0 | ±<br>53.0<br>0.7               |
| OOD  |                   | 11.0<br>±<br>0.0 | 35.7<br>±<br>0.4 | 37.1<br>±<br>1.2 | 37.6<br>±<br>0.9 | 37.8<br>±<br>1.1               |

| Table 4: Applied to | contrastive fine-tuning of CLIP using FLYP                                      | SAflex<br>(Goyal et al., 2023), |
|---------------------|---------------------------------------------------------------------------------|---------------------------------|
|                     | also enhances the performance of standard image augmentations like RandAugment. |                                 |

As evidenced in Table [4,](#page-10-1) incorporating RandAugment alongside FLYP yields favorable outcomes. Moreover, the introduction of SAflex amplifies performance gains for both ID and OOD tasks, irrespective of whether ensembling is applied. This observation is particularly noteworthy, as it demonstrates that SAflex is compatible with contrastive loss, which is a key component of many training paradigms, including self-supervised learning.

## 6 Conclusions

Our study presents SAflex, a novel solution to current challenges in data augmentation. At its core, SAflex offers a paradigm shift from traditional, one-size-fits-all augmentation strategies to a more adaptive, data-driven approach. It allows for the learning of low-dimensional sample weights and soft labels for each augmented instance, thereby circumventing the complexities and limitations inherent in direct augmentation feature learning. Our method demonstrates universal compatibility, underscoring its vast potential for diverse data types in learning scenarios. Extensive empirical evaluations confirm SAflex's prowess, proving its adaptability from medical imaging contexts to the nuances of tabular and natural image datasets. While SAflex demonstrates promising results, there are certain factors to consider for optimal performance. A substantial and high-quality validation set is beneficial. A suboptimal set could limit its effectiveness. Additionally, the type of upstream augmentation methods selected plays a role, as it impacts the overall performance of SAflex. Our approach also entails some computational overhead due to frequent gradient evaluations. These considerations will be the focus of future studies to further refine the methodology. In essence, SAflex stands as a testament to the advancements in learnable data augmentation, ushering in a more adaptive and customized era of data-centric AI.

#### Acknowledgments

Ding, An, Xu, Satheesh, and Huang are supported by National Science Foundation NSF-IIS-2147276 FAI, DOD-ONR-Office of Naval Research under award number N00014-22-1-2335, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness against Deception (GARD) HR00112020007, Adobe, Capital One and JP Morgan faculty fellowships.

### References

- <span id="page-12-9"></span>Sercan O Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In ¨ Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 6679–6687, 2021.
- <span id="page-12-7"></span>Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J Fleet. Synthetic data from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023.
- <span id="page-12-2"></span>Jonathan F Bard. Practical bilevel optimization: algorithms and applications, volume 30. Springer Science & Business Media, 2013.
- <span id="page-12-3"></span>Binod Bhattarai, Seungryul Baek, Rumeysa Bodur, and Tae-Kyun Kim. Sampling strategies for gan synthetic data. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2303–2307. IEEE, 2020.
- <span id="page-12-5"></span>Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18392–18402, 2023.
- <span id="page-12-4"></span>Razvan Caramalau, Binod Bhattarai, and Tae-Kyun Kim. Sequential graph convolutional network for active learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9583–9592, 2021.
- <span id="page-12-8"></span>Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785–794, 2016.
- <span id="page-12-0"></span>Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 113–123, 2019.
- <span id="page-12-1"></span>Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 702–703, 2020.
- <span id="page-12-6"></span>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.
- <span id="page-13-11"></span>Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor Darrell, Joseph E Gonzalez, Aditi Raghunathan, and Anna Rohrbach. Using language to extend to unseen domains. In The Eleventh International Conference on Learning Representations, 2022.
- <span id="page-13-3"></span>Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E Gonzalez, and Trevor Darrell. Diversify your vision datasets with automatic diffusion-based augmentation. arXiv preprint arXiv:2305.16289, 2023.
- <span id="page-13-7"></span>Jiahui Gao, Renjie Pi, LIN Yong, Hang Xu, Jiacheng Ye, Zhiyong Wu, Weizhong Zhang, Xiaodan Liang, Zhenguo Li, and Lingpeng Kong. Self-guided noise-free data generation for efficient zeroshot learning. In The Eleventh International Conference on Learning Representations, 2022.
- <span id="page-13-4"></span>Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.
- <span id="page-13-9"></span>Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19338–19347, 2023.
- <span id="page-13-12"></span>Beliz Gunel, Jingfei Du, Alexis Conneau, and Veselin Stoyanov. Supervised contrastive learning for pre-trained language model fine-tuning. In International Conference on Learning Representations, 2020.
- <span id="page-13-6"></span>Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. Advances in neural information processing systems, 31, 2018.
- <span id="page-13-8"></span>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
- <span id="page-13-2"></span>Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and XI-AOJUAN QI. Is synthetic data from generative models ready for image recognition? In The Eleventh International Conference on Learning Representations, 2022.
- <span id="page-13-5"></span>Chih-Hui Ho and Nuno Nvasconcelos. Contrastive learning with adversarial examples. Advances in Neural Information Processing Systems, 33:17081–17093, 2020.
- <span id="page-13-0"></span>Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efficient learning of augmentation policy schedules. In International Conference on Machine Learning, pp. 2731–2741. PMLR, 2019.
- <span id="page-13-1"></span>Sheng-Wei Huang, Che-Tsung Lin, Shu-Ping Chen, Yen-Yi Wu, Po-Hao Hsu, and Shang-Hong Lai. Auggan: Cross domain adaptation with gan-based data augmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 718–731, 2018.
- <span id="page-13-10"></span>Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry. Distilling model failures as directions in latent space. In The Eleventh International Conference on Learning Representations, 2022.
- <span id="page-14-12"></span>Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:18661–18673, 2020.
- <span id="page-14-7"></span>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
- <span id="page-14-8"></span>Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637–5664. PMLR, 2021.
- <span id="page-14-4"></span>Nikola Konstantinov and Christoph Lampert. Robust learning from untrusted sources. In International conference on machine learning, pp. 3488–3498. PMLR, 2019.
- <span id="page-14-6"></span>Jan Kremer, Fei Sha, and Christian Igel. Robust active label correction. In International conference on artificial intelligence and statistics, pp. 308–316. PMLR, 2018.
- <span id="page-14-1"></span>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84–90, 2017.
- <span id="page-14-11"></span>Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Finetuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2021.
- <span id="page-14-3"></span>Hunter Lang, Aravindan Vijayaraghavan, and David Sontag. Training subset selection for weak supervision. In Advances in Neural Information Processing Systems, 2022.
- <span id="page-14-2"></span>Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Advances in Neural Information Processing Systems, 32, 2019.
- <span id="page-14-13"></span>Chi-Heng Lin, Chiraag Kaushik, Eva L Dyer, and Vidya Muthukumar. The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective. arXiv preprint arXiv:2210.05021, 2022.
- <span id="page-14-10"></span>Bo Liu, Mao Ye, Stephen Wright, Peter Stone, and Qiang Liu. Bome! bilevel optimization made easy: A simple first-order approach. Advances in Neural Information Processing Systems, 35: 17248–17262, 2022.
- <span id="page-14-0"></span>Yang Liu, Shen Yan, Laura Leal-Taix´e, James Hays, and Deva Ramanan. Soft augmentation for image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16241–16250, 2023.
- <span id="page-14-9"></span>Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In International conference on artificial intelligence and statistics, pp. 1540–1552. PMLR, 2020.
- <span id="page-14-5"></span>Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah Erfani, Shutao Xia, Sudanthi Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In International Conference on Machine Learning, pp. 3355–3364. PMLR, 2018.
- <span id="page-15-0"></span>Saypraseuth Mounsaveng, Issam Laradji, Ismail Ben Ayed, David Vazquez, and Marco Pedersoli. Learning data augmentation with online bilevel optimization for image classification. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1691–1700, 2021.
- <span id="page-15-1"></span>Saypraseuth Mounsaveng, Issam Laradji, David V´azquez, Marco Perdersoli, and Ismail Ben Ayed. Automatic data augmentation learning using bilevel optimization for histopathological images. arXiv preprint arXiv:2307.11808, 2023.
- <span id="page-15-4"></span>Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. In International conference on machine learning, pp. 2642–2651. PMLR, 2017.
- <span id="page-15-8"></span>Soma Onishi and Shoya Meguro. Rethinking data augmentation for tabular data in deep learning. arXiv preprint arXiv:2305.10308, 2023.
- <span id="page-15-12"></span>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763. PMLR, 2021.
- <span id="page-15-11"></span>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684–10695, 2022.
- <span id="page-15-5"></span>Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8503–8512, 2018.
- <span id="page-15-6"></span>Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, and Clinton Fookes. Diversity is definitely needed: Improving model-agnostic zero-shot classification via stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 769–778, 2023.
- <span id="page-15-3"></span>Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of big data, 6(1):1–48, 2019.
- <span id="page-15-2"></span>Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural networks applied to visual document analysis. In Icdar, volume 3. Edinburgh, 2003.
- <span id="page-15-9"></span>Gowthami Somepalli, Avi Schwarzschild, Micah Goldblum, C Bayan Bruss, and Tom Goldstein. Saint: Improved neural networks for tabular data via row attention and contrastive pre-training. In NeurIPS 2022 First Table Representation Workshop, 2022.
- <span id="page-15-10"></span>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.
- <span id="page-15-7"></span>Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and Jamal Mohd-Yusof. Combating label noise in deep learning using abstention. In International Conference on Machine Learning, pp. 6234–6243. PMLR, 2019.
- <span id="page-16-5"></span>Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective data augmentation with diffusion models. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.
- <span id="page-16-9"></span>Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltechucsd birds-200-2011 dataset. 2011.
- <span id="page-16-7"></span>Zhenyi Wang, Li Shen, Donglin Zhan, Qiuling Suo, Yanjun Zhu, Tiehang Duan, and Mingchen Gao. Metamix: Towards corruption-robust continual learning with temporally self-adaptive data transformation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 24521–24531, 2023.
- <span id="page-16-10"></span>Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7959–7971, 2022.
- <span id="page-16-12"></span>Sen Wu, Hongyang Zhang, Gregory Valiant, and Christopher R´e. On the generalization effects of linear transformations in data augmentation. In International Conference on Machine Learning, pp. 10410–10420. PMLR, 2020.
- <span id="page-16-8"></span>Jiancheng Yang, Rui Shi, and Bingbing Ni. Medmnist classification decathlon: A lightweight automl benchmark for medical image analysis. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pp. 191–195. IEEE, 2021.
- <span id="page-16-1"></span>Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. Scientific Data, 10(1):41, 2023.
- <span id="page-16-0"></span>Kaiwen Yang, Yanchao Sun, Jiahao Su, Fengxiang He, Xinmei Tian, Furong Huang, Tianyi Zhou, and Dacheng Tao. Adversarial auto-augment with label preservation: A representation learning principle guided approach. Advances in Neural Information Processing Systems, 35:22035–22048, 2022a.
- <span id="page-16-6"></span>Kaiwen Yang, Tianyi Zhou, Xinmei Tian, and Dacheng Tao. Identity-disentangled adversarial augmentation for self-supervised learning. In International Conference on Machine Learning, pp. 25364–25381. PMLR, 2022b.
- <span id="page-16-3"></span>Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6023–6032, 2019.
- <span id="page-16-2"></span>Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018.
- <span id="page-16-11"></span>Yifan Zhang, Bryan Hooi, Dapeng Hu, Jian Liang, and Jiashi Feng. Unleashing the power of contrastive self-supervised visual models via contrast-regularized fine-tuning. Advances in Neural Information Processing Systems, 34:29848–29860, 2021.
- <span id="page-16-4"></span>Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. In International Conference on Learning Representations, 2020.

### <span id="page-17-0"></span>A Method and Algorithm Details

In this appendix, we provide more details about the proposed method and algorithm. We first show the derivation details behind results in Theorem [1.](#page-5-0) Then, we provide more details about the SAflex algorithm on contrastive losses discussed at the end of Section [3.](#page-4-0)

Proof of Theorem [1:](#page-5-0) As outlined in Section [3,](#page-4-0) we start from approximating the validation loss up to first order around the current parameter θt−1. By the first-order approximation, we shall rewrite the optimization problem in Eq. [\(3\)](#page-4-2) as follows:

<span id="page-17-1"></span>
$$
\max_{\substack{(w_1,\ldots,w_B),\ (y_1,\ldots,y_B);\\\sum_{i=1}^B w_i=1,\ y_i\in\Delta^K,\forall i\in[B]}}\left\langle\nabla_{\theta}\mathcal{L}(\mathcal{D}_{val},\theta_{t-1}),\nabla_{\theta}\mathcal{L}(\mathcal{D}_{train}^{batch}\cup\{(w_i^{aug},x_i^{aug},\mathbf{y}_i^{aug})\}_{i=1}^B,\theta_{t-1})\right\rangle\tag{4}
$$

where we also explicitly write out the learnable parts in the augmented batch.

Clearly, the set of constraints, P<sup>B</sup> <sup>i</sup>=1 w<sup>i</sup> = 1 and P <sup>k</sup>=1[Ky<sup>i</sup> ]<sup>k</sup> = 1 for ∀i ∈ [B], are linear. To show that the objective function is also linear, we consider the form of cross-entropy loss:

$$
\mathcal{L}_{\text{CE}}(\mathcal{D}, \theta) = -\sum_{i=1}^{B} \log \frac{\exp [f(x_i)]_{y_i}}{\sum_{k=1}^{K} \exp [f(x_i)]_k}
$$
(5)

Since f (·) : X → ∆<sup>K</sup> is assumed to have the Softmax function applied on the outputs (see Section [2\)](#page-2-0), we have P<sup>K</sup> <sup>k</sup>=1 exp [f (xi)]<sup>k</sup> = 1, and the cross-entropy loss can be rewritten as:

$$
\mathcal{L}_{CE}(\mathcal{D}, \theta) = -\sum_{i=1}^{B} \log[f(x_i)]_{y_i}
$$
\n(6)

When sample weights and soft labels are introduced, the cross-entropy loss becomes:

$$
\mathcal{L}_{\text{CE}}(\mathcal{D}, \theta) = -\sum_{i=1}^{B} w_i \cdot \sum_{k=1}^{K} [\mathbf{y}_i]_k \log[f(x_i)]_k \tag{7}
$$

From the above equation, we can see that the objective function L Dbatch train ∪{(w aug i , x aug i , y aug i )} B <sup>i</sup>=1, θt−<sup>1</sup> in Eq. [\(4\)](#page-17-1) is indeed linear with respect to sample weigths (w1, . . . , wB) and soft labels (y1, . . . , yB).

Given these, we conclude, the resulted optimizaiton task, Eq. [\(4\)](#page-17-1), is a linear programming problem, which can be solved efficiently. Moreover, the set of linear constraints are independent, which means the solution for sample weight w and soft labels y for an augmented sample x aug ∈ Dbatch aug are independent of other augmented samples and the training sample batch Dbatch train . For an arbitrary augmented sample x aug ∈ Dbatch aug , replacing the gradient vector on the entire batch of training and augmented samples with the gradient vector on this single augmented sample,

$$
\mathcal{L}_{CE}(\{(w^{\text{aug}}, x^{\text{aug}}), \mathbf{y}^{\text{aug}})\}, \theta) = w^{\text{aug}} \cdot \sum_{k=1}^{K} [\mathbf{y}^{\text{aug}}]_k \log[f(x^{\text{aug}})]_k
$$
(8)

it is not hard to see that if the gradient inner product is denoted by

$$
\Pi = \nabla_{\theta} f \left( x^{\text{aug}} \right) \mid_{\theta = \theta_{t-1}} \nabla_{\theta} \mathcal{L}(\mathcal{D}_{\text{val}}, \theta_{t-1}) \tag{9}
$$

the optimal solution for w aug and y aug are:

$$
\mathbf{y} = \text{OneHot}\left(\arg\max_{k}[\Pi]_k\right) \tag{10}
$$

where OneHot(·) denotes one-hot encoding, and,

$$
w = 1 \text{ if } \sum_{k=1}^{K} [\Pi]_k \ge 0, \text{ otherwise } w = 0 \tag{11}
$$

□

For the adaptation of SAflex to contrastive losses, in Section [3,](#page-4-0) we have already shown the main idea. Here we take a closer look at some typical contrastive losses like,

<span id="page-18-0"></span>
$$
\mathcal{L}_{\text{contrast}}(\mathcal{D}, \theta) = -\sum_{i=1}^{B} \log \frac{\exp \left(\bar{\phi}\left(x_i\right) \cdot \bar{\phi}\left(x_i^+\right) / \tau\right)}{\exp \left(\bar{\phi}\left(x_i\right) \cdot \bar{\phi}\left(x_i^+\right) / \tau\right) + \sum_{j=1}^{B-1} \exp \left(\bar{\phi}\left(x_i\right) \cdot \bar{\phi}\left(x_j^-\right) / \tau\right)}
$$
(12)

where x + i is the positive example of x<sup>i</sup> , and x − j is the j-th negative example of x<sup>i</sup> . And ϕ¯ (·) is the L<sup>2</sup> normalized encoder, τ is the temperature. And the contrastive pre-training (which is also use for finetuning in [\(Goyal et al., 2023\)](#page-13-9)),

<span id="page-18-1"></span>
$$
\mathcal{L}_{CLIP}(\mathcal{D}, \theta) := \sum_{i=1}^{B} -\log \frac{\exp \left(\bar{g} \left(I_i\right) \cdot \bar{h} \left(T_i\right)\right)}{\sum_{j=1}^{B} \exp \left(\bar{g} \left(I_i\right) \cdot \bar{h} \left(T_j\right)\right)} + \sum_{i=1}^{B} -\log \frac{\exp \left(\bar{g} \left(I_i\right) \cdot \bar{h} \left(T_i\right)\right)}{\sum_{j=1}^{B} \exp \left(\bar{g} \left(I_j\right) \cdot \bar{h} \left(T_i\right)\right)},\tag{13}
$$

where I<sup>i</sup> is the image, and T<sup>i</sup> is the text for the i-th sample. ¯g (·) and h¯ (·) are the L<sup>2</sup> normalized image and text encoders, respectively.

We confirm that one can perceive the contrastive training objectives in Eq. [\(12\)](#page-18-0) and Eq. [\(13\)](#page-18-1) as proxy classification tasks. Here, we posit that the batch of size B can be construed as containing B classes: one positive example coupled with B − 1 negative examples. This interpretation paves the way to introduce the notion of (soft) labels over this surrogate classification task with its B distinct classes.

Taking the CLIP loss as an example, we shall generalize the first term in Eq. [\(13\)](#page-18-1) to the following:

$$
\sum_{i=1}^{B} -w_i \cdot \sum_{j=1}^{B} [\mathbf{y}_i]_j \log \frac{\exp \left(\bar{g} \left(I_i\right) \cdot \bar{h} \left(T_j\right)\right)}{\sum_{k=1}^{B} \exp \left(\bar{g} \left(I_i\right) \cdot \bar{h} \left(T_j\right)\right)}
$$
(14)

where there are B proxy-classes.

Under this paradigm, the loss function remains linear concerning the soft labels and sample weights, making the methodology in Theorem [1](#page-5-0) applicable.

### <span id="page-18-2"></span>B Related Work

In this section, we compare our approach with established augmentation methods, including traditional heuristical transformations, autonomous data augmentation, and methods leveraging large pretrained models or adversarial perturbation. We then discuss our methodology's connections to noise-robust learning and hyperparameter optimization. For a detailed background on our experimental tasks and other connected areas.

Traditional data augmentation operations are usually crafted and chosen heuristically based on domain expertise [\(Krizhevsky et al., 2017;](#page-14-1) [Simard et al., 2003\)](#page-15-2). For natural images, common transformations include random flipping, cropping, and color shifting [\(Shorten & Khoshgoftaar,](#page-15-3) [2019\)](#page-15-3). Mixup-based [\(Zhang et al., 2018\)](#page-16-2) augmentations like cutmix [\(Yun et al., 2019\)](#page-16-3) enhance data diversity by merging patches from two images, which is also widely adopted for tabular datasets. Although we, like mixup, introduce soft labels, ours are not the outcome of merging two data instances. Nevertheless, traditional methods enjoy no guarantee of effectiveness or universality, limiting their applicability across varied data types and tasks.

Autonomous data augmentation has a rich history, while classical works generally bifurcate into AutoAugment-based and GAN-based approaches. AutoAugment [\(Cubuk et al., 2019\)](#page-12-0) learns sequences of transformations to optimize classifier performance on a validation set. Subsequent works [\(Lim et al., 2019;](#page-14-2) [Ho et al., 2019\)](#page-13-0) have proposed alternative learning algorithms. Among them, [\(Mounsaveng et al., 2021,](#page-15-0) [2023\)](#page-15-1) propose to learn the augmentation transformation using bilevel optimization at the cost that only differentiable affine transformations can be considered. Subsequently, RandAugment [\(Cubuk et al., 2020\)](#page-12-1) demonstrates equivalent performance to AutoAugment by employing random transformation selection. However, such approaches still rely on a priori knowledge of beneficial transformations. On the other hand, GAN-generated images conditioned on their class can be used as augmented samples [\(Odena et al., 2017;](#page-15-4) [Sankaranarayanan](#page-15-5) [et al., 2018;](#page-15-5) [Huang et al., 2018\)](#page-13-1). However, the inherent assumption of GANs, that augmented data should mimic the original distribution, often restricts potential enhancements [\(Shorten &](#page-15-3) [Khoshgoftaar, 2019\)](#page-15-3).

Pretrained large generative models, like diffusion models, offer the capability to synthesize training data in zero or few shot scenarios [\(He et al., 2022;](#page-13-2) [Shipard et al., 2023\)](#page-15-6) as well as generate hard training examples [\(Jain et al., 2022\)](#page-13-10). Nonetheless, models exclusively trained on diffusionproduced data often underperform compared to their counterparts trained on real datasets [\(Azizi](#page-12-7) [et al., 2023\)](#page-12-7). To address this, recent studies [\(Dunlap et al., 2023;](#page-13-3) [Trabucco et al., 2023\)](#page-16-5) proposed the use of diffusion models for image editing with text prompts, yielding augmentations closer to original training data without necessitating finetuning. In contrast to conventional GAN-based methods, diffusion-based augmentations leverage knowledge from large pretrained datasets. However, they can sometimes produce subtle image edits and corrupt class-relevant information, highlighting the importance of noise reduction techniques like filtering [\(Dunlap et al., 2023\)](#page-13-3). While such filtering relies on heuristic metrics, it can be viewed as a specific case of learning sample weights in our work. Another line of research models augmentation as adversarial perturbations [\(Goodfellow et al.,](#page-13-4) [2015\)](#page-13-4), aiming to generate more challenging positive and negative samples [\(Yang et al., 2022a,](#page-16-0)[b;](#page-16-6) [Ho & Nvasconcelos, 2020\)](#page-13-5). However, these models usually suffer from inherent complexity and instability issues.

Noise robust learning bears relevance to our approach since we treat upstream augmented samples as noisy data. Learning sample weights and soft labels parallel noise reduction strategies such as dataset resampling [\(Han et al., 2018;](#page-13-6) [Lang et al., 2022\)](#page-14-3), loss reweighting [\(Thulasidasan](#page-15-7) [et al., 2019;](#page-15-7) [Konstantinov & Lampert, 2019;](#page-14-4) [Gao et al., 2022\)](#page-13-7), and label correction [\(Ma et al.,](#page-14-5) [2018;](#page-14-5) [Kremer et al., 2018\)](#page-14-6). Our method is efficient yet principled as we formulate to optimize the model performance on the validation set, similar to standard hyperparameter search paradigms. Our algorithm bears relevance to continuous hyperparameter optimization [Lorraine et al.](#page-14-9) [\(2020\)](#page-14-9) in its use of bilevel optimization algorithms [\(Liu et al., 2022\)](#page-14-10), but we introduce a novel bilevel approach. Data augmentation is greedily learned in our formulation, in sync with the ongoing training dynamics.

Medical image classification MedMNIST [\(Yang et al., 2023,](#page-16-1) [2021\)](#page-16-8) is a comprehensive dataset of biomedical images, offering both 2D and 3D standardized images pre-processed to small sizes with classification labels. ResNets [\(He et al., 2016\)](#page-13-8) popular models for medical image classification. [\(Yang et al., 2022a\)](#page-16-0) and [\(Mounsaveng et al., 2023\)](#page-15-1) are augmentation methods that have been shown to improve performance on medical image classification tasks. [\(Yang et al., 2022a\)](#page-16-0) introduces a novel, prior-free autonomous data augmentation approach that leverages representation learning to create hard positive examples as augmentations, enhancing performance in various machine learning tasks without the need for a separate generative model. [\(Mounsaveng et al., 2023\)](#page-15-1) proposes an automatic data augmentation learning method for histopathological images, wherein the augmentation parameters are determined as learnable using a bilevel optimization approach, proving more efficient and effective than predefined transformations. However, [\(Mounsaveng et al.,](#page-15-1) [2023\)](#page-15-1) is not evaluated on MedMNIST and the adaptation is non-trivial.

Tabular data classification Classical models, such as XGBoost [\(Chen & Guestrin, 2016\)](#page-12-8), have been the cornerstone for tabular data processing, providing interpretability and handling diverse feature types effectively, including those with missing values. Multilayer perceptrons (MLPs) have also been a staple in the domain, offering flexibility in modeling non-linear relationships in tabular datasets. TabNet [\(Arik & Pfister, 2021\)](#page-12-9), a more recent innovation, employs neural networks to emulate decision trees, focusing selectively on specific features at every layer. Lastly, SAINT [\(Somepalli](#page-15-9) [et al., 2022\)](#page-15-9) presents a hybrid deep learning solution tailored for tabular data. It employs attention mechanisms over both rows and columns and introduces an improved embedding technique. On tabular data, cutmix [\(Yun et al., 2019\)](#page-16-3) is widely adopted and considered as a standard augmentation method.

Diffusion-model-based image augmentations Recent studies have shed light on the prowess of diffusion models in image augmentations. [\(Dunlap et al., 2022\)](#page-13-11) introduces ALIA, a technique integrating both vision and language models. Using natural language descriptions of a dataset's classes or domains, ALIA edit the image using image-to-image diffusion models [\(Brooks et al.,](#page-12-5) [2023\)](#page-12-5), ensuring the augmented data is not only visually consistent with the original but also encompasses a broader range of diversity, particularly beneficial for fine-grained classification tasks. [\(Trabucco et al., 2023\)](#page-16-5) propose to change the inherent semantics of images, generalizing to novel visual concepts from a few labeled examples, making it especially valuable for tasks demanding semantic diversification.

Robust finetuning of vision models, particularly the cutting-edge variants, has witnessed significant progress in recent times. Notably, image-text pre-trained models like CLIP [\(Radford et al.,](#page-15-12) [2021\)](#page-15-12) have heralded unprecedented levels of robustness, as demonstrated in CLIP and subsequent studies [\(Wortsman et al., 2022;](#page-16-10) [Kumar et al., 2021\)](#page-14-11). While standard fine-tuning methodologies possess substantial potential, there's evidence to suggest that they might diminish robustness, especially in zero-shot paradigms. The culmination of methodologies from [\(Kumar et al., 2021\)](#page-14-11) (LP-FT) and [\(Wortsman et al., 2022\)](#page-16-10) (weight ensembling) represents a notable benchmark in the literature. Meanwhile, the approach by [\(Goyal et al., 2023\)](#page-13-9) introduces a nuanced strategy to the fine-tuning landscape. Harnessing a simple yet effective technique that mimics contrastive pretraining, it casts downstream class labels as text prompts and then optimizes the contrastive loss between image embeddings and these prompt embeddings, terming it "contrastive finetuning". This method has achieved remarkable results, outstripping benchmarks in multiple areas such as distribution shifts, transfer learning, and few-shot learning. Especially on the WILDS-iWILDCam, the FLYP approach championed by [\(Goyal et al., 2023\)](#page-13-9) has set new performance standards, surpassing both traditional finetuning and existing state-of-the-art approaches. The research solidifies contrastive finetuning as a premier, intuitive strategy for the supervised finetuning of image-text models like CLIP.

Supervised learning via contrastive loss has taken center stage in recent research undertakings. The methodology advocates for the fine-tuning of zero-shot models in a fashion similar to their pretraining phase by capitalizing on contrastive loss. Various studies, such as [\(Khosla et al., 2020\)](#page-14-12) have investigated this concept in a fully supervised setting without the support of a pre-trained model. In contrast, [\(Gunel et al., 2020\)](#page-13-12) ventured into the realm of fine-tuning vast language models, while [\(Zhang et al., 2021\)](#page-16-11) concentrated on vision-only models. A salient distinction in the approach becomes evident when considering the addition of loss functions: while certain works have paired contrastive loss with cross-entropy, it has been observed that integrating cross-entropy with FLYP loss might negatively impact results. Direct comparisons between the two loss functions have showcased the superior accuracy credentials of contrastive loss over cross-entropy.

Generalization aspects and theoretical understanding of data augmentation is a less explored area. Data augmentation plays a pivotal role in boosting performance, especially in tasks such as image and text classification. [\(Wu et al., 2020\)](#page-16-12) delves into the reasons behind the efficacy of various augmentations, specifically linear transformations, within the context of overparametrized linear regression. The study reveals that certain transformations can either enhance estimation by expanding the span of the training data or act as regularization agents. Based on these insights, the authors present an augmentation strategy that tailors transformations to the model's uncertainty about the transformed data, validating its potency across image and text datasets. On the other hand, [\(Lin et al., 2022\)](#page-14-13) offers a fresh perspective on data augmentation (DA), challenging traditional beliefs. While classic augmentations, like translations in computer vision, are thought to create new data from the same distribution, this fails to explain the success of newer techniques that dramatically shift this distribution. The study introduces a theoretical framework that posits that DA imposes implicit spectral regularization, achieved through manipulating the eigenvalues of the data covariance matrix and boosting its entire spectrum via ridge regression. This framework provides a profound understanding of DA's varying impacts on generalization, serving as a foundational platform for innovative augmentation design.

Other augmentation methods that use soft labels and sample weights. There is a recent paper, Soft-Augmentation [\(Liu et al., 2023\)](#page-14-0), which also considers soft labels/targets and soft sample weights (i.e., loss reweighting). However, we believe there are huge methodological differences between the two methods in how they model the soft labels and weights. These methodological distinctions lead to significant differences in applicability. Below, we elaborate on the methodological and applicability differences between the two approaches and provide empirical comparisons to further highlight the novelty and improved performance of our method.

Our SAflex employs a learnable, augmentation-method agnostic, and more automatic and principled approach for generating soft labels and sample weights. In Soft-Augmentation, the authors implement a specific approach to generating soft labels, namely through label smoothing. Label smoothing modifies the indicator value "1" (representing the ground-truth class label) with p = 1 − α(ϕ), where the adaptive smoothing factor α(ϕ) is determined by the degree/strength ϕ of the specific sampled augmentation applied to input x<sup>i</sup> . Notably, the remaining probability mass α(ϕ) is uniformly distributed across all other class labels. The formula of α(·) requires human modeling with domain expertise. And since different upstream augmentation methods have different definitions of the strength factor ϕ, remodeling of α(·) for each new augmentation method is required. The discussion in Soft-Augmentation mainly focuses on crop augmentations on images, which impressively draws insights from human visual classification experiments. Our SAflex, in contrast, differs in these key aspects: (a) Flexible Soft Labels: SAflex employs a more flexible approach to modeling soft labels, moving beyond label smoothing's limitations. We believe that uniformly distributing the probability mass across all classes may not always be the most effective strategy. This limitation of Soft-Augmentation is also acknowledged in the paper. (b) Learned Soft Labels and Sample Weights: In SAflex, both soft labels and sample weights are learned from a bilevel optimization problem, which is agnostic to the type and strength of the upstream augmentation method. (c) Bilevel Optimization Problem: SAflex confronts the inherent challenge of soft augmentation by framing it as a bilevel optimization problem. This approach represents the first rigorous formulation of the problem, underscoring an important theoretical contribution. Additionally, we introduce novel and efficient algorithms specifically designed to tackle this bilevel optimization challenge.

Our SAflex approach offers broader applicability compared to the Soft-Augmentation method. Unlike Soft-Augmentation, which requires an explicit augmentation strength parameter ϕ, SAflex seamlessly integrates with any upstream data augmentation mechanism, including diffusion models that lack the strength parameter ϕ. This versatility enables SAflex to effectively handle a wider range of data types, including medical and tabular data. SAflex demonstrates its versatility by effectively handling a variety of tasks, including (standard) classification, fine-grained classification, out-of-distribution (OOD) generalization, and self-supervised learning. This broad applicability is evident in our comprehensive experiments. Conversely, Soft-Augmentation primarily focuses on image classification, with specific emphasis on model occlusion performance and calibration error, thus limiting its applicability to a narrower range of tasks.

### <span id="page-22-0"></span>C Experimental Setups and Implementation Details

In this section, we provide more details about the experimental setups and implementation details.

The experiments are conducted on 4 NVIDIA Tesla V100 GPUs with 32GB memory each. For the hyperparameter setting of SAflex algorithm, we usually set the penalty coefficient β = 0, and only set it β = 1 for experiments on the tabular datasets. We often keep the temperature τ = 0.01, and only set it to be τ = 0.1 on the CLIP finetuning experiment. We do not conduct hyperparameter search for the hyperparameters of SAflex algorithm, and we believe the performance can be further improved by hyperparameter search.

We then describe the infomation of datasets. The information about tabular datasets are listed below.

The specific subsets of iWILDCam and CUB datasets used in diffusion-generated augmentation experiments are adopted form [\(Dunlap et al., 2023\)](#page-13-3).

Next, we show some more experiment results. The performance of Soft-Augmentation [\(Liu](#page-14-0) [et al., 2023\)](#page-14-0) on MedMNIST datasets is listed below. Since the Soft-Augmentation paper focuses on improving crop augmentation and does not provide formulas to generate soft labels and sample weights for the upstream augmentations we considered, we test it with crop augmentation on the MedMNIST medical image datasets. We use the tuned hyperparameters for crop augmentation

| Dataset    | Task               | #<br>Features | #<br>Categorical | #<br>Continuous | Dataset<br>Size | #<br>Positives | #<br>of<br>Neg. | %<br>of<br>Positives |
|------------|--------------------|---------------|------------------|-----------------|-----------------|----------------|-----------------|----------------------|
| Appetency  | Binary             | 39            | 3                | 36              | 494,021         | 97,278         | 396,743         | 19.69                |
| Arrhythmia | Binary             | 226           | 0                | 226             | 452             | 66             | 386             | 14.60                |
| Click      | Binary             | 12            | 7                | 5               | 39,948          | 6,728          | 33,220          | 16.84                |
| Credit     | Binary             | 29            | 0                | 29              | 284,807         | 492            | 284,315         | 0.17                 |
| QSAR       | Binary             | 41            | 0                | 41              | 1,055           | 356            | 699             | 33.74                |
| Shrutime   | Binary             | 11            | 3                | 8               | 10,000          | 2,037          | 7,963           | 20.37                |
| Volkert    | Multiclass<br>(10) | 147           | 0                | 147             | 58,310          | —              | —               | —                    |

Table 5: Statistics of the tabular datasets.

| Dataset    | Download Link                                               |
|------------|-------------------------------------------------------------|
| Appetency  | http://kdd.ics.uci.edu/databases/kddcup99                   |
| Arrhythmia | http://odds.cs.stonybrook.edu/arrhythmia-dataset/           |
| Click      | https://kdd.org/kdd-cup/view/kdd-cup-2012-track-2           |
| Credit     | https://www.kaggle.com/jacklizhi/creditcard                 |
| QSAR       | https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation |
| Shrutime   | https://www.kaggle.com/shrutimechlearn/churn-modelling      |
| Volkert    | http://automl.chalearn.org/data                             |

Table 6: Links of the tabular datasets.

and Soft-Augmentation as described in the paper.

| Method                               | Path               | Derma              | Tissue             | Blood              | OCT                | OrganA             | OrganC             | OrganS             |
|--------------------------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|
| Crop                                 | 92.68<br>±<br>0.82 | 76.61<br>±<br>0.14 | 67.38<br>±<br>0.19 | 95.38<br>±<br>0.12 | 77.50<br>±<br>0.11 | 94.46<br>±<br>0.14 | 90.29<br>±<br>0.09 | 80.19<br>±<br>0.06 |
| Soft<br>Augmentation<br>(w/<br>Crop) | 91.95<br>±<br>0.59 | 77.05<br>±<br>0.24 | 67.06<br>±<br>0.44 | 95.96<br>±<br>0.28 | 76.92<br>±<br>0.46 | 93.90<br>±<br>0.25 | 91.44<br>±<br>0.24 | 80.92<br>±<br>0.17 |

Table 7: Soft-Augment's performance on MedMNIST images, ResNet-18 backbone is used.

We see that except on the DermaMNIST dataset, the performance of Soft-Augmentation is even lower than the No Augmentation baseline. While SAflex's performance is consistently higher than the 'No Augmentation' baseline. Applying crop augmentation directly decreases the performance on most of the MedMNIST datasets. This is not surprising as we observed that applying RandAugment or Mixup directly also lowers the performance. However, the main reason for Soft-Augmentation's relatively poor performance is that it cannot consistently improve performance over the crop augmentation baseline (it shows improvement on Derma, Blood, OrganC, OrganS, but decreases performance on Path, Tissue, OCT, OrganA). This suggests that in situations with a high prevalence of poor-quality augmented samples (e.g., crop augmentation on medical images), Soft-Augmentation's relatively conservative strategy is inadequate in overcoming the significant noise and label errors introduced by these samples.

The performance of LP-A3 [\(Yang et al., 2022a\)](#page-16-0) on MedMNIST datasets (copied from the original paper) is listed below for reference.

For the efficiency result, we found that on the eight MedMNIST datasets considered, the overhead of SAflex measure as wall-clock time is 42% on average, while more specifically, 54% with

| Method | Path               | Derma              | Tissue             | Blood              | OCT                | OrganA             | OrganC             | OrganS             |
|--------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|
| LP-A3  | 94.42<br>±<br>0.24 | 76.22<br>±<br>0.27 | 68.63<br>±<br>0.14 | 96.97<br>±<br>0.06 | 80.27<br>±<br>0.54 | 94.73<br>±<br>0.21 | 92.41<br>±<br>0.22 | 82.28<br>±<br>0.38 |

Table 8: LP-A3's performance on MedMNIST images, ResNet-18 backbone is used.

RandAugment and 31% with Mixup. On the seven tabular datasets, on average the overhead of SAflex is 81% of the original training time per epoch.