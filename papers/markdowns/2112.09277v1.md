# <span id="page-0-0"></span>DNA: Dynamic Network Augmentation

Scott Mahan<sup>1</sup> Tim Doster<sup>2</sup> Henry Kvinge<sup>2</sup>

<sup>1</sup>University of California, San Diego <sup>2</sup> Pacific Northwest National Laboratory

#### Abstract

In many classification problems, we want a classifier that is robust to a range of non-semantic transformations. For example, a human can identify a dog in a picture regardless of the orientation and pose in which it appears. There is substantial evidence that this kind of invariance can significantly improve the accuracy and generalization of machine learning models. A common technique to teach a model geometric invariances is to augment training data with transformed inputs. However, which invariances are desired for a given classification task is not always known. Determining an effective data augmentation policy can require domain expertise or extensive data pre-processing. Recent efforts like AutoAugment optimize over a parameterized search space of data augmentation policies to automate the augmentation process. While AutoAugment and similar methods achieve state-of-the-art classification accuracy on several common datasets, they are limited to learning one data augmentation policy. Often times different classes or features call for different geometric invariances. We introduce Dynamic Network Augmentation (DNA), which learns input-conditional augmentation policies. Augmentation parameters in our model are outputs of a neural network and are implicitly learned as the network weights are updated. Our model allows for dynamic augmentation policies and performs well on data with geometric transformations conditional on input features.

### 1 Introduction

Deep neural networks are powerful classification models that often require large amounts of labeled data to be trained well. Data augmentation (DA) is the technique of augmenting a dataset with transformed inputs to artificially increase the quantity and diversity of training data [\[15\]](#page-9-0). Proper DA policies enhance the generalization of a machine learning model and can lead to more efficient training.

The transformations applied during DA can teach a model to be invariant to task-independent modes of variation within the data. For example, in image data, horizontal flips and other geometric transformations are often non-semantic for object classification. Many image classification tasks are insensitive to small color adjustments, and other data domains can be invariant to their own group actions (such as permutation of inputs). When training data is augmented using these transformations, the model can learn the corresponding invariances, resulting in better generalization at test time.

Some neural network architectures encode geometric invariances using built-in model constraints. Convolutional neural networks are approximately translation-invariant by design since the same filters are applied across various patches of an input image [\[16\]](#page-9-1). Other architectures use weightsharing and pooling operations to enforce invariance to rotations, permutations, or other group actions [\[4,](#page-8-0) [5,](#page-8-1) [28\]](#page-10-0). Such models can be very efficient when these hard-coded invariances accurately reflect non-semantic transformations in the data. However, most of them are limited to full group invariances which may negatively impact performance (e.g., some labels in MNIST [\[17\]](#page-9-2) are not invariant to 180-degree rotations). Some models allow for partial group actions [\[2\]](#page-8-2) by sampling <span id="page-1-0"></span>from a subset of transformations (e.g., rotation between −30 and 30 degrees), but even this DA framework can be too rigid for effective generalization. Moreover, hard-coding invariances requires domain knowledge of which transformations do not impact classification for a given dataset.

DA is an alternative to hard-coding invariance into a network architecture. It only requires that we can simulate the transformations with which we are trying to build invariance. Additionally, DA techniques are flexible in that they augment the training data according to some predetermined set of transformations that need not follow a specific algebraic structure. However, designing DA policies still requires a great deal of domain knowledge, as effective DA implementation can vary greatly from one data set to another. For example, image reflections can be an effective way to generate new data for CIFAR-10 [\[14\]](#page-9-3), but would likely reduce accuracy on MNIST where chirality plays an important role in digit identity. Thus, there is a need for new DA strategies which are tailored to a specific dataset.

Recent efforts have been made to optimize DA policies from training data, which would further automate the machine learning pipeline. These methods include generative DA [\[18,](#page-9-4) [23,](#page-9-5) [27\]](#page-9-6) as well as parameterized policy optimization [\[6,](#page-8-3) [10,](#page-8-4) [20,](#page-9-7) [19\]](#page-9-8). Automating the augmentation process eliminates the need for domain expertise while still allowing for great model performance.

However, one limitation of all previously mentioned augmentation methods – both manual and automated – is that they learn one augmentation policy for an entire dataset. While this is a common approach in DA, the range and types of variation present differs across classes and even individual instances in a dataset. For example, if a dataset contains overhead images of airplanes, it is critical that a network learns rotational invariance to these since they have no preferred orientation. On the other hand, buildings do not often appear upside down, so learning rotation invariance for this class may be less important or may even harm performance. We address this problem with Dynamic Network Augmentation (DNA), which draws from the input-dependent nature of dynamic neural networks [\[9\]](#page-8-5) to optimize augmentation policies. Our model features an augmentation network that learns input-conditional parameterizations of DA policies. We then use a differentiable relaxation of augmentation sampling [\[12\]](#page-9-9) before feeding the augmented input into a classification network. The result is a fully differentiable end-to-end model with learnable input-conditional data augmentation.

We test DNA on CIFAR-10, CIFAR-100 [\[14\]](#page-9-3), and SVHN [\[22\]](#page-9-10) by first training the augmentation network and then using the learned input-conditional DA policy to train the classification network. Our algorithm requires similar amounts of time for policy search and classifier training to the most recent automatic DA methods, and it achieves comparable accuracy to other methods on these datasets. Moreover, the input-conditional nature of our data augmentation allows us to explore invariances of our model on a finer scale.

### 2 Related Work

We focus on using DA for building a model that is robust to non-semantic geometric transformations, rather than using networks with hard-coded invariances. Manually selected DA policies for common datasets have existed for a long time, relying on domain expertise. For example, it has long been common practice to improve performance on MNIST by using geometric transformations like translation, scaling, shearing, and rotating [\[24\]](#page-9-11). On the other hand, natural image data like CIFAR-10 is typically augmented with cropping, mirroring, and color-changing operations to improve model generalization [\[15\]](#page-9-0). These are simple datasets for which DA has been studied extensively, but new datasets require their own expertise and investigation into effective augmentation strategies. Other augmentations that have found success in the image domain include Cutout [\[7\]](#page-8-6), Mixup [\[31\]](#page-10-1), and CutMix [\[29\]](#page-10-2). Each of these operations replaces or covers a random patch of the image in some way.

Approaches to automate DA generally fall into one of two categories. The first approach generates entirely new samples from existing training data. For example, Smart Augmentation [\[18\]](#page-9-4) merges two samples from the same class to create new data. Other models use Generative Adversarial Networks <span id="page-2-0"></span>(GANs) [\[8\]](#page-8-7) to generate augmented data by either refining synthetic images [\[23\]](#page-9-5) or sampling from the approximate training distribution using an expectation maximization algorithm [\[27\]](#page-9-6). Each of these methods seeks to generate new samples that are likely to come from the same distribution as the actual training examples, thus increasing the quantity and diversity of traning data.

The other approach to automated DA involves optimizing over a parameterized policy search space that includes a predetermined list of transformations. These algorithms are inspired by neural architecture search, where reinforcement learning or population based optimization is used to optimize model architectures from data [\[25,](#page-9-12) [26\]](#page-9-13). In the DA setting, optimization occurs over the parameterized policy search space, and this optimization is guided by evaluating DA policies on training data. AutoAugment (AA) [\[6\]](#page-8-3) introduces a very general parameterized augmentation policy search space consisting of many common geometric and color transformations, allowing for robust DA that can be learned from training data. AA optimizes the augmentation policy using reinforcement learning, but proves very costly as each step of the reinforcement learning algorithm involves training a network on a newly augmented training set. Population Based Augmentation (PBA) [\[10\]](#page-8-4) performs population based optimization on the DA policy, while Fast AutoAugment (Fast AA) [\[20\]](#page-9-7) uses Bayesian optimization. Both methods vastly improve the computation time of AA while obtaining similar accuracy on test data. To make DA policy search even more efficient, Differentiable Automatic Data Augmentation (DADA) [\[19\]](#page-9-8) relaxes policy selection to a differentiable process which can then be optimized using standard network backpropagation. DADA in particular draws from differentiable architecture search algorithms [\[21\]](#page-9-14). Each of these models performs well on image datasets without any prior knowledge of which augmentations are beneficial.

Finally, our DNA model learns input-conditional DA policies with a separate augmentation network, allowing for dynamic data augmentation. We note that none of the aforementioned methods address the situation where the types of augmentation that should be applied depend on the particular instance at hand. This is the primary novelty of our proposed DNA method.

## 3 Dynamic Network Augmentation (DNA)

At a high level, DNA uses a policy search space whose augmentation parameters are functions of the model's input. We use a neural network to approximate these functions and then augment the data accordingly before feeding it through a separate network for classification. The augmentation network and classification network are jointly optimized as one end-to-end model during the policy search phase, before the weights of the augmentation network are frozen. This network then serves as an input-conditional DA policy used to train the classification network. The entire model architecture is shown in Figure [1.](#page-3-0)

DNA shares some similarities with dynamic neural networks, which are networks whose weights are functions of the input [\[9\]](#page-8-5). In both cases, the operation performed on the input varies from one instance to another, although it is only the augmentation that is input-dependent in our DNA model. Hence, our model allows for truly dynamic augmentation policies that can learn to apply different transformations depending on features of the input data.

### 3.1 Policy Search Space

We define our policy similar to AA [\[6\]](#page-8-3) and other optimization-based automatic DA methods. Let O be a predetermined set of transformations O : X → X , where X is some fixed image space. Each image operation O has a parameter m ∈ [0, 1] which dictates the magnitude of the operation (e.g., for rotation, m ∈ [0, 1] corresponds to how many degrees to rotate the image by in the range [−180◦ , 180◦ ]). Some operations (e.g., horizontal flip) do not use the magnitude parameter but still have one. We introduce another parameter p ∈ [0, 1] and define a probabilistic operation O¯ which corresponds to O with probability p and no operation with probability 1 − p. The difference in

<span id="page-3-1"></span><span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)

**Caption:** Figure 1 illustrates the architecture of the Dynamic Network Augmentation (DNA) model, which employs an augmentation network to generate input-dependent augmentation policies (p(x), m(x), π(x)). During the policy search phase, both networks are jointly optimized, while the augmentation network remains fixed during classifier training. This design enables dynamic data augmentation tailored to specific input features, enhancing model generalization across datasets.

Figure 1: The overall architecture of the DNA model. During the policy search and classifier training phases, input images are fed through an augmentation network that outputs an input-dependent augmentation policy (p(x),m(x),π(x)). Note that p(x) and m(x) are sigmoid outputs while π(x) is a softmax output to be consistent with the parameterization of the search space. The policy is then applied to the original image, and the resulting augmented version is fed through a classifier network to obtain the final output. Both networks are trained during the search phase, but during classifier training the augmentation network remains frozen. During testing, input images are fed through the classifier network only.

the augmentation policies used in DNA, compared to existing approaches, is that the augmentation parameters p and m are functions of the input x. Thus, a single operation in DNA is given by

$$
\bar{O}(x; p(x), m(x)) = \begin{cases} O(x; m(x)) & \text{with prob. } p(x) \\ x & \text{with prob. } 1 - p(x). \end{cases}
$$
\n(1)

A sub-policy s is defined as a composition of k such operations

$$
x_{(\ell)} = \bar{O}_{\ell}^{s}(x_{(\ell-1)}; p_{\ell}^{s}(x), m_{\ell}^{s}(x)), \quad \ell = 1, ..., k
$$
 (2)

where x(0) = x and x(k) = s(x; p s (x),m<sup>s</sup> (x)). Notice that the parameters p s ` and m<sup>s</sup> ` only depend on the original input x even though the operations O¯<sup>s</sup> ` are applied sequentially.

We fix O to include the same image operations used in AA: ShearX/Y, TranslateX/Y, Rotate, AutoContrast, Invert, Equalize, Solarize, Posterize, Contrast, Color, Brightness, Sharpness, and Cutout (the only AA operation we exclude is Sample Pairing [\[11\]](#page-8-8) because it uses two training images). As in AA, we use sub-policies of length k = 2 operations. The entire list of sub-policies consists of all 15 2 = 105 combinations of these 15 operations (each combination only appears once; e.g., ('TranslateX','Rotate') is a sub-policy but ('Rotate','TranslateX') is not).

Finally, a complete DNA augmentation policy includes an input-dependent probability vector π(x) over the space of all sub-policies. The value πs(x) indicates the probability that sub-policy s is applied to x during the augmentation step while training the DNA model. An entire DA policy is defined uniquely by its parameter functions (p(x),m(x),π(x)). The DNA model seeks to optimize this policy during the policy search phase by approximating p(x), m(x), and π(x) with a deep neural network as depicted in Figure [1.](#page-3-0)

#### 3.2 Policy Sampling

DNA augmentation requires sampling from a categorical distribution with parameter vector π(x), as well as sampling from k Bernoulli distributions with parameters p s 1 (x), . . . , p<sup>s</sup> k (x) to determine whether an operation should be applied. However, sampling from these distributions is not differentiable. Following DADA [\[19\]](#page-9-8), we introduce differentiable relaxations of these sampling operations so that the entire DNA model is differentiable.

<span id="page-4-3"></span>To make the sampling operations differentiable, we first write the selected sub-policy as

$$
s^*(x) = \sum_{s \in \mathcal{S}} c_x^s s(x),\tag{3}
$$

where S is the set of all sub-policies and c<sup>x</sup> is a one-hot vector sampled from the categorical distribution with probability vector π(x). In the DNA model, π(x) is given by a softmax output

$$
\pi_s(x) = \frac{\exp(\alpha_s(x))}{\sum_{s' \in \mathcal{S}} \exp(\alpha_{s'}(x))}
$$
(4)

where α(x) is the output prior to the softmax layer. Sampling directly from this distribution would require backpropagating with respect to the parameters of the distribution. Instead, we can approximate π(x) with a Gumbel-Softmax [\[12\]](#page-9-9) distribution

<span id="page-4-0"></span>
$$
\pi_s(x) \approx \frac{\exp((\alpha_s(x) + g_s)/\tau)}{\sum_{s' \in \mathcal{S}} \exp((\alpha_{s'}(x) + g_{s'})/\tau))}
$$
(5)

where g<sup>s</sup> = − log(− log(us)) with u<sup>s</sup> i.i.d. ∼ Uniform(0, 1), and τ is a temperature parameter. We now have the relaxation c<sup>x</sup> = one-hot(argmax<sup>s</sup> (αs(x) + gs)). This is an example of a reparameterization trick since g<sup>s</sup> has a fixed distribution unlike πs(x). While the argmax operation is still not differentiable, we can use this sampling in the model's forward pass and backpropagate with respect to the deterministic parameters in Equation [\(5\)](#page-4-0). Here we are using the straight-through gradient estimator [\[1\]](#page-8-9) by effectively ignoring the argmax during backpropagation.

Once a sub-policy s has been selected, we apply the sequence of operations O¯<sup>s</sup> 1 , . . . , O¯<sup>s</sup> k in that sub-policy. We can write

<span id="page-4-2"></span>
$$
\bar{O}_i^s(x; p_i^s(x), m_i^s(x)) = bO_i^s(x; m_i^s(x)) + (1 - b)x \tag{6}
$$

where b ∼ Bernoulli(p s i (x)). We again cannot sample directly from this distribution, but we use a reparameterization trick

<span id="page-4-1"></span>
$$
\hat{b} \approx \sigma \left( \lambda^{-1} \left( \log \frac{p_i^s(x)}{1 - p_i^s(x)} + \log \frac{u_i^s}{1 - u_i^s} \right) \right) \tag{7}
$$

where σ is the sigmoid function, λ is a temperature parameter, and u s i i.i.d. ∼ Uniform(0, 1). We set b = 1{ ˆb > 0.5} during the forward pass but use the straight-through gradient estimator to backpropagate with respect to the parameters in Equation [\(7\)](#page-4-1). Finally, we use the straight-through gradient estimator for m<sup>s</sup> i in Equation [\(6\)](#page-4-2) since some of the image operations O<sup>s</sup> i (e.g., horizontal flipping) are not differentiable.

### 4 Experiments and Results

We assess our DNA model on the CIFAR-10 [\[14\]](#page-9-3), CIFAR-100 [\[14\]](#page-9-3), and SVHN [\[22\]](#page-9-10) datasets and compare the results to other automated data augmentations models (namely AA [\[6\]](#page-8-3), PBA [\[10\]](#page-8-4), Fast AA [\[20\]](#page-9-7), and DADA [\[19\]](#page-9-8)). Using DNA involves a search phase and a training phase. During the search phase, we jointly optimize the augmentation network and classification network so that the model can learn an effective input-conditional augmentation strategy. After the policy search is complete, the weights in the augmentation network are frozen. During training, the augmentation network is used to augment each input image, but this network is not updated. Instead, the classification network receives the augmented inputs and is trained just like any classifier would normally be on an augmented dataset. Finally, we evaluate the trained model on test data and use test accuracy

<span id="page-5-1"></span><span id="page-5-0"></span>

| Dataset   | AA [6] | PBA [10] | Fast AA [20] | DADA [19] | DNA |
|-----------|--------|----------|--------------|-----------|-----|
| CIFAR-10  | 5000   | 5        | 3.5          | 0.1       | 0.4 |
| CIFAR-100 | –      | –        | –            | 0.2       | 0.5 |
| SVHN      | 1000   | 1        | 1.5          | 0.1       | 0.1 |

Table 1: GPU hours spent on DA policy search. Hours for AA, PBA, Fast AA, and DADA are reported in their respective papers. Hours for DNA show the search cost using an NVIDIA V100 GPU.

to determine whether the augmentation network learned an effective DA policy that improves generalization. Our implementation uses some existing code assets from Fast AA and DADA for the policy search space and augmentation but primarily uses original code for model specification and training.

Search Phase We conduct DA policy search by optimizing the augmentation network on reduced versions of each dataset. Following AA, we randomly select 4,000 training examples for the reduced CIFAR-10 and CIFAR-100 datasets and 1,000 samples for the reduced SVHN dataset. In each case, we use stratified sampling to preserve the percentage of each class in the dataset. CIFAR data are also pre-processed using horizontal flips with 50% probability, zero-padding with random crops, and Cutout [\[7\]](#page-8-6) with size 16 × 16 patches, but on SVHN we only apply Cutout. This pre-processing follows AA and provides a baseline for effective training on these datasets. We conduct policy search for 20 epochs on all datasets.

During the search phase, we use a Wide-ResNet-40-2 [\[30\]](#page-10-3) architecture for CIFAR data and a Wide-ResNet-28-10 architecture for SVHN (with the same architecture for both the augmentation network and the classification network). The augmentation network is trained using an Adam optimizer [\[13\]](#page-9-15) with learning rate 0.005, momentum β = (0.5, 0.999), and weight decay of 0. The classification network is trained using an SGD optimizer with momentum 0.9, weight decay 2×10<sup>−</sup><sup>4</sup> , and a cosine annealing learning rate schedule. The initial learning rate and batch size are set to 0.1 and 128 for CIFAR-10, compared to 0.025 and 32 for CIFAR-100 and SVHN. Finally, temperature parameters for the relaxed categorical and Bernoulli distributions were set to τ = λ = 0.5.

Table [1](#page-5-0) shows the cost of DA policy search in GPU hours for each model on each dataset. Note that AA, PBA, and Fast AA did not perform policy search on CIFAR-100 due to the computationally intensive nature of their algorithms. They instead transferred the DA policy found during CIFAR-10 search to train their models on CIFAR-100.

Most notably, DNA requires much less search time than AA, PBA, and Fast AA since those methods use expensive techniques like reinforcement learning and population based optimization. Policy search is faster with DNA by orders of magnitude because it benefits from the single-pass differentiable nature of the model as with DADA. DNA requires similar or slightly more time for policy search than DADA, likely because the input-conditional augmentation policy is learned with a neural network rather than a single set of parameters.

Training Phase After DA policy search is conducted, the weights of the augmentation network are frozen. This gives us a learned input-conditional augmentation policy that can be used to augment training data. During training, each image is still fed through the augmentation network but without updating the network's weights. The resulting policy is used to augment the original image, which is fed through the classification network for the final output. We train our DNA models on the full CIFAR-10, CIFAR-100, and SVHN training sets (the "core" SVHN training set, not the "extra" one) for 200 epochs. Again, all training data is pre-processed as in the search phase to be consistent with AA and other DA literature on the datasets.

AA, Fast AA, and DADA sometimes refer to this training phase as policy evaluation (and

<span id="page-6-1"></span><span id="page-6-0"></span>

| Dataset   | Baseline | Cutout [7] | AA [6] | Fast AA [20] | DADA [19] | DNA  |
|-----------|----------|------------|--------|--------------|-----------|------|
| CIFAR-10  | 5.3      | 4.1        | 3.7    | 3.6          | 3.6       | 4.0  |
| CIFAR-100 | 26.0     | 25.2       | 20.7   | 20.7         | 20.9      | 22.0 |
| SVHN      | 1.5      | 1.3        | 1.1    | 1.1          | 1.2       | 3.4  |

Table 2: Test error rates (%) on each dataset using a Wide-ResNet-40-2 network architecture for CIFAR data and a Wide-ResNet-28-10 for SVHN. Baseline refers to conventional pre-processing augmentation without Cutout. Error rates for AA, Fast AA, and DADA are reported in their respective papers. Error rates for DNA are computed on the standard CIFAR-10, CIFAR-100, and SVHN test datasets. PBA [\[10\]](#page-8-4) does not use the same architecture and hence is not compared.

distinguish it from model evaluation that occurs at test time). This idea of policy evaluation is especially relevant for AA, PBA, and Fast AA since performance on training data is used as a signal for the overall optimization scheme. On the other hand, DADA and DNA use a one-pass optimization strategy since the model is fully differentiable.

During training, both networks again use a Wide-ResNet-40-2 architecture for CIFAR and a Wide-ResNet-28-10 architecture for SVHN. We use an SGD optimizer with the same parameters as the search phase, except the initial learning rate is adjusted to 0.1 for CIFAR and 0.01 for SVHN and the batch size is adjusted to 512 for all datasets, and we add Nesterov momentum [\[3\]](#page-8-10). This optimizer is applied only to the classification network. All training parameters and the number of epochs trained are consistent with those used in AA, Fast AA, and DADA.

Model Testing After DA policy search and training have been completed, we test our DNA model on the standard test datasets for CIFAR-10, CIFAR-100, and SVHN. The test images are not augmented in any way, nor are they fed through the augmentation network. Augmentation was used during training with the goal of exposing the model to new data and improving generalization. At test time, images are fed straight through the classification network.

Table [2](#page-6-0) shows error rates on each test dataset for baseline pre-processing, Cutout, AA, Fast AA, DADA, and DNA using the same network architecture. Each of the automated data augmentation algorithms AA, Fast AA, and DADA perform better than baseline pre-processing and Cutout, which are known to perform very well on these natural image datasets. Our DNA model performs better than baseline pre-processing and Cutout but not quite as well as the other DA methods. We believe that optimization of our training hyperparameters could improve our error rates to be more competitive with the other methods, as the different structure of our augmentation policy likely warrants different hyperparameters.

#### 4.1 Input-Conditional Augmentation Policies

Tables [1](#page-5-0) and [2](#page-6-0) show that our DNA model conducts DA policy search very efficiently and generalizes well on test data. One additional benefit of DNA is that it learns an input-conditional DA policy. By investigating aggregate statistics of the found augmentation policies over the entire training set, we can gain insight into which operations are beneficial for certain images or classes.

Part of the motivation for DNA's input-conditional augmentation is the idea that different class labels may be invariant to different transformations. To investigate whether this phenomenon occurs in natural image data, we look at the average DA policy for each class in the CIFAR-10 training set. In particular, for a given class label y, we look at the average probability vector E[π(x)|Y = y]. Of interest are the five largest values in this vector, corresponding to the five most commonly used sub-policies for class y. Table [3](#page-7-0) shows the top five sub-policies averaged over each class in the CIFAR-10 training set compared to the sub-policies found by AA on CIFAR-10.

Table [3](#page-7-0) offers some interesting insights about the input-conditional augmentation policies learned

<span id="page-7-1"></span><span id="page-7-0"></span>

| airplane                                                   | automobile                                 | bird                                         | cat                                                                                         | deer                   | AA [6]                   |
|------------------------------------------------------------|--------------------------------------------|----------------------------------------------|---------------------------------------------------------------------------------------------|------------------------|--------------------------|
| sub-policy 1 (TranslateX, Solarize)                        | (TranslateX, Solarize)                     | (TranslateX, Solarize)                       | (TranslateX, Solarize)                                                                      | (TranslateX, Solarize) | (Invert, Contrast)       |
| sub-policy 2 (Equalize, Posterize)                         | (Equalize, Posterize)                      | (Equalize, Posterize)                        | (Equalize, Posterize)                                                                       | (Equalize, Posterize)  | (Rotate, TranslateX)     |
| sub-policy 3 (ShearY, Contrast)                            | (ShearY, Contrast)                         |                                              | (TranslateY, Contrast) (TranslateY, Contrast) (TranslateY, Contrast) (Sharpness, Sharpness) |                        |                          |
| sub-policy 4 (Equalize, Brightness)                        | (Equalize, Brightness)                     | (Invert, Cutout)                             | (Invert, Cutout)                                                                            | (Invert, Cutout)       | (ShearY, TranslateY)     |
| sub-policy 5 (Invert, Cutout)                              | (Invert, Cutout)                           | (ShearY, Contrast)                           | (ShearY, Contrast)                                                                          | (ShearY, Contrast)     | (AutoContrast, Equalize) |
| dog                                                        | frog                                       | horse                                        | ship                                                                                        | truck                  | (ShearY, Posterize)      |
| sub-policy 1 (TranslateX, Solarize)                        | (TranslateX, Solarize)                     | (TranslateX, Solarize)                       | (TranslateX, Solarize)                                                                      | (TranslateX, Solarize) | (Color, Brightness)      |
| sub-policy 2 (Equalize, Posterize)                         | (Equalize, Posterize)                      | (Equalize, Posterize)                        | (ShearY, Contrast)                                                                          | (Equalize, Posterize)  | (Sharpness, Brightness)  |
| sub-policy 3 (TranslateY, Contrast) (TranslateY, Contrast) |                                            | (TranslateY, Contrast) (Equalize, Posterize) |                                                                                             | (ShearY, Contrast)     | (Equalize, Equalize)     |
| sub-policy 4 (Invert, Cutout)                              | (Invert, Cutout)                           | (Invert, Cutout)                             | (Equalize, Brightness)                                                                      | (Equalize, Brightness) | (Contrast, Sharpness)    |
| sub-policy 5 (ShearY, Contrast)                            | (TranslateX, Posterize) (ShearY, Contrast) |                                              | (Invert, Cutout)                                                                            | (Invert, Cutout)       | (Color, TranslateX)      |

Table 3: Ordered top five DNA sub-policies for each CIFAR-10 class compared to ordered top eleven sub-policies for AA on all of CIFAR-10. Sub-policies are ordered by the probability π of sampling that sub-policy from largest to smallest, averaged over each class for DNA.

by our DNA model on CIFAR-10. The sub-policy (TranslateX, Solarize) is most common for each class, followed by (Equalize, Posterize) for every class except ship. After the top two sub-policies there are some additional differences across classes, but in total we only see seven unique subpolicies appear in the top five across all CIFAR-10 classes. Thus, it appears that DNA learned a few sub-policies that improve generalization for all CIFAR-10 classes while also picking up some minor differences in DA policies between classes.

Notably, DNA appears to have learned two groups of DA policies – one on images of animals and one on images of vehicles. The top five sub-policies are identical for the bird, cat, deer, dog, and horse classes. Additionally, only sub-policy 5 is different between these classes and the frog class, suggesting that images of animals benefit from similar augmentations. On the other hand, the top five sub-policies are identical for the airplane, automobile, and truck classes. Moreover, the ship class only differs in the order of sub-policies 2 and 3. Overall, the animal classes have very similar sub-policies as do the vehicle classes, and there are a few noticeable differences between these two groups in sub-policies 3-5.

Our DNA model favors color-based transformations over geometric transformations among the top five sub-policies on CIFAR-10. Color-based augmentation is often preferred for natural image data, and this agrees with AA, which also finds fewer geometric transformations among its top subpolicies. In total, a breakdown of the DNA augmentation policies by class gives a more thorough picture of which augmentations are beneficial for certain classes or images.

### 5 Discussion

DNA offers a more general model for automated data augmentation that allows for input-conditional augmentation policies. While this approach is compelling and potentially very robust to diverse datasets, it could have some negative societal impacts. Most notably, if the DNA model is deployed on data with historical bias, it is conceivable that it could learn those biases even more than other models due to the input-dependent nature of its data augmentation. To mitigate this concern, we encourage continued awareness in the machine learning community about biases present in data and processes to avoid misuse.

Our results show that DNA does not perform quite as well as other automated DA approaches, suggesting some limitations of the model. It is quite possible that the input-conditional augmentation policy's additional complexity makes the learning process slightly slower. Moreover, on datasets when augmentation would not benefit from being input-dependent, it is likely that DNA provides little additional benefit over DADA. However, we believe that our model gives a compelling foundation for automated conditional data augmentation that could improve model performance and generalization, especially on more varied datasets. We plan to further experiment with DNA using different training hyperparameters and datasets to better understand the effect that conditional data augmentation has on downstream model performance.

### 6 Conclusions

In this paper we propose Dynamic Network Augmentation (DNA), a method for learning inputconditional data augmentation policies from training data alone. The DNA architecture features a differentiable end-to-end model where one network is trained to learn augmentation policy functions and another network is trained as a classifier.

While DNA does not outperform comparable models in terms of accuracy, its novel ability to efficiently search for input-conditional policies provides new opportunities for data-driven data augmentation strategies. Investigating the top sub-policies learned from CIFAR-10 by class reveals that DNA is choosing more color-based transformations, which is consistent with other augmentation strategies on natural images. Grouping instances by sub-policy also reveals that our learned augmentations correlate with natural semantic boundaries (e.g., animal vs. vehicle), suggesting that DNA learns meaningful and insightful DA policies that fit a particular dataset.

### References

- <span id="page-8-9"></span>[1] Yoshua Bengio, Nicholas L´eonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv:1308.3432v1, 2013. [5](#page-4-3)
- <span id="page-8-2"></span>[2] Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. Learning invariances in neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pages 1–16, 2020. [1](#page-0-0)
- <span id="page-8-10"></span>[3] Aleksandar Botev, Guy Lever, and David Barber. Nesterov's accelerated gradient and momentum as approximations to regularised update descent. In International Joint Conference on Neural Networks (IJCNN), 2017. [7](#page-6-1)
- <span id="page-8-0"></span>[4] Taco S. Cohen and Max Welling. Group equivariant convolutional networks. In Proceedings of the 33rd International Conference on Machine Learning (ICML), volume 48, pages 2990–2999, 2016. [1](#page-0-0)
- <span id="page-8-1"></span>[5] Taco S. Cohen and Max Welling. Steerable CNNs. arXiv:1612.08498, 2016. [1](#page-0-0)
- <span id="page-8-3"></span>[6] Ekin D. Cubuk, Barret Zoph, Dandelion Man´e, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 113–123, 2019. [2,](#page-1-0) [3,](#page-2-0) [5,](#page-4-3) [6,](#page-5-1) [7,](#page-6-1) [8](#page-7-1)
- <span id="page-8-6"></span>[7] Terrance DeVries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv:1708.04552v2, 2017. [2,](#page-1-0) [6,](#page-5-1) [7](#page-6-1)
- <span id="page-8-7"></span>[8] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NeurIPS), pages 2672–2680, 2014. [3](#page-2-0)
- <span id="page-8-5"></span>[9] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: A survey. arXiv:2102.04906v3, 2021. [2,](#page-1-0) [3](#page-2-0)
- <span id="page-8-4"></span>[10] Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi Chen. Population based augmentation: Efficient learning of augmentation policy schedules. In Proceedings of the 36th International Conference on Machine Learning (ICML), pages 2731–2741, 2019. [2,](#page-1-0) [3,](#page-2-0) [5,](#page-4-3) [6,](#page-5-1) [7](#page-6-1)
- <span id="page-8-8"></span>[11] Hiroshi Inoue. Data augmentation by pairing samples for images classification. arXiv:1801.02929v2, 2018. [4](#page-3-1)
- <span id="page-9-9"></span>[12] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations (ICLR), 2017. [2,](#page-1-0) [5](#page-4-3)
- <span id="page-9-15"></span>[13] Diederik P. Kingma and Jimmy L. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980v9, 2017. [6](#page-5-1)
- <span id="page-9-3"></span>[14] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. [2,](#page-1-0) [5](#page-4-3)
- <span id="page-9-0"></span>[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pages 1097–1105, 2012. [1,](#page-0-0) [2](#page-1-0)
- <span id="page-9-1"></span>[16] Yann LeCun and Yoshua Bengio. Convolutional Networks for Images, Speech, and Time Series, pages 255–258. MIT Press, Cambridge, MA, USA, 1998. [1](#page-0-0)
- <span id="page-9-2"></span>[17] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. [1](#page-0-0)
- <span id="page-9-4"></span>[18] Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation learning an optimal data augmentation strategy. IEEE Access, 5:5858–5869, 2017. [2](#page-1-0)
- <span id="page-9-8"></span>[19] Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M. Robertson, and Yongxin Yang. Differentiable automatic data augmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 580–595, 2020. [2,](#page-1-0) [3,](#page-2-0) [4,](#page-3-1) [5,](#page-4-3) [6,](#page-5-1) [7](#page-6-1)
- <span id="page-9-7"></span>[20] Sungbin Lim, Ildoo Kim, Taesup Kim, and Chiheon Kim. Fast autoaugment. In Advances in Neural Information Processing Systems (NeurIPS), pages 6665–6675, 2019. [2,](#page-1-0) [3,](#page-2-0) [5,](#page-4-3) [6,](#page-5-1) [7](#page-6-1)
- <span id="page-9-14"></span>[21] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In International Conference on Learning Representations (ICLR), 2019. [3](#page-2-0)
- <span id="page-9-10"></span>[22] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. [2,](#page-1-0) [5](#page-4-3)
- <span id="page-9-5"></span>[23] Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russ Webb. Learning from simulated and unsupervised images through adversarial training. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2107–2116, 2017. [2,](#page-1-0) [3](#page-2-0)
- <span id="page-9-11"></span>[24] P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of the 7th International Conference on Document Analysis and Recognition (ICDAR), pages 958–963, 2003. [2](#page-1-0)
- <span id="page-9-12"></span>[25] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Advances in Neural Information Processing Systems (NeurIPS), 2015. [3](#page-2-0)
- <span id="page-9-13"></span>[26] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–9, 2015. [3](#page-2-0)
- <span id="page-9-6"></span>[27] Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, and Ian Reid. A bayesian data augmentation approach for learning deep models. In Advances in Neural Information Processing Systems (NeurIPS), pages 2797–2806, 2017. [2,](#page-1-0) [3](#page-2-0)
- <span id="page-10-0"></span>[28] Maurice Weiler and Gabriele Cesa. General E(2)-equivariant steerable CNNs. In Advances in Neural Information Processing Systems (NeurIPS), pages 14334–14345, 2019. [1](#page-0-0)
- <span id="page-10-2"></span>[29] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. CutMix: regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6023–6032, 2019. [2](#page-1-0)
- <span id="page-10-3"></span>[30] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British Machine Vision Conference (BMVC), 2016. [6](#page-5-1)
- <span id="page-10-1"></span>[31] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: beyond empirical risk minimization. arXiv:1710.09412v2, 2018. [2](#page-1-0)