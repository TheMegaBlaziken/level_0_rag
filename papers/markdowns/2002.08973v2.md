# Affinity and Diversity: Quantifying Mechanisms of Data Augmentation

Raphael Gontijo-Lopes<sup>∗</sup> Google Brain Mountain View, CA 94043

Ekin D. Cubuk Google Brain Mountain View, CA 94043 cubuk@google.com

Sylvia J. Smullin<sup>∗</sup> Google Mountain View, CA 94043

Ethan Dyer Google Mountain View, CA 94043 edyer@google.com

# Abstract

Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of either distribution shift or augmentation diversity. Inspired by these, we seek to quantify how data augmentation improves model generalization. To this end, we introduce interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.

![](_page_0_Figure_7.jpeg)

**Caption:** Figure 1 illustrates the relationship between Affinity and Diversity in data augmentation for CIFAR-10. Each point represents an augmentation yielding test accuracy above 88.7%, with color indicating accuracy. The bubble size reflects diversity, while overlap indicates distributional similarity. Generally, higher accuracy is found in the upper right, suggesting that effective augmentations combine both metrics.

<span id="page-0-0"></span>Figure 1: Affinity and Diversity parameterize the performance of a model trained with augmentation. (a) CIFAR-10: Color shows the final test accuracy. \* marks the clean baseline. Each point represents a different augmentation that yields test accuracy greater than 88.7%. (b) Representation of how clean data and augmented data are related in the space of these two metrics. Higher diversity is represented by a larger bubble while distributional similarity is depicted through the overlap of bubbles. Test accuracy generally improves to the upper right in this space. Adding real new data to the training set is expected to be in the far upper right corner.

<sup>∗</sup>Equal contribution

# 1 Introduction

Models that achieve state-of-the-art in image classification often use heavy data augmentation strategies. The best techniques use various transforms applied sequentially and stochastically. Though the effectiveness of this is well-established, the mechanism through which these transformations work is not well-understood.

Since early uses of data augmentation, it has been assumed that augmentation works because it simulates realistic samples from the true data distribution: "[augmentation strategies are] reasonable since the transformed reference data is now extremely close to the original data. In this way, the amount of training data is effectively increased" [\[1\]](#page-8-0). Because of this, augmentations have often been designed with the heuristic of incurring minimal distribution shift from the training data.

This rationale does not explain why unrealistic distortions such as cutout [\[2\]](#page-9-0), SpecAugment [\[3\]](#page-9-1), and mixup [\[4\]](#page-9-2) significantly improve generalization performance. Furthermore, methods do not always transfer across datasets—Cutout, for example, is useful on CIFAR-10 and not on ImageNet [\[5\]](#page-9-3). Additionally, many augmentation policies heavily modify images by stochastically applying multiple transforms to a single image. Based on this observation, some have proposed that augmentation strategies are effective because they increase the diversity of images seen by the model.

In this complex landscape, claims about diversity and distributional similarity remain unverified heuristics. Without more precise data augmentation science, finding state-of-the-art strategies requires brute force that can cost thousands of GPU hours [\[6,](#page-9-4) [7\]](#page-9-5). This highlights a need to specify and measure the relationship between the original training data and the augmented dataset, as relevant to a given model's performance.

In this paper, we quantify these heuristics. Seeking to understand the mechanisms of augmentation, we focus on single transforms as a foundation. We present an extensive study of 204 different augmentations on CIFAR-10 and 223 on ImageNet, varying both broad transform families and finer transform parameters. Our contributions are:

- 1. We introduce Affinity and Diversity: interpretable, easy-to-compute metrics for parameterizing augmentation performance. Affinity quantifies how much an augmentation shifts the training data distribution from that learned by a model. Diversity quantifies the complexity of the augmented data with respect to the model and learning procedure.
- 2. We show that performance is dependent on *both* metrics. In the Affinity-Diversity plane, the best augmentation strategies jointly optimize the two (see Fig [1\)](#page-0-0).
- 3. We connect augmentation to other familiar forms of regularization, such as `<sup>2</sup> and learning rate scheduling, observing common features of the dynamics: performance can be improved and training accelerated by turning off regularization at an appropriate time.
- 4. We show that performance is only improved when a transform increases the total number of unique training examples. The utility of these new training examples is informed by the augmentation's Affinity and Diversity.

# 2 Related Work

Since early uses of data augmentation in training neural networks, there has been an assumption that effective transforms for data augmentation are those that produce images from an "overlapping but different" distribution [\[1,](#page-8-0) [8\]](#page-9-6). Indeed, elastic distortions as well as distortions in the scale, position, and orientation of training images have been used on MNIST [\[9](#page-9-7)[–12\]](#page-9-8), while horizontal flips, random crops, and random distortions to color channels have been used on CIFAR-10 and ImageNet [\[13–](#page-9-9) [15\]](#page-9-10). For object detection and image segmentation, one can also use object-centric cropping [\[16\]](#page-9-11) or cut-and-paste new objects [\[17–](#page-9-12)[19\]](#page-9-13).

In contrast, researchers have also successfully used more generic transformations that are less domainspecific, such as Gaussian noise [\[5,](#page-9-3) [20\]](#page-10-0), input dropout [\[21\]](#page-10-1), erasing random patches of the training samples during training [\[2,](#page-9-0) [3,](#page-9-1) [22\]](#page-10-2), and adversarial noise [\[23\]](#page-10-3). Mixup [\[4\]](#page-9-2) and Sample Pairing [\[24\]](#page-10-4) are two augmentation methods that use convex combinations of training samples.

It is also possible to improve generalization by combining individual transformations. For example, reinforcement learning has been used to choose more optimal combinations of data augmentation transformations [\[6,](#page-9-4) [25\]](#page-10-5). Follow-up research has lowered the computation cost of such optimization, by using population based training [\[26\]](#page-10-6), density matching [\[27\]](#page-10-7), adversarial policy-design that evolves throughout training [\[7\]](#page-9-5), or a reduced search space [\[28\]](#page-10-8). Despite producing unrealistic outputs, such combinations of augmentations can be highly effective in different tasks [\[29–](#page-10-9)[33\]](#page-10-10).

Across these different examples, the role of distribution shift in training remains unclear. Lim et al. [\[27\]](#page-10-7), Hataya et al. [\[34\]](#page-10-11) have found augmentation policies by minimizing the distance between the distributions of augmented data and clean data. Recent work found that after training with augmented data, fine-tuning on clean training data can be beneficial [\[35\]](#page-10-12), while Touvron et al. [\[36\]](#page-10-13) found it beneficial to fine-tune with a test-set resolution that aligns with the training-set resolution.

The true input-space distribution from which a training dataset is drawn remains elusive. To better understand the effect of distribution shift on model performance, many works attempt to estimate it. Often these techniques require training secondary models, such as those based on variational methods [\[37](#page-10-14)[–40\]](#page-11-0). Others have tried to augment the training set by modelling the data distribution directly [\[41\]](#page-11-1). Recent work has suggested that even unrealistic distribution modelling can be beneficial [\[42\]](#page-11-2).

These methods try to specify the distribution separately from the model they are trying to optimize. As a result, they are insensitive to any interaction between the model and data distribution. Instead, we are interested in a measure of how much the data shifts along directions that are most relevant to the model's performance.

# <span id="page-2-1"></span>3 Methods

We performed extensive experiments with various augmentations on CIFAR-10 and ImageNet. Experiments on CIFAR-10 used the WRN-28-2 model [\[14\]](#page-9-14), trained for 78k steps with cosine learning rate decay. Results are the mean over 10 initializations and reported errors (often too small to show on figures) are the standard error on the mean. Details on the error analysis are in Sec. [C.](#page-16-0)

Experiments on ImageNet used the ResNet-50 model [\[43\]](#page-11-3), trained for 112.6k steps with a weight decay rate of 1e-4, and a learning rate of 0.2, which is decayed by 10 at epochs 30, 60, and 80.

Images were pre-processed by dividing each pixel value by 255 and normalizing by the data set statistics. Random crop was also applied on all ImageNet models. These pre-processed data without further augmentation are "clean data" and a model trained on it is the "clean baseline". We followed the same implementation details as Cubuk et al. [\[6\]](#page-9-4)[2](#page-2-0) , including for most augmentation operations. Further implementation details are in Sec. [A.](#page-13-0)

For CIFAR-10, test accuracy on the clean baseline is 89.7 ± 0.1%. The validation accuracy is 89.9 ± 0.2%. On ImageNet, the test accuracy is 76.06%.

Unless specified otherwise, data augmentation was applied following standard practice: each time an image is drawn, the given augmentation is applied with a given probability. We call this mode *dynamic* augmentation. Due to whatever stochasticity is in the transform itself (such as randomly selecting the location for a crop) or in the policy (such as applying a flip only with 50% probability), the augmented image could be different each time. Thus, most of the tested augmentations increase the number of possible distinct images that can be shown during training.

We also performed select experiments using *static* training. In static augmentation, the augmentation policy (one or more transforms) is applied once to the entire clean training set. Static augmentation does not change the number of unique images in the dataset.

### 3.1 Affinity: a simple metric for distribution shift

Thus far, heuristics of distribution shift have motivated design of augmentation policies. Inspired by this focus, we introduce a simple metric to quantify how augmentation shifts data *with respect to the decision boundary of the clean baseline model*.

We start by noting that a trained model is often sensitive to the distribution of the training data. That is, model performance varies greatly between new samples from the true data distribution and samples from a shifted distribution.

Importantly, the model's sensitivity to distribution shift is not purely a function of the input data distribution, since training dynamics and the model's implicit biases affect performance. Because

<span id="page-2-0"></span><sup>2</sup>Available at bit.ly/2v2FojN

the goal of augmentation is improving model performance, measuring shifts with respect to the distribution captured by the model is more meaningful than measuring shifts in the distribution of the input data alone.

We thus define Affinity to be the difference between the validation accuracy of a model trained on clean data and tested on clean data, and the accuracy of the same model tested on an augmented validation set. Here, the augmentation is applied to the validation dataset in one pass, as a static augmentation. More formally we define:

Definition 1. *Let* D*train and* D*val be training and validation datasets drawn IID from the same* clean *data distribution, and let* D<sup>0</sup> *val be derived from* D*val by applying a stochastic augmentation strategy,* a*, once to each image in* D*val,* D<sup>0</sup> *val* = {(a(x), y) : ∀ (x, y) ∈ D*val*}*. Further let* m *be a model trained on* D*train and* A(m, D) *denote the model's accuracy when evaluated on dataset* D*. The Affinity,* T [a; m; D*val*]*, is given by*

$$
\mathcal{T}[a; m; D_{val}] = \mathcal{A}(m, D'_{val}) - \mathcal{A}(m, D_{val}). \qquad (1)
$$

With this definition, Affinity of zero represents no shift and a negative number suggests that the augmented data is out-of-distribution for the model.

![](_page_3_Figure_5.jpeg)

**Caption:** Figure 2 depicts Affinity as a model-sensitive measure of distribution shift, showing contours of equal Affinity and KL Divergence in a two-class Gaussian mixture. Affinity varies with shifts relevant to the model's decision boundary, while KL Divergence changes with any data shift. This highlights Affinity's utility in quantifying how augmentations affect model performance.

<span id="page-3-0"></span>Figure 2: Affinity is a model-sensitive measure of distribution shift. Contours indicate lines of equal (a) Affinity, or (b) KL Divergence between the joint distribution of the original data and targets and the shifted data. The two axes indicate the actual shifts that define the augmentation. Affinity captures modeldependent features, such as the decision boundary.

In Fig. [2](#page-3-0) we illustrate Affinity with a two-class classification task on a mixture of two Gaussians. Augmentation in this example comprises shift of the means of the Gaussians of the validation data compared to those used for training. Under this shift, we calculate both Affinity and KL divergence of the shifted data with respect to the original data. Affinity changes only when the shift in the data is with respect to the model's decision boundary, whereas the KL divergence changes even when data is shifted in the direction that is irrelevant to the classification task. In this way, Affinity captures what is relevant to a model: shifts that impact predictions.

This same metric has been used as a measure of a model's robustness to image corruptions that do not change images' semantic content [\[20,](#page-10-0) [44–](#page-11-4)[48\]](#page-11-5). Here we, turn this around and use it to quantify the shift of augmented data compared to clean data.

Affinity has the following advantages as a metric:

- 1. It is easy to measure. It requires only clean training of the model in question.
- 2. It is independent of any confounding interaction between the data augmentation and the training process, since augmentation is only used on the validation set and applied statically.
- 3. It is a measure of distance sensitive to properties of both the data distribution *and* the model.

We gain confidence in this metric by comparing it to other potential model-dependant measures of distribution shift. We consider the mean log likelihood of augmented test images[\[49\]](#page-11-6), and the Watanabe–Akaike information criterion (WAIC) [\[50\]](#page-11-7). These other metrics have high correlation with Affinity. Details can be found in Sec. [F.](#page-19-0)

#### 3.2 Diversity: A measure of augmentation complexity

Inspired by the observation that multi-factor augmentation policies such as FlipLR+Crop+Cutout and RandAugment[\[28\]](#page-10-8) greatly improve performance, we propose another axis on which to view augmentation policies, which we dub *Diversity*. This measure is intended to quantify the intuition that augmentations prevent models from over-fitting by increasing the number of samples in the training set; the importance of this is shown in Sec. [4.3.](#page-6-0)

Based on the intuition that more diverse data should be more difficult for a model to fit, we propose a model-based measure. The Diversity metric in this paper is the final training loss of a model trained with a given augmentation:

Definition 2. *Let* a *be an augmentation and* D<sup>0</sup> *train be the augmented training data resulting from applying the augmentation,* a*, stochastically. Further, let* L*train be the training loss for a model,* m*, trained on* D<sup>0</sup> *train. We define the Diversity,* D[a; m; D*train*]*, as*

$$
\mathcal{D}[a; m; D_{train}] := \mathbb{E}_{D'_{train}}[L_{train}] \tag{2}
$$

Though determining the training loss requires the same amount of work as determining final test accuracy, here we focus on this metric as a tool for understanding.

As with Affinity, this definition of Diversity has the advantage that it can capture model-dependent elements, i.e. it is informed by the class of priors implicit in choosing a model and optimization scheme as well as by the stopping criterion used in training.

Another potential diversity measure is the entropy of the transformed data, DEnt. This is inspired by the intuition that augmentations with more degrees of freedom perform better. For discrete transformations, we consider the conditional entropy of the augmented data.

> DEnt := H(X<sup>0</sup> |X) = −E<sup>X</sup> [Σx<sup>0</sup>p(x 0 |X) log(p(x 0 |X))] .

Here x ∈ X is a clean training image and x <sup>0</sup> ∈ X<sup>0</sup> is an augmented image. This measure has the property that it can be evaluated without any training or reference to model architecture. However, the appropriate entropy for continuously-varying transforms is less straightforward.

A third proxy for Diversity is the training time needed for a model to reach a given training accuracy threshold. In Sec. [E,](#page-18-0) we show that these three metrics correlate well with each other.

In the remaining sections we describe how the complementary metrics of Diversity and Affinity can be used to characterize and understand augmentation performance.

### 4 Results

#### 4.1 Augmentation performance is determined by both Affinity and Diversity

Despite the original inspiration to mimic realistic transformations and minimize distribution shift, many state-of-the-art augmentations yield unrealistic images. This suggests that distribution shift alone does not fully describe or predict augmentation performance.

<span id="page-4-0"></span>![](_page_4_Figure_13.jpeg)

**Caption:** Figure 3 demonstrates that augmentation performance is influenced by both Affinity and Diversity. Test accuracy is plotted against these metrics for CIFAR-10 and ImageNet, revealing that neither metric alone predicts performance. Successful augmentations often exhibit low Affinity yet high test accuracy, emphasizing the need to optimize both metrics for effective augmentation strategies.

<span id="page-4-2"></span><span id="page-4-1"></span>Figure 3: Augmentation performance is determined by both Affinity and Diversity. (a) Test accuracy plotted against each of Affinity and Diversity for the two datasets, showing that neither metric alone predicts performance. In the CIFAR-10 plots (top), blue highlights (also in inset) are the augmentations that increase test accuracy above the clean baseline. Dashed lines indicate the clean baseline. (b) and (c) show test accuracy on the color scale in the plane of Affinity and Diversity. The three star markers in (b) are (left to right) RandAugment, AutoAugment, and mixup. The \* on the color bar indicates the clean baseline case. For fixed values of Affinity, test accuracy generally increases with higher values of Diversity. For fixed values of Diversity, test accuracy generally increases with higher values of Affinity. Note that the gains observed on ImageNet are expected to be small, in line with previous work on single-transformation policies [\[48\]](#page-11-5).

Figure [3\(a\)](#page-4-0) (left) measures Affinity across 204 different augmentations for CIFAR-10 and 223 for ImageNet respectively. We find that for the most important augmentations—those that help performance—Affinity is a poor predictor of accuracy. Furthermore, we find many successful augmentations with low Affinity. For example, Rotate(fixed, 45deg, 50%), Cutout(16), and combinations of FlipLR, Crop(32), and Cutout(16) all have Affinity< −15% and test accuracy> 2% above clean baseline on CIFAR-10. Augmentation details are in Sec. [B.](#page-13-1)

As Affinity does not fully characterize the performance of an augmentation, we seek another metric. To assess the importance of an augmentation's complexity, we measure Diversity across the same set of augmentations. We find that Diversity is complementary in explaining how augmentations can increase test performance. As shown in Fig. [3\(b\)](#page-4-1) and [\(c\),](#page-4-2) Affinity and Diversity together provide a much clearer parameterization of an augmentation policy's benefit to performance. For a fixed level of Diversity, augmentations with higher Affinity are consistently better. Similarly, for a fixed Affinity, it is generally better to have higher Diversity.

A simple case study is presented in Fig. [4.](#page-5-0) The probability of the transform Rotate(fixed, 60deg) is varied. The accuracy and Affinity are not monotonically related, with the peak accuracy falling at an intermediate value of Affinity. Similarly, accuracy is correlated with Diversity for low probability transformations, but does not track for higher probabilities. The optimal probability for Rotate(fixed, 60deg) lies at an intermediate value of Affinity and Diversity.

![](_page_5_Figure_3.jpeg)

**Caption:** Figure 4 shows how test accuracy varies with the probability of applying the Rotate(fixed, 60deg) augmentation on CIFAR-10. As probability increases, Affinity decreases linearly, while accuracy changes non-monotonically, peaking at intermediate Affinity values. This indicates a complex relationship between augmentation probability, Affinity, and model performance.

<span id="page-5-0"></span>Figure 4: Test accuracy varies differently than either Affinity or Diversity. Here, the probability of Rotate(fixed, 60deg) on CIFAR-10 is varied from 10% to 90%. Left: as probability increases, Affinity decreases linearly while the accuracy changes non-monotonically. Center: accuracy and Diversity vary differently from each other as probability is changed. Right: test accuracy is maximized at intermediate values.

To situate the tested augmentations—mostly single transforms—within the context of the state-of-theart, we tested three high-performance augmentations from literature: mixup [\[4\]](#page-9-2), AutoAugment [\[6\]](#page-9-4), and RandAugment [\[28\]](#page-10-8). These are highlighted with a star marker in Fig. [3\(b\).](#page-4-1)

More than either of the metrics alone, Affinity and Diversity together provide a useful parameterization of an augmentation's performance. We now turn to investigating the utility of this tool for explaining other observed phenomena of data augmentations.

#### 4.2 Turning augmentations off may adjust Affinity, Diversity, and performance

The term "regularizer" is ill-defined in the literature, often referring to any technique used to reduce generalization error without necessarily reducing training error [\[51\]](#page-11-8). With this definition, it is widely acknowledged that commonly-used augmentations act as regularizers [\[52](#page-11-9)[–54\]](#page-11-10). Though this is a broad definition, we notice another commonality across seemingly different kinds of regularizers: various regularization techniques yield boosts in performance (or at least no degradation) if the regularization is *turned off* at the right time during training. For instance:

- 1. Decaying a large learning rate on an appropriate schedule can be better than maintaining a large learning rate throughout training [\[14\]](#page-9-14).
- 2. Turning off `<sup>2</sup> regularization at the right time in training does not hurt performance [\[55\]](#page-11-11).
- 3. Relaxing architectural constraints mid-training can boost final performance [\[56\]](#page-11-12).
- 4. Turning augmentations off and fine-tuning on clean data can improve final test accuracy [\[35\]](#page-10-12).

To further study augmentation as a regularizer, we compare the constant augmentation case (with the same augmentation throughout) to the case where the augmentation is turned off partway through training and training is completed with clean data. For each transform, we test over a range of switchoff points and select the one that yields the best final validation or test accuracy on CIFAR-10 and

ImageNet respectively. The Switch-off Lift is the resulting increase in final test accuracy, compared to training with augmented data the entire time.

<span id="page-6-2"></span><span id="page-6-1"></span>![](_page_6_Figure_1.jpeg)

**Caption:** Figure 5 illustrates the 'slingshot effect' of switching off regularizers during training. It shows how turning off augmentations can boost validation accuracy, with examples demonstrating that this strategy can improve performance significantly. The results suggest that timing in regularization can enhance model training efficiency and final accuracy.

<span id="page-6-4"></span><span id="page-6-3"></span>Figure 5: [\(a\)](#page-6-1) Switching off regularizers yields a performance boost: Three examples of how turning off a regularizer increases the validation accuracy. This slingshot effect can speed up training and improve the best validation accuracy. Top: training with no augmentation (clean baseline), compared to constant augmentation, and augmentation that is turned off at 55k steps. Here, the augmentation is Rotate(fixed, 20deg,100%). Middle: Baseline with constant `2. This is compared to turning off `<sup>2</sup> regularization part way through training. Bottom: Constant learning rate of 0.1 compared to training where the learning rate is decayed in one step by a factor of 10. [\(b\)](#page-6-2) Bad augmentations can become helpful if switched off: Colored lines connect the test accuracy with augmentation applied throughout training (top) to the test accuracy with switching mid-training. Color indicates the amount of Switch-off Lift; blue is positive and orange is negative. [\(c\)](#page-6-3) Switch-off Lift varies with Affinity and Diversity. Where Switch-off Lift is negative, it is mapped to 0 on the color scale.

For some poor-performing augmentations, this gain can actually bring the final test accuracy above the baseline, as shown in Fig. [5\(b\).](#page-6-2) We additionally observe (Fig. [5\(a\)\)](#page-6-1) that this test accuracy improvement can happen quite rapidly for both augmentations and for the other regularizers tested. This suggests an opportunity to accelerate training without hurting performance by appropriately switching off regularization. We call this a *slingshot* effect.

Interestingly, we find the best time for turning off an augmentation is not always close to the end of training, contrary to what is shown in He et al. [\[35\]](#page-10-12). For example, without switching, FlipUD(100%) decreases test accuracy by almost 50% compared to clean baseline. When the augmentation is used for only the first third of training, final test accuracy is above the baseline.

He et al. [\[35\]](#page-10-12) hypothesized that the gain from turning augmentation off is due to recovery from a distribution shift. Indeed, for many detrimental transformations, the test accuracy gained by turning off the augmentation merely recovers the clean baseline performance. However, in Fig. [5\(c\),](#page-6-3) we see that for a given value of Affinity, Switch-off Lift can vary. This result suggests that the Switch-off Lift is derived from more than simply correction of a distribution shift.

A few of the tested augmentations, such as FlipLR(100%), are fully deterministic. Thus, each time an image is drawn in training, it is augmented the same way. When such an augmentation is turned off partway through training, the model then sees images—the clean ones—that are now new. Indeed, when FlipLR(100%) is switched off at the right time, its final test accuracy exceeds that of FlipLR(50%) without switching. In this way, switching augmentation off may adjust for not only low Affinity but also low Diversity.

#### <span id="page-6-0"></span>4.3 Increased effective training set size is crucial for data augmentation

Most augmentations we tested and those used in practice have inherent stochasticity and thus may alter a given training image differently each time the image is drawn. In the typical *dynamic* training mode, these augmentations increase the number of unique inputs seen across training epochs.

To further study how augmentations act as regularizers, we seek to discriminate this increase in effective dataset size from other effects. We train models with *static* augmentation, as described

in Sec. [3.](#page-2-1) This altered training set is used without further modification during training so that the number of unique training inputs is the same between the augmented and the clean training settings.

For almost all tested augmentations, using static augmentation yields lower test accuracy than the clean baseline. Where static augmentation shows a gain (versions of crop), the difference is less than the standard error on the mean. As in the dynamic case, poorer performance in the static case is for transforms that have lower Affinity and lower Diversity.

Static augmentations also always perform worse than their non-deterministic, dynamic counterparts, as shown in Fig. [6.](#page-7-0) This may be because the Diversity of a static augmentation is always less than the dynamic case (see also Sec. [E\)](#page-18-0). The decrease in Diversity in the static case suggests a connection between Diversity and the number of training examples.

![](_page_7_Figure_3.jpeg)

**Caption:** Figure 6 compares the performance of static versus dynamic augmentations on CIFAR-10. It shows that static augmentations consistently yield lower test accuracy than the clean baseline and dynamic counterparts. The results indicate that dynamic augmentations enhance diversity and effective training set size, which are crucial for improving model performance.

<span id="page-7-0"></span>Figure 6: Static augmentations decrease diversity and performance. CIFAR-10, static augmentation performance is less than the clean baseline, (0, 0), and less than the dynamic augmentation case. Augmentations with no stochasticity are excluded because they are trivially equal on the two axes (left). Diversity in the static case is less than in the dynamic case.(right) Diagonal line indicates where static and dynamic cases would be equal.

Together, these results point to the following conclusion: *Increased effective training set size is crucial to the performance benefit of data augmentation. An augmentation's Affinity and Diversity inform how useful the additional training examples are.*

### 5 Discussion

In this work, we focused on single transforms in an attempt to understand the essential parts of augmentation in a controlled context. This builds a foundation for using these metrics to quantify and design more complex and powerful combinations of augmentations.

Though earlier work has often explicitly focused on just one of these metrics, chosen priors have implicitly ensured reasonable values for both. One way to achieve Diversity is to use combinations of many single augmentations, as in AutoAugment [\[6\]](#page-9-4). Because transforms and hyperparameters in Cubuk et al. [\[6\]](#page-9-4) were chosen by optimizing performance on proxy tasks, the optimal policies include high and low Affinity transforms. Fast AutoAugment [\[27\]](#page-10-7), CTAugment [\[29,](#page-10-9) [57\]](#page-12-0), and differentiable RandAugment [\[28\]](#page-10-8) all aim to increase Affinity by what Lim et al. [\[27\]](#page-10-7) called "density-matching". However these methods use the search space of AutoAugment and thus inherit its Diversity.

On the other hand, Adversarial AutoAugment [\[7\]](#page-9-5) focused on increasing Diversity by optimizing policies to increase the training loss. While this method did not explicitly aim to increase Affinity, it also used transforms and hyperparameters from the AutoAugment search space. Without such a prior, which includes useful Affinity, the goal of maximizing training loss with no other constraints would lead to data augmentation policies that erase all the information from the images.

Our results motivate casting an even wider net when searching for augmentation strategies. Firstly, our work suggests that explicitly optimizing along axes of both Affinity and Diversity yields better performance. Furthermore, we have seen that poor-performing augmentations can actually be helpful if turned off during training (Fig. [5\)](#page-6-4). With inclusion of scheduling in augmentation optimization, we expect there are opportunities for including a different set of augmentations in an ideal policy. Ho et al. [\[26\]](#page-10-6) observes trends in how probability and magnitude of various transforms change during training for an optimized augmentation schedule. We suggest that with further study, Diversity and Affinity can provide priors for optimization of augmentation schedules.

# 6 Conclusion

We attempted to quantify common intuition that more in-distribution and more diverse augmentation policies perform well. To this end, we introduced two easy-to-compute metrics, Affinity and Diversity, intended to measure to what extent a given augmentation is in-distribution and how complex the augmentation is to learn. Because they are model-dependent, these metrics capture the data shifts that affect model performance.

With these tools, we have conducted a study over a large class of augmentations for CIFAR-10 and ImageNet and found that neither feature alone is a perfect predictor of performance. Rather, we presented evidence that Diversity and Affinity play dual roles in determining augmentation quality. Optimizing for either metric separately is sub-optimal and the best augmentations balance the two.

Additionally, we found that an increased number of training examples, connected to Diversity, was a necessary ingredient of beneficial augmentation.

Finally, we found that augmentations share an important feature with other regularizers: switching off regularization at the right time can improve performance. In some cases, this can cause an otherwise poorly-performing augmentation to be beneficial.

We hope our findings provide a foundation for continued scientific study of data augmentation.

# 7 Broader Impact

#### Data augmentation has the potential to amplify bias

Data augmentation takes a smaller, potentially biased training set and recycles this as the basis of a larger augmented training program. A central finding of this work is that the success of an augmentation policy varies with the dual metrics of Affinity and Diversity; as Affinity is explicitly model-dependent, it depends on biases present in the model. This data reuse and model-dependence of successful augmentation suggest the possibility that augmentation may amplify biases in the data or model and warrants future investigation.

#### Robust data augmentation can reduce social, environmental, and financial costs

At its best, data augmentation provides a means for less well-funded or data-rich practitioners to design performant models by supplementing a smaller training data set with additional transformed images. Commonly-used policies, however, such as those found by AutoAugment [\[6\]](#page-9-4) have relied on expensive brute force searches which cost thousands of GPU-hours, replacing the need for extensive data collections with the need for financially and environmentally expensive compute. We hope that by understanding the mechanisms behind successful data augmentation we can design guided augmentation policies for new datasets and models, and mitigate the social and financial costs of data collection without undue compute expense.

### Fundamental understanding facilitates impact assessment

More broadly, the central aim of this work is to better understand the elements driving successful augmentation policies. Truly understanding the conceptual mechanisms at play is crucial in making informed judgements about the impact of data augmentation.

# Acknowledgements

The authors would like to thank Alex Alemi, Justin Gilmer, Guy Gur-Ari, Albin Jones, Behnam Neyshabur, Zan Armstrong, and Ben Poole for thoughtful discussions on this work.

# References

<span id="page-8-0"></span>[1] J. R. Bellegarda, P. V. de Souza, A. J. Nadas, D. Nahamoo, M. A. Picheny, and L. R. Bahl. Robust speaker adaptation using a piecewise linear acoustic mapping. In *ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing*, 1992.

- <span id="page-9-0"></span>[2] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. *arXiv preprint arXiv:1708.04552*, 2017.
- <span id="page-9-1"></span>[3] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition. *arXiv preprint arXiv:1904.08779*, 2019.
- <span id="page-9-2"></span>[4] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. *arXiv preprint arXiv:1710.09412*, 2017.
- <span id="page-9-3"></span>[5] Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, and Ekin D Cubuk. Improving robustness without sacrificing accuracy with patch gaussian augmentation. *arXiv preprint arXiv:1906.02611*, 2019.
- <span id="page-9-4"></span>[6] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. *arXiv preprint arXiv:1805.09501*, 2018.
- <span id="page-9-5"></span>[7] Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial autoaugment, 2019.
- <span id="page-9-6"></span>[8] Yoshua Bengio, Frédéric Bastien, Arnaud Bergeron, Nicolas Boulanger-Lewandowski, Thomas Breuel, Youssouf Chherawala, Moustapha Cisse, Myriam Côté, Dumitru Erhan, Jeremy Eustache, et al. Deep learners benefit more from out-of-distribution examples. In *Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics*, pages 164–172, 2011.
- <span id="page-9-7"></span>[9] Dan Ciregan, Ueli Meier, and Jürgen Schmidhuber. Multi-column deep neural networks for image classification. In *Proceedings of IEEE Conference on Computer Vision and Pattern Recognition*, pages 3642–3649. IEEE, 2012.
- [10] Ikuro Sato, Hiroki Nishimura, and Kensuke Yokoi. Apac: Augmented pattern classification with neural networks. *arXiv preprint arXiv:1505.03229*, 2015.
- [11] Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural networks applied to visual document analysis. In *Proceedings of International Conference on Document Analysis and Recognition*, 2003.
- <span id="page-9-8"></span>[12] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In *International Conference on Machine Learning*, pages 1058–1066, 2013.
- <span id="page-9-9"></span>[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In *Advances in Neural Information Processing Systems*, 2012.
- <span id="page-9-14"></span>[14] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In *British Machine Vision Conference*, 2016.
- <span id="page-9-10"></span>[15] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In *Proceedings of IEEE Conference on Computer Vision and Pattern Recognition*, 2017.
- <span id="page-9-11"></span>[16] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In *European conference on computer vision*, pages 21–37. Springer, 2016.
- <span id="page-9-12"></span>[17] Debidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut, paste and learn: Surprisingly easy synthesis for instance detection. In *Proceedings of the IEEE International Conference on Computer Vision*, pages 1301–1310, 2017.
- [18] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting instance segmentation via probability map guided copy-pasting. In *Proceedings of the IEEE International Conference on Computer Vision*, pages 682–691, 2019.
- <span id="page-9-13"></span>[19] Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang, Yuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Alsharif, Patrick Nguyen, et al. Starnet: Targeted computation for object detection in point clouds. *arXiv preprint arXiv:1908.11069*, 2019.
- <span id="page-10-0"></span>[20] Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk. Adversarial examples are a natural consequence of test error in noise. *arXiv preprint arXiv:1901.10513*, 2019.
- <span id="page-10-1"></span>[21] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. *The journal of machine learning research*, 15(1):1929–1958, 2014.
- <span id="page-10-2"></span>[22] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. *arXiv preprint arXiv:1708.04896*, 2017.
- <span id="page-10-3"></span>[23] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. *arXiv preprint arXiv:1312.6199*, 2013.
- <span id="page-10-4"></span>[24] Hiroshi Inoue. Data augmentation by pairing samples for images classification. *arXiv preprint arXiv:1801.02929*, 2018.
- <span id="page-10-5"></span>[25] Alexander J Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher Ré. Learning to compose domain-specific transformations for data augmentation. In *Advances in Neural Information Processing Systems*, pages 3239–3249, 2017.
- <span id="page-10-6"></span>[26] Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi Chen. Population based augmentation: Efficient learning of augmentation policy schedules. *arXiv preprint arXiv:1905.05393*, 2019.
- <span id="page-10-7"></span>[27] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. *arXiv preprint arXiv:1905.00397*, 2019.
- <span id="page-10-8"></span>[28] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical data augmentation with no separate search. *arXiv preprint arXiv:1909.13719*, 2019.
- <span id="page-10-9"></span>[29] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. *arXiv preprint arXiv:1911.09785*, 2019.
- [30] Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. *arXiv preprint arXiv:1905.11946*, 2019.
- [31] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. *arXiv preprint arXiv:1911.09070*, 2019.
- [32] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan Yuille, and Quoc V Le. Adversarial examples improve image recognition. *arXiv preprint arXiv:1911.09665*, 2019.
- <span id="page-10-10"></span>[33] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation. *arXiv preprint arXiv:1904.12848*, 2019.
- <span id="page-10-11"></span>[34] Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster autoaugment: Learning augmentation strategies using backpropagation, 2019.
- <span id="page-10-12"></span>[35] Zhuoxun He, Lingxi Xie, Xin Chen, Ya Zhang, Yanfeng Wang, and Qi Tian. Data augmentation revisited: Rethinking the distribution gap between clean and augmented data, 2019.
- <span id="page-10-13"></span>[36] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution discrepancy, 2019.
- <span id="page-10-14"></span>[37] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, *Advances in Neural Information Processing Systems 27*, pages 2672–2680. Curran Associates, Inc., 2014. URL <http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf>.
- [38] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun, editors, *ICLR*, 2014. URL [http://dblp.uni-trier.de/db/conf/iclr/](http://dblp.uni-trier.de/db/conf/iclr/iclr2014.html#KingmaW13) [iclr2014.html#KingmaW13](http://dblp.uni-trier.de/db/conf/iclr/iclr2014.html#KingmaW13).
- [39] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization, 2016.
- <span id="page-11-0"></span>[40] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. *Journal of the American Statistical Association*, 112(518):859–877, Feb 2017. ISSN 1537-274X. doi: 10.1080/01621459.2017.1285773. URL [http://dx.doi.org/10.](http://dx.doi.org/10.1080/01621459.2017.1285773) [1080/01621459.2017.1285773](http://dx.doi.org/10.1080/01621459.2017.1285773).
- <span id="page-11-1"></span>[41] Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, and Ian Reid. A bayesian data augmentation approach for learning deep models. In *Advances in Neural Information Processing Systems*, pages 2794–2803, 2017.
- <span id="page-11-2"></span>[42] Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Russ R Salakhutdinov. Good semi-supervised learning that requires a bad gan. In *Advances in neural information processing systems*, pages 6510–6520, 2017.
- <span id="page-11-3"></span>[43] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 770–778, 2016.
- <span id="page-11-4"></span>[44] Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small image transformations? *arXiv preprint arXiv:1805.12177*, 2018.
- [45] Samuel Dodge and Lina Karam. A study and comparison of human and deep learning recognition performance under visual distortions. In *2017 26th international conference on computer communication and networks (ICCCN)*, pages 1–7. IEEE, 2017.
- [46] Dan Hendrycks and Thomas G Dietterich. Benchmarking neural network robustness to common corruptions and surface variations. *arXiv preprint arXiv:1807.01697*, 2018.
- [47] Amir Rosenfeld, Richard Zemel, and John K Tsotsos. The elephant in the room. *arXiv preprint arXiv:1808.03305*, 2018.
- <span id="page-11-5"></span>[48] Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D Cubuk, and Justin Gilmer. A fourier perspective on model robustness in computer vision. *arXiv preprint arXiv:1906.08988*, 2019.
- <span id="page-11-6"></span>[49] Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one, 2019.
- <span id="page-11-7"></span>[50] Sumio Watanabe. Asymptotic equivalence of bayes cross validation and widely applicable information criterion in singular learning theory. *J. Mach. Learn. Res.*, 11:3571–3594, December 2010. ISSN 1532-4435.
- <span id="page-11-8"></span>[51] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. *Deep Learning*. MIT Press, 2016. <http://www.deeplearningbook.org>.
- <span id="page-11-9"></span>[52] Alex Hernández-García and Peter König. Further advantages of data augmentation on convolutional neural networks. In *International Conference on Artificial Neural Networks*, pages 95–103. Springer, 2018.
- [53] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. *arXiv preprint arXiv:1611.03530*, 2016.
- <span id="page-11-10"></span>[54] Tri Dao, Albert Gu, Alexander J Ratner, Virginia Smith, Christopher De Sa, and Christopher Ré. A kernel theory of modern data augmentation. *Proceedings of machine learning research*, 97:1528, 2019.
- <span id="page-11-11"></span>[55] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence, 2019.
- <span id="page-11-12"></span>[56] Stéphane d'Ascoli, Levent Sagun, Joan Bruna, and Giulio Biroli. Finding the needle in the haystack with convolutions: on the benefits of architectural bias, 2019.

<span id="page-12-0"></span>[57] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. *arXiv preprint arXiv:2001.07685*, 2020.

# SUPPLEMENTARY MATERIAL

# <span id="page-13-0"></span>A Training methods

Cifar10 models were trained using code based on AutoAugment code[3](#page-13-2) using the following choices:

- 1. Learning rate was decayed following a cosine decay schedule, starting with a value of 0.1
- 2. 78050 training steps were used, with data shuffled after every epoch.
- 3. As implemented in the AutoAugment code, the WRN-28-2 model was used with stochastic gradient descent and momentum. The optimizer used cross entropy loss with `<sup>2</sup> weight decay of 0.0005.
- 4. Before selecting the validation set, the full training set was shuffled and balanced such that the subset selected for training was balanced across classes.
- 5. Validation set was the last 5000 samples of the shuffled CIFAR-10 training data.
- 6. Models were trained using Python 2.7 and TensorFlow 1.13 .

A training time of 78k steps was chosen because it showed reasonable convergence with the standard data augmentation of FlipLR, Crop, and Cutout In the clean baseline case, test accuracy actually reached its peak much earlier than 78k steps.

With CIFAR-10, experiments were also performed for training dataset sizes of 1024, 4096, and 16384. At smaller dataset sizes, the impact of augmentation and the Switch-off Lift tended to be larger. These results are not shown in this paper.

ImageNet models were ResNet-50 trained using the Cloud TPU codebase[4](#page-13-3) . Models were trained for 112.6k steps with a weight decay rate of 1e-4, and a learning rate of 0.2, which was decayed by 10 at epochs 30, 60, and 80. Batch size was set to be 1024.

# <span id="page-13-1"></span>B Details of augmentation

## B.1 CIFAR-10

<span id="page-13-2"></span>3

On CIFAR-10, both color and affine transforms were tested, as given in the full results (see Sec. [G\)](#page-19-1). Most augmentations were as defined in Cubuk et al. [\[6\]](#page-9-4) and additional conventions for augmentations as labeled in Fig. [7](#page-14-0) are defined here. For Rotate, *fixed* means each augmented image was rotated by exactly the stated amount, with a randomly-chosen direction. *Variable* means an augmented image was rotated a random amount up to the given value in a randomly-chosen direction. Shear is defined similarly. Rotate(square) means that an image was rotated by an amount chosen randomly from [0◦ , 90◦ , 180◦ , 270◦ ].

Crop included a padding before the random-location crop so that the final image remained 32 × 32 in size. The magnitude given for Crop is the number of pixels that were added in each dimension. The magnitude given in the label for Cutout is the size, in pixels, of each dimension of the square cutout.

PatchGaussian was defined as in Lopes et al. [\[5\]](#page-9-3), with the patch specified to be contained entirely within the image domain. In Fig. [7,](#page-14-0) it is labeled by two hyperparameters: the size of the square patch (in pixels) that was applied and σmax, which is the maximum standard deviation of the noise that could be selected for any given patch. Here, "fixed" means the patch size was always the same.

Since FlipLR, Crop, and Cutout are part of standard pipelines for CIFAR-10, we tested combinations of the three augmentations (varying probabilities of each) as well as these three augmentations plus an single additional augmentation. As in standard processing of CIFAR-10 images, the first augmentation applied was anything that is not one of FlipLR, Crop, or Cutout. After that, augmentations were applied in the order Crop, then FlipLR, then Cutout.

Finally, we tested the CIFAR-10 AutoAugment policy [\[6\]](#page-9-4), RandAugment [\[28\]](#page-10-8), and mixup [\[4\]](#page-9-2). The hyperparameters for these augmentations followed the guidelines described in the respective papers.

available at github.com/tensorflow/models/tree/master/research/autoaugment

<span id="page-13-3"></span><sup>4</sup> available at https://github.com/tensorflow/tpu/tree/master/models/official/resnet

These augmentations are labeled in Fig. [7.](#page-14-0)

![](_page_14_Figure_1.jpeg)

**Caption:** Figure 7 presents a labeled map of tested augmentations on CIFAR-10, plotted in the Affinity-Diversity plane. Different colors represent various hyperparameters for each transform. This visualization aids in understanding how different augmentations relate to their performance metrics, providing insights into effective augmentation strategies.

<span id="page-14-0"></span>Figure 7: CIFAR-10: Labeled map of tested augmentations on the plane of Affinity and Diversity. Color distinguishes different hyperparameters for a given transform. Legend is below.

### B.2 ImageNet

On ImageNet, we experimented with PatchGaussian, Cutout, operations from the PIL imaging library[5](#page-14-1) , and techniques from the AutoAugment code, as described above for CIFAR-10. In addition to PatchGaussian(fixed), we also tested PatchGaussian(variable), where the patch size was uniformly sampled up to a maximum size. The implementation here did not constrain the patch to be

<span id="page-14-1"></span><sup>5</sup> <https://pillow.readthedocs.io/en/5.1.x/>

| Clean<br>Autoaugment                                                     | Invert(100%)<br>Invert(50%)                                 | Equalize(100%)<br>Equalize(50%)                                                             |
|--------------------------------------------------------------------------|-------------------------------------------------------------|---------------------------------------------------------------------------------------------|
| Mixup                                                                    | ShearX(variable, 0.1, 100%)                                 | FlipLR(50%) + Crop(4,100%)                                                                  |
| Randaug                                                                  | ShearX(variable, 0.1, 50%)                                  | FlipLR(100%) + Crop(4,100%) + Cutout(16,100%)                                               |
| PatchGaussian(fixed,12, 0.1, 100%)                                       | ShearX(variable, 0.1, 75%)                                  | FlipLR(100%) + Crop(4,100%) + Cutout(16,25%)                                                |
| PatchGaussian(fixed,12, 0.2, 100%)                                       | ShearX(variable, 0.3, 100%)                                 | FlipLR(100%) + Crop(4,100%) + Cutout(16,50%)                                                |
| PatchGaussian(fixed,12, 0.3, 100%)                                       | ShearX(variable, 0.3, 50%)                                  | FlipLR(100%) + Crop(4,100%) + Cutout(16,75%)                                                |
| PatchGaussian(fixed,12, 0.5, 100%)                                       | ShearX(variable, 0.3, 75%)                                  | FlipLR(100%) + Crop(4,25%) + Cutout(16,100%)                                                |
| PatchGaussian(fixed,12, 0.8, 100%)                                       | ShearX(fixed ,0.1, 100%)                                    | FlipLR(100%) + Crop(4,25%) + Cutout(16,25%)                                                 |
| PatchGaussian(fixed,12, 1.0, 100%)                                       | ShearX(fixed, 0.1, 50%)                                     | FlipLR(100%) + Crop(4,25%) + Cutout(16,50%)                                                 |
| PatchGaussian(fixed,12, 1.5, 100%)                                       | ShearX(fixed, 0.1, 75%)                                     | FlipLR(100%) + Crop(4,25%) + Cutout(16,75%)                                                 |
| PatchGaussian(fixed,12, 2.0, 100%)                                       | ShearX(fixed, 0.3, 100%)                                    | FlipLR(100%) + Crop(4,50%) + Cutout(16,100%)                                                |
| PatchGaussian(fixed,16, 0.1, 100%)                                       | ShearX(fixed, 0.3, 50%)                                     | FlipLR(100%) + Crop(4,50%) + Cutout(16,25%)                                                 |
| PatchGaussian(fixed,16, 0.2, 100%)                                       | ShearX(fixed, 0.3, 75%)                                     | FlipLR(100%) + Crop(4,50%) + Cutout(16,50%)                                                 |
| PatchGaussian(fixed,16, 0.3, 100%)                                       | Rotate(variable, 20deg, 100%)                               | FlipLR(100%) + Crop(4,50%) + Cutout(16,75%)                                                 |
| PatchGaussian(fixed,16, 0.5, 100%)                                       | Rotate(variable, 20deg, 50%)                                | FlipLR(100%) + Crop(4,75%) + Cutout(16,100%)                                                |
| PatchGaussian(fixed,16, 0.8, 100%)                                       | Rotate(variable, 20deg, 75%)                                | FlipLR(100%) + Crop(4,75%) + Cutout(16,25%)                                                 |
| PatchGaussian(fixed,16, 1.0, 100%)                                       | Rotate(variable, 45, 100%)                                  | FlipLR(100%) + Crop(4,75%) + Cutout(16,50%)                                                 |
| PatchGaussian(fixed,16, 1.5, 100%)<br>PatchGaussian(fixed,16, 2.0, 100%) | Rotate(variable, 5deg, 100%)<br>Rotate(variable, 5deg, 50%) | FlipLR(100%) + Crop(4,75%) + Cutout(16,75%)<br>FlipLR(25%) + Crop(4,100%) + Cutout(16,100%) |
| PatchGaussian(fixed,20, 0.1, 100%)                                       | Rotate(variable, 5deg, 75%)                                 | FlipLR(25%) + Crop(4,100%) + Cutout(16,25%)                                                 |
| PatchGaussian(fixed,20, 0.2, 100%)                                       | Rotate(variable, 60deg, 100%)                               | FlipLR(25%) + Crop(4,100%) + Cutout(16,50%)                                                 |
| PatchGaussian(fixed,20, 0.3, 100%)                                       | Rotate(fixed, 15deg, 50%)                                   | FlipLR(25%) + Crop(4,100%) + Cutout(16,75%)                                                 |
| PatchGaussian(fixed,20, 0.5, 100%)                                       | Rotate(fixed, 20deg, 100%)                                  | FlipLR(25%) + Crop(4,25%) + Cutout(16,100%)                                                 |
| PatchGaussian(fixed,20, 0.8, 100%)                                       | Rotate(fixed, 20deg, 50%)                                   | FlipLR(25%) + Crop(4,25%) + Cutout(16,25%)                                                  |
| PatchGaussian(fixed,20, 1.0, 100%)                                       | Rotate(fixed, 20deg, 75%)                                   | FlipLR(25%) + Crop(4,25%) + Cutout(16,50%)                                                  |
| PatchGaussian(fixed,20, 1.5, 100%)                                       | Rotate(fixed, 45deg, 50%)                                   | FlipLR(25%) + Crop(4,25%) + Cutout(16,75%)                                                  |
| PatchGaussian(fixed,20, 2.0, 100%)                                       | Rotate(fixed, 5deg, 10%)                                    | FlipLR(25%) + Crop(4,50%) + Cutout(16,100%)                                                 |
| PatchGaussian(fixed,24, 0.1, 100%)                                       | Rotate(fixed, 5deg, 100%)                                   | FlipLR(25%) + Crop(4,50%) + Cutout(16,25%)                                                  |
| PatchGaussian(fixed,24, 0.2, 100%)                                       | Rotate(fixed, 5deg, 20%)                                    | FlipLR(25%) + Crop(4,50%) + Cutout(16,50%)                                                  |
| PatchGaussian(fixed,24, 0.3, 100%)                                       | Rotate(fixed, 5deg, 30%)                                    | FlipLR(25%) + Crop(4,50%) + Cutout(16,75%)                                                  |
| PatchGaussian(fixed,24, 0.5, 100%)                                       | Rotate(fixed, 5deg, 40%)                                    | FlipLR(25%) + Crop(4,75%) + Cutout(16,100%)                                                 |
| PatchGaussian(fixed,24, 0.8, 100%)                                       | Rotate(fixed, 5deg, 50%)                                    | FlipLR(25%) + Crop(4,75%) + Cutout(16,25%)                                                  |
| PatchGaussian(fixed,24, 1.0, 100%)                                       | Rotate(fixed, 5deg, 60%)                                    | FlipLR(25%) + Crop(4,75%) + Cutout(16,50%)                                                  |
| PatchGaussian(fixed,24, 1.5, 100%)                                       | Rotate(fixed, 5deg, 70%)                                    | FlipLR(25%) + Crop(4,75%) + Cutout(16,75%)                                                  |
| PatchGaussian(fixed,24, 2.0, 100%)                                       | Rotate(fixed, 5deg, 75%)                                    | FlipLR(50%) + Crop(4,100%) + Cutout(16,100%)                                                |
| PatchGaussian(fixed,28, 0.1, 100%)                                       | Rotate(fixed, 5deg, 80%)                                    | FlipLR(50%) + Crop(4,100%) + Cutout(16,25%)                                                 |
| PatchGaussian(fixed,28, 0.2, 100%)<br>PatchGaussian(fixed,28, 0.3, 100%) | Rotate(fixed, 5deg, 90%)<br>Rotate(fixed, 60deg, 10%)       | FlipLR(50%) + Crop(4,100%) + Cutout(16,50%)<br>FlipLR(50%) + Crop(4,100%) + Cutout(16,75%)  |
| PatchGaussian(fixed,28, 0.5, 100%)                                       | Rotate(fixed, 60deg, 100%)                                  | FlipLR(50%) + Crop(4,25%) + Cutout(16,100%)                                                 |
| PatchGaussian(fixed,28, 0.8, 100%)                                       | Rotate(fixed, 60deg, 20%)                                   | FlipLR(50%) + Crop(4,25%) + Cutout(16,25%)                                                  |
| PatchGaussian(fixed,28, 1.0, 100%)                                       | Rotate(fixed, 60deg, 30%)                                   | FlipLR(50%) + Crop(4,25%) + Cutout(16,50%)                                                  |
| PatchGaussian(fixed,28, 1.5, 100%)                                       | Rotate(fixed, 60deg, 40%)                                   | FlipLR(50%) + Crop(4,25%) + Cutout(16,75%)                                                  |
| PatchGaussian(fixed,28, 2.0, 100%)                                       | Rotate(fixed, 60deg, 50%)                                   | FlipLR(50%) + Crop(4,50%) + Cutout(16,100%)                                                 |
| PatchGaussian(fixed,32, 0.1, 100%)                                       | Rotate(fixed, 60deg, 60%)                                   | FlipLR(50%) + Crop(4,50%) + Cutout(16,25%)                                                  |
| PatchGaussian(fixed,32, 0.2, 100%)                                       | Rotate(fixed, 60deg, 70%)                                   | FlipLR(50%) + Crop(4,50%) + Cutout(16,50%)                                                  |
| PatchGaussian(fixed,32, 0.3, 100%)                                       | Rotate(fixed, 60deg, 80%)                                   | FlipLR(50%) + Crop(4,50%) + Cutout(16,75%)                                                  |
| PatchGaussian(fixed,32, 0.5, 100%)                                       | Rotate(fixed, 60deg, 90%)                                   | FlipLR(50%) + Crop(4,75%) + Cutout(16,100%)                                                 |
| PatchGaussian(fixed,32, 0.8, 100%)                                       | Rotate(square, 100%)                                        | FlipLR(50%) + Crop(4,75%) + Cutout(16,25%)                                                  |
| PatchGaussian(fixed,32, 1.0, 100%)                                       | Rotate(square, 50%)                                         | FlipLR(50%) + Crop(4,75%) + Cutout(16,50%)                                                  |
| PatchGaussian(fixed,32, 1.5, 100%)                                       | Blur(100%)                                                  | FlipLR(50%) + Crop(4,75%) + Cutout(16,75%)                                                  |
| PatchGaussian(fixed,32, 2.0, 100%)                                       | Blur(50%)                                                   | FlipLR(75%) + Crop(4,100%) + Cutout(16,100%)                                                |
| PatchGaussian(fixed,4, 0.1, 100%)                                        | FlipLR(100%)                                                | FlipLR(75%) + Crop(4,100%) + Cutout(16,25%)                                                 |
| PatchGaussian(fixed,4, 0.2, 100%)                                        | FlipLR(25%)                                                 | FlipLR(75%) + Crop(4,100%) + Cutout(16,50%)                                                 |
| PatchGaussian(fixed,4, 0.3, 100%)                                        | FlipLR(50%)                                                 | FlipLR(75%) + Crop(4,100%) + Cutout(16,75%)                                                 |
| PatchGaussian(fixed,4, 0.5, 100%)                                        | FlipLR(75%)                                                 | FlipLR(75%) + Crop(4,25%) + Cutout(16,100%)                                                 |
| PatchGaussian(fixed,4, 0.8, 100%)<br>PatchGaussian(fixed,4, 1.0, 100%)   | FlipUD(100%)<br>FlipUD(25%)                                 | FlipLR(75%) + Crop(4,25%) + Cutout(16,25%)<br>FlipLR(75%) + Crop(4,25%) + Cutout(16,50%)    |
| PatchGaussian(fixed,4, 1.5, 100%)                                        | FlipUD(50%)                                                 | FlipLR(75%) + Crop(4,25%) + Cutout(16,75%)                                                  |
| PatchGaussian(fixed,4, 2.0, 100%)                                        | FlipUD(75%)                                                 | FlipLR(75%) + Crop(4,50%) + Cutout(16,100%)                                                 |
| PatchGaussian(fixed,8, 0.1, 100%)                                        | Crop(4, 25%)                                                | FlipLR(75%) + Crop(4,50%) + Cutout(16,25%)                                                  |
| PatchGaussian(fixed,8, 0.2, 100%)                                        | Crop(4, 50%)                                                | FlipLR(75%) + Crop(4,50%) + Cutout(16,50%)                                                  |
| PatchGaussian(fixed,8, 0.3, 100%)                                        | Crop(4, 75%)                                                | FlipLR(75%) + Crop(4,50%) + Cutout(16,75%)                                                  |
| PatchGaussian(fixed,8, 0.5, 100%)                                        | Crop(4,100%)                                                | FlipLR(75%) + Crop(4,75%) + Cutout(16,100%)                                                 |
| PatchGaussian(fixed,8, 0.8, 100%)                                        | Cutout(16, 100%)                                            | FlipLR(75%) + Crop(4,75%) + Cutout(16,25%)                                                  |
| PatchGaussian(fixed,8, 1.0, 100%)                                        | Cutout(16, 25%)                                             | FlipLR(75%) + Crop(4,75%) + Cutout(16,50%)                                                  |
| PatchGaussian(fixed,8, 1.5, 100%)                                        | Cutout(16, 50%)                                             | FlipLR(75%) + Crop(4,75%) + Cutout(16,75%)                                                  |
| PatchGaussian(fixed,8, 2.0, 100%)                                        | Cutout(16, 75%)                                             | FlipLR(50%) + Crop(4, 100%) + Cutout(16, 100%) + Equalize(50%)                              |
|                                                                          |                                                             | FlipLR(50%) + Crop(4, 100%) + Cutout(16, 100%) + Rotate(fixed, 15deg, 50%)                  |

entirely contained within the image. Additionally, we experimented with SolarizeAdd. SolarizeAdd is similar to Solarize from the PIL library, but has an additional hyperparameter which determines how much value was added to each pixel that is below the threshold. Finally, we also experimented with Full Gaussian and Random Erasing on ImageNet. Full Gaussian adds Gaussian noise to the whole image. Random Erasing is similar to Cutout, but randomly samples the values of the pixels in the patch [\[22\]](#page-10-2) (whereas Cutout sets them to a constant, gray pixel).

These augmentations are labeled in Fig. [8.](#page-16-1)

We note that the gains on ImageNet are expected to be small. This is in-line with the magnitude of the gains observed by related works with single transformations [\[48\]](#page-11-5). While combinations of transformations can lead to bigger improvements [\[28\]](#page-10-8), our focus is on understanding single augmentations as a foundation for future work on their combinations.

![](_page_16_Figure_0.jpeg)

**Caption:** Figure 8 displays a labeled map of tested augmentations on ImageNet, similar to Figure 7 for CIFAR-10. The color coding indicates different hyperparameters, facilitating the analysis of how augmentations perform in terms of Affinity and Diversity. This map serves as a reference for evaluating augmentation strategies across datasets.

<span id="page-16-1"></span>Figure 8: ImageNet: Labeled map of tested augmentations on the plane of Affinity and Diversity. Color distinguishes different hyperparameters for a given transform. Legend is below.

Each augmentation was applied with a certain probability (given as a percentage in the label). Each time an image was pulled for training, the given image was augmented with that probability.

### <span id="page-16-0"></span>C Error analysis

All of the CIFAR-10 experiments were repeated with 10 different initialization. In most cases, the resulting standard error on the mean (SEM) is too small to show as error bars on plots. The error on each measurement is given in the full results (see Sec. [G\)](#page-19-1).

| Clean                                    | Random Erasing(variable, 448, 100%) | Brightness(0.1, 100%)         |
|------------------------------------------|-------------------------------------|-------------------------------|
|                                          |                                     |                               |
| Patch Gaussian(variable, 100, 0.2, 100%) | Random Erasing(fixed, 120, 100%)    | Brightness(0.2, 100%)         |
| Patch Gaussian(variable, 100, 0.5, 100%) | Random Erasing(fixed, 150, 100%)    | Brightness(0.3, 100%)         |
| Patch Gaussian(variable, 100, 0.8, 100%) | Random Erasing(fixed, 180, 100%)    | Brightness(0.4, 100%)         |
|                                          |                                     |                               |
| Patch Gaussian(variable, 100, 1.0, 100%) | Random Erasing(fixed, 30, 100%)     | Brightness(0.5, 100%)         |
| Patch Gaussian(variable, 100, 2.0, 100%) | Random Erasing(fixed, 60, 100%)     | Brightness(0.6, 100%)         |
| Patch Gaussian(variable, 150, 0.2, 100%) | Random Erasing(fixed, 90, 100%)     | Brightness(0.7, 100%)         |
|                                          |                                     |                               |
| Patch Gaussian(variable, 150, 0.5, 100%) | Solarize(0, 100%)                   | Color(0.1, 100%)              |
| Patch Gaussian(variable, 150, 0.8, 100%) | Solarize(100, 100%)                 | Color(0.2, 100%)              |
| Patch Gaussian(variable, 150, 1.0, 100%) | Solarize(150, 100%)                 | Color(0.3, 100%)              |
|                                          |                                     |                               |
| Patch Gaussian(variable, 150, 2.0, 100%) | Solarize(200, 100%)                 | Color(0.4, 100%)              |
| Patch Gaussian(variable, 200, 0.2, 100%) | Solarize(250, 100%)                 | Color(0.5, 100%)              |
|                                          |                                     |                               |
| Patch Gaussian(variable, 200, 0.5, 100%) | Solarize(50, 100%)                  | Color(0.6, 100%)              |
| Patch Gaussian(variable, 200, 0.8, 100%) | Solarize Add(-002, 000, 100%)       | Color(0.7, 100%)              |
| Patch Gaussian(variable, 200, 1.0, 100%) | Solarize Add(-002, 050, 100%)       | Contrast(0.1, 100%)           |
|                                          |                                     |                               |
| Patch Gaussian(variable, 200, 2.0, 100%) | Solarize Add(-002, 100, 100%)       | Contrast(0.2, 100%)           |
| Patch Gaussian(variable, 250, 0.1, 100%) | Solarize Add(-002, 150, 100%)       | Contrast(0.3, 100%)           |
| Patch Gaussian(variable, 250, 0.2, 100%) | Solarize Add(-002, 200, 100%)       | Contrast(0.4, 100%)           |
|                                          |                                     |                               |
| Patch Gaussian(variable, 250, 0.3, 100%) | Solarize Add(-002, 250, 100%)       | Contrast(0.5, 100%)           |
| Patch Gaussian(variable, 250, 0.5, 100%) | Solarize Add(-027, 000, 100%)       | Contrast(0.6, 100%)           |
| Patch Gaussian(variable, 250, 0.8, 100%) | Solarize Add(-027, 050, 100%)       | Contrast(0.7, 100%)           |
|                                          |                                     |                               |
| Patch Gaussian(variable, 250, 1.0, 100%) | Solarize Add(-027, 100, 100%)       | Cutout(variable, 448, 100%)   |
| Patch Gaussian(variable, 250, 1.5, 100%) | Solarize Add(-027, 150, 100%)       | Cutout(fixed, 120, 100%)      |
| Patch Gaussian(variable, 250, 2.0, 100%) | Solarize Add(-027, 200, 100%)       | Cutout(fixed, 150, 100%)      |
|                                          |                                     |                               |
| Patch Gaussian(variable, 300, 0.2, 100%) | Solarize Add(-027, 250, 100%)       | Cutout(fixed, 180, 100%)      |
| Patch Gaussian(variable, 300, 0.5, 100%) | Solarize Add(-052, 000, 100%)       | Cutout(fixed, 30, 100%)       |
| Patch Gaussian(variable, 300, 0.8, 100%) | Solarize Add(-052, 050, 100%)       | Cutout(fixed, 60, 100%)       |
|                                          |                                     |                               |
| Patch Gaussian(variable, 300, 1.0, 100%) | Solarize Add(-052, 100, 100%)       | Cutout(fixed, 90, 100%)       |
| Patch Gaussian(variable, 300, 2.0, 100%) | Solarize Add(-052, 150, 100%)       | FullGaussian(0.1, 100%)       |
| Patch Gaussian(variable, 350, 0.2, 100%) | Solarize Add(-052, 200, 100%)       | FullGaussian(0.2, 100%)       |
|                                          |                                     |                               |
| Patch Gaussian(variable, 350, 0.5, 100%) | Solarize Add(-052, 250, 100%)       | FullGaussian(0.3, 100%)       |
| Patch Gaussian(variable, 350, 0.8, 100%) | Solarize Add(-077, 000, 100%)       | FullGaussian(0.5, 100%)       |
| Patch Gaussian(variable, 350, 1.0, 100%) | Solarize Add(-077, 050, 100%)       | FullGaussian(0.8, 100%)       |
|                                          |                                     |                               |
| Patch Gaussian(variable, 350, 2.0, 100%) | Solarize Add(-077, 100, 100%)       | FullGaussian(1.0, 100%)       |
| Patch Gaussian(variable, 400, 0.2, 100%) | Solarize Add(-077, 150, 100%)       | FullGaussian(1.5, 100%)       |
| Patch Gaussian(variable, 400, 0.5, 100%) | Solarize Add(-077, 200, 100%)       | FullGaussian(2.0, 100%)       |
|                                          |                                     |                               |
| Patch Gaussian(variable, 400, 0.8, 100%) | Solarize Add(-077, 250, 100%)       | Rotate(square, 100%)          |
| Patch Gaussian(variable, 400, 1.0, 100%) | Solarize Add(-102, 000, 100%)       | Posterize(0, 100%)            |
| Patch Gaussian(variable, 400, 2.0, 100%) | Solarize Add(-102, 050, 100%)       | Posterize(1, 100%)            |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 100, 0.0, 100%)    | Solarize Add(-102, 100, 100%)       | Posterize(2, 100%)            |
| Patch Gaussian(fixed, 100, 0.2, 100%)    | Solarize Add(-102, 150, 100%)       | Posterize(3, 100%)            |
| Patch Gaussian(fixed, 100, 0.5, 100%)    | Solarize Add(-102, 200, 100%)       | Posterize(4, 100%)            |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 100, 0.8, 100%)    | Solarize Add(-102, 250, 100%)       | Posterize(5, 100%)            |
| Patch Gaussian(fixed, 100, 1.0, 100%)    | Solarize Add(-127, 000, 100%)       | Posterize(6, 100%)            |
| Patch Gaussian(fixed, 100, 2.0, 100%)    | Solarize Add(-127, 050, 100%)       | Posterize(7, 100%)            |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 150, 0.2, 100%)    | Solarize Add(-127, 100, 100%)       | Rotate(variable, 0deg, 100%)  |
| Patch Gaussian(fixed, 150, 0.5, 100%)    | Solarize Add(-127, 150, 100%)       | Rotate(variable, 10deg, 100%) |
| Patch Gaussian(fixed, 150, 0.8, 100%)    | Solarize Add(-127, 200, 100%)       | Rotate(variable, 15deg, 100%) |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 150, 1.0, 100%)    | Solarize Add(-127, 250, 100%)       | Rotate(variable, 20deg, 100%) |
| Patch Gaussian(fixed, 150, 2.0, 100%)    | Solarize Add(0023, 050, 100%)       | Rotate(variable, 25deg, 100%) |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 200, 0.2, 100%)    | Solarize Add(0023, 100, 100%)       | Rotate(variable, 30deg, 100%) |
| Patch Gaussian(fixed, 200, 0.5, 100%)    | Solarize Add(0023, 150, 100%)       | Rotate(variable, 5deg, 100%)  |
| Patch Gaussian(fixed, 200, 0.8, 100%)    | Solarize Add(0023, 200, 100%)       | Sharpness(0.1, 100%)          |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 200, 1.0, 100%)    | Solarize Add(0023, 250, 100%)       | Sharpness(0.2, 100%)          |
| Patch Gaussian(fixed, 200, 2.0, 100%)    | Solarize Add(0048, 050, 100%)       | Sharpness(0.3, 100%)          |
| Patch Gaussian(fixed, 250, 0.2, 100%)    | Solarize Add(0048, 100, 100%)       | Sharpness(0.4, 100%)          |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 250, 0.5, 100%)    | Solarize Add(0048, 150, 100%)       | Sharpness(0.5, 100%)          |
| Patch Gaussian(fixed, 250, 0.8, 100%)    | Solarize Add(0048, 200, 100%)       | Sharpness(0.6, 100%)          |
| Patch Gaussian(fixed, 250, 1.0, 100%)    | Solarize Add(0048, 250, 100%)       | Sharpness(0.7, 100%)          |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 250, 2.0, 100%)    | Solarize Add(0073, 100, 100%)       | ShearX(variable, 0.1, 100%)   |
| Patch Gaussian(fixed, 300, 0.2, 100%)    | Solarize Add(0073, 150, 100%)       | ShearX(variable, 0.2, 100%)   |
| Patch Gaussian(fixed, 300, 0.5, 100%)    | Solarize Add(0073, 200, 100%)       | ShearX(variable, 0.3, 100%)   |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 300, 0.8, 100%)    | Solarize Add(0073, 250, 100%)       | ShearX(variable, 0.4, 100%)   |
| Patch Gaussian(fixed, 300, 1.0, 100%)    | Solarize Add(0098, 100, 100%)       | ShearX(variable, 0.5, 100%)   |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 300, 2.0, 100%)    | Solarize Add(0098, 150, 100%)       | TranslateX(0, 100%)           |
| Patch Gaussian(fixed, 350, 0.2, 100%)    | Solarize Add(0098, 200, 100%)       | TranslateX(10, 100%)          |
| Patch Gaussian(fixed, 350, 0.5, 100%)    | Solarize Add(0098, 250, 100%)       | TranslateX(20, 100%)          |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 350, 0.8, 100%)    | Solarize Add(0123, 150, 100%)       | TranslateX(30, 100%)          |
| Patch Gaussian(fixed, 350, 1.0, 100%)    | Solarize Add(0123, 200, 100%)       | TranslateX(40, 100%)          |
| Patch Gaussian(fixed, 350, 2.0, 100%)    | Solarize Add(0123, 250, 100%)       | TranslateX(50, 100%)          |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 400, 0.2, 100%)    | Invert(100%)                        | TranslateX(60, 100%)          |
| Patch Gaussian(fixed, 400, 0.5, 100%)    | AutoContrast(100%)                  | TranslateX(70, 100%)          |
| Patch Gaussian(fixed, 400, 0.8, 100%)    | Equalize(100%)                      | TranslateX(80, 100%)          |
|                                          |                                     |                               |
| Patch Gaussian(fixed, 400, 1.0, 100%)    | FlipUD(100%)                        | TranslateX(90, 100%)          |
| Patch Gaussian(fixed, 400, 2.0, 100%)    |                                     |                               |

Affinity and Switch-off Lift both were computed from differences between runs that share the same initialization. For Affinity, the same trained model was used for inference on clean validation data and on augmented validation data. Thus, the variance of Affinity for the clean baseline is not independent of the variance of Affinity for a given augmentation. The difference between the augmentation case and the clean baseline case was taken on a per-experiment basis (for each initialization of the clean baseline model) before the error was computed.

In the switching experiments, the final training *without* augmentation was completed starting from a given checkpoint in the model that was trained *with* augmentation. Thus, each switching experiment shared an initialization with an experiment that had no switching. Again, in this case the difference was taken on a per-experiment basis before the error (based on the standard deviation) was computed. All ImageNet experiments shown are with one initialization. Thus, there are not statistics from which to analyze the error.

### D Switching off augmentations

For CIFAR-10, switching times were tested in increments of approximately 5k steps between ∼ 25k and ∼ 75k steps. The best point for switching was determined by the final validation accuracy.

On ImageNet, we tested turning augmentation off at 50, 60, 70, and 80 epochs. Total training took 90 epochs. The best point for switching was determined by the final test accuracy.

The Switch-off Lift was derived from the experiment at the best switch-off point for each augmentation.

For CIFAR-10, there are some augmentations where the validation accuracy was best at 25k, which means that further testing is needed to find if the actual optimum switch off point is lower or if the best case is to not train at all with the given augmentation. Some of the best augmentations have a small negative Switch-off Lift, indicating that it is better to train the entire time with the given augmentations.

For each augmentation, the best time for switch-off is listed in the full results (see Sec. [G\)](#page-19-1).

### <span id="page-18-0"></span>E Diversity metrics

![](_page_18_Figure_8.jpeg)

**Caption:** Figure 9 correlates three diversity metrics for CIFAR-10 augmentations, showing strong relationships among entropy, final training loss, and training steps to accuracy threshold. This correlation suggests that higher entropy augmentations lead to better model performance, reinforcing the importance of diversity in data augmentation strategies.

<span id="page-18-1"></span>Figure 9: CIFAR-10: Three different diversity metrics are strongly correlated for high entropy augmentations. Here, the entropy is calculated only for discrete augmentations.

We computed three possible diversity metrics, shown in Fig. [9:](#page-18-1) Entropy, Final Training Loss, Training Steps to Accuracy Threshold. The entropy was calculated only for augmentations that have a discrete stochasticity (such as Rotate(fixed) and not for augmentations that have a continuous variation (such as Rotate(variable) or PatchGaussian). Final Training Loss is the batch statistic at the last step of training. For CIFAR-10 experiments, this was averaged across the 10 initializations. For ImageNet, it was averaged over the last 10 steps of training. Training Steps to Accuracy Threshold is the number of training steps at which the training accuracy first hits a threshold of 97%. A few of the tested augmentation (extreme versions of PatchGaussian) did not reach this threshold in the given time and that column is left blank in the full results.

Entropy is unique in that it is independent of the model or data set and it is a counting of states. However, it is difficult to compare between discrete and continuously-varying transforms and it is not clear how proper it is to compare even across different types of transforms.

Final Training Loss and Training Steps to Accuracy Threshold correlate well across the tested transforms. Entropy is highly correlated to these measures for PatchGaussian and versions of FlipLR, Crop, and Cutout where only probabilities are varying. For Rotate and Shear where magnitudes are varying as well, the correlation between Entropy and the other two measures is less clear.

Building intuition for what Diversity means in this case, the Final Training Loss was compared in the case of static augmentation to the case of dynamic augmentation. As shown in Fig. [6,](#page-7-0) in the case of static augmentation, the Diversity was always less than in the typical case of dynamic augmentation. Moreover, across this large range of augmentations, the numerical span of Diversity was very small in

the case of static augmentation, compared to dynamic augmentation. This suggests that this particular measure of Diversity is indeed connected to the number of unique or useful training images that can be created with a given augmentation. In the case of static augmentation, the number of unique images is exactly the same for all augmentations; dynamic augmentations allow for more unique images and both the number and utility of unique images will vary with augmentation.

### <span id="page-19-0"></span>F Comparing Affinity to other related measures

We gain confidence in the Affinity measure by comparing it to other potential model-dependant measures of distribution shift. In Fig [10,](#page-19-2) we show the correlation between Affinity and these two measures: the mean log likelihood of augmented test images[\[49\]](#page-11-6) (labeled as "logsumexp(logits)") and the Watanabe–Akaike information criterion (labeled as "WAIC") [\[50\]](#page-11-7).

Like Affinity, these other two measures indicate how well a model trained on clean data comprehends augmented data.

![](_page_19_Figure_4.jpeg)

**Caption:** Figure 10 illustrates the correlation between Affinity and two other measures of distribution shift: logsumexp of logits and WAIC. Both measures reflect how well a model trained on clean data understands augmented data. The plots indicate that Affinity is a reliable metric for assessing the relationship between augmented images and model performance.

<span id="page-19-2"></span>Figure 10: Affinity correlates with two other measures of how augmented images are related to a trained model's distribution: logsumexp of the logits (left, for CIFAR-10, and right, for ImageNet) is the mean log likelihood for the image. WAIC (middle, for CIFAR-10) corrects for a possible bias in that estimate. In all three plots, numbers are referenced to the clean baseline, which is assigned a value of 0.

## <span id="page-19-1"></span>G Full results

The plotted data for CIFAR-10 and ImageNet are given in .csv files uploaded at [https://storage.](https://storage.googleapis.com/public_research_data/augmentation/data.zip) [googleapis.com/public\\_research\\_data/augmentation/data.zip](https://storage.googleapis.com/public_research_data/augmentation/data.zip). In these .csv files, blank cells generally indicate that a given experiment (such as switching) was not done for the specified augmentation. In the case of the training accuracy threshold as a proxy for diversity, a blank cell indicates that for the given augmentation, the training accuracy did not reach the specified threshold during training.