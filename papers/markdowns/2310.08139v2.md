# DUALAUG: EXPLOITING ADDITIONAL HEAVY AUG-MENTATION WITH OOD DATA REJECTION

Zehao Wang<sup>1</sup> , Yiwen Guo<sup>2</sup> , Qizhang Li<sup>1</sup> , Guanglei Yang1<sup>∗</sup> , Wangmeng Zuo<sup>1</sup>

<sup>1</sup>Harbin Institute of Technology <sup>2</sup> Independent Researcher

zehaowang0704@gmail.com guoyiwen89@gmail.com liqizhang95@gmail.com yangguanglei@hit.edu.cn wmzuo@hit.edu.cn

# ABSTRACT

Data augmentation is a dominant method for reducing model overfitting and improving generalization. Most existing data augmentation methods tend to find a compromise in augmenting the data, *i.e.*, increasing the amplitude of augmentation carefully to avoid degrading some data too much and doing harm to the model performance. We delve into the relationship between data augmentation and model performance, revealing that the performance drop with heavy augmentation comes from the presence of out-of-distribution (OOD) data. Nonetheless, as the same data transformation has different effects for different training samples, even for heavy augmentation, there remains part of in-distribution data which is beneficial to model training. Based on the observation, we propose a novel data augmentation method, named DualAug, to keep the augmentation in distribution as much as possible at a reasonable time and computational cost. We design a data mixing strategy to fuse augmented data from both the basic- and the heavy-augmentation branches. Extensive experiments on supervised image classification benchmarks show that DualAug improve various automated data augmentation method. Moreover, the experiments on semi-supervised learning and contrastive self-supervised learning demonstrate that our DualAug can also improve related method. Code is available at [https://github.com/shuguang99/DualAug.](https://github.com/shuguang99/DualAug)

### 1 INTRODUCTION

Deep neural networks lead to advances in various computer vision tasks, such as image classification [\(Krizhevsky et al., 2012;](#page-10-0) [He et al., 2016;](#page-9-0) [Dosovitskiy et al., 2021\)](#page-9-1), object detection [\(Girshick et al., 2014;](#page-9-2) [Ren](#page-10-1) [et al., 2015;](#page-10-1) [He et al., 2017\)](#page-9-3), and semantic segmentation [\(Chen et al., 2017\)](#page-9-4). The training of deep neural networks often relies on data augmentation to relieve overfitting, including recent automated data augmentations which increases the amount and diversity of data by transforming it following some policies [\(Cubuk et al., 2019;](#page-9-5) [2020;](#page-9-6) [Lim et al., 2019;](#page-10-2) [Zheng et al., 2022\)](#page-11-0). Applying it normally requires searching for appropriate transformation operators, range of magnitudes, *etc.* Although a moderate level of data transformation can improve model accuracy, heavy augmentation which significantly increases the data diversity sometimes destroys semantic context in the aug-

<sup>∗</sup>Corresponding Author.

<span id="page-0-0"></span>![](_page_0_Figure_10.jpeg)

**Caption:** Figure 1 illustrates the performance of RandAugment (RA) on CIFAR-100 using WideResNet-28-2, showing a peak at moderate transformations before declining with excessive augmentation. Filtering out out-of-distribution (OOD) data enhances performance even under heavy augmentation, indicating the importance of OOD management in data augmentation strategies.

Figure 1: Red Line: The performance of RandAugment (RA) peaks with a carefully selected moderate number of transformations, but it rapidly declines if the number is increased further. Green Line: After simply filtering out OOD data, the performance of RA further improves even with heavy augmentation. The experiment is conducted on CIFAR-100 using WideResNet-28-2.

arXiv:2310.08139v2 [cs.CV] 16 Oct 2023

<span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)

**Caption:** Figure 2 depicts the motivation behind DualAug, illustrating the limitations of current augmentation methods. It highlights how heavy augmentation can lead to OOD data, and proposes a strategy to convert OOD samples back to in-distribution, thereby improving the overall augmentation process and model training.

Figure 2: Motivation of DualAug. *Circle*, *Square* and *Cross* represent different data point. (a) Current automated augmentations with conservative parameters need to keep the data in distribution thus they are insufficient for data like the *Circle* and *Square* in the figure. (b) When heavy augmentation is introduced, the *Cross* become OOD. (c) Modify the OOD data (*i.e.*, *Cross*) into in-distribution data appropriately. (d) Visualization of basic augmentation, which is not sufficiently. (e) Visualization of heavy augmentation, which is more sufficiently (augmented data is in in-distribution Area). (f) Visualization of heavy augmentation (augmented data is in OOD Area)

mented visual data and leads to unsatisfactory performance, as shown in Figure [1](#page-0-0) (heavy augmentation part of the red line) and Figure [2\(](#page-1-0)b).

Despite being somewhat effective, a moderate level of transformation in fact augments the whole dataset conservatively, and many training samples may not be sufficiently augmented. See Figure [2](#page-1-0) for an illustration of this problem. Comparing Figure [2\(](#page-1-0)a) and Figure [2\(](#page-1-0)b), we can see that, when searching for the optimal data augmentation strategy, there exists a trade-off between the diversity of augmentation and the distraction of semantically meaningless augmentations (shown in Figure [2\(](#page-1-0)f)). From our perspective, the samples without meaningful semantic contexts are OOD samples, hence, in this paper, we consider the possibility of improving diversity without generating OOD samples. A direct idea for achieving this is to detect OOD augmentation results and get rid of them. See Figure [2\(](#page-1-0)c) for expected results.

Based on this intuition, we propose dual augmentation (DualAug), which applies heavy data augmentation while keeping the in-distribution augmentation data as much as possible. DualAug consists of two different data augmentation branches: one is the basic data augmentation branch which is the same as existing augmentation methods [\(Cubuk et al., 2019;](#page-9-5) [2020;](#page-9-6) [Zheng et al., 2022\)](#page-11-0), and the other is for heavy augmentation (*i.e.*,more types or larger magnitudes of transformations). We take advantage of the training model for building the OOD detector and we calculate the OOD score according to an estimation of the distribution of augmented data in the basic branch. The threshold for filtering out OOD augmented data is chosen by utilizing the 3σ rule of thumb. Meanwhile, augmented data that does not meet the 3σ rule, from the heavy augmentation branch, will be regarded as OOD. These OOD samples are caused by excessive transformation, and they will be replaced with their basic augmentation version to keep the augmentation results in distribution as much as possible. Finally, the in-distribution heavy augmentation results together with some basic augmentation results (which are replacements of the OOD results) will be used in model training.

To summarize, the contribution of this paper is as follows:

- We demonstrate that, although heavy augmentation destroys semantic contexts in some data, there still exists augmentation results which are informative enough and can be beneficial to model training.
- We investigate the existence of OOD augmentation result, and we show that, to achieve better performance, it is important to filter OOD data as much as possible when using heavy augmentation.
- Based on our observation, we propose a two-branch data augmentation framework called DualAug. DualAug makes data further augmented while avoiding the production of OOD data as much as possible.

• Extensive experiments on image classification tasks with four datasets prove that DualAug improve various data augmentation methods. Furthermore, DualAug can also improve the performance of semi-supervised learning and contrastive self-supervised learning.

### 2 RELATED WORK

#### 2.1 AUTOMATED DATA AUGMENTATION

Over the past few years, data augmentation and automated data augmentation have developed rapidly [\(Mumuni & Mumuni, 2022\)](#page-10-3). A pioneer automated augmentation method is AutoAugment [\(Cubuk et al., 2019\)](#page-9-5), which proposes a well-designed search space and employs reinforcement learning to search the optimal policy. Although it has demonstrated significant performance gains, its high computational cost can pose a significant challenge in certain applications. To address this issue, several different works have been proposed to reduce the computation cost of AutoAugment. Fast AutoAugment (FAA) [\(Lim et al., 2019\)](#page-10-2) considers data augmentation as density matching, which can be efficiently implemented using Bayesian optimization. RandAugment (RA) [\(Cubuk et al., 2020\)](#page-9-6) explores a simplified search space that involves two easily understandable hyper-parameters, which can be optimized with grid search. Recently, Deep AutoAugment (DeepAA) [\(Zheng et al., 2022\)](#page-11-0), a new multi-layer data augmentation search method outperforms all these methods.

Although data augmentation increases the diversity of data, it still faces some problems if not restricted [\(Wei et al., 2020;](#page-11-1) [Lopes et al., 2021;](#page-10-4) [Gong et al., 2021;](#page-9-7) [Gudovskiy et al., 2021;](#page-9-8) [Sinha et al.,](#page-11-2) [2021;](#page-11-2) [Pinto et al., 2022;](#page-10-5) [Suzuki, 2022;](#page-11-3) [Wang et al., 2022;](#page-11-4) [Liu et al., 2023\)](#page-10-6). For instance, [Suzuki](#page-11-3) [\(2022\)](#page-11-3) discovers that adversarial augmentation [\(Zhang et al., 2019\)](#page-11-5) can produce meaningless or difficult-to-recognize images, such as black and noise images, if it is not constrained properly. To address this problem, the proposed method TeachAugment [\(Suzuki, 2022\)](#page-11-3) uses a teacher model to avoid meaningless augmentations. Although TeachAugment makes reasonable improvements, it has a significant drawback: it involves alternative optimization that relies on an extra model, which significantly increases the training complexity. Our method also handles unexpected augmented data, without introducing any extra model.

#### 2.2 OUT-OF-DISTRIBUTION DETECTION

OOD detection is critical in ensuring the safety of machine learning applications [\(Yang et al., 2021\)](#page-11-6). [Hendrycks & Gimpel](#page-10-7) [\(2016\)](#page-10-7) uses the maximum softmax probability (MSP) score to detect OOD examples. In ODIN [\(Liang et al., 2017\)](#page-10-8), the temperature is utilized in the computation of MSP, and it has been demonstrated to be more effective in distinguishing between ID samples and OOD samples. Since then, many studies have been conducted to improve the OOD detection performance in different scenarios and settings [\(Techapanurak et al., 2020;](#page-11-7) [Liu et al., 2020;](#page-10-9) [Sun & Li, 2022\)](#page-11-8).

It is important to note that OOD detection is a broad concept that has been systematically researched and classified by [Yang et al.](#page-11-6) [\(2021\)](#page-11-6). Generalized OOD detection includes sensory anomaly detection, one-class novelty detection, multi-class novelty detection, open set recognition, outlier Detection, *etc.*As the first attempt of adopting OOD detection to data augmentation, our DuagAug uses the most basic method for OOD detection (*i.e.*, MSP).

### 3 METHOD

### 3.1 PRELIMINARIES

An image classification model f parameterized by θ is trained using a training set D = (x<sup>i</sup> , yi) N <sup>i</sup>=1 to correctly classify images. Data augmentation has emerged as a popular technique for addressing the problem of overfitting. The whole data augmentation pipeline can be regarded as a cascade of M individual transformations:

<span id="page-2-0"></span>
$$
\Phi(x, K) = \phi_M \circ \phi_{M-1} \circ \cdots \circ \phi_2 \circ \phi_1 \tag{1}
$$

where K = [κ1, κ<sup>2</sup> · · · κM] is the set of transformation parameters. In Φ(x, K), each individual transformation ϕ is defined as:

$$
\tilde{x} = \phi(x, \kappa) \tag{2}
$$

<span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)

**Caption:** Figure 3 presents an overview of the DualAug framework, featuring two branches: a Basic Augmentation Branch utilizing existing methods like AutoAugment, and a Heavy Augmentation Branch employing more aggressive transformations. The OOD detection mechanism is illustrated, showing how OOD samples are replaced with basic augmentations to maintain data quality.

Figure 3: The overview of the proposed DualAug, which has two branches for data augmentation. (a) Basic Augmentation Branch: an automated augmentation are chose such as AutoAugment [\(Cubuk](#page-9-5) [et al., 2019\)](#page-9-5), RandAugment [\(Cubuk et al., 2020\)](#page-9-6) and DeepAA [\(Zheng et al., 2022\)](#page-11-0). (b) Heavy Augmentation Branch: more aggressive augmentation are used than basic augmentation. (c) OOD Detection: the trained model evaluates S basic and S heavy of two branch. τ is computed by µ(S basic) and σ(S basic). If s heavy <sup>i</sup> ∈ S heavy is smaller than τ , the corresponding heavy augmented data x heavy i is regarded as OOD. (d) Mix of two branch: OOD data in heavy augmentation branch are replaced by corresponding basic augmentation version.

where κ represents the parameter for controling the transformation.

In this work, we propose a two-branch data augmentation framework, DualAug, as shown in Figure [3.](#page-3-0) The two branches are the basic augmentation branch and the heavy augmentation branch. The heavy augmentation branch enriches the data diversity of basic augmentations, while the basic augmented data and the training model are used to detect OOD samples from the heavy augmentation branch and replace them to corresponding basic augmentation version. In the following two subsections, we will introduce the two branches in more details.

#### 3.2 THE BASIC AUGMENTATION BRANCH

We choose a baseline automated augmentation method to build the basic augmentation branch. For the convenience of description, AutoAugment [\(Cubuk et al., 2019\)](#page-9-5) which seeks augmentation policies via reinforcement learning is taken as an example [1](#page-3-1) . Its augmentation pipeline Φ(·, K) consists of 5 sub-groups of augmentation operations, each containing two individual transformations ϕ(·, κ), where κ contains three parameters: 1) the type of transformation, 2) the probability of applying the transformation, and 3) the magnitude of the transformation. For each image, a random sub-group is selected to apply.

The basic augmentation is formulated similar to Eq. [\(1\)](#page-2-0):

<span id="page-3-2"></span>
$$
\tilde{x}_i^{basic} = \Phi(x_i, K^{basic})
$$
\n(3)

where Kbasic denotes the augmentation parameters in the baseline automated augmentation method.

### <span id="page-3-3"></span>3.3 THE HEAVY AUGMENTATION BRANCH

In the heavy augmentation branch, we use more aggressive augmentation parameters Kheavy than in the basic augmentation branch. There are several different ways of realizing heavy augmentation, such as increasing the number of transformations, growing the magnitude of transformations, adding more types of transformations, and enriching their combination. We implement it by increasing

<span id="page-3-1"></span><sup>1</sup>Besides AutoAugment [\(Cubuk et al., 2019\)](#page-9-5), there are a large number of methods to be chosen from, such as RandAugment [\(Cubuk et al., 2020\)](#page-9-6), Fast AutoAugment [\(Lim et al., 2019\)](#page-10-2) and Deep Autoaugment [\(Zheng et al.,](#page-11-0) [2022\)](#page-11-0),*etc.*

the number of transformations due to its convenience of being applied to automated augmentation methods.

Like the basic augmentation, heavy augmentation can be formualted as:

<span id="page-4-4"></span>
$$
\tilde{x}_i^{heavy} = \Phi(x_i, K^{heavy})
$$
\n(4)

and we can rewrite it as follows:

$$
\Phi(\cdot, K^{heavy}) = \Phi(\cdot, K^{basic}) \circ \Phi(\cdot, K^{extra})
$$
\n(5)

where Kbasic is the same as in Eq. [\(3\)](#page-3-2) and Kextra denotes the extra parameters. The effectiveness of different heavy augmentation implementations is discussed in Section [4.4.](#page-7-0)

#### <span id="page-4-3"></span>3.4 OOD DETECTION AND THE MIX OF TWO BRANCHES

Following [Hendrycks & Gimpel](#page-10-7) [\(2016\)](#page-10-7); [Liang et al.](#page-10-8) [\(2017\)](#page-10-8), MSP (with temperature) is adopted as a score to decide whether an augmented sample x˜ ∈ X˜ is in-distribution or OOD data. The model training on the fly is used for calculating the probabilities, and an apparent advantage of using it instead of a pre-trained model is the saving of memory and computation. The OOD score of x˜<sup>i</sup> can be obtained by

<span id="page-4-5"></span>
$$
s_i = \max_{j \in \{1, ..., C\}} \frac{\exp(f_\theta^j(\tilde{x}_i)/T)}{\sum_{c=1}^C \exp(f_\theta^c(\tilde{x}_i)/T)},
$$
(6)

where f j θ (˜xi) represents the logit of j-th class and T represents the temperature. Therefore, we have s basic i for x˜ basic i and s heavy i for x˜ heavy i , respectively.

Samples with high scores are classified as in-distribution samples, and samples showing lower scores are considered as OOD. Let Sbasic = {s basic i } be the set which collects all scores of the augmented data from the basic branch, we know that most elements of Sbasic are in-distribution, based on which we simply assume all elements of Sbasic form an Gaussian distribution and set the threshold for filtering OOD data following the 3σ rule of thumb. That is, we use

<span id="page-4-2"></span>
$$
\tau = \mu(S^{basic}) - \lambda \cdot \sigma(S^{basic}) \tag{7}
$$

as the threshold, where λ is a hyper-parameter.

In order not to increase the sample complexity of augmentation, we mix augmented data from the two branches, in the following manner

$$
\tilde{x}_i^{dual} = \begin{cases}\n\tilde{x}_i^{heavy}, & if \ s_i^{heavy} > \tau \\
\tilde{x}_i^{basic}, & otherwise\n\end{cases}
$$
\n(8)

That is, X˜ dual = {x˜ dual i } N <sup>i</sup>=1 is used to train the deep network fθ.

### 4 EXPERIMENTS

In this section, we evaluate DualAug in three different tasks: supervised learning (Section [4.1\)](#page-4-0), semisupervised learning (Section [4.2\)](#page-6-0), and contrastive self-supervised learning (Section [4.2\)](#page-6-1). DualAug effectively enhances the performance of data augmentation in the mentioned tasks. Additionally, we provide an ablation study in Section [4.4.](#page-7-0)

#### <span id="page-4-0"></span>4.1 SUPERVISED LEARNING

#### <span id="page-4-1"></span>4.1.1 EXPERIMENTAL SETTINGS

In our experiments, the augmentation parameters of DualAug include Kbasic and Kextra . Kbasic can choose from various automated data augmentation methods, such as AutoAugment (AA) [\(Cubuk](#page-9-5) [et al., 2019\)](#page-9-5), RandAugment (RA) [\(Cubuk et al., 2020\)](#page-9-6), and Deep AutoAugment (DeepAA) [\(Zheng](#page-11-0) [et al., 2022\)](#page-11-0). We follow the original augmentation parameters or policy of each corresponding method. The extra augmentation include M extra operations, we have Kextra = {κ}<sup>M</sup> <sup>i</sup>=1. We make M a random number and sample it uniformly from the range [1, 10] for CIFAR-10/100 and SVHN-Core, [1, 6] for ImageNet, respectively. For calculating the OOD score and threshold, the temperature T and λ are set to 1000 and 1, respectively. Since the OOD Score of the model is unreliable in the early stage of training, we will only use the basic augmentation branch at the initial training iterations (which is referred to as the warm-up stage of our DualAug). The warm-up stage is set to be the first 20% training epochs in all experiments. Other information including the training batch size, number of train epochs, and the optimizer will be introduced in appendix.

#### 4.1.2 CIFAR-10/100 AND SVHN-CORE

Results. Table [1](#page-5-0) reports the Top-1 test accuracy on CIFAR-10/100 [\(Krizhevsky et al., 2009\)](#page-10-10), and SVHN-Core [\(Netzer et al., 2011\)](#page-10-11) for WideResNet-28-10 and WideResNet-40-2 [\(Zagoruyko &](#page-11-9) [Komodakis, 2016\)](#page-11-9), respectively. When combined with automated data augmentation, DualAug demonstrates superior on all networks and datasets.

It can prove that the parameters proposed by previous automated augmentation methods are tend to being conservative, which becomes an obstacle to further growth in performance. Due to the heavy augmentation branch and effective mix strategy, DualAug further augments data to boost model's performance.

|  |  |  |  | Table 1: Top-1 test accuracy (%) on CIFAR-10, CIFAR-100 and SVHN-Core. Better results in |  |  |  |
|--|--|--|--|------------------------------------------------------------------------------------------|--|--|--|
|  |  |  |  | comparison are shown in bold. Statistics are computed from three runs.                   |  |  |  |

<span id="page-5-0"></span>

| Dataset             | Metric    | AA                         | AA+Ours                    | RA                         | RA+Ours                    | DeepAA      | DeepAA+Ours |
|---------------------|-----------|----------------------------|----------------------------|----------------------------|----------------------------|-------------|-------------|
| CIFAR-10            | WRN-40-2  | 96.34 ± .10                | 96.44 ± .04                | 96.30 ± .14                | 96.48 ± .10                | 96.45 ± .07 | 96.48 ± .08 |
|                     | WRN-28-10 | 97.36 ± .05                | 97.44 ± .09                | 97.13 ± .05                | 97.32 ± .12                | 97.50 ± .08 | 97.59 ± .07 |
| CIFAR-100           | WRN-40-2  | 79.63 ± .26                | 79.83 ± .19                | 78.13 ± .12                | 78.57 ± .07                | 78.41 ± .07 | 78.47 ± .11 |
|                     | WRN-28-10 | 83.04 ± .20                | 83.42 ± .21                | 83.11 ± .26                | 83.60 ± .23                | 84.14 ± .12 | 84.39 ± .21 |
| SVHN-Core* WRN-40-2 | WRN-28-10 | 97.71 ± .07<br>97.76 ± .19 | 98.00 ± .09<br>98.08 ± .11 | 97.95 ± .12<br>97.97 ± .15 | 98.11 ± .03<br>98.15 ± .06 | -<br>-      | -<br>-      |

\* DeepAA does not report its policy on SVHN-Core, thus we do not test our method with it on SVHN-Core.

#### 4.1.3 IMAGENET

Compared method. We compare our DualAug with the following methods: 1) AA [\(Cubuk et al.,](#page-9-5) [2019\)](#page-9-5), 2) RA [\(Cubuk et al., 2020\)](#page-9-6), 3) DeepAA [\(Zheng et al., 2022\)](#page-11-0), 4) TeachAugment [\(Suzuki, 2022\)](#page-11-3), 5) FastAA [\(Lim et al., 2019\)](#page-10-2), 6) Faster AA [\(Hataya et al., 2020\)](#page-9-9), 7) UA [\(LingChen et al., 2020\)](#page-10-12), 8) TA [\(Muller & Hutter, 2021\)](#page-10-13). The code of training AA and RA on ImageNet is not released, thus we ¨ reproduce them using the training settings of DeepAA.

Evaluation protocol. In addition to the standard ImageNet-1K test set, we have chosen to assess the effectiveness of our DualAug using more challenging datasets. These datasets include ImageNet-C [\(Hendrycks & Dietterich, 2019\)](#page-10-14), ImageNet-C¯ [\(Mintun et al., 2021\)](#page-10-15), ImageNet-A [\(Hendrycks et al., 2021\)](#page-10-16), and ImageNet-V2 [\(Recht et al., 2019\)](#page-10-17). It is noted that,

to refrain from potential domain conflicts, we have eliminated contrast and brightness variations from the ImageNet-C dataset. Furthermore, we incorporate the RMS calibration error on ImageNet-1K test set as a metric to evaluate the robustness of models, following previous work [\(Hendrycks et al., 2019\)](#page-10-18).

Result. In Table [2,](#page-5-1) we present the results of DualAug on the ImageNet dataset. By combining DualAug with AA, RA, and DeepAA, we observe an improvement in the performance of all three data augmentation techniques. This illustrates the effectiveness of DualAug on large-scale datasets. Specifically, DualAug with DeepAA (the state-of-the-art automated data augmentation method) achieves the highest top-1 accuracy (78.44%). In Table [3,](#page-6-2) we evaluate DualAug on more challenging ImageNet variant datasets. When combined with DualAug, automated data augmentation show improvements in all of them. This demonstrates that

Table 2: Top-1 test accuracy (%) on ImageNet.

<span id="page-5-1"></span>

| Method        | Accuracy (%) |
|---------------|--------------|
| Default       | 76.40        |
| FastAA*       | 77.60        |
| Faster AA*    | 76.50        |
| UA*           | 77.63        |
| TA*           | 78.04        |
| TeachAugment* | 77.80        |
| AA            | 77.30        |
| AA+Ours       | 77.46        |
| RA            | 76.90        |
| RA+Ours       | 77.25        |
| DeepAA        | 78.30        |
| DeepAA+Ours   | 78.44        |
|               |              |

\* Reported in previous papers.

<span id="page-6-2"></span>Table 3: Result of calibration RMS on ImageNet and accuracy (%) on variants of ImageNet (including ImageNet-C, ImageNet-C¯, ImageNet-A, and ImageNet-V2) when different basic augmentations are combined with our DualAug. Lower is better for the calibration RMS. The result in bold is better in comparison.

| Method      | Calibration RMS (↓) | ImageNet-C | ImageNet-C¯ | ImageNet-A | ImageNet-V2 |
|-------------|---------------------|------------|-------------|------------|-------------|
| AA          | 6.77                | 38.95      | 39.67       | 5.72       | 66.00       |
| AA+Ours     | 6.75                | 40.99      | 40.12       | 6.05       | 65.80       |
| RA          | 7.91                | 38.99      | 38.96       | 4.09       | 65.10       |
| RA+Ours     | 7.65                | 41.67      | 40.44       | 4.87       | 65.30       |
| DeepAA      | 6.03                | 42.82      | 40.94       | 6.57       | 65.80       |
| DeepAA+Ours | 5.59                | 43.85      | 42.08       | 6.83       | 66.20       |

DualAug's effective mixing strategy successfully prevents excessive out-of-distribution (OOD) data from hindering the learning process. Due to its more aggressive augmentations DualAug naturally has an advantage in handling challenging tasks.

Discussion about TeachAugment. Similar to our DualAug in motivation, TeachAugment [\(Suzuki,](#page-11-3) [2022\)](#page-11-3) also considers augmentation data that is meaningless or difficult to recognize in Adversarial AutoAugment [\(Zhang et al., 2019\)](#page-11-5). As shown in Table [2,](#page-5-1) our DualAug outperforms TeachAugment which utilizes a teacher model and requires updating it with less time and memory cost. In addition, DualAug can be combined with a broader range of augmentation methods, while TeachAugment is limited to Adversarial AutoAugment only.

#### <span id="page-6-0"></span>4.2 SEMI-SUPERVISED LEARNING

Semi-supervised learning [\(Yang et al., 2022\)](#page-11-10) improves data efficiency in machine learning by utilizing both labeled data and unlabeled data. A series of semi-supervised learning methods utilize consistency training. Examples of these methods include UDA [\(Xie et al., 2020\)](#page-11-11), FixMatch [\(Sohn et al., 2020\)](#page-11-12), SoftMatch [\(Chen et al., 2023\)](#page-9-10), among others. These methods constraint consistency between the outputs of two different data views, allowing the model to learn features from unlabeled data. Data augmentation plays a crucial role in helping the models learn

<span id="page-6-1"></span>

| Table 4: Semi-supervised learning results on  |
|-----------------------------------------------|
| CIFAR-10 using WideResNet-28-2. "4000         |
| Labels" denotes that 4,000 images have labels |
| while the other 4,6000 do not.<br>Similar for |
| "250 Labels".                                 |

| Method        | 250 Labels  | 4000 Labels |
|---------------|-------------|-------------|
| FixMatch      | 95.04 ± .68 | 95.77 ± .06 |
| FixMatch+Ours | 95.23 ± .45 | 96.10 ± .05 |

invariants of data, guided by some consistency constraints. Therefore, having high-quality data augmentation is of utmost importance in these methods.

In this section, we demonstrate the effectiveness of DualAug in semi-supervised learning by combining it with FixMatch as an example. FixMatch first generates pseudo-labels by using the model's predictions on weakly-augmented unlabeled data. A pseudo-label is only kept if the model makes a highly confident prediction for a given data. The model is then trained to predict the pseudo-label when being fed a strongly-augmented version of the same data. We apply our DualAug only to the strongly-augmented data. The experiment setting of FixMatch is presented in appendix. The setting for DualAug is the same as the one described in Section [4.1.1.](#page-4-1)

As shown in Table [4,](#page-6-1) we present the results of FixMatch with DualAug on CIFAR-10 using 250 and 4000 labelled samples only. The results show that our DualAug consistently improves the performance of FixMatch.

### 4.3 CONTRASTIVE SELF-SUPERVISED LEARNING

Contrastive learning, such as MoCo [\(He et al., 2020\)](#page-9-11), SimCLR [\(Chen et al., 2020\)](#page-9-12), SwAV [\(Caron](#page-9-13) [et al., 2020\)](#page-9-13), and SimSiam [\(Chen & He, 2021\)](#page-9-14), is a crucial technique for self-supervised learning. One key step in many contrastive learning methods is to generate two different views of each single training image using data augmentations. Our DualAug significantly increases the data diversity while preserving the data distribution, thus it is capble of providing more challenging and meaningful views for contrastive learning. In this regard, we consider integrating our DualAug into contrastive learning be beneficial.

<span id="page-7-3"></span>![](_page_7_Figure_0.jpeg)

**Caption:** Figure 4 shows the score distribution of the CIFAR-100 training set at the 100th epoch, comparing different augmentation strategies. The empirical probability density function indicates how DualAug effectively filters OOD data, enhancing the learning process by ensuring that most samples remain above the defined threshold.

Figure 4: Score distribution of the total CIFAR-100 training set, which is obtained from the currently trained model (using WRN-40-2) at the 100-th epoch. "Empirical pdf" refers to the empirical probability density function, "Basic/Heavy/Dual Aug" refer to basic/heavy/dual augmentation, and "No Aug" refers to the original data without augmentation. τ is threshold computed by Eq [\(7\)](#page-4-2) and averaged by iterations in one epoch. AutoAugment is chosen as basic augmentation.

We try our DualAug on SimSiam [\(Chen & He, 2021\)](#page-9-14). We selected the original data augmentations mentioned in the SimSiam paper to serve as the basic augmentations for our DualAug, which include geometric augmentation, color augmentation, and blurring augmentation. The pre-train is conducted using ResNet-18 for 800 epochs on CIFAR-10, and ResNet-50 for 100 epochs on ImageNet, respectively. The linear classification evaluation is conducted for 100 epochs on CIFAR-10 and 90 epochs on ImageNet, respectively. We follow other experimental settings of SimSiam (which details is presented in appendix), and specific settings for DualAug are consistent with those detailed in Section [4.1.1.](#page-4-1)

Table 5: The linear evaluation results of contrastive self-supervised learning with DualAug.

<span id="page-7-1"></span>

| CIFAR-10 | ImageNet                           |
|----------|------------------------------------|
|          | 68.10                              |
|          | 68.20                              |
|          | 68.00                              |
|          | 62.70                              |
|          | 67.85                              |
| -        | 68.30                              |
|          | 68.23                              |
| 92.29    | 68.67                              |
|          | 91.80<br>-<br>-<br>-<br>-<br>91.61 |

<sup>1</sup> Reported in SimSiam.

<sup>2</sup> Reported in Teach Augment.

<sup>3</sup> Reported in YOCO.

As shown in Table [5,](#page-7-1) we show the linear evaluation results of pretrained SimSiam model on CIFAR-10 and

ImageNet when integrated with DualAug. Automated data augmentation (RA, TA, and AA) has a negative impact on SimSiam. TeachAug [\(Suzuki, 2022\)](#page-11-3) and YOCO [\(Han et al., 2022\)](#page-9-15) enhance SimSiam's performance. DualAug consistently also improves SimSiam and outperforms previous data augmentation methods.

#### <span id="page-7-0"></span>4.4 ABLATION STUDY

Component. As shown in Table [6,](#page-7-2) we show the ablation study to analyze the impact of different components in our DualAug. These components include the basic augmentation branch, the heavy augmentation branch, and the OOD detector. For the basic augmentation, we chose AutoAugment. Firstly, we observe a significant performance drop when only the heavy augmentation is applied (2nd row) compared to 1st row. Secondly, when we add the OOD detector without using basic augmentation (3rd row), the performance slightly improved compared to the 1st row. Thirdly, when randomly mixing the heavy augmented data and basic augmented data without the OOD

detector (4th row), a slight performance drop is observed compared to the 1st row. Finally, when all three components of DualAug are used (5th row), we achieve the best performance. Moreover, compared with the 1st, 2nd, and 5th row's score distribution in Figure [4,](#page-7-3) DualAug shifts the score distribution to a larger extent while ensuring that most samples' scores are still above the threshold. This confirms that DualAug makes data sufficiently augmented and filters OOD data generated by heavy augmentation as much as possible.

<span id="page-7-2"></span>Table 6: Ablation study about DualAug component using WRN-28-2 on CIFAR-100. "Basic/Heavy Aug" refers to the basic/heavy augmentation branch.

| Basic Aug | Heavy Aug | OOD Detector | Accuracy(%) |
|-----------|-----------|--------------|-------------|
| ✓         | ✗         | ✗            | 78.34 ± .10 |
| ✗         | ✓         | ✗            | 73.32 ± .25 |
| ✗         | ✓         | ✓            | 78.40 ± .19 |
| ✓         | ✓         | ✗            | 77.43 ± .24 |
| ✓         | ✓         | ✓            | 78.89 ± .21 |
|           |           |              |             |

<span id="page-8-1"></span>![](_page_8_Figure_0.jpeg)

**Caption:** Figure 5 analyzes the hyper-parameter settings of DualAug on CIFAR-100, demonstrating the impact of various parameters on model accuracy. The dashed lines represent the performance of basic augmentation, highlighting the improvements achieved through careful tuning of the augmentation strategy.

Figure 5: Hyper-parameters analysis of DualAug. Experiments are conducted on CIFAR-100 using WideResNet-28-2. The dashed lines represent the accuracy of basic augmentation(AutoAugment).

OOD detector. We also make an ablation study on OOD detector generation options. As shown in Table [7,](#page-8-0) the "Online (✓)" yields better results compared to the "Online (✗)". Furthermore, the "Online (✓)" is more efficient for time and memory saving.

Hyper-parameter analysis. Then, we investigate the effect of three hyper-parameter of DualAug, including λ of 3σ rule mentioned in Section [3.4,](#page-4-3) upper limit of M and the percentage of epochs in warm-up phase mentioned in Section [4.1.1.](#page-4-1) We perform experiments on CIFAR-100 dataset with WRN-28-2 backbone and evaluate on AutoAugment. According to the result shown in Figure [5\(](#page-8-1)a), it is reasonable to set λ in [0.6,1.2]

<span id="page-8-0"></span>Table 7: Ablation study about OOD Detector generation using WRN-28-10 on CIFAR-100. Online (✓): currently-trained model used as OOD detector. Online (✗) : pretrained model used as OOD detector.

| Method          | Online | Accuracy(%)                |
|-----------------|--------|----------------------------|
| AA<br>AA + Ours | -<br>✗ | 83.04 ± .20<br>83.34 ± .15 |
| AA + Ours       | ✓      | 83.42 ± .21                |

for all our experiments. According to the result shown in Figure [5\(](#page-8-1)b), more aggresive upper limit of M bring better performance, M = 8/10 is suitable. According to the result shown in Figure [5\(](#page-8-1)c), 20% epochs of total stage bring the best performance.

Heavy augmentation implementation analysis. As mentioned in Section [3.3,](#page-3-3) there are various implementation ways to use heavy augmentation. In Table [8,](#page-8-2) RA [\(Cubuk et al.,](#page-9-6) [2020\)](#page-9-6) is chosen as an example to analyze the optimal heavy augmentation implementation manner. In detail, three different methods for heavy augmentation are chosen. Firstly, more types of transformations such as Gaussian, Blur, Sample Pairing, *etc.*are included into the transformations set in heavy augmentation. Additionally, a higher degree of transformation magnitude is used. While RA uses an magnitude of 14 in their original paper, Our heavy Augmentation uses an magnitude of 18. Finally, the number of augmentation transformations are increased, which is the DualAug's implementation. Results indicate that increasing the number of transformations is the best choice, and it can be easily applied to various basic augmentations.

### <span id="page-8-2"></span>Table 8: Ablation study about heavy augmentation implementation using WRN-28-10 on CIFAR-100. The word in parentheses represents implementations of heavy augmentation, which includes more *Type*, bigger *Magnitude* and extra *Number* of transformations.

| Method            | Accuracy (%) |
|-------------------|--------------|
| RA                | 83.11 ± .26  |
| +Ours (Type)      | 82.10 ± .18  |
| +Ours (Magnitude) | 83.21 ± .14  |
| +Ours (Number)    | 83.60 ± .23  |

# 5 CONCLUSION

This paper revealed that a large amount of well-augmented data can still be exploited in heavyaugmentation while the weakness introduced by some OOD data outweighs the benefit of these well-augmented data. Based on this observation, we proposed a generic yet straightforward dualbranch framework, named DualAug, for automatic data augmentation. Specifically, the network is encouraged to take advantage of heavy-augmentation by data mixing strategy, which makes augmented data from the basic- and the heavy-augmentation branches wisely fuse. Extensive experiments show the effectiveness of the proposed DualAug. Moreover, the experiments of semisupervised consistent learning and contrastive self-supervised learning prove that our DualAug can be generalized to other task settings.

### REFERENCES

- <span id="page-9-13"></span>Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. *Advances in neural information processing systems*, 33:9912–9924, 2020.
- <span id="page-9-10"></span>Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, and Marios Savvides. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. *arXiv preprint arXiv:2301.10921*, 2023.
- <span id="page-9-4"></span>Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. *IEEE transactions on pattern analysis and machine intelligence*, 40(4):834–848, 2017.
- <span id="page-9-12"></span>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In *International conference on machine learning*, pp. 1597–1607. PMLR, 2020.
- <span id="page-9-14"></span>Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 15750–15758, 2021.
- <span id="page-9-5"></span>Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In *CVPR*, pp. 113–123, 2019.
- <span id="page-9-6"></span>Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In *CVPR workshops*, pp. 702–703, 2020.
- <span id="page-9-16"></span>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In *CVPR*, pp. 248–255. Ieee, 2009.
- <span id="page-9-1"></span>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. *ICLR*, 2021.
- <span id="page-9-2"></span>Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In *CVPR*, pp. 580–587, 2014.
- <span id="page-9-7"></span>Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu. Keepaugment: A simple information-preserving data augmentation approach. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 1055–1064, 2021.
- <span id="page-9-8"></span>Denis Gudovskiy, Luca Rigazio, Shun Ishizaka, Kazuki Kozuka, and Sotaro Tsukizawa. Autodo: Robust autoaugment for biased data with label noise via scalable probabilistic implicit differentiation. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 16601–16610, 2021.
- <span id="page-9-15"></span>Junlin Han, Pengfei Fang, Weihao Li, Jie Hong, Mohammad Ali Armin, Ian Reid, Lars Petersson, and Hongdong Li. You only cut once: Boosting data augmentation with a single cut. In *International Conference on Machine Learning*, pp. 8196–8212. PMLR, 2022.
- <span id="page-9-9"></span>Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster autoaugment: Learning augmentation strategies using backpropagation. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV 16*, pp. 1–16. Springer, 2020.
- <span id="page-9-0"></span>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *CVPR*, 2016.
- <span id="page-9-3"></span>Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In ´ *ICCV*, pp. 2961–2969, 2017.
- <span id="page-9-11"></span>Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 9729–9738, 2020.
- <span id="page-10-14"></span>Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In *ICLR*, 2019.
- <span id="page-10-7"></span>Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. *arXiv preprint arXiv:1610.02136*, 2016.
- <span id="page-10-18"></span>Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. *arXiv preprint arXiv:1912.02781*, 2019.
- <span id="page-10-16"></span>Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 15262–15271, 2021.
- <span id="page-10-10"></span>Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
- <span id="page-10-0"></span>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In *NeurIPS*, 2012.
- <span id="page-10-8"></span>Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. *arXiv preprint arXiv:1706.02690*, 2017.
- <span id="page-10-2"></span>Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. *Advances in Neural Information Processing Systems*, 32, 2019.
- <span id="page-10-12"></span>Tom Ching LingChen, Ava Khonsari, Amirreza Lashkari, Mina Rafi Nazari, Jaspreet Singh Sambee, and Mario A Nascimento. Uniformaugment: A search-free probabilistic data augmentation approach. *arXiv preprint arXiv:2003.14348*, 2020.
- <span id="page-10-9"></span>Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. *Advances in neural information processing systems*, 33:21464–21475, 2020.
- <span id="page-10-6"></span>Yang Liu, Shen Yan, Laura Leal-Taixe, James Hays, and Deva Ramanan. Soft augmentation for ´ image classification. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 16241–16250, 2023.
- <span id="page-10-4"></span>Raphael Gontijo Lopes, Sylvia J Smullin, Ekin Dogus Cubuk, and Ethan Dyer. Tradeoffs in data augmentation: An empirical study. In *ICLR*, 2021.
- <span id="page-10-15"></span>Eric Mintun, Alexander Kirillov, and Saining Xie. On interaction between augmentations and corruptions in natural corruption robustness. *Advances in Neural Information Processing Systems*, 34:3571–3583, 2021.
- <span id="page-10-13"></span>Samuel G Muller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data augmenta- ¨ tion. In *ICCV*, pp. 774–782, 2021.
- <span id="page-10-3"></span>Alhassan Mumuni and Fuseini Mumuni. Data augmentation: A comprehensive survey of modern approaches. *Array*, pp. 100258, 2022.
- <span id="page-10-11"></span>Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.
- <span id="page-10-5"></span>Francesco Pinto, Harry Yang, Ser Nam Lim, Philip Torr, and Puneet Dokania. Using mixup as a regularizer can surprisingly improve accuracy & out-of-distribution robustness. *Advances in Neural Information Processing Systems*, 35:14608–14622, 2022.
- <span id="page-10-17"></span>Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In *International conference on machine learning*, pp. 5389–5400. PMLR, 2019.
- <span id="page-10-1"></span>Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. *Advances in neural information processing systems*, 28, 2015.
- <span id="page-11-2"></span>Abhishek Sinha, Kumar Ayush, Jiaming Song, Burak Uzkent, Hongxia Jin, and Stefano Ermon. Negative data augmentation. *arXiv preprint arXiv:2102.05113*, 2021.
- <span id="page-11-12"></span>Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. *Advances in neural information processing systems*, 33:596–608, 2020.
- <span id="page-11-8"></span>Yiyou Sun and Yixuan Li. Dice: Leveraging sparsification for out-of-distribution detection. In *ECCV*, pp. 691–708. Springer, 2022.
- <span id="page-11-3"></span>Teppei Suzuki. Teachaugment: Data augmentation optimization using teacher knowledge. In *CVPR*, pp. 10904–10914, 2022.
- <span id="page-11-7"></span>Engkarat Techapanurak, Masanori Suganuma, and Takayuki Okatani. Hyperparameter-free out-ofdistribution detection using cosine similarity. In *ACCV*, 2020.
- <span id="page-11-4"></span>Chenyang Wang, Junjun Jiang, Xiong Zhou, and Xianming Liu. Resmooth: Detecting and utilizing ood samples when training with data augmentation. *IEEE Transactions on Neural Networks and Learning Systems*, 2022.
- <span id="page-11-1"></span>Longhui Wei, An Xiao, Lingxi Xie, Xiaopeng Zhang, Xin Chen, and Qi Tian. Circumventing outliers of autoaugment with knowledge distillation. In *European Conference on Computer Vision*, pp. 608–625. Springer, 2020.
- <span id="page-11-11"></span>Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. *Advances in neural information processing systems*, 33:6256–6268, 2020.
- <span id="page-11-6"></span>Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. *arXiv preprint arXiv:2110.11334*, 2021.
- <span id="page-11-10"></span>Xiangli Yang, Zixing Song, Irwin King, and Zenglin Xu. A survey on deep semi-supervised learning. *IEEE Transactions on Knowledge and Data Engineering*, 2022.
- <span id="page-11-9"></span>Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. *arXiv preprint arXiv:1605.07146*, 2016.
- <span id="page-11-5"></span>Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial autoaugment. *arXiv preprint arXiv:1912.11188*, 2019.

<span id="page-11-0"></span>Yu Zheng, Zhi Zhang, Shen Yan, and Mi Zhang. Deep autoaugmentation. In *ICLR*, 2022.

# A ALGORITHM

<span id="page-12-0"></span>Algorithm [1](#page-12-0) describe the pseudo code of Dual Augmentation.

Algorithm 1 Dual Augmentation

Input: Dataset D = {(x<sup>i</sup> , yi)} N <sup>i</sup>=1, model fθ, temperature T of Maximum Softmax Probability, λ of 3σ rule, warmup period epoch w, mini-batch size B; Initialization: Perform random initialization for θ of the model; for each epoch do for each mini-batch do Sample mini-batch data X = {xi} B <sup>i</sup>=1, Y = {yi} B <sup>i</sup>=1 from D Augment x ∈ X to x˜ basic <sup>i</sup> <sup>∈</sup> <sup>X</sup>˜ basic and <sup>x</sup>˜ heavy <sup>i</sup> <sup>∈</sup> <sup>X</sup>˜ heavy by Eq [3](#page-3-2) and Eq [4](#page-4-4) Calculate s basic <sup>i</sup> <sup>∈</sup> <sup>S</sup>˜basic and <sup>s</sup> heavy <sup>i</sup> <sup>∈</sup> <sup>S</sup>˜heavy by Eq [6](#page-4-5) τ = µ(S basic) − λ · σ(S basic) if epoch ≥ w then X˜ dual = {x˜ dual i | ( x˜ heavy i , if s heavy <sup>i</sup> > τ x˜ basic i , otherwise } else X˜ dual = X˜ basic end if Compute Loss of X˜ dual and Y Update θ end for end for Output: model f<sup>θ</sup>

# B TRAINING SETTINGS

For all basic augmentations, our experiments use the original settings and policy of augmentation, including the range, probability, and magnitude of transformations.

### B.1 SUPERVISED LEARNING

For CIFAR-10/100 and SVHN-Core the WideResNet-40-2 and WideResNet-28-10 [\(Zagoruyko](#page-11-9) [& Komodakis, 2016\)](#page-11-9) network is trained for 200 epochs using SGD with Nesterov Momentum, a learning rate of 0.1, a batch size of 128, a weight decay of 5e-4, and cosine learning rate decay.

For ImageNet [\(Deng et al., 2009\)](#page-9-16), the ResNet-50 [\(He et al., 2016\)](#page-9-0) is trained using the training setup of DeepAA [\(Zheng et al., 2022\)](#page-11-0). The training process lasts for 270 epochs, with each GPU using a batch size of 512. The training is conducted on 2 A6000 GPUs, using image crops of size 224 x 224. The initial learning rate is set to 0.1. A stepwise 10-fold reduction is applied after 90, 180, and 240 epochs. A linear warmup factor of 8 over the first 5 epochs is used.

# B.2 SEMI-SUPERVISED LEARNING

We train the FixMatch for 1024 epochs on a 2080Ti GPU using a batch size of 64. The initial learning rate is set to 0.03, and we adopt a cosine learning rate schedule. The weight decay is set at 5e−4. The coefficient for the unlabeled batch size is 7, the coefficient for the unlabeled loss is 1, the pseudo-label temperature is 1, and the pseudo-label threshold is 0.95.

# B.3 CONTRASTIVE SEMI-SUPERVISED LEARNING

For CIFAR-10, we pre-train SimSiam on a single 3060Ti GPU using a batch size of 512 for 800 epochs. The learning rate adjusts using cosine scheduling, starting at 0.05, with a weight decay of 5e − 4. For the linear evaluation of SimSiam, it runs for 100 epochs with a batch size of 256.

<span id="page-13-1"></span>![](_page_13_Figure_0.jpeg)

**Caption:** Figure 6 visualizes the original, basic, heavy, and dual augmented images on CIFAR-100. It illustrates how heavy augmentation can produce varying quality images, with the dual augmented images effectively combining the strengths of both basic and heavy augmentations while filtering out OOD samples.

Figure 6: Visualization on CIFAR-100 of the original image, basic augmented image, heavy augmented image, and dual augmented image. Basic/Heavy/Dual Aug refer to basic/heavy/Dual augmentation, and No Aug refers to the original data without augmentation. The Heavy Aug row is sorted from min to max based on the image's score, with corresponding images in the other rows.

The initial learning rate is set to 30 and decays by a factor of 0.1 at the 60th and 80th epochs. The momentum is set at 0.9.

For ImageNet, we pre-train SimSiam on two A6000 GPUs with a batch size of 512. The learning rate follows a cosine adjustment, starting from 0.05, with a weight decay of 5e − 4. During the linear evaluation of SimSiam, which lasts 100 epochs with a batch size of 256, the initial learning rate is set to 0.1 and employs a cosine learning rate schedule. The momentum is set at 0.9.

### C DUALAUG WITH FASTAUTOAUGMENT AND UNIFORMAUGMENT

As a supplement, the results of combining DualAug with FastAutoAugment [\(Lim et al., 2019\)](#page-10-2) and UniformAugment [\(LingChen et al., 2020\)](#page-10-12) are presented in Table [9.](#page-13-0) It can be seen that DualAug effectively improves the performance of basic augmentation, except for the comparable performance of WideResNet-40-2 on CIFAR-100 for FastAutoAugment.

<span id="page-13-0"></span>Table 9: Result of DualAug with FasetAA and UA. Result in bold is better in comparison.

| Dataset   |           |          | FastAA   | UA       |          |
|-----------|-----------|----------|----------|----------|----------|
|           | Model     | Ours (✗) | Ours (✓) | Ours (✗) | Ours (✓) |
| CIFAR-10  | WRN-40-2  | 96.40    | 96.65    | 96.25    | 96.53    |
|           | WRN-28-10 | 97.30    | 97.54    | 97.33    | 97.58    |
| CIFAR-100 | WRN-40-2  | 79.40    | 79.40    | 79.01    | 79.74    |
|           | WRN-28-10 | 82.70    | 83.16    | 82.82    | 83.34    |

### D VISUALIZATION OF DUAL AUGMENTED IMAGE

Figure [6](#page-13-1) shows the visualization on CIFAR-100 of the original image, image of basic augmentation, heavy augmentation, and dual augmentation. Among them, the images in the row of Heavy Aug are sorted from minimum to maximum according to the Score of the image, and the other rows are the corresponding images of the Heavy Aug row. As described in Section 3.4, heavy augmented images that is out-of-distribution is degraded to basic augmented images, and dual augmented images mix heavy augmented images and basic augmented images.