# Cross-Domain Feature Augmentation for Domain Generalization

Yingnan Liu<sup>1</sup>,<sup>2</sup> , Yingtian Zou<sup>1</sup>,<sup>2</sup> , Rui Qiao<sup>1</sup> , Fusheng Liu<sup>2</sup> , Mong Li Lee<sup>1</sup>,<sup>2</sup> and Wynne Hsu<sup>1</sup>,<sup>2</sup>

<sup>1</sup>School of Computing, National University of Singapore

Institute of Data Science, National University of Singapore

{liu.yingnan, yingtian, rui.qiao, fusheng}@u.nus.edu, {leeml, whsu}@comp.nus.edu.sg

## Abstract

2

Domain generalization aims to develop models that are robust to distribution shifts. Existing methods focus on learning invariance across domains to enhance model robustness, and data augmentation has been widely used to learn invariant predictors, with most methods performing augmentation in the input space. However, augmentation in the input space has limited diversity whereas in the feature space is more versatile and has shown promising results. Nonetheless, feature semantics is seldom considered and existing feature augmentation methods suffer from a limited variety of augmented features. We decompose features into class-generic, class-specific, domain-generic, and domain-specific components. We propose a crossdomain feature augmentation method named XDomainMix that enables us to increase sample diversity while emphasizing the learning of invariant representations to achieve domain generalization. Experiments on widely used benchmark datasets demonstrate that our proposed method is able to achieve state-of-the-art performance. Quantitative analysis indicates that our feature augmentation approach facilitates the learning of effective models that are invariant across different domains. Our code is available at [https://github.com/NancyQuris/](https://github.com/NancyQuris/XDomainMix) [XDomainMix.](https://github.com/NancyQuris/XDomainMix)

## 1 Introduction

Deep learning methods typically assume that training and testing data are independent and identically distributed. However, this assumption is often violated in the real world, leading to a decrease in model performance when faced with a different distribution [\[Torralba and Efros, 2011\]](#page-8-0). The field of domain generalization aims to mitigate this issue by learning a model from one or more distinct yet related training domains, with the goal of generalizing effectively to domains that have not been previously encountered. Studies suggest that the poor generalization on unseen distributions can be attributed to the failure of learning the *invariance* across different domains during the training phase [\[Muandet](#page-8-1) *et al.*, 2013;

<span id="page-0-0"></span>![](_page_0_Figure_9.jpeg)

**Caption:** Figure 1 illustrates reconstructed images from original features using DSU and XDomainMix. The XDomainMix-generated elephant features exhibit a complex background, while the horse shows distinct characteristics. In contrast, DSU's reconstructions lack diversity, closely resembling the originals, highlighting XDomainMix's superior feature augmentation capabilities.

Figure 1: Samples of images reconstructed from features produced by DSU [Li *et al.*[, 2022\]](#page-8-2) and the proposed XDomainMix. The elephant reconstructed from XDomainMix's features shows a more complex background. The horse reconstructed from XDomainMix's features displays different characteristics such as a white belly. The top floor of the house generated by XDomainMix's features shows solid walls, instead of glass walls in the original. In contrast, images reconstructed from DSU's features have limited diversity and appear largely similar to the original images.

Li *et al.*[, 2018\]](#page-7-0). To tackle this, research has focused on representation learning and data augmentation as key to learning invariance.

Invariant representation learning aims to align representation across domains [Shi *et al.*[, 2022\]](#page-8-3), and learn invariant causal predictors [\[Arjovsky](#page-7-1) *et al.*, 2019]. They usually impose regularization, which may result in a hard optimization problem [Yao *et al.*[, 2022a\]](#page-8-4). In contrast, data augmentation techniques propose to generate additional samples for the learning of invariance, and avoid the complexities in the regularization approach [\[Mancini](#page-8-5) *et al.*, 2020]. Data augmentation can be generally classified into two types: input space and feature space augmentation. The former often encounters limitations due to a lack of diversity in the augmented data [\[Li](#page-8-6) *et al.*[, 2021b\]](#page-8-6), while the latter offers more versatility and has yielded promising outcomes [Zhou *et al.*[, 2021\]](#page-9-0).

Despite the versatility of feature space augmentation, existing methods such as MixStyle [Zhou *et al.*[, 2021\]](#page-9-0) and DSU [Li *et al.*[, 2022\]](#page-8-2) do not consider feature semantics during the augmentation process. Instead, they alter feature statistics which often leads to a limited range of diversity. This lack of diversity in the generated features motivates us to decompose the features according to feature semantics. We build on prior research which suggests that the features learned for each class can be viewed as a combination of class-specific and class-generic components [Chu *et al.*[, 2020\]](#page-7-2). The classspecific component carries information unique to a class, while the class-generic component carries information that is shared across classes. We observe that, even within the same class, features of samples from different domains can be distinguished, indicating that these features may contain domain-specific information. As such, we broaden our understanding of features to include domain-specific and domaingeneric components.

We introduce a method called XDomainMix that changes domain-specific components of a feature while preserving class-specific components. With this, the model is able to learn features that are not tied to specific domains, allowing it to make predictions based on features that are invariant across domains. Figure [1](#page-0-0) shows samples of original images and reconstructed images based on existing feature augmentation technique (DSU) and the proposed XDomainMix. We visualize the augmented features using a pre-trained autoencoder [\[Huang and Belongie, 2017\]](#page-7-3). From the reconstructed images, we see that DSU's augmented features remain largely the same as that of the original image feature. On the other hand, the images reconstructed from the features obtained using XDomainMix have richer variety while preserving the salient features of the class.

Results of experiments on benchmark datasets show the superiority of XDomainMix for domain generalization. We quantitatively measure the invariance of learned representation and prediction to show that the models trained with XDomainMix's features are more invariant across domains compared to state-of-the-art feature augmentation methods. Our measurement of the divergence between original features and augmented features shows that XDomainMix results in more diverse augmentation.

## 2 Related Work

To learn invariance, existing domain generalization approaches can be categorized into representation learning methods and data augmentation methods. Works on learning invariant representation employ regularizers to align representations or gradients[\[Sun and Saenko, 2016;](#page-8-7) Li *et al.*[, 2020;](#page-7-4) Kim *et al.*[, 2021;](#page-7-5) [Mahajan](#page-8-8) *et al.*, 2021; Shi *et al.*[, 2022;](#page-8-3) [Rame](#page-8-9) *et al.*[, 2022;](#page-8-9) Yao *et al.*[, 2022b\]](#page-9-1) across different domains, enforce the optimal classifier on top of the representation space to be the same across all domains [\[Arjovsky](#page-7-1) *et al.*, 2019; Ahuja *et al.*[, 2021\]](#page-7-6), or uses distributionally robust optimization [\[Sagawa](#page-8-10) *et al.*, 2020]. However, the use of regularization terms during learning of invariant representation could make the learning process more complex and potentially limit the model's expressive power.

Another approach is to employ data augmentation to learn invariance. Existing work that operates in the input space includes network-learned transformation [Zhou *et al.*[, 2020;](#page-9-2) Li *et al.*[, 2021a\]](#page-7-7), adversarial data augmentation [\[Volpi](#page-8-11) *et al.*, [2018;](#page-8-11) [Shankar](#page-8-12) *et al.*, 2018], mixup [\[Mancini](#page-8-5) *et al.*, 2020; [Yao](#page-8-4) *et al.*[, 2022a\]](#page-8-4), and Fourier-based transformation [Xu *[et al.](#page-8-13)*, [2021\]](#page-8-13). Each of these techniques manipulates the input data in different ways to create variations that help the model learn invariant features. However, the range of transformations that can be applied in the input space is often limited.

On the other hand, feature augmentation can offer more flexibility and potential for learning more effective invariant representations. Prior work has generated diverse distributions in the feature space by changing feature statistics [Zhou *et al.*[, 2021;](#page-9-0) Jeon *et al.*[, 2021;](#page-7-8) Wang *et al.*[, 2022;](#page-8-14) Li *et al.*[, 2022;](#page-8-2) Fan *et al.*[, 2023\]](#page-7-9), adding noise [\[Li](#page-8-6) *et al.*[, 2021b\]](#page-8-6), or mixing up features from different domains [\[Mancini](#page-8-5) *et al.*, 2020; [Qiao and Peng, 2021\]](#page-8-15). For example, MixStyle [Zhou *et al.*[, 2021\]](#page-9-0) synthesizes new domains by mixing the feature statistics of two features. DSU [\[Li](#page-8-2) *et al.*[, 2022\]](#page-8-2) extends the idea by modeling feature statistics as a probability distribution and using new feature statistics drawn from the distribution to augment features. In addition to generating diverse distributions, RSC [\[Huang](#page-7-10) *et al.*, 2020] adopts a different approach by discarding the most activated features instead of generating diverse data. This encourages the model to use less-activated features that might be associated with labels relevant to data outside the domain.

In contrast to existing methods, our work carefully considers feature semantics by leveraging class-label information and domain information to augment features. This increases intra-class variability and helps the model to learn a broader understanding of each class, thus improving its ability to handle new, unseen data.

## 3 Proposed Method

We consider the problem where we have a set of source domains D<sup>S</sup> = {S1, · · · , S<sup>N</sup> }, N > 1 and a set of unseen domains D<sup>U</sup> . Each source domain S<sup>i</sup> = {(x (i) j , y (i) j )} n<sup>i</sup> <sup>j</sup>=1 has a joint distribution on the input x and the label y. Domains in D<sup>U</sup> have distinct joint distributions from those of the domains in DS. We assume that all domains in D<sup>S</sup> and D<sup>U</sup> share the same label space but the class distribution across domains need not be the same. The goal is to learn a mapping g : x → y using the source domains in D<sup>S</sup> such that the error is minimized when g is applied to samples in D<sup>U</sup> .

In deep learning, g is typically realized as a composition of two functions: a feature extractor f : x → Z that maps input x to Z in the latent feature space, followed by a classifier c : Z → y that maps Z to the output label y. Ideally, f should extract features that are domain invariant yet retain class-specific information. The features of a given input can be decomposed into two components: class-specific and class-generic. The class-specific component consists of feature semantics that are strongly correlated with class labels, making them more important in discriminating a target class from other classes.

Furthermore, features can also be decomposed into

<span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)

**Caption:** Figure 2 provides an overview of the XDomainMix method. It decomposes input features into class-specific and domain-specific components, allowing for effective augmentation by mixing features from different domains. This approach enhances diversity while preserving essential class information, crucial for improving domain generalization.

Figure 2: Overview of XDomainMix. To perform augmentation, the feature of an input is decomposed into four components based on the semantics' correlation with class and domain. Afterward, features of other two samples from different domains, one from the same class and one from a different class are used to augment features by changing domain-specific feature components.

domain-specific and domain-generic components. This is because samples from different domains, even if they belong to the same class, possess unique feature characteristics to their respective domains. We extend these notions to decompose the features extracted by f into four distinct components: class-specific domain-specific (Zc,d), class-specific domaingeneric (Zc,¬d), class-generic domain-specific (Z<sup>¬</sup>c,d), and class-generic domain-generic (Z<sup>¬</sup>c,¬d) component.

We determine whether an extracted feature contains information that is specific to a class or a domain by its importance to the prediction of the class and domain respectively. In other words, if a feature is important to the prediction of a specific class and a specific domain, it is considered a class-specific domain-specific component. If a feature is crucial in class prediction but not domain prediction, it falls into the classspecific domain-generic category. Similarly, features that are important for domain but not class predictions are categorized as class-generic domain-specific, and those not significant to either are labeled as class-generic domain-generic.

Our proposed feature augmentation strategy, XDomain-Mix, performs augmentation in the feature space by modifying the domain-specific component of features in a way that preserves class-related information. To discourage the use of domain-specific features for class prediction and encourage the exploitation of less activated features, class-specific domain-specific component is discarded with some probability during training. Details of feature decomposition and augmentation are described in the following subsections. Figure [2](#page-2-0) shows an overview of our proposed method.

#### 3.1 Feature Decomposition

Suppose the feature extractor f extracts Z = f(x) ∈ R K. Let z <sup>k</sup> be the k th dimension of Z. We determine whether z k is class-specific or class-generic via the class importance score, which is computed as the product of feature value and the derivative of class classifier c's predicted logit v<sup>c</sup> of x's ground truth class [\[Selvaraju](#page-8-16) *et al.*, 2017; Chu *et al.*[, 2020\]](#page-7-2) as

they show how much z k contributes to vc:

<span id="page-2-1"></span>
$$
\text{class\_score}(z^k) = \frac{\partial v_c}{\partial z^k} z^k \tag{1}
$$

To determine if z k is domain-specific, we employ a domain classifier d that has an identical architecture as the class classifier c. d is trained to predict domain labels of features extracted by the feature extractor. Similar to Equation [1,](#page-2-1) domain importance score is computed using the derivative of d's predicted logit v<sup>d</sup> of x's ground truth domain:

<span id="page-2-4"></span>domain\_score
$$
(z^k)
$$
 =  $\frac{\partial v_d}{\partial z^k} z^k$  (2)

Let τ<sup>c</sup> and τ<sup>d</sup> be predefined thresholds for filtering feature dimensions that are class-specific and domain-specific respectively. We obtain a class-specific mask M<sup>c</sup> ∈ R <sup>K</sup> and a domain-specific mask M<sup>d</sup> ∈ R <sup>K</sup> on Z for {z k} K <sup>k</sup>=1 where their respective k th entries are given as follows:

$$
M_c[k] = \begin{cases} 1 & \text{if class\_score}(z^k) > \tau_c \\ 0 & \text{otherwise} \end{cases},
$$
  

$$
M_d[k] = \begin{cases} 1 & \text{if domain\_score}(z^k) > \tau_d \\ 0 & \text{otherwise} \end{cases}
$$
 (3)

<span id="page-2-3"></span>Complementary class-generic mask and domain-generic mask are obtained by 1−M<sup>c</sup> and 1−M<sup>d</sup> where 1 is the tensor of values 1 and of the same size as Z. Class-specific domainspecific (Zc,d), class-specific domain-generic (Zc,¬d), classgeneric domain-specific (Z¬c,d), and class-generic domaingeneric (Z<sup>¬</sup>c,¬d) feature components are obtained by

$$
Z_{c,d} = M_c \odot M_d \odot Z
$$
  
\n
$$
Z_{c,\neg d} = M_c \odot (1 - M_d) \odot Z
$$
  
\n
$$
Z_{\neg c,d} = (1 - M_c) \odot M_d \odot Z
$$
  
\n
$$
Z_{\neg c,\neg d} = (1 - M_c) \odot (1 - M_d) \odot Z
$$
\n(4)

<span id="page-2-2"></span>where ⊙ is element-wise multiplication. Note that Zc,d + Zc,¬<sup>d</sup> + Z<sup>¬</sup>c,d + Z<sup>¬</sup>c,¬<sup>d</sup> = Z.

#### 3.2 Cross Domain Feature Augmentation

To achieve domain invariance and reduce reliance on domainspecific information presented in training domains during prediction, we manipulate domain-specific feature components to enhance diversity from a domain perspective. Further, the augmentation should increase feature diversity while preserving class semantics using existing data. This is achieved by mixing the class-specific domain-specific feature component of a sample with the class-specific domainspecific feature component of a same-class sample from other domains. For class-generic domain-specific feature component, it is mixed with the class-generic domain-specific feature component of a different-class sample from other domains, introducing further diversity.

Specifically, for the feature Z extracted from input x, we randomly sample two inputs x<sup>i</sup> and x<sup>j</sup> whose domains are different from x. Further, x<sup>i</sup> has the same class label as x while x<sup>j</sup> is from a different class. Let Z<sup>i</sup> be the feature extracted from input x<sup>i</sup> and Z<sup>j</sup> be the feature extracted from x<sup>j</sup> . Then we have

$$
\tilde{Z}_{c,d} = \lambda_1 Z_{c,d} + (1 - \lambda_1) Z_{ic,d}, \n\tilde{Z}_{\neg c,d} = \lambda_2 Z_{\neg c,d} + (1 - \lambda_2) Z_{j \neg c,d}
$$
\n(5)

<span id="page-3-1"></span>where λ<sup>1</sup> and λ<sup>2</sup> are the mixup ratios independently sampled from a uniform distribution U(0, 1).

A new feature Z˜ with the same class label as Z is generated by replacing the domain-specific component in Z as follows:

$$
\tilde{Z} = \tilde{Z}_{c,d} + \tilde{Z}_{-c,d} + Z_{c,-d} + Z_{-c,-d}
$$
 (6)

To further encourage the model to focus on domaininvariant features and exploit the less activated feature during class prediction, we discard the class-specific domainspecific feature component with some probability pdiscard as follows:

<span id="page-3-0"></span>
$$
\tilde{Z} = \begin{cases} \tilde{Z}_{\neg c,d} + Z_{c,\neg d} + Z_{\neg c,\neg d} & \text{if } p \le p_{\text{discard}}\\ \tilde{Z}_{c,d} + \tilde{Z}_{\neg c,d} + Z_{c,\neg d} + Z_{\neg c,\neg d} & \text{otherwise} \end{cases} \tag{7}
$$

where p is randomly sampled from a uniform distribution U(0, 1).

#### 3.3 Training Procedure

Prior research has shown that empirical risk minimization (ERM) [\[Vapnik, 1999\]](#page-8-17) is a competitive baseline [\[Gulrajani](#page-7-11) [and Lopez-Paz, 2021;](#page-7-11) Wiles *et al.*[, 2022\]](#page-8-18). The objective function of ERM is given by

$$
\mathcal{L}_{erm} = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{n_i} \sum_{j=1}^{n_i} \ell(\hat{y}_j^{(i)}, y_j^{(i)})
$$
(8)

where ℓ is the loss function to measure the error between the predicted class yˆ (i) j and the ground truth y (i) j . N is the number of training domains and n<sup>i</sup> is the number of training samples in domain i.

We train the model in two phases. During the warm-up phase, the feature extractor f and class classifier c are trained on the original dataset for class label prediction following Lerm. The domain classifier d is trained using Z, the features extracted by f, to predict domain labels. The objective function of d is given by

$$
\mathcal{L}_d = \frac{1}{N} \sum_{i=1}^N \frac{1}{n_i} \sum_{j=1}^{n_i} \ell(d(Z_j^{(i)}), i)
$$
(9)

where ℓ is the loss function to measure the error between the predicted domain d(Z (i) j ) and the ground truth i.

When the warm-up phase is completed, we use Equation [4](#page-2-2) to decompose the features obtained from f. Augmented features are then derived using Equation [7.](#page-3-0) Both feature extractor f and class classifier c are trained using the original and augmented features with the following objective function:

$$
\mathcal{L}_{aug} = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{2n_i} \sum_{j=1}^{n_i} \left[ \ell(c(Z_j^{(i)}), y_j^{(i)}) + \ell(c(\tilde{Z}_j^{(i)}), y_j^{(i)}) \right]
$$
\n(10)

where Z˜ (i) j is the augmented feature derived from Z (i) j . c(Z (i) j ) is the predicted class given Z (i) j , and c(Z˜ (i) j ) is the predicted class given Z˜ (i) j . y (i) j is the ground truth class.

We also train the domain classifier d using Ld. Note that the domain classifier d is not trained using this set of augmented features, as the augmented features do not have assigned domain labels since they need not follow the distribution of existing domains.

## 4 Performance Study

We implement our proposed solution using PyTorch 1.12.0 and perform a series of experiments on NVIDIA Tesla V100 GPU to evaluate the effectiveness of the proposed XDomain-Mix. The following benchmark datasets are used:

- Camelyon17 [Bandi *et al.*[, 2018\]](#page-7-12) from Wilds [Koh *[et al.](#page-7-13)*, [2021\]](#page-7-13). This dataset contains 455,954 tumor and normal tissue slide images from 5 hospitals (domains). Distribution shift arises from variations in patient population, slide staining, and image acquisition.
- FMoW [\[Christie](#page-7-14) *et al.*, 2018] from Wilds. This dataset contains 141,696 satellite images from 62 land use categories across 16 years from 5 regions (domains).
- PACS [Li *et al.*[, 2017\]](#page-7-15). This dataset contains 9,991 images of 7 objects in 4 visual styles (domains): art painting, cartoon, photo, and sketch.
- TerraIncognita [Beery *et al.*[, 2018\]](#page-7-16). The dataset contains 24,788 images from 10 categories of wild animals taken from 4 different locations (domains).
- DomainNet [Peng *et al.*[, 2019\]](#page-8-19). This dataset contains 586,575 images from 365 classes in 6 visual styles (domains): clipart, infograph, painting, quickdraw, real, and sketch.

The class importance thresholds τ<sup>c</sup> and domain importance thresholds τ<sup>d</sup> in Equation [3](#page-2-3) are set as follows: τ<sup>c</sup> is set to be the 50%-quantile of the class importance scores of {z k}

<span id="page-4-0"></span>

| Method     | Camelyon17 | FMoW      | PACS      | TerraIncognita | DomainNet |
|------------|------------|-----------|-----------|----------------|-----------|
| ERM        | 70.3±6.4   | 32.3±1.3  | 85.5±0.2  | 46.1±1.8       | 43.8±0.1  |
| GroupDRO   | 68.4±7.3   | 30.8±0.8  | 84.4±0.8  | 43.2±1.1       | 33.3±0.2  |
| RSC        | 77.0±4.9ˆ  | 32.6±0.5ˆ | 85.2±0.9  | 46.6±1.0       | 38.9±0.5  |
| MixStyle   | 62.6±6.3ˆ  | 32.9±0.5ˆ | 85.2±0.3  | 44.0±0.7       | 34.0±0.1  |
| DSU        | 69.6±6.3ˆ  | 32.5±0.6ˆ | 85.5±0.6ˆ | 41.5±0.9ˆ      | 42.6±0.2ˆ |
| LISA       | 77.1±6.5   | 35.5±0.7  | 83.1±0.2ˆ | 47.2±1.1ˆ      | 42.3±0.3ˆ |
| Fish       | 74.7±7.1   | 34.6±0.2  | 85.5±0.3  | 45.1±1.3       | 42.7±0.2  |
| XDomainMix | 80.9±3.2   | 35.9±0.8  | 86.4±0.4  | 48.2±1.3       | 44.0±0.2  |

Table 1: Domain generalization performance of XDomainMix compared with state-of-the-art methods performance published in [Yao *[et al.](#page-8-4)*, [2022a;](#page-8-4) [Gulrajani and Lopez-Paz, 2021;](#page-7-11) Cha *et al.*[, 2021\]](#page-7-17). Results with ˆ are produced by us.

in a feature Z so that 50% dimensions are considered classspecific, while the remaining 50% are class-generic. τ<sup>d</sup> controls the strength of the augmentation as it determines the identification of domain-specific feature components. We employ a cyclic changing scheme for τ<sup>d</sup> to let the model learn gradually from weak augmentation to strong augmentation and give the domain classifier more time to adapt to a more domain-invariant feature extractor. The value is initially set to be 90%-quantile of domain importance scores of {z <sup>k</sup>} in a feature Z. As the training proceeds, τ<sup>d</sup> is decreased by 10% quantile for every n step until it reaches the 50%-quantile, where it also remains for n steps. The same cycle is repeated where τ<sup>d</sup> is set to be 90%-quantile again. Input x<sup>i</sup> , x<sup>j</sup> used in augmentation (Equation [5\)](#page-3-1) are samples from the same training batch. pdiscard is set to 0.2.

For Camelyon17 and FMoW datasets, we follow the setup in LISA [Yao *et al.*[, 2022a\]](#page-8-4). Non-pretrained DenseNet-121 is used for Canmelyon17 and pretrained DenseNet-121 is used for FMoW. We use the same partitioning in Wilds [Koh *[et al.](#page-7-13)*, [2021\]](#page-7-13) to obtain the training, validation, and test domains. The batch size is set to 32, and the model is trained for 2 epochs for Camelyon17 and 5 epochs for FMoW. The learning rate and weight decay are set to 1e-4 and 0. The warm-up phase is set to 4000 steps. We tune the step n in {100, 500} for changing τd. The best model is selected based on its performance in the validation domain.

For PACS, TerraIncognita and DomainNet datasets, we follow the setup in DomainBed [\[Gulrajani and Lopez-Paz,](#page-7-11) [2021\]](#page-7-11), and use a pre-trained ResNet-50. Each domain in the dataset is used as a test domain in turn, with the remaining domains serving as training domains. The batch size is set to 32 (24 for DomainNet), and the model is trained for 5000 steps (15000 steps for DomainNet). We tune the learning rate in {2e-5, 3e-5, 4e-5, 5e-5, 6e-5} and weight decay in (1e-6, 1e-2) using the DomainBed framework. The warm-up phase is set to 3000 steps and n is set to 100 steps. The best model is selected based on its performance on the validations split of the training domains.

#### 4.1 Domain Generalization Performance

We compare our proposed XDomainMix with ERM [\[Vapnik,](#page-8-17) [1999\]](#page-8-17) and the following state-of-the-art methods:

- GroupDRO [\[Sagawa](#page-8-10) *et al.*, 2020] minimizes worst-case loss for distributionally robust optimization.
- RSC [\[Huang](#page-7-10) *et al.*, 2020] discards features that have

higher activation to activate the remaining features appear to be applicable to out-of-domain data.

- MixStyle [Zhou *et al.*[, 2021\]](#page-9-0) synthesizes new domains by mixing feature statistics of two features.
- DSU [Li *et al.*[, 2022\]](#page-8-2) synthesizes new domains by renormalizing feature statistics of features with the ones drawn from a probability distribution.
- LISA [Yao *et al.*[, 2022a\]](#page-8-4) selectively mixes up samples to learn an invariant predictor.
- Fish [Shi *et al.*[, 2022\]](#page-8-3) aligns gradients across domains by maximizing the gradient inner product.

Classification accuracy, which is the ratio of the number of correct predictions to the total number of samples is reported. Following the instruction of datasets, average accuracy on the test domain over 10 runs is reported for the Camelyon17 dataset; worst-group accuracy on the test domain over 3 runs is reported for the FMoW dataset. For PACS, TerraIncognita and DomainNet datasets, the averaged accuracy of the test domains over 3 runs is reported.

Table [1](#page-4-0) shows the results. Our method consistently achieves the highest average accuracy across all the datasets, outperforming SOTA methods. This result suggests that XDomainMix is able to train models with good domain generalization ability.

### 4.2 Model Invariance

One advantage of XDomainMix is that is able to learn invariance across training domains. We quantify the invariance in terms of representation invariance and predictions invariance. Representation invariance refers to the disparity between representations of the same class across different domains. The distance between second-order statistics (covariances) [\[Sun](#page-8-7) [and Saenko, 2016\]](#page-8-7) can be used to measure representation invariance. Prediction invariance considers the variation in predictions across different domains. We employ risk variance [Yao *et al.*[, 2022a\]](#page-8-4) which measures how similar the model performs across domains.

Let {Z (i) j |y (i) <sup>j</sup> = yc} be the set of representations of class label y<sup>c</sup> from domain i. We use C (i) yc to denote the covariance matrix of the representations. Given the set of class labels Y and the set of training domains DS, the measurement result is given by <sup>1</sup> |Y||D<sup>S</sup> | P yc∈Y P i,i′∈D<sup>S</sup> ||C (i) <sup>y</sup><sup>c</sup> −C (i ′ ) yc ||2 F . ||·||<sup>2</sup> F denotes the squared matrix Frobenius norm. A smaller distance

<span id="page-5-0"></span>

| Method     | Camelyon17 | FMoW      | PACS      | TerraIncognita | DomainNet  |
|------------|------------|-----------|-----------|----------------|------------|
| ERM        | 0.47±0.18  | 0.35±0.09 | 0.69±0.12 | 0.50±0.09      | 3.60±0.38  |
| GroupDRO   | 0.21±0.02  | 0.40±0.04 | 0.77±0.16 | 0.43±0.05      | 2.13±0.09  |
| RSC        | 0.32±0.17  | 0.55±0.14 | 42.3±12.8 | 29.9±3.02      | 17.3±1.43  |
| MixStyle   | 0.28±0.26  | 0.36±0.05 | 0.69±0.03 | 0.38±0.09      | 3.39±0.17  |
| DSU        | 0.07±0.02  | 0.32±0.02 | 0.33±0.04 | 9.18±1.84      | 4.61±0.24  |
| LISA       | 0.19±0.05  | 0.29±0.05 | 0.04±0.00 | 0.13±0.01      | 0.72± 0.06 |
| Fish       | 3.95±3.28  | 0.47±0.02 | 0.64±0.34 | 0.34±0.04      | 2.60±0.26  |
| XDomainMix | 0.19±0.07  | 0.28±0.01 | 0.02±0.00 | 0.04±0.00      | 0.11±0.02  |

(a) Representation invariance measured by distance of covariance matrix of same class representations across domains.

| Method     | Camelyon17 | FMoW     | PACS      | TerraIncognita | DomainNet |
|------------|------------|----------|-----------|----------------|-----------|
| ERM        | 1.55±0.27  | 139±50.0 | 9.89±2.09 | 12.7±2.6       | 553±26.5  |
| GroupDRO   | 0.93±0.11  | 161±171  | 398±65.1  | 603±22.3       | 668±17.9  |
| RSC        | 1.90±0.50  | 181±70.8 | 6.34±0.91 | 10.3±3.6       | 631±7.06  |
| MixStyle   | 1.67±0.91  | 110±88.9 | 6.51±1.20 | 9.10±0.56      | 563±4.68  |
| DSU        | 3.85±1.30  | 237±136  | 16.2±5.24 | 10.6±1.5       | 567±21.1  |
| LISA       | 1.81±1.14  | 111±15.2 | 3.02±0.47 | 9.39±0.51      | 520±11.9  |
| Fish       | 5.68±1.81  | 251±45.3 | 9.08±5.38 | 9.37±1.59      | 567±13.2  |
| XDomainMix | 0.90±0.28  | 109±11.7 | 2.10±0.21 | 8.22±1.05      | 504±15.8  |

<span id="page-5-1"></span>(b) Prediction invariance measured by variance of risk across domains. The results are reported in the unit of 1e-3.

Table 2: Results of model invariance.

| Method     | Camelyon17 | FMoW       | PACS       | TerraIncognita | DomainNet  |
|------------|------------|------------|------------|----------------|------------|
| MixStyle   | 6.65±0.18  | 8.58±0.50  | 3.11±0.34  | 3.20±0.26      | 2.81±0.11  |
| DSU        | 26.77±2.53 | 20.93±1.87 | 6.97±0.03  | 10.85±0.32     | 5.75±0.01  |
| XDomainMix | 38.82±0.28 | 34.06±0.11 | 14.82±0.11 | 14.36±0.12     | 10.27±0.02 |

Table 3: Divergence of the augmented feature and original feature measured by MMD in the unit of 1e-2.

suggests that same-class representations across domains are more similar.

Let R<sup>i</sup> be the loss in predicting the class labels of inputs from domain i. The risk variance is given by the variance among training domains, Var{R<sup>1</sup> , R<sup>2</sup> , ..., R|D<sup>S</sup> <sup>|</sup>}. Lower risk variance suggests a more consistent model performance across domains.

Table [2](#page-5-0) shows the results. We see that our method has the smallest covariance distance in the FMoW, PACS, TerraIncognita, and DomainNet dataset, and the second-smallest in the Camelyon17 dataset. The results indicate that the representations of the same class learned by our method have the least divergence across domains. Additionally, XDomain-Mix has the lowest risk variance, suggesting that it is able to maintain consistent performance in predictions across domains. Overall, the results demonstrate that our approach is able to learn invariance at both the representation level and the prediction level.

## 4.3 Diversity of Augmented Features

To show that XDomainMix can generate more diverse features, we measure the distance between the original and augmented features using maximum mean discrepancy (MMD). A higher MMD suggests that the distance between the original and augmented features is further. The same set of original features is used to ensure fairness and comparability of the measurement result. Average and standard deviation over three runs are reported. Table [3](#page-5-1) shows the results. Features augmented by XDomainMix consistently have the highest MMD compared to MixStyle and DSU which are two SOTA feature augmentation methods. This suggests that the features augmented by XDomainMix exhibit the most deviation from the original features, leading to a more varied augmentation. Visualization of sample images reconstructed from augmented features are given in Supplementary.

## 4.4 Experiments on the Identified Features

In this set of experiments, we demonstrate that XDomainMix is able to identify features that are important for class and domain prediction. We evaluate the model performance for class or domain prediction after eliminating those features with the highest importance score computed in Equations [1](#page-2-1) and [2.](#page-2-4) A decrease in accuracy suggests that the features that have been removed are important for the predictions.

For comparison, we implement two alternative selection strategies: a random method that arbitrarily selects features to remove, and a gradient norm approach, where features are chosen for removal based on the magnitude of the gradient in the importance score computation. Samples in the validation set of PACS dataset are used in this experiment.

Figure [3](#page-6-0) shows the results. Our method shows the largest drop in both class prediction and domain prediction accuracies compared to random removal and gradient norm methods. This indicates that XDomainMix is able to identify fea-

<span id="page-6-0"></span>![](_page_6_Figure_0.jpeg)

**Caption:** Figure 3 demonstrates the impact of removing features with the highest importance scores on prediction accuracy. The results indicate that XDomainMix effectively identifies critical features for class and domain predictions, as evidenced by the significant accuracy drop when these features are removed, outperforming random and gradient norm selection methods.

Figure 3: Prediction accuracy after removing x% of features with the highest importance scores.

<span id="page-6-1"></span>![](_page_6_Figure_2.jpeg)

**Caption:** Figure 4 visualizes features extracted from the PACS dataset, showcasing class-specific and domain-specific features. The class-specific features are well-separated, indicating effective discrimination, while domain-specific features exhibit clearer delineation compared to domain-invariant features, underscoring the method's capability in feature identification.

(b) Domain-related features

Figure 4: Visualization of features from different classes/domains, indicated by the different colors.

tures that are specific to the domain and class effectively.

To visualize the extracted features, we map the high dimensional feature vectors obtained by f to a lower dimensional space, This transformation is carried out using two linear layers, as described in [Zhang *et al.*[, 2021\]](#page-9-3). Figure [4\(](#page-6-1)a) provides a visualization for the model that is trained on the PACS dataset, with Art as the unseen domain. We see that the features identified as class-specific are well separated by class. This is in contrast to the features that are generic across classes, which are not as clearly delineated.

Similarly, we also visualize the extracted domain-specific

and domain-invariant features. As shown in Figure [4\(](#page-6-1)b), the domain-specific features are noticeably better separated compared to the domain-invariant features.

### 4.5 Ablation Study

To understand the contribution of each component in the augmentation, we perform ablation studies on Camelyon17 and FMoW datasets. Table [4](#page-6-2) shows the result. Compared to the baseline, mixing class-specific domain-specific feature components (Zc,d) only, or mixing class-generic domain-specific feature components (Z¬c,d) only in the augmentation can improve the performance. This suggests that by manipulating domain-specific feature components, models that are better at domain generalization can be learned. Mixing Z¬c,d leads to greater improvement, indicating that enriching diversity by content from other classes is more helpful than simply intraclass augmentation.

Augmenting both Zc,d and Z¬c,d does not consistently lead to performance improvement, possibly due to dataset-specific characteristics. Probabilistically discarding Zc,d seems to encourage the model to use less domain-specific information and exploit less activated features in prediction, which improves the domain generalization performance.

<span id="page-6-2"></span>

| mix  | mix   | discard | Camelyon17 | FMoW     |
|------|-------|---------|------------|----------|
| Zc,d | Z¬c,d | Zc,d    |            |          |
|      |       |         | 70.3±6.4   | 32.3±1.3 |
| ✓    |       |         | 78.3±5.5   | 32.9±2.2 |
|      | ✓     |         | 79.1±6.0   | 33.6±1.1 |
| ✓    | ✓     |         | 79.6±7.0   | 31.9±0.4 |
| ✓    | ✓     | ✓       | 80.9±3.2   | 35.9±0.8 |

Table 4: Ablation study.

## 5 Conclusion and Future Work

In this work, we have developed a feature augmentation method to address the domain generalization problem. Our approach aims to enhance data diversity within the feature space for learning models that are invariant across domains by mixing domain-specific components of features from different domains while retaining class-related information. We have also probabilistically discarded domain-specific features to discourage the model from using such features for their predictions, thereby achieving good domain generalization performance. Our experiments on multiple datasets demonstrate the effectiveness of the proposed method.

While our method presents a promising approach to solving the domain generalization problem, there are several limitations. Our method needs more than one training domain to perform cross-domain feature augmentation. Our method assumes that the datasets across different domains share the same label space and similar class distributions, and its performance may be affected if this is not the case. Our method is mainly empirically validated, and a theoretical analysis or guarantee of its performance is still lacking. Further research is needed to provide a deeper theoretical understanding of the proposed method and its performance bounds.

## Acknowledgements

This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG-GC-2019-001-2B). We thank Dr Wenjie Feng for the helpful discussions.

## References

- <span id="page-7-6"></span>[Ahuja *et al.*, 2021] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. *Advances in Neural Information Processing Systems*, 34:3438–3450, 2021.
- <span id="page-7-1"></span>[Arjovsky *et al.*, 2019] Martin Arjovsky, Leon Bottou, ´ Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. *arXiv preprint arXiv:1907.02893*, 2019.
- <span id="page-7-12"></span>[Bandi *et al.*, 2018] Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. *IEEE Transactions on Medical Imaging*, 2018.
- <span id="page-7-16"></span>[Beery *et al.*, 2018] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In *Proceedings of the European conference on computer vision (ECCV)*, pages 456–473, 2018.
- <span id="page-7-17"></span>[Cha *et al.*, 2021] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. *Advances in Neural Information Processing Systems*, 34:22405–22418, 2021.
- <span id="page-7-14"></span>[Christie *et al.*, 2018] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 2018.
- <span id="page-7-2"></span>[Chu *et al.*, 2020] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for long-tailed data. In *European Conference on Computer Vision*, pages 694–710. Springer, 2020.
- <span id="page-7-19"></span>[Dosovitskiy *et al.*, 2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*, 2020.
- <span id="page-7-9"></span>[Fan *et al.*, 2023] Qi Fan, Mattia Segu, Yu-Wing Tai, Fisher Yu, Chi-Keung Tang, Bernt Schiele, and Dengxin Dai. Towards robust object detection invariant to real-world domain shifts. In *The Eleventh International Conference on Learning Representations*, 2023.
- <span id="page-7-20"></span>[Fang *et al.*, 2013] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In *Proceedings of the IEEE International Conference on Computer Vision*, pages 1657–1664, 2013.
- <span id="page-7-18"></span>[Foret *et al.*, 2021] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In *International Conference on Learning Representations*, 2021.
- <span id="page-7-11"></span>[Gulrajani and Lopez-Paz, 2021] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In *International Conference on Learning Representations*, 2021.
- <span id="page-7-3"></span>[Huang and Belongie, 2017] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*, Oct 2017.
- <span id="page-7-10"></span>[Huang *et al.*, 2020] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves crossdomain generalization. In *European Conference on Computer Vision*, pages 124–140. Springer, 2020.
- <span id="page-7-8"></span>[Jeon *et al.*, 2021] Seogkyu Jeon, Kibeom Hong, Pilhyeon Lee, Jewook Lee, and Hyeran Byun. Feature stylization and domain-aware contrastive learning for domain generalization. In *Proceedings of the 29th ACM International Conference on Multimedia*, pages 22–31, 2021.
- <span id="page-7-5"></span>[Kim *et al.*, 2021] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Selfsupervised contrastive regularization for domain generalization. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 9619–9628, 2021.
- <span id="page-7-13"></span>[Koh *et al.*, 2021] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In *International Conference on Machine Learning*, pages 5637– 5664. PMLR, 2021.
- <span id="page-7-15"></span>[Li *et al.*, 2017] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In *Proceedings of the IEEE international conference on computer vision*, pages 5542–5550, 2017.
- <span id="page-7-0"></span>[Li *et al.*, 2018] Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. Domain generalization via conditional invariant representations. In *Proceedings of the AAAI conference on artificial intelligence*, volume 32, number 1, 2018.
- <span id="page-7-4"></span>[Li *et al.*, 2020] Haoliang Li, YuFei Wang, Renjie Wan, Shiqi Wang, Tie-Qiang Li, and Alex Kot. Domain generalization for medical imaging classification with lineardependency regularization. *Advances in neural information processing systems*, 33:3118–3129, 2020.
- <span id="page-7-7"></span>[Li *et al.*, 2021a] Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xiaoyue Mi, Zhengze Yu, Xiaoya Li, and Boyang Xia. Progressive domain expansion network for single domain generalization. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 224–233, 2021.
- <span id="page-8-6"></span>[Li *et al.*, 2021b] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature augmentation for domain generalization. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 8886–8895, 2021.
- <span id="page-8-2"></span>[Li *et al.*, 2022] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and LINGYU DUAN. Uncertainty modeling for out-of-distribution generalization. In *International Conference on Learning Representations*, 2022.
- <span id="page-8-8"></span>[Mahajan *et al.*, 2021] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In *International Conference on Machine Learning*, pages 7313–7324. PMLR, 2021.
- <span id="page-8-5"></span>[Mancini *et al.*, 2020] Massimiliano Mancini, Zeynep Akata, Elisa Ricci, and Barbara Caputo. Towards recognizing unseen categories in unseen domains. In *European Conference on Computer Vision*, pages 466–483. Springer, 2020.
- <span id="page-8-1"></span>[Muandet *et al.*, 2013] Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via in- ¨ variant feature representation. In *International Conference on Machine Learning*, pages 10–18. PMLR, 2013.
- <span id="page-8-19"></span>[Peng *et al.*, 2019] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In *Proceedings of the IEEE International Conference on Computer Vision*, pages 1406–1415, 2019.
- <span id="page-8-15"></span>[Qiao and Peng, 2021] Fengchun Qiao and Xi Peng. Uncertainty-guided model generalization to unseen domains. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 6790–6800, 2021.
- <span id="page-8-21"></span>[Radford *et al.*, 2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In *International conference on machine learning*, pages 8748–8763. PMLR, 2021.
- <span id="page-8-9"></span>[Rame *et al.*, 2022] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution generalization. In *International Conference on Machine Learning*, pages 18347–18377. PMLR, 2022.
- <span id="page-8-10"></span>[Sagawa *et al.*, 2020] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In *International Conference on Learning Representations*, 2020.
- <span id="page-8-16"></span>[Selvaraju *et al.*, 2017] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In *Proceedings of the IEEE international conference on computer vision*, pages 618–626, 2017.
- <span id="page-8-12"></span>[Shankar *et al.*, 2018] Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and

Sunita Sarawagi. Generalizing across domains via crossgradient training. *arXiv preprint arXiv:1804.10745*, 2018.

- <span id="page-8-3"></span>[Shi *et al.*, 2022] Yuge Shi, Jeffrey Seely, Philip Torr, Siddharth N, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In *International Conference on Learning Representations*, 2022.
- <span id="page-8-7"></span>[Sun and Saenko, 2016] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In *European conference on computer vision*, pages 443–450. Springer, 2016.
- <span id="page-8-0"></span>[Torralba and Efros, 2011] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In *CVPR 2011*, pages 1521–1528. IEEE, 2011.
- <span id="page-8-17"></span>[Vapnik, 1999] Vladimir Vapnik. *The nature of statistical learning theory*. Springer science & business media, 1999.
- <span id="page-8-22"></span>[Venkateswara *et al.*, 2017] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 5018– 5027, 2017.
- <span id="page-8-11"></span>[Volpi *et al.*, 2018] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. *Advances in neural information processing systems*, 31, 2018.
- <span id="page-8-14"></span>[Wang *et al.*, 2022] Yue Wang, Lei Qi, Yinghuan Shi, and Yang Gao. Feature-based style randomization for domain generalization. *IEEE Transactions on Circuits and Systems for Video Technology*, 2022.
- <span id="page-8-20"></span>[Wang *et al.*, 2023] Pengfei Wang, Zhaoxiang Zhang, Zhen Lei, and Lei Zhang. Sharpness-aware gradient matching for domain generalization. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3769–3778, 2023.
- <span id="page-8-18"></span>[Wiles *et al.*, 2022] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena, Krishnamurthy Dj Dvijotham, and Ali Taylan Cemgil. A finegrained analysis on distribution shift. In *International Conference on Learning Representations*, 2022.
- <span id="page-8-13"></span>[Xu *et al.*, 2021] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generalization. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 14383–14392, 2021.
- <span id="page-8-4"></span>[Yao *et al.*, 2022a] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-of-distribution robustness via selective augmentation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, *Proceedings of the 39th International Conference on Machine Learning*, volume 162 of *Proceedings of Machine Learning Research*, pages 25407–25437. PMLR, 17–23 Jul 2022.
- <span id="page-9-1"></span>[Yao *et al.*, 2022b] Xufeng Yao, Yang Bai, Xinyun Zhang, Yuechen Zhang, Qi Sun, Ran Chen, Ruiyu Li, and Bei Yu. Pcl: Proxy-based contrastive learning for domain generalization. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 7097– 7107, 2022.
- <span id="page-9-3"></span>[Zhang *et al.*, 2021] Yifan Zhang, Bryan Hooi, Dapeng Hu, Jian Liang, and Jiashi Feng. Unleashing the power of contrastive self-supervised visual models via contrastregularized fine-tuning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, *Advances in Neural Information Processing Systems*, 2021.
- <span id="page-9-2"></span>[Zhou *et al.*, 2020] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel domains for domain generalization. In *European conference on computer vision*, pages 561–578. Springer, 2020.
- <span id="page-9-0"></span>[Zhou *et al.*, 2021] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In *International Conference on Learning Representations*, 2021.

## A Comparison with Sharpness-aware Methods

Apart from learning invariance across domains, learning a flat minimum is another approach to improve domain generalization, as recent work suggests that flat minima bring better generalization than sharp minima. As a result, several domain generalization works seek flat minima by optimization that leads to flatter loss landscapes.

Here we compare the performance of XDomainMix with two sharpness-aware methods:

- SAM [Foret *et al.*[, 2021\]](#page-7-18) seeks parameters that lie in neighborhoods that have uniformly low loss.
- SAGM [Wang *et al.*[, 2023\]](#page-8-20) aligns the gradient direction between the SAM loss and the empirical risk.

The results are shown in Table [5.](#page-11-0) We follow the same experiment and reporting protocol as that in Table 1. XDomainMix outperforms SAM and SAGM on Camelyon17 and FMoW datasets. On the PACS and TerraIncognita datasets, XDomainMix comes as a close second to SAGM. The result is expected as the superiority of considering sharpness in domain generalization has been empirically demonstrated.

It is worth mentioning that sharpness-aware methods can be easily incorporated into XDomainMix to learn a flatter minimum. We see that XDomainMix+SAM achieves better performance on Camelyon17, FMoW, PACS, and Domain-Net datasets, indicating that incorporating sharpness-related strategies can further boost the performance of XDomainMix.

## B Additional Results on Experiments of the Identified Features

We present the results of eliminating features with the highest importance score computed in Equations 1 and 2 on other datasets in Figure [5.](#page-10-0) As that we see on the PACS dataset, XDomainMix outperforms random selection and gradient norm methods by exhibiting the most significant decline in both class and domain prediction accuracies on other datasets. This highlights its effective identification of class and domain-specific features.

## C Scalability

Large-scale foundation models have emerged as a prominent trend. XDomainMix can be applied to any model architecture that allows for the decomposition of its features, including larger and more complex models like Vision Transformer [\[Dosovitskiy](#page-7-19) *et al.*, 2020]. We applied the ViT-B/16 CLIP model [\[Radford](#page-8-21) *et al.*, 2021] to XDomainMix by using its image encoder to extract features and fine-tuning the classifier. Feature augmentation is performed on the features extracted by the image encoder.

We compared with CLIP's zero-shot prediction and ERM fine-tuning of the classifier. We use the prompt template "a photo of a {class name}" for zero-shot prediction. For ERM and XDomainMix fine-tuning, the image encoder is frozen, and only the classifier is updated. We follow the same experiment and reporting protocol as that in Table 1 for finetuning. Table [6](#page-11-1) shows the result. On the Camelyon17 dataset,

<span id="page-10-0"></span>![](_page_10_Figure_12.jpeg)

**Caption:** Figure 5 illustrates the prediction accuracy after removing features with the highest importance scores across various datasets. The results reveal that XDomainMix consistently identifies critical features, as indicated by the substantial accuracy decline when these features are eliminated, demonstrating its effectiveness in feature selection.

Figure 5: Prediction accuracy after removing x% of features with the highest importance scores on additional datasets.

CLIP's zero-shot prediction achieves fair performance. Finetuning the classifier further improves the performance, and

<span id="page-11-0"></span>

| Method         | Camelyon17 | FMoW     | PACS     | TerraIncognita | DomainNet |
|----------------|------------|----------|----------|----------------|-----------|
| SAM            | 75.8±5.9   | 35.4±1.4 | 85.8±0.2 | 43.3±0.7       | 44.3±0.0  |
| SAGM           | 80.0±2.9   | 31.0±1.6 | 86.6±0.2 | 48.8±0.9       | 45.0±0.2  |
| XDomainMix     | 80.9±3.2   | 35.9±0.8 | 86.4±0.4 | 48.2±1.3       | 44.0±0.2  |
| XDomainMix+SAM | 81.4±3.1   | 36.1±1.3 | 86.7±0.4 | 44.4±0.4       | 45.1±0.1  |

Table 5: Domain generalization performance of XDomainMix compared with sharpness-aware methods.

XDomainMix gives better result than ERM. On the FMoW dataset, XDomainMix finetuning gives the best performance. CLIP's subpar zero-shot prediction suggests that the image encoder may not be optimal for FMoW. While finetuning enhances performance, it falls short of achieving the levels seen in Table 1.

<span id="page-11-1"></span>

| Method     | Camelyon17 | FMoW     |
|------------|------------|----------|
| zero-shot  | 68.2       | 12.9     |
| ERM        | 86.4±0.3   | 26.6±0.4 |
| XDomainMix | 86.6±0.3   | 26.9±0.2 |

Table 6: Domain generalization performance with ViTB/16 CLIP.

## D Visualization of Augmented Features

We visualize the augmented features by employing a pretrained autoencoder [\[Huang and Belongie, 2017\]](#page-7-3) [1](#page-11-2) to map these features back in the input space. Figure [6](#page-11-3) shows the reconstructed images using both the original and augmented features generated from the Camelyon17 dataset.

Cell nuclei and the general structural features of the tissue are highlighted by the stain. In general, tumor cells are larger than normal cells [\[Bandi](#page-7-12) *et al.*, 2018]. For both classes, XDomainMix is able to generate augmented features with greater diversity, while DSU's augmented features show only limited differences. In addition, XDomainMix also preserves class semantics as no large cells are included in the generated results for the normal class. This demonstrates the effectiveness of using XDomainMix for diverse feature augmentation.

Additionally, we also visualize the XDomainMix augmented features in a lower dimensional space (see Figure [7\)](#page-12-0). The augmented features clearly lie in the same cluster formed by the original features of their respective classes, indicating that XDomainMix is able to preserve class-specific information.

## E Alternative Design of XDomainMix

Sample selection in the interpolation of class-generic domain-specific Feature Component To augment the feature Z extracted from input x, we randomly sample two inputs x<sup>i</sup> and x<sup>j</sup> whose domains are different from x. x<sup>i</sup> has the same class label as x and is used to interpolate Zc,d. x<sup>j</sup> is from a different class and is used to interpolate Z<sup>¬</sup>c,d. We select x<sup>j</sup> from a different class and a different domain so that the augmented feature has greater diversity, compared to samples from the same class or domain.

<span id="page-11-3"></span>![](_page_11_Picture_12.jpeg)

**Caption:** Figure 6 compares reconstructed images from augmented features generated by DSU and XDomainMix. The XDomainMix method produces more diverse samples, preserving class semantics, while DSU's results show limited variation, emphasizing the effectiveness of XDomainMix in generating varied and meaningful augmented features.

(b) Tumor tissues

Figure 6: Visualization of images reconstructed using augmented features obtained from DSU and XDomainMix. Features from XDomainMix result in samples that are more diverse than DSU method.

We compare the maximum mean discrepancy (MMD) between the original and augmented features on the Camelyon17 dataset when different x<sup>j</sup> selection strategies are used in the interpolation of Z<sup>¬</sup>c,d. A higher MMD suggests that the features are more diverged. Table [7](#page-12-1) shows the result. When same-class or same-domain inputs are sampled, the augmented features always have a lower divergence. Selecting samples from different classes and different domains results in the highest MMD, which implies the greatest diversity.

Training domain classifier with augmented features We performed an experiment to train the domain classifier with and without augmented features when Zc,d is not discarded.

<span id="page-11-2"></span><sup>1</sup>Weights are from https://github.com/naoto0804/pytorch-AdaIN.

<span id="page-12-0"></span>![](_page_12_Figure_0.jpeg)

**Caption:** Figure 7 visualizes original and augmented features in a lower-dimensional space. The augmented features from XDomainMix cluster closely with their respective original features, indicating that the method successfully maintains class-specific information while enhancing feature diversity, crucial for robust domain generalization.

Figure 7: Visualization of original and augmented features.

<span id="page-12-1"></span>

| Sample used                          | MMD (1e-2) |
|--------------------------------------|------------|
| same as xi                           | 35.94±0.11 |
| same class different domain (not xi) | 36.00±0.03 |
| different class same domain          | 38.24±0.23 |
| different class different domain     | 38.82±0.28 |

Table 7: Feature divergence of different sample selection in the augmentation of Z<sup>¬</sup>c,d on Camelyon17 dataset

We give each augmented feature Z˜ a soft domain label. Suppose N domains are present in training. the original feature Z is from domain d. Z<sup>i</sup> is from domain d<sup>i</sup> and Z<sup>j</sup> is from domain d<sup>j</sup> . The ratio to interpolate Zc,d with Zic,d is λ1, and the ratio to interpolate Z<sup>¬</sup>c,d with Zjc,d is λ2. The domain label of Z˜, ˜d ∈ R <sup>N</sup> at position d is <sup>λ</sup>1+λ<sup>2</sup> 2 . The value of ˜d at position d<sup>i</sup> is <sup>1</sup>−λ<sup>1</sup> 2 , and at position d<sup>j</sup> is <sup>1</sup>−λ<sup>2</sup> 2 . Other positions are set to 0. Binary cross-entropy loss is used in the training.

Table [8](#page-12-2) shows the results on the Camelyon17 dataset. Classification accuracy on the test domain is reported. It suggests that training the domain classifier with augmented features could harm the domain generalization performance. Augmented features may not follow the distribution of existing domains.

<span id="page-12-2"></span>

| Training domain classifier | Test domain accuracy |
|----------------------------|----------------------|
| with aug features          | 77.4±7.1             |
| without aug features       | 79.6±7.0             |

Table 8: Training domain classifier with and without augmented features on the Camelyon17 dataset.

## F Additional Results on Domain Generalization Performance

We include the domain generalization performance of XDomainMix on two more datasets, VLCS and OfficeHome.

- VLCS [Fang *et al.*[, 2013\]](#page-7-20). This dataset contains 10,729 photos of 5 classes from 4 existing datasets (domains).
- OfficeHome [\[Venkateswara](#page-8-22) *et al.*, 2017]. This dataset contains 15,588 images of 65 office and home objects

in 4 visual styles (domains): art painting, clipart, product (images without background), and real-world (images captured with a camera).

In addition, we include the results of two methods that were proposed earlier.

- CORAL [\[Sun and Saenko, 2016\]](#page-8-7) aligns the secondorder statistics of the representations across different domains.
- IRM [\[Arjovsky](#page-7-1) *et al.*, 2019] learns a representation such that the optimal classifier matches all domains.

To perform experiments on VLCS and OfficeHome, We follow the setup in DomainBed, and use a pre-trained ResNet-50. Each domain in the dataset is used as a test domain in turn, with the remaining domains serving as training domains. Hyperparameters are the same as what we used for PACS and TerraIncognita. The best model is selected based on its performance on the validations split of the training domains. The averaged classification accuracy of the test domains over 3 runs is reported.

Table [9](#page-13-0) shows the result. Our method does not perform well on VLCS and ranks third on the OfficeHome dataset. Overall, our method still yields the highest average performance on benchmark datasets.

## G Domain Generalization Experiment Details

Details of the domain generalization experiments of state-ofthe-art methods produced by us are as below.

Camelyon17 and FMoW dataset We run the experiments of RSC, MixStyle, and DSU in the testbed provided by LISA. Following the instruction of the publisher of the datasets, nonpretrained DenseNet-121 is used as the backbone for Camelyon17. Pretrained DenseNet-121 is used as the backbone for FMoW.

Our implementation of RSC follows that in DomainBed. The drop factors are set to 1/3, the value recommended in RSC.

For MixStyle, our implementation is based on the code published by the author. MixStyle module is inserted after the first and second DenseBlock. All hyperparameters are set to be the value recommended by the author. The MixStyle mode is random, which means that feature statistics are mixed from two randomly drawn features. The probability of using MixStyle is set to 0.5. α, the parameter of the Beta distribution is set to 0.1. The scaling parameter to avoid numerical issues, ϵ is set to 1e-6.

We implement DSU following the published code by the author. All hyperparameters are set to be the value recommended by the author. DSU module is inserted after the first convolutional layer, first maxpool layer, and every transition block. The probability of using DSU is set to 0.5. The scaling parameter to avoid numerical issues, ϵ is set to 1e-6.

For the three experiments, the batch size is set to 32, and the model is trained for the same number of epochs as we used to train our method. We tune the learning rate in {1e-4, 1e-3, 1e-2} and select the optimal learning rate based on the performance on the validation domain. Weight decay is set to 0.

<span id="page-13-0"></span>

| Method     | Camelyon17 | FMoW      | PACS      | VLCS      | OfficeHome | TerraIncognita | DomainNet | Average |
|------------|------------|-----------|-----------|-----------|------------|----------------|-----------|---------|
| ERM        | 70.3±6.4   | 32.3±1.3  | 85.5±0.2  | 77.5±0.4  | 66.5±0.3   | 46.1±1.8       | 43.8±0.1  | 60.3    |
| CORAL      | 59.5±7.7   | 32.8±0.7  | 86.2±0.3  | 78.8±0.6  | 68.7±0.3   | 47.6±1.0       | 41.5±0.1  | 59.3    |
| IRM        | 64.2±8.1   | 30.0±1.4  | 83.5±0.2  | 78.5±0.5  | 64.3±2.2   | 47.6±0.8       | 33.9±2.8  | 57.4    |
| GroupDRO   | 68.4±7.3   | 30.8±0.8  | 84.4±0.8  | 76.7±0.6  | 66.0±0.7   | 43.2±1.1       | 33.3±0.2  | 57.5    |
| RSC        | 77.0±4.9ˆ  | 32.6±0.5ˆ | 85.2±0.9  | 77.1±0.5  | 65.5±0.9   | 46.6±1.0       | 38.9±0.5  | 60.4    |
| MixStyle   | 62.6±6.3ˆ  | 32.9±0.5ˆ | 85.2±0.3  | 77.9±0.5  | 60.4±0.3   | 44.0±0.7       | 34.0±0.1  | 56.7    |
| DSU        | 69.6±6.3ˆ  | 32.5±0.6ˆ | 85.5±0.6ˆ | 77.2±0.5ˆ | 65.7±0.4ˆ  | 41.5±0.9ˆ      | 42.6±0.2ˆ | 59.2    |
| LISA       | 77.1±6.5   | 35.5±0.7  | 83.1±0.2ˆ | 76.8±1.0ˆ | 67.4±0.2ˆ  | 47.2±1.1ˆ      | 42.3±0.3ˆ | 61.3    |
| Fish       | 74.7±7.1   | 34.6±0.2  | 85.5±0.3  | 77.8±0.3  | 68.6±0.4   | 45.1±1.3       | 42.7±0.2  | 61.3    |
| XDomainMix | 80.9±3.2   | 35.9±0.8  | 86.4±0.4  | 76.3±0.5  | 68.1±0.2   | 48.2±1.3       | 44.0±0.2  | 62.8    |

Table 9: Performance of XDomainMix compared with state-of-the-art methods performance. Results with ˆ are produced by us.

PACS, VLCS, OfficeHome, TerraIncognita and Domain-

Net dataset We use DomainBed as the testbed to experiment DSU and LISA. ResNet-50 pretrained on ImageNet is used as the backbone.

Our implementation of LISA is based on the code published by the author. Following the author's suggestion, CuMix is used to mix up input samples. The mix up alpha is set to 2.

The official code of DSU for ResNet-50 is used. Other settings are the same as what we used for Camelyon17 and FMoW experiments.

The model is trained for the same number of steps as we used to train our method. We tune the learning rate, weight decay, and batch size in the range listed by DomainBed. The learning rate is tuned in (1e-5, 1e-3.5). The weight decay is tuned in (1e-6, 1e-2). The batch size is tuned in [32, 45] (24 for DomainNet). 20 groups of hyperparameters are searched. The optimal hyperparameters are selected based on the performance of the validation split of training domains.