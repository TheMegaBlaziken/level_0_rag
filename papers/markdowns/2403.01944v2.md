# <span id="page-0-1"></span>Fourier-basis functions to bridge augmentation gap: Rethinking frequency augmentation in image classification

Puru Vaish<sup>∗</sup> Shunxin Wang<sup>∗</sup> Nicola Strisciuglio University of Twente

{p.vaish, s.wang-2, n.strisciuglio}@utwente.nl

## Abstract

*Computer vision models normally witness degraded performance when deployed in real-world scenarios, due to unexpected changes in inputs that were not accounted for during training. Data augmentation is commonly used to address this issue, as it aims to increase data variety and reduce the distribution gap between training and test data. However, common visual augmentations might not guarantee extensive robustness of computer vision models. In this paper, we propose Auxiliary Fourier-basis Augmentation (AFA), a complementary technique targeting augmentation in the frequency domain and filling the robustness gap left by visual augmentations. We demonstrate the utility of augmentation via Fourier-basis additive noise in a straightforward and efficient adversarial setting. Our results show that AFA benefits the robustness of models against common corruptions, OOD generalization, and consistency of performance of models against increasing perturbations, with negligible deficit to the standard performance of models. It can be seamlessly integrated with other augmentation techniques to further boost performance.*

# 1. Introduction

Computer vision models usually encounter performance degradation when deployed in real-world scenarios due to unexpected image variations [\[9,](#page-8-0) [14,](#page-8-1) [17\]](#page-8-2). Improving the robustness of computer vision models to out-of-distribution (OOD) data is thus essential for their reliable practical use. Among the methods addressing the robustness and generalization of computer vision models [\[1,](#page-8-3) [7,](#page-8-4) [8,](#page-8-5) [10,](#page-8-6) [38,](#page-9-0) [39,](#page-9-1) [47,](#page-9-2) [50,](#page-9-3) [53\]](#page-9-4), data augmentation is mostly used for its easy-toapply characteristics and effectiveness at reducing the distribution gap between training and test data [\[45\]](#page-9-5). Popular augmentation techniques, such as AugMix [\[15\]](#page-8-7), Aug-Max [\[42\]](#page-9-6), AutoAugment [\[2\]](#page-8-8), TrivialAugment [\[34\]](#page-9-7), and PRIME [\[33\]](#page-9-8) have shown great improvements in corruption and perturbation robustness benchmarks and OOD

<span id="page-0-0"></span>![](_page_0_Figure_8.jpeg)

**Caption:** Figure 1 illustrates the impact of frequency augmentation using Fourier-basis functions, which serve as adversarial examples. These augmentations appear unnatural, enhancing model robustness against unseen variations and complementing traditional visual augmentations.

Figure 1. Frequency augmentation with Fourier-basis functions is complementary to common visual augmentations. They appear *unnatural* and can be used as adversarial examples.

datasets for generalisation, e.g. ImageNet-C, ImageNet-C¯, ImageNet-3DCC, ImageNet-P, ImageNet-R and ImageNetv2 [\[13,](#page-8-9) [14,](#page-8-1) [18,](#page-8-10) [32,](#page-9-9) [35\]](#page-9-10). These approaches mainly focus on adding visual variations to images through random or policy-based combinations [\[2,](#page-8-8) [15,](#page-8-7) [16,](#page-8-11) [26,](#page-8-12) [27,](#page-8-13) [30,](#page-9-11) [31,](#page-9-12) [34\]](#page-9-7) of visual transformations aiming at increasing the diversity of training images (expanding on their domain, see visual augmentations in Fig. [1\)](#page-0-0), and adversarial-based augmentations, which address the hardness of training samples but are computationally heavy (see Tab. [1,](#page-1-0) AugMax). However, even if trained with visual augmentations, models are still sensitive to image variations not included in the training [\[25\]](#page-8-14) and frequency perturbations [\[49\]](#page-9-13). This occurs due to the pre-defined frequency characteristics of visual transformations, which cannot ensure the complete robustness of models against noise with different frequency characteristics from those encountered during training. Attackers may exploit this weakness and degrade model performance in operational settings [\[23\]](#page-8-15). This raises a question: *Is there a complementary augmentation technique that can bridge the gap left by visual augmentations?*

Common visual augmentations impact different frequency components of images simultaneously, which are

<sup>\*</sup>Equal contribution

<span id="page-1-1"></span>difficult to explicitly control, and might not encompass all possible frequency variations present in unseen corruptions or variantions happening in real-world scenarios [\[36\]](#page-9-14). We thus rethink image augmentation in the frequency domain, and complement visual augmentation strategies with explicit use of Fourier basis functions in an adversarial setting. There has been exploration into frequency-based augmentations to discover capabilities beyond what visual augmentations can achieve. [\[5,](#page-8-16) [41,](#page-9-15) [48\]](#page-9-16) swap or mix partial amplitude spectrum between images, aiming to induce more phase-reliance for classification. [\[43\]](#page-9-17) augments images with shortcut features to reduce their specificity for classification. AugSVF [\[37\]](#page-9-18) introduces frequency noise within the AugMix framework and [\[24,](#page-8-17) [28\]](#page-8-18) adversarially perturb the frequency components of images. These augmentations are computationally heavy, due to the complicated augmentation framework [\[37\]](#page-9-18), computation of multiple Fourier transforms for training images and their augmented versions [\[5,](#page-8-16) [41,](#page-9-15) [48\]](#page-9-16), identification of learned frequency shortcuts [\[43\]](#page-9-17), or adversarial training [\[24,](#page-8-17) [28\]](#page-8-18).

In this work, we propose Auxiliary Fourier-basis Augmentation (AFA). We use additive noise based on Fourierbasis functions to augment the frequency spectrum in a more efficient way than other methods that apply frequency manipulations [\[5,](#page-8-16) [37,](#page-9-18) [43\]](#page-9-17). The effect of additive Fourierbasis functions on image appearance is complementary to those of other augmentations (see Fig. [1\)](#page-0-0). These images can be interpreted as samples representing an adversarial distribution, distinct from those augmented by common visual transformations. We thus expand upon the conventional idea of adversarial augmentation, moving beyond the generation of imperceptible noise through gradient backpropagation. We employ a training architecture and strategy with an auxiliary component to address the adversarial distribution, and a main component for the original distribution, similarly to AugMax [\[42\]](#page-9-6). However, the adversarial distribution that we construct using additive Fourier-basis is much less computationally expensive than that of AugMax (and other visual augmentation methods - see Tab. [1\)](#page-1-0). It contributes to comparable or higher generalization results, while allowing for the training of larger models on larger datasets (e.g. ImageNet). Our contributions are:

- We propose a straightforward and computationally efficient augmentation technique called AFA. We show that it enhances robustness of models to common image corruptions, improves OOD generalization and consistency of prediction w.r.t. perturbations;
- We expand the augmentation space, complementary to that of visual augmentations, by exploiting amplitudeand phase-adjustable frequency noise, and use it in an adversarial setting. Our method reduces the augmentation gap of common visual augmentations.

<span id="page-1-0"></span>

|                       | AFA (ours)<br>w/o aux. | AFA | (ours) AugMix†    | AFA<br>w/ AugMix PRIME |                   | AFA<br>w/ PRIME AugMax† |             |
|-----------------------|------------------------|-----|-------------------|------------------------|-------------------|-------------------------|-------------|
| FLOPs<br>Memory ×1.02 | ×1                     | ×2  | ×3<br>×1.62 ×2.66 | ×2<br>×1.83            | ×1<br>×2.50 ×3.06 | ×2                      | ×8<br>×2.35 |

Table 1. AFA adds minimal computational burden to existing methods and is more efficient compared to other adversarial methods. It requires only ×1.62 memory and just ×2 the FLOPs of standard augmentation [\[12\]](#page-8-19) training whereas AugMax uses ×2.35 the memory and ×8 the FLOPs when using 5 PGD steps. Methods with † denote the use of loss with JSD.

## 2. Related works

Data augmentation includes a set of techniques to increase data variety, thus reducing the distribution gap between training and test data. Generalization and robustness performance of models normally benefits from the use of data augmentation for training [\[45\]](#page-9-5) or at test-time [\[19\]](#page-8-20).

Image-based augmentations. Common image augmentation techniques include transformations, e.g. cropping, flipping, rotation, among others [\[45\]](#page-9-5). Applying the transformations with fixed configuration lacks flexibility when the models encounter more variations in the inputs at testing time. Thus, algorithms were designed to combine transformations randomly, e.g. AugMix [\[15\]](#page-8-7), RandAug [\[3\]](#page-8-21), TrivialAugment [\[34\]](#page-9-7), MixUp [\[52\]](#page-9-19), and CutMix [\[51\]](#page-9-20). However, random combinations might not be optimal. In [\[2\]](#page-8-8), AutoAugment was proposed, based on using reinforcement learning to find the best policy on how to combine basic transformations for augmentation. AugMax [\[42\]](#page-9-6) instead combines transformations adversarially, aiming at complementing augmentations based on diversity with others that favour hardness of training data. PRIME [\[33\]](#page-9-8) samples transformations with maximum-entropy distributions. [\[40\]](#page-9-21) augments images based on knowledge distilled by a teacher model. However, these approaches address variations limited by visually-plausible transformations only.

Frequency-based augmentations. In [\[49\]](#page-9-13), it was discovered that models trained with visual transformations might be vulnerable to noise impacting certain parts of the frequency spectrum (e.g. high-frequency components), demonstrating that visual augmentations do not completely guarantee robustness. Complementary augmentation techniques are thus required to fill the augmentation gap left by visual augmentations. The straightforward approach is augmentation in the frequency domain. For example, [\[5\]](#page-8-16) mixes the amplitude spectrum of images to reduce reliance on the amplitude part of the spectrum and induce phase-reliance for classification. [\[41,](#page-9-15) [48\]](#page-9-16) swap or mix the amplitude spectrum of images. [\[43\]](#page-9-17) augments images with shortcut features to reduce their specificity for classification, mitigating frequency shortcut learning. [\[37\]](#page-9-18) introduces frequency noise in the AugMix framework. [\[24,](#page-8-17) [29\]](#page-8-22) adversarially perturb images in the frequency domain. While these techniques address what visual augmentations may overlook, <span id="page-2-2"></span>they also have limitations. Most frequency augmentation methods are based on manipulation of the frequency components of images. They usually have high computational requirements to identify frequency shortcuts [\[43\]](#page-9-17) (f.i. using [\[44,](#page-9-22) [46\]](#page-9-23)), implement adversarial training setup [\[24\]](#page-8-17) or calculate multiple Fourier transforms of original and augmented images [\[5,](#page-8-16) [41,](#page-9-15) [43,](#page-9-17) [48\]](#page-9-16).

We instead propose to use Fourier-basis functions as additive noise in the frequency domain. Our augmentation technique requires only one extra step during training rather than multiple pre-processing and expensive computations during training time as in other methods [\[5,](#page-8-16) [41,](#page-9-15) [43,](#page-9-17) [48\]](#page-9-16), and works to complement image-based augmentations. Furthermore, we simplify the adversarial training framework of AugMax [\[42\]](#page-9-6), not requiring an optimization process to maximize the hardness of adversarial augmentation, and achieving comparable or higher robustness. This allows the use of adversarial augmentations at larger-scale. We account for the induced distribution shifts in the frequency domain via an auxiliary component. The benefit of AFA is complementary to visual augmentations, and we can incorporate them seamlessly to further boost model robustness.

## 3. Preliminary: Fourier-basis functions

We utilize Fourier-basis functions in our augmentation strategy as an additive perturbation to the images. They are sinusoidal wave functions used as basic components of the Fourier transform to represent signals and images. A real Fourier basis function has two parameters, namely a frequency f and direction ω, and is denoted as:

$$
A_{f,\omega}(u,v) = R\sin(2\pi f(u\cos(\omega)+v\sin(\omega)-\pi/4)),
$$
 (1)

where Af,ω(u, v) represents the amplitude of the wave at position (u, v). The function involves the sine of a 2D spatial frequency 2πf to produce a planar wave with a specific frequency f, and angle ω that indicates the direction of propagation. R is chosen such that the planar wave has unit l2-norm. A particular Fourier basis function, characterized by specific frequency (f) and direction (ω), can be associated with a Dirac delta function in the spectral domain. Therefore, when employed in an additive manner, as in our augmentation strategy, this Fourier-basis function facilitates the targeted modification of particular frequency components of images. Examples of Fourier-basis waves superimposed on images are shown in Fig. [2.](#page-2-0)

## 4. Auxiliary Fourier-basis Augmentation

The Auxiliary Fourier-basis Augmentation (AFA) that we propose is based on two lines of augmentations, one considered in-distribution (using visual augmentations) and another considered out-of-distribution or adversarial (using frequency-based noise) as shown in Fig. [3.](#page-3-0) We generate the adversarial augmented images by sampling a Fourier-basis

<span id="page-2-0"></span>![](_page_2_Picture_8.jpeg)

**Caption:** Figure 2 displays examples of natural images augmented with Fourier-basis functions, resulting in grating patterns that obscure spatial information. This demonstrates the effectiveness of frequency-based augmentations in generating adversarial samples.

Figure 2. Example of Fourier-basis functions added to natural images. They appear as *gratings* that obscure spatial information.

and a strength parameter per colour channel, and adding them to the original images. Visually augmented and adversarially augmented training images are then processed using a main component and an auxiliary component, respectively. Joint optimisation of two cross-entropy functions encourages robust and consistent classification, as it promotes correctness under adversarially augmented images. Details of the different parts of the method are reported below.

Generation of adversarial augmented images. Randomly sampling augmentations and applying them to images with random strengths was shown to be sufficient to outperform more complex strategies [\[34\]](#page-9-7).

We follow this design principle in our method to generate adversarial augmented images with Fourier basis functions, which allows us to avoid optimization steps to determine the worst-case combination of augmentations as in Aug-Max [\[42\]](#page-9-6). We produce adversarial augmented images by adding a different Fourier basis function Af,ω per channel of the original RGB image. We generate the Fourier basis functions by sampling f and ω from uniform distributions as f ∼ U[1,M] and ω ∼ U[0,π] , where M is the image size. The sampling space of all Fourier-basis is denoted as V. We add the generated Fourier basis functions per channel c with a weight factor sampled from an exponential distribution σ<sup>c</sup> ∼ Exp(1/λ), with c ∈ {R, G, B}. The selection of the exponential distribution for sampling augmentation magnitude is motivated by the concept of event rate, where perturbations with larger magnitudes become progressively less likely, albeit still possible. This is controlled by adjusting λ, ensuring a balance between maintaining diversity in sampled values while minimizing the occurrence of extremely large augmentation perturbations. In Sec. [5.3,](#page-7-0) we show how the parameter λ affects the augmentation results.

The proposed augmentation process results in a 3 channel image x <sup>a</sup> = [x a R, x<sup>a</sup> G, x<sup>a</sup> <sup>B</sup>], where:

<span id="page-2-1"></span>
$$
x_c^a = \text{Clamp}_{[0,1]}(x_c + \sigma_c A_{f_c,\omega_c}), \quad c \in \{R, G, B\}. \tag{2}
$$

An example of image x a augmented with additive Fourierbasis functions is shown in our method schema in Fig. [3.](#page-3-0) The augmentation also results in the augmentation at exactly the particular sampled frequency and phase in the

<span id="page-3-1"></span><span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)

**Caption:** Figure 3 presents the schema of the Auxiliary Fourier-basis Augmentation (AFA) pipeline. It details how images are augmented with Fourier waves per channel, enhancing robustness through joint optimization of main and auxiliary components.

Figure 3. Schema of the AFA augmentation pipeline. The image x is augmented using AFA, which adds a planar wave per channel c of the image at a strength value σ<sup>c</sup> sampled from an exponential distribution (eq[.2\)](#page-2-1). The AFA augmented image x a is used for training, processed through the auxiliary component of the parallel batch normalisation layer (for models that use batch normalization to track batch statistics, e.g. ResNet). Other visual augmentations are applied in parallel, and used for training via the main component of the normalization layer. Finally, we train via optimizing two cross-entropy losses, one for the main and the other for the auxiliary component.

Fourier domain (Appendix [F\)](#page-15-0) therefore being effective at perturbing the samples in Fourier domain with precision.

Auxiliary component for distribution shifts. As shown in Figs. [2](#page-2-0) and [3,](#page-3-0) the Fourier-basis augmentations result in images with an unnatural appearance due to substantial frequency perturbations. The presence of planar waves across the augmented images determines the *unnaturalness* of image appearance, which can be seen as adversarial attacks on the images (experimentally verified in Appendix [D.2\)](#page-12-0). These augmentations disrupt the learned mean and variance in batch normalization layers, which are inconsistent with the distribution shifts induced by our augmentation and lead to inconsistent activations (Appendix [D.1\)](#page-12-1). This results in a negative impact on model convergence and generalization abilities.

We address these issues by deploying architectural components in the training, capable of handling distribution shifts explicitly by tracking statistics and adjusting the loss function accordingly. Namely, we incorporate auxiliary components into the model, such as Parallel Batch Normalization layers and an additional cross-entropy term in the loss function to specifically account for these adversarial augmented images. These modifications to the model architecture and training enhance performance, particularly in the presence of distribution shifts, contributing to better generalization, robustness to common corruptions and consistency to time-dependent increasing perturbations. The introduction of parallel batch normalization layers is motivated by the need to account for distribution shifts induced by adversarial (Fourier-basis) augmentations, as observed in [\[42\]](#page-9-6). With the parallel batch normalisation, the affine parameters and statistics of main and auxiliary distributions are recorded separately. This allows independent learning of distribution of the visually and adversarially augmented images. Without these additional normalization layers, the model training assumes a single-modal sample distribution, limiting its ability to differentiate between the main and the adversarial distribution, thus negatively affecting overall performance. In Sec. [5.3,](#page-6-0) we show the result of not employing the auxiliary components.

It is worth noting that for models that do not employ batch normalization layers (e.g. CCT that uses layer normalization and does not track statistics), the parallel normalization layers are not needed. However, the extra term in the loss function (see next paragraph) to generate consistent predictions across distribution shifts serves as a regularization mechanism (verified in Appendix [E\)](#page-15-1).

Loss function. We work in the supervised learning setting with a training dataset D consisting of clean images x with labels y. We train the model in the main architecture stream (see Fig. [3\)](#page-3-0) using a cross-entropy loss LCE(ˆy, y), where y is the ground-truth label and yˆ is the predicted label for images augmented with a given visual augmentation strategy (e.g. standard, PRIME, etc.). Under the non-auxiliary setting, models thus optimise the standard cross entropy loss.

In the auxiliary setting, we add an extra cross-entropy loss term LCE(y a , y), which optimise the model to predict the correct label on adversarial augmented images whose predicted label is denoted by y a , contributing to robustness of the model w.r.t. aggressive distribution shifts. We refer <span id="page-4-2"></span>to the combined loss function LACE, taking the average of the two cross-entropy terms, as the Auxiliary Cross Entropy (ACE) Loss:

$$
\mathcal{L}_{\text{ACE}}(\hat{y}, y^a, y) = \frac{1}{2} \left[ \mathcal{L}_{\text{CE}}(\hat{y}, y) + \mathcal{L}_{\text{CE}}(y^a, y) \right]. \tag{3}
$$

It contributes to achieve comparable performance, with lower training time and complexity, than using the Jensen-Shannon Divergence (JSD) loss [\[15,](#page-8-7) [42\]](#page-9-6). Our motivation to not employ the JSD loss is the reduced training time due to less computational complexity. In our experiments, for comparison purposes, we also use the JSD loss in the auxiliary setting, where training batches are augmented using AFA and go through auxiliary components. We report results in Sec. [5.3](#page-6-1) (Fig. [6\)](#page-7-1).

## 5. Experiments and results

We compare AFA with other popular augmentation techniques, evaluating robustness to common corruptions, generalization abilities and consistency to time-dependent increasing perturbations, on benchmark datasets.

#### 5.1. Experiment setup

Datasets. We trained models on the CIFAR-10 (C10) [\[20\]](#page-8-23), CIFAR-100 (C100) [\[21\]](#page-8-24), TinyImageNet (TIN) [\[22\]](#page-8-25) and ImageNet (IN) [\[4\]](#page-8-26) datasets and evaluate them on the corresponding robustness benchmark datasets, namely C10-C, C100-C, TIN-C, IN-C [\[14\]](#page-8-1), IN-C [ ¯ [32\]](#page-9-9), and IN-3DCC [\[18\]](#page-8-10). For ImageNet-trained models, we further evaluate their generalisation performance on the IN-v2 [\[35\]](#page-9-10) and IN-R datasets [\[13\]](#page-8-9), and consistency of performance on timedependent increasing perturbations on the IN-P dataset [\[14\]](#page-8-1). Architectures and training details. We train ResNet [\[12\]](#page-8-19) and Compact Convolution Transformers (CCTs) [\[6\]](#page-8-27). We train ResNet-18 and CCT-7/3x1 (32 resolution) on C-10, C-100, and only ResNet-18 on TIN. In the case of ImageNet, we train ResNet-18, ResNet-50 and CCT-14/7x2 (224 resolution). Under auxiliary setting, we use the DuBIN variant of ResNet [\[42\]](#page-9-6). We always use standard transforms [\[12\]](#page-8-19) before other augmentations. Implementation details and hyperparameter configurations are in Appendix [A.](#page-10-0) We release code and models[\\*](#page-4-0) .

Evaluation metrics. We evaluate the classification accuracy on the original test set, which we refer to as standard accuracy (SA), and the average classification accuracy over all corruptions in the robustness benchmarks as robustness accuracy (RA). This provides direct comparison between model performance on original and corruption benchmark datasets. We also compute the mean corruption error (mCE) [\[14\]](#page-8-1) for TIN and IN (for CIFAR there are no baselines advised) to evaluate the normalized robustness of models against image corruptions, the mean flip rate (mFR) and the mean top-5 distance (mT5D) to evaluate the consistency performance of models against increasing perturbations. For the evaluation of generalization performance, we compute the accuracy on the ImageNet-R and ImageNet-v2 test sets (note that ImageNet-v2 has 3 test sets, and we report the average accuracy on them). More details about the metrics are in Appendix [B.](#page-10-1)

#### 5.2. Results

Comparison with AugMax. We first report a direct comparison with AugMax [\[42\]](#page-9-6) in Tab. [2,](#page-4-1) as AFA addresses the computational shortcomings of generating adversarial augmentations via PGD iterations, and of using a JSD loss for alignment of the distribution of original and (adversarially) augmented images. We use AugMix as main augmentation, as in AugMax, and ablate on the use of JSD and ACE loss.

We show that AFA achieves comparable (or better) performance than AugMax, despite it being much less computational intensive. We indeed demonstrate that we can generate adversarial augmentations by only adding (weighted) Fourier-basis waves per color channel, not requiring PGD steps, and can train the models using an extra cross-entropy instead of the expensive JSD loss. The improvements granted by our approach are particularly evident in the case of ImageNet (using ACE), where we gain 1.6% of standard accuracy and 4.1% of robust accuracy (5.6% mCE) performance w.r.t. AugMax. Considering the increased computational efficiency and the simplicity of adversarial augmentation method, AFA is a more versatile and effective tool than AugMax. Hence, in the rest of the paper, we do not report further results of the AugMax framework, due to its high computational requirements, which complicate the training

<span id="page-4-1"></span>

| -    | Main    | Auxiliary | SA↑   | RA↑   | mCE↓  |
|------|---------|-----------|-------|-------|-------|
|      | AugMix† | ✗         | 95.47 | 86.48 | -     |
| C10  | AugMix† | AugMax    | 95.76 | 90.36 | -     |
|      | AugMix† | AFA       | 95.24 | 89.96 | -     |
|      | AugMix  | AFA       | 95.44 | 89.81 | -     |
|      | AugMix† | ✗         | 78.72 | 61.61 | -     |
| C100 | AugMix† | AugMax    | 78.69 | 65.75 | -     |
|      | AugMix† | AFA       | 78.99 | 65.96 | -     |
|      | AugMix  | AFA       | 77.80 | 66.69 | -     |
|      | AugMix† | ✗         | 64.65 | 36.30 | 83.90 |
| TIN  | AugMix† | AugMax    | 62.21 | 38.67 | 80.72 |
|      | AugMix† | AFA       | 64.34 | 38.53 | 80.79 |
|      | AugMix  | AFA       | 62.51 | 38.67 | 80.83 |
|      | AugMix† | ✗         | 65.2  | 31.5  | 87.1  |
| IN   | AugMix† | AugMax    | 66.5  | 36.5  | 80.6  |
|      | AugMix† | AFA       | 65.0  | 36.8  | 80.4  |
|      | AugMix  | AFA       | 68.1  | 41.1  | 75.0  |

Table 2. Comparison of AFA and AugMax (with AugMix for visual augmentation [\[42\]](#page-9-6)), with a ResNet18 backbone. The mark † indicates the use of the JSD loss, otherwise the ACE loss is used.

<span id="page-4-0"></span><sup>\*</sup>Code and models: <https://github.com/nis-research/afa-augment>

<span id="page-5-1"></span><span id="page-5-0"></span>

|          |         |     |        | Robustness |         |        |           |        |         | Generalisation |               | Consistency |          |
|----------|---------|-----|--------|------------|---------|--------|-----------|--------|---------|----------------|---------------|-------------|----------|
|          |         |     |        |            | IN-C    |        | ¯<br>IN-C |        | IN-3DCC | IN-R           | IN-v2         |             | IN-P     |
|          | Main    | Aux | SA (↑) | RA (↑)     | mCE (↓) | RA (↑) | mCE (↓)   | RA (↑) | mCE (↓) | Acc. (↑)       | Avg. Acc. (↑) | mFP (↓)     | mT5D (↓) |
|          | -       | ✗   | 68.9   | 32.9       | 84.7    | 34.8   | 87.0      | 34.9   | 84.4    | 33.1           | 64.3          | 72.8        | 87.0     |
|          | -       | AFA | 68.2   | 35.9       | 81.0    | 41.7   | 78.3      | 37.1   | 81.7    | 32.8           | 63.7          | 64.2        | 76.8     |
|          | AugMix† | ✗   | 65.2   | 31.5       | 87.1    | 34.6   | 87.3      | 32.1   | 88.3    | 28.2           | 59.5          | 80.2        | 86.2     |
|          | AugMix† | AFA | 65.0   | 36.8       | 80.4    | 40.9   | 79.3      | 36.0   | 83.2    | 30.6           | 60.9          | 60.1        | 68.5     |
| ResNet18 | AugMix  | AFA | 68.1   | 41.1       | 75.0    | 45.2   | 73.3      | 38.9   | 79.4    | 35.2           | 63.2          | 68.5        | 81.7     |
|          | PRIME   | ✗   | 66.0   | 43.6       | 72.0    | 42.0   | 78.1      | 42.4   | 75.2    | 36.9           | 61.4          | 54.7        | 65.3     |
|          | PRIME   | AFA | 67.2   | 47.2       | 67.8    | 47.3   | 71.1      | 43.8   | 73.5    | 37.8           | 63.0          | 52.3        | 63.7     |
|          | TA+     | ✗   | 68.9   | 36.9       | 80.1    | 35.9   | 85.6      | 38.6   | 79.7    | 32.6           | 63.7          | 68.1        | 81.4     |
|          | TA+     | AFA | 67.8   | 41.4       | 74.7    | 42.9   | 76.7      | 41.1   | 76.5    | 35.4           | 62.7          | 59.9        | 72.3     |
|          | -       | ✗   | 75.6   | 39.2       | 76.7    | 39.9   | 79.4      | 41.2   | 76.1    | 36.2           | 70.8          | 58.0        | 78.4     |
|          | -       | AFA | 76.5   | 46.2       | 68.0    | 47.6   | 69.4      | 46.2   | 69.8    | 38.1           | 72.0          | 48.0        | 67.2     |
|          | APR-SP  | ✗   | 71.9   | 42.9       | 72.7    | 45.9   | 72.5      | 39.8   | 78.4    | 34.9           | 67.2          | 60.2        | 75.4     |
|          | APR-SP  | AFA | 74.4   | 47.6       | 66.7    | 51.4   | 64.9      | 42.6   | 74.6    | 38.7           | 69.3          | 54.9        | 72.6     |
| ResNet50 | AugMix† | ✗   | 74.7   | 43.4       | 72.0    | 44.6   | 73.3      | 41.9   | 75.5    | 33.0           | 70.0          | 60.9        | 72.5     |
|          | AugMix† | AFA | 75.6   | 50.6       | 62.9    | 51.8   | 64.0      | 47.6   | 68.3    | 36.3           | 71.2          | 44.5        | 56.1     |
|          | AugMix  | AFA | 76.6   | 49.1       | 64.7    | 52.5   | 62.9      | 46.3   | 69.6    | 41.0           | 71.8          | 52.2        | 72.2     |
|          | PRIME   | ✗   | 72.1   | 49.2       | 64.9    | 46.4   | 71.5      | 47.2   | 68.8    | 38.5           | 67.8          | 45.4        | 58.1     |
|          | PRIME   | AFA | 74.5   | 53.9       | 59.2    | 54.2   | 61.3      | 50.2   | 65.0    | 40.9           | 69.8          | 40.4        | 54.8     |
|          | TA+     | ✗   | 75.9   | 43.4       | 71.7    | 41.8   | 77.1      | 44.7   | 71.6    | 37.1           | 70.3          | 51.9        | 70.4     |
|          | TA+     | AFA | 76.6   | 50.3       | 63.1    | 49.7   | 66.7      | 49.6   | 65.4    | 40.0           | 72.2          | 45.1        | 64.5     |
|          | -       | ✗   | 76.4   | 43.9       | 70.7    | 50.3   | 65.6      | 43.4   | 73.2    | 35.6           | 71.2          | 48.3        | 72.9     |
|          | -       | AFA | 76.9   | 51.9       | 61.0    | 58.5   | 55.4      | 50.7   | 64.4    | 39.0           | 71.9          | 38.4        | 61.8     |
|          | AugMix  | ✗   | 76.1   | 47.3       | 66.8    | 52.2   | 63.1      | 45.3   | 71.0    | 37.9           | 70.7          | 49.3        | 72.8     |
| CCT      | AugMix  | AFA | 77.4   | 56.5       | 55.6    | 60.8   | 52.2      | 51.8   | 62.8    | 41.0           | 72.5          | 37.9        | 59.9     |
|          | PRIME   | ✗   | 73.6   | 54.1       | 58.6    | 54.5   | 60.8      | 50.7   | 64.4    | 39.2           | 68.7          | 36.1        | 53.0     |
|          | PRIME   | AFA | 76.6   | 58.7       | 52.8    | 61.2   | 52.0      | 54.5   | 59.4    | 43.2           | 71.9          | 31.9        | 51.2     |
|          | TA+     | ✗   | 77.1   | 50.2       | 63.2    | 54.1   | 60.7      | 49.3   | 65.8    | 38.2           | 72.1          | 41.8        | 66.3     |
|          | TA+     | AFA | 76.9   | 56.0       | 56.0    | 59.1   | 54.6      | 53.1   | 61.1    | 41.1           | 72.1          | 36.4        | 58.5     |

Table 3. Robustness, generalization and consistency results on ImageNet-based benchmarks. Models with † use the JSD loss. TrivialAugment (TA) has overlapping augmentations with IN-C (<sup>+</sup>), and no other overlaps with other datasets. The green colour indicates an improvement when the main augmentation is combined with AFA, while red indicates no improvement. Results marked with bold/bold are the best for a particular architecture.

of larger models (e.g. ResNet-50 and CCT).

Robustness, generalization and consistency. In Tab. [3,](#page-5-0) we report results achieved by AFA combined with different visual augmentation methods, AugMix, PRIME, TrivialAugment (TA), to train different architectures (ResNet, CCT). We evaluate robustness to common corruptions on IN-C, IN-C and IN-3DCC, OOD generalisation on IN-v2 and IN- ¯ R, and consistency w.r.t. increasing perturbations on IN-P.

AFA generally contributes to a boost of performance (green colored results in Tab. [3\)](#page-5-0) when combined with different visual augmentation techniques, reducing the robustness and generalization gap for different model architectures. When compared to another Fourier-based augmentation technique, APR-SP [\[5\]](#page-8-16), AFA outperforms it on all benchmarks when trained with only standard augmentation techniques. Also in the case of AugMix and AFA, we record better overall performance over AugMix alone, when using both the ACE loss and loss with JSD. For the transformer architecture CCT, training with AFA contributes to an even stronger improvement in all tests. These results stay consistent for smaller resolution datasets (CIFAR and TIN), as we report at the end of this section.

Robustness to high-severity corruptions. AFA contributes to a consistent improvement of robustness of models at increasing corruption severity. We compute the relative corruption error, namely the difference between the corruption error of models trained with a visual augmentation technique only and those trained with both visual augmentations and AFA, and report it in Fig. [4](#page-6-2) for different corruption severity. A positive value indicates that models trained with the addition of AFA have better robustness. For higher corruption severity, AFA contributes to stronger robustness, measured by an increase in the relative corruption error in Fig. [4.](#page-6-2) The improvements obtained by AFA on IN-3DCC are slightly less pronounced than those on IN-C and IN-C¯. This is attributable to the specific corruptions in IN-3DCC that concern 3D geometric information, and are somewhat more complicated image transformations. However, AFA contributes to a substantial improvement w.r.t. to models trained without it. We thus highlight that AFA is

<span id="page-6-5"></span><span id="page-6-2"></span>![](_page_6_Figure_0.jpeg)

**Caption:** Figure 4 shows the relative error per corruption severity, highlighting the robustness improvements of models trained with AFA. Positive values indicate better performance with AFA, particularly under high-severity corruptions.

Figure 4. Relative error per corruption severity, computed by subtracting the classification error of models trained with PRIME, TrivialAugment, and AugMix with that of corresponding models trained with PRIME+AFA, TrivialAugment+AFA, and AugMix+AFA.

<span id="page-6-3"></span>![](_page_6_Figure_2.jpeg)

**Caption:** Figure 5 features Fourier heatmaps for ResNet18 models, illustrating classification errors under frequency perturbations. Models trained with AFA exhibit enhanced robustness, especially against middle-high frequency noise.

Figure 5. Fourier heatmaps of ResNet18 trained with standard setup, and PRIME and TrivialAugment, with and without AFA.

very beneficial for increasing robustness to aggressive corruptions of the test images. Details of the results at different severity are in Appendix [C.3.](#page-11-0)

Fourier heatmap: robustness in the frequency spectrum. We further evaluate the robustness of models to perturbations at specific frequencies, using test images perturbed with frequency noises according to [\[49\]](#page-9-13). We present the results in the form of Fourier heatmaps, see Fig. [5](#page-6-3) for heatmaps of ResNet18 models (trained on ImageNet), and in Appendix [C.2](#page-11-1) for the heatmaps of CCT models. The intensity of a pixel at location (u, v) in the heatmap indicates the classification error of a model tested on images perturbed by Fourier noise at frequency (u, v) in the frequency spectrum (implementation details are in Appendix [B\)](#page-10-1). ResNet18 trained with standard augmentations setting (baseline) is very sensitive to perturbations at low and middle-high frequency (see Fig. [5\)](#page-6-3), while those trained with visual augmentations like PRIME and TrivialAugment (TA) still show vulnerability at low and middle-high frequency noise. When training models with AFA, i.e. PRIME+AFA and TA+AFA, the models become more robust to frequency pertubations, especially at middle-high frequency. AFA can provide extensive robustness to frequency perturbations and bridge the robustness gap that visual augmentation might not cover.

Results on CIFAR and TIN. In Tab. [4,](#page-6-4) we present the robustness results on smaller resolution datasets, C10 and

<span id="page-6-4"></span>

|          |         |           | C10-C |       | C100-C |       |  |
|----------|---------|-----------|-------|-------|--------|-------|--|
| -        | Main    | Auxiliary | SA↑   | RA↑   | SA↑    | RA↑   |  |
|          | -       | ✗         | 94.15 | 73.67 | 78.27  | 48.30 |  |
|          | -       | AFA       | 94.69 | 88.22 | 77.91  | 62.53 |  |
| ResNet18 | AugMix† | ✗         | 95.47 | 86.48 | 78.72  | 61.61 |  |
|          | AugMix† | AFA       | 95.24 | 89.96 | 78.99  | 65.96 |  |
|          | PRIME   | ✗         | 94.38 | 89.81 | 75.49  | 66.16 |  |
|          | PRIME   | AFA       | 94.54 | 90.64 | 76.16  | 68.48 |  |
|          | -       | ✗         | 95.67 | 80.45 | 78.37  | 54.20 |  |
|          | -       | AFA       | 95.94 | 88.13 | 77.47  | 61.40 |  |
| CCT      | AugMix  | ✗         | 95.10 | 85.42 | 75.79  | 60.83 |  |
|          | AugMix  | AFA       | 95.93 | 90.57 | 77.22  | 66.18 |  |
|          | PRIME   | ✗         | 95.30 | 90.56 | 76.65  | 67.92 |  |
|          | PRIME   | AFA       | 95.49 | 91.40 | 76.50  | 67.89 |  |
| CVT      | -       | ✗         | 94.31 | 77.02 | 75.53  | 48.25 |  |
|          | -       | AFA       | 94.53 | 87.03 | 76.96  | 60.12 |  |
| VIT      | -       | ✗         | 94.46 | 75.97 | 74.26  | 50.88 |  |
|          | -       | AFA       | 94.58 | 86.71 | 75.13  | 58.25 |  |

Table 4. Results for C10-C and C100-C with ResNet18, CCT. CVT and ViT-Lite. Models with † use loss with JSD.

C100. The results on TIN are in Appendix [C.1.](#page-11-2) There results are inline with those reported on IN in Tab. [3.](#page-5-0)

#### <span id="page-6-1"></span>5.3. Ablation

<span id="page-6-0"></span>Auxiliary components. We investigate the contribution and importance of the auxiliary components in improving model robustness. We trained models with AFA-augmented images, passing through only the main components or the auxiliary components. The results in Tab. [5,](#page-7-0) i.e. lower RA and higher mCE of models trained with AFA applied only in the main components, highlight the importance of AFA auxiliary components. The auxiliary components play a crucial role in mitigating the impact of aggressive adversarial distribution shifts induced by AFA. By doing so, they

<span id="page-7-0"></span>

| -    | Main | Auxiliary | SA↑   | RA↑   | mCE↓   |
|------|------|-----------|-------|-------|--------|
|      | -    | ✗         | 94.15 | 73.67 | -      |
| C10  | AFA  | ✗         | 92.36 | 83.25 | -      |
|      | -    | AFA       | 94.69 | 88.22 | -      |
|      | -    | ✗         | 78.27 | 48.30 | -      |
| C100 | AFA  | ✗         | 72.34 | 58.70 | -      |
|      | -    | AFA       | 77.91 | 62.53 | -      |
|      | -    | ✗         | 61.64 | 23.91 | 100.00 |
| TIN  | AFA  | ✗         | 59.04 | 28.87 | 93.45  |
|      | -    | AFA       | 62.52 | 33.35 | 87.58  |
|      | -    | ✗         | 68.9  | 32.9  | 84.7   |
| IN   | AFA  | ✗         | 66.7  | 33.3  | 84.4   |
|      | -    | AFA       | 68.2  | 35.9  | 81.0   |

Table 5. Ablation results ResNet18 trained with and without Auxiliary Components on C10, C100, TinyImageNet and ImageNet.

contribute to model ability to learn from the original distribution, while AFA facilitates learning robustness to distribution shifts. This is also highlighted in the substantial decrease in SA for models not employing auxiliary components. While model robustness improves under both settings, the performance gain for the auxiliary setting is three to five percentage points higher across all datasets.

ACE vs JSD. As part of our method, we replaced the use of JSD with ACE which is less computationally burdening. We thus performed an ablation analysis of the tradeoff of using JSD. We report results for robustness using mCE and Robust Accuracy (RA) in Fig. [6,](#page-7-1) and observe that JSD does not significantly improve the robustness of our model to image corruptions, despite it being more computationally heavy than using ACE. Using JSD also results in slightly worse robustness on C100. Given the minimal differences, we opt for the simpler ACE loss for training with the AFA augmentation pipeline and only using JSD if other techniques (e.g. AugMix) employ them.

Effect of hyperparameter 1/λ. We studied also the contribution of the mean 1/λ of the exponential distribution that we use to sample the weight factor for the channelwise application of the Fourier-basis augmentations. We provide the results in Fig. [7,](#page-7-2) and observe that our method has low sensitivity to the choice of the rate parameter. This is attributable to the choice of the exponential distribution that allows larger values to be sampled even if they are less likely. We indeed observe that larger values of 1/λ, which result in larger perturbations (in the range of 10 to 15), result in stronger gains in robustness. At the same time, there is no clear trend in the standard accuracy on the clean dataset, with only minimal variations for the larger values, indicating that the choice of the 1/λ value does not have a specific influence on the correct functioning of AFA.

## 6. Conclusions

We proposed an efficient data augmentation technique called AFA, which complements existing visual augmenta-

<span id="page-7-1"></span>![](_page_7_Figure_7.jpeg)

**Caption:** Figure 6 compares the performance of models trained with and without the JSD term in the auxiliary component. Results indicate that using JSD does not significantly enhance robustness, suggesting AFA's effectiveness without added complexity.

Figure 6. Comparison of using objective with and without the JSD term. All models are ResNet-18 trained with only AFA in the auxiliary component and no other augmentations. When used with JSD two batches passed through Auxiliary components and there was no main augmentation (in total 3 batches, 1 clean and 2 AFA).

<span id="page-7-2"></span>![](_page_7_Figure_9.jpeg)

**Caption:** Figure 7 depicts the trend of mean corruption error (mCE) and standard accuracy (SA) against the rate parameter in AFA training. The results show that larger perturbations improve robustness while maintaining stable standard accuracy.

Figure 7. Trend of the mCE and SA with respect to the rate parameter. The models were trained using AFA in the auxiliary setting and no other augmentations for the main.

tion techniques by filling the augmentation gap, that they do not cover in the Fourier domain. AFA perturbs the frequency components of images and generates adversarial samples. By leveraging Fourier-basis functions and the auxiliary augmentation setting we demonstrate that AFA allows the models to learn from aggressive/adversarial input changes. We performed extensive experiments on benchmark datasets, and demonstrated that AFA benefits the robustness of models against common image corruptions, the consistency of predictions when facing increasing perturbations, and the OOD generalization performance. Being complementary to other augmentation techniques, AFA can further boost the robustness of models, especially against strong corruptions and perturbation, and it also results in better robustness in the frequency spectrum. We foresee that investigating the use of Fourier-basis functions on the training process of neural networks would provide promising improvement to model performance, thus encouraging their reliability in real scenarios.

## References

- <span id="page-8-3"></span>[1] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations, 2020. [1](#page-0-1)
- <span id="page-8-8"></span>[2] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation policies from data, 2019. [1,](#page-0-1) [2](#page-1-1)
- <span id="page-8-21"></span>[3] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, *Advances in Neural Information Processing Systems*, volume 33, pages 18613– 18624. Curran Associates, Inc., 2020. [2](#page-1-1)
- <span id="page-8-26"></span>[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In *2009 IEEE Conference on Computer Vision and Pattern Recognition*, pages 248–255, 2009. [5](#page-4-2)
- <span id="page-8-16"></span>[5] Chen *et al*. Amplitude-phase recombination: Rethinking robustness of convolutional neural networks in frequency domain, 2021. [2,](#page-1-1) [3,](#page-2-2) [6](#page-5-1)
- <span id="page-8-27"></span>[6] Hassani *et al*. Escaping the big data paradigm with compact transformers, 2022. [5](#page-4-2)
- <span id="page-8-4"></span>[7] Fartash Faghri, Hadi Pouransari, Sachin Mehta, Mehrdad Farajtabar, Ali Farhadi, Mohammad Rastegari, and Oncel Tuzel. Reinforce data, multiply impact: Improved model accuracy and robustness with dataset reinforcement, 2023. [1](#page-0-1)
- <span id="page-8-5"></span>[8] Zhiqiang Gao, Kaizhu Huang, Rui Zhang, Dawei Liu, and Jieming Ma. Towards better robustness against common corruptions for unsupervised domain adaptation. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 18882–18893, October 2023. [1](#page-0-1)
- <span id="page-8-0"></span>[9] Antonio Greco, Nicola Strisciuglio, Mario Vento, and Vincenzo Vigilante. Benchmarking deep networks for facial emotion recognition in the wild. *Multimedia Tools and Applications*, 82(8):11189–11220, 2023. [1](#page-0-1)
- <span id="page-8-6"></span>[10] Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston Zhang, Wanqian Zhang, Bo Li, and Mu Li. Mixgen: A new multi-modal data augmentation. In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops*, pages 379–389, January 2023. [1](#page-0-1)
- <span id="page-8-28"></span>[11] Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen Li, and Humphrey Shi. Escaping the Big Data Paradigm with Compact Transformers. *arXiv*, Apr. 2021. [11](#page-10-2)
- <span id="page-8-19"></span>[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015. [2,](#page-1-1) [5](#page-4-2)
- <span id="page-8-9"></span>[13] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization, 2021. [1,](#page-0-1) [5](#page-4-2)
- <span id="page-8-1"></span>[14] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations, 2019. [1,](#page-0-1) [5,](#page-4-2) [11](#page-10-2)
- <span id="page-8-7"></span>[15] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty. *arXiv*, Dec. 2019. [1,](#page-0-1) [2,](#page-1-1) [5,](#page-4-2) [11](#page-10-2)
- <span id="page-8-11"></span>[16] Ignacio Hounie, Luiz F. O. Chamon, and Alejandro Ribeiro.

Automatic data augmentation via invariance-constrained learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, *Proceedings of the 40th International Conference on Machine Learning*, volume 202 of *Proceedings of Machine Learning Research*, pages 13410–13433. PMLR, 23–29 Jul 2023. [1](#page-0-1)

- <span id="page-8-2"></span>[17] Christoph Kamann and Carsten Rother. Benchmarking the robustness of semantic segmentation models with respect to common corruptions. *International Journal of Computer Vision*, 129(2):462–483, Feb 2021. [1](#page-0-1)
- <span id="page-8-10"></span>[18] Oguzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir ˘ Zamir. 3d common corruptions and data augmentation, 2022. [1,](#page-0-1) [5](#page-4-2)
- <span id="page-8-20"></span>[19] Ildoo Kim, Younghoon Kim, and Sungwoong Kim. Learning loss for test-time augmentation. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, *Advances in Neural Information Processing Systems*, volume 33, pages 4163–4174. Curran Associates, Inc., 2020. [2](#page-1-1)
- <span id="page-8-23"></span>[20] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). [5](#page-4-2)
- <span id="page-8-24"></span>[21] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced research). [5](#page-4-2)
- <span id="page-8-25"></span>[22] Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. 2015. [5](#page-4-2)
- <span id="page-8-15"></span>[23] Xiu-Chuan Li, Xu-Yao Zhang, Fei Yin, and Cheng-Lin Liu. F-mixup: Attack cnns from fourier perspective. In *2020 25th International Conference on Pattern Recognition (ICPR)*, pages 541–548, 2021. [1](#page-0-1)
- <span id="page-8-17"></span>[24] Chang Liu, Wenzhao Xiang, Yuan He, Hui Xue, Shibao Zheng, and Hang Su. Improving model generalization by on-manifold adversarial augmentation in the frequency domain, 2023. [2,](#page-1-1) [3](#page-2-2)
- <span id="page-8-14"></span>[25] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey, 2023. [1](#page-0-1)
- <span id="page-8-12"></span>[26] Siao Liu, Zhaoyu Chen, Yang Liu, Yuzheng Wang, Dingkang Yang, Zhile Zhao, Ziqing Zhou, Xie Yi, Wei Li, Wenqiang Zhang, and Zhongxue Gan. Improving generalization in visual reinforcement learning via conflict-aware gradient agreement augmentation. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 23436–23446, October 2023. [1](#page-0-1)
- <span id="page-8-13"></span>[27] Yang Liu, Shen Yan, Laura Leal-Taixe, James Hays, and ´ Deva Ramanan. Soft augmentation for image classification. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 16241– 16250, June 2023. [1](#page-0-1)
- <span id="page-8-18"></span>[28] Yuyang Long, Qilong Zhang, Boheng Zeng, Lianli Gao, Xianglong Liu, Jian Zhang, and Jingkuan Song. Frequency domain model augmentation for adversarial attack. In Shai Avidan, Gabriel Brostow, Moustapha Cisse, Giovanni Maria ´ Farinella, and Tal Hassner, editors, *Computer Vision – ECCV 2022*, pages 549–566, Cham, 2022. Springer Nature Switzerland. [2](#page-1-1)
- <span id="page-8-22"></span>[29] Yuyang Long, Qilong Zhang, Boheng Zeng, Lianli Gao, Xianglong Liu, Jian Zhang, and Jingkuan Song. Frequency domain model augmentation for adversarial attack. In Shai Avidan, Gabriel Brostow, Moustapha Cisse, Giovanni Maria ´

Farinella, and Tal Hassner, editors, *Computer Vision – ECCV 2022*, pages 549–566, Cham, 2022. Springer Nature Switzerland. [2](#page-1-1)

- <span id="page-9-11"></span>[30] Guozheng Ma, Linrui Zhang, Haoyu Wang, Lu Li, Zilin Wang, Zhen Wang, Li Shen, Xueqian Wang, and Dacheng Tao. Learning better with less: Effective augmentation for sample-efficient visual reinforcement learning, 2023. [1](#page-0-1)
- <span id="page-9-12"></span>[31] Juliette Marrie, Michael Arbel, Diane Larlus, and Julien Mairal. Slack: Stable learning of augmentations with coldstart and kl regularization. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 24306–24314, June 2023. [1](#page-0-1)
- <span id="page-9-9"></span>[32] Eric Mintun, Alexander Kirillov, and Saining Xie. On interaction between augmentations and corruptions in natural corruption robustness, 2021. [1,](#page-0-1) [5](#page-4-2)
- <span id="page-9-8"></span>[33] Apostolos Modas, Rahul Rade, Guillermo Ortiz-Jimenez, ´ Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. PRIME: A few primitives can boost robustness to common corruptions. *arXiv*, Dec. 2021. [1,](#page-0-1) [2,](#page-1-1) [13](#page-12-2)
- <span id="page-9-7"></span>[34] Samuel G. Muller and Frank Hutter. TrivialAugment: ¨ Tuning-free Yet State-of-the-Art Data Augmentation. *arXiv*, Mar. 2021. [1,](#page-0-1) [2,](#page-1-1) [3](#page-2-2)
- <span id="page-9-10"></span>[35] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet?, 2019. [1,](#page-0-1) [5](#page-4-2)
- <span id="page-9-14"></span>[36] Tonmoy Saikia, Cordelia Schmid, and Thomas Brox. Improving robustness against common corruptions with frequency biased models. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 10211–10220, October 2021. [2](#page-1-1)
- <span id="page-9-18"></span>[37] Ryan Soklaski, Michael Yee, and Theodoros Tsiligkaridis. Fourier-Based Augmentations for Improved Robustness and Uncertainty Calibration. *arXiv*, Feb. 2022. [2](#page-1-1)
- <span id="page-9-0"></span>[38] Nicola Strisciuglio and George Azzopardi. Visual response inhibition for increased robustness of convolutional networks to distribution shifts. In *NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications*, 2022. [1](#page-0-1)
- <span id="page-9-1"></span>[39] Nicola Strisciuglio, Manuel Lopez-Antequera, and Nicolai Petkov. Enhanced robustness of convolutional networks with a push–pull inhibition layer. *Neural Computing and Applications*, 32(24):17957–17971, 2020. [1](#page-0-1)
- <span id="page-9-21"></span>[40] Teppei Suzuki. Teachaugment: Data augmentation optimization using teacher knowledge. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 10904–10914, June 2022. [2](#page-1-1)
- <span id="page-9-15"></span>[41] An Wang, Mobarakol Islam, Mengya Xu, and Hongliang Ren. Curriculum-based augmented fourier domain adaptation for robust medical image segmentation. *IEEE Transactions on Automation Science and Engineering*, pages 1–13, 2023. [2,](#page-1-1) [3](#page-2-2)
- <span id="page-9-6"></span>[42] Haotao Wang, Chaowei Xiao, Jean Kossaifi, Zhiding Yu, Anima Anandkumar, and Zhangyang Wang. AugMax: Adversarial Composition of Random Augmentations for Robust Training. *arXiv*, Oct. 2021. [1,](#page-0-1) [2,](#page-1-1) [3,](#page-2-2) [4,](#page-3-1) [5,](#page-4-2) [11,](#page-10-2) [13](#page-12-2)
- <span id="page-9-17"></span>[43] Shunxin Wang, Christoph Brune, Raymond Veldhuis, and Nicola Strisciuglio. DFM-x: Augmentation by leveraging prior knowledge of shortcut learning. In *4th Visual Inductive Priors for Data-Efficient Deep Learning Workshop*, 2023. [2,](#page-1-1)

[3](#page-2-2)

- <span id="page-9-22"></span>[44] Shunxin Wang, Raymond Veldhuis, Christoph Brune, and Nicola Strisciuglio. Frequency shortcut learning in neural networks. In *NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications*, 2022. [3](#page-2-2)
- <span id="page-9-5"></span>[45] Shunxin Wang, Raymond Veldhuis, Christoph Brune, and Nicola Strisciuglio. Larger is not Better: A Survey on the Robustness of Computer Vision Models against Common Corruptions. *arXiv*, May 2023. [1,](#page-0-1) [2](#page-1-1)
- <span id="page-9-23"></span>[46] Shunxin Wang, Raymond Veldhuis, Christoph Brune, and Nicola Strisciuglio. What do neural networks learn in image classification? a frequency shortcut perspective. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 1433–1442, October 2023. [3](#page-2-2)
- <span id="page-9-2"></span>[47] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves imagenet classification. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2020. [1](#page-0-1)
- <span id="page-9-16"></span>[48] Qinwei Xu, Ruipeng Zhang, Ziqing Fan, Yanfeng Wang, Yi-Yan Wu, and Ya Zhang. Fourier-based augmentation with applications to domain generalization. *Pattern Recognition*, 139:109474, 2023. [2,](#page-1-1) [3](#page-2-2)
- <span id="page-9-13"></span>[49] Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D. Cubuk, and Justin Gilmer. A Fourier Perspective on Model Robustness in Computer Vision. *arXiv*, June 2019. [1,](#page-0-1) [2,](#page-1-1) [7,](#page-6-5) [12](#page-11-3)
- <span id="page-9-3"></span>[50] Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, and Pinar Duygulu. Hybridaugment++: Unified frequency spectra perturbations for model robustness, 2023. [1](#page-0-1)
- <span id="page-9-20"></span>[51] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features, 2019. [2](#page-1-1)
- <span id="page-9-19"></span>[52] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In *International Conference on Learning Representations*, 2018. [2](#page-1-1)
- <span id="page-9-4"></span>[53] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the Robustness of Deep Neural Networks via Stability Training. *arXiv*, Apr. 2016. [1](#page-0-1)

## <span id="page-10-2"></span><span id="page-10-0"></span>A. Implementation Details

Below, we report the training setup in detail. For all methods, and a particular dataset and architecture, the same training setup was used unless stated otherwise.

Convolution Neural Networks For CIFAR-100 and Tiny ImageNet we use the SGD optimiser with an initial learning rate of 0.2, Nesterov momentum of 0.9 with a batch size of 128 training for 100 epochs. We use a weight decay of 0.0005 and we do not decay the affine parameters of normalisation. For CIFAR-10, we follow the same setup as above, except we train for 200 epochs with a batch size of 256 and an initial learning rate of 0.1. The learning rate is decayed with a cosine annealing schedule to 0 which is stepped step-wise. For all models, we always employ the standard transformation of random crop with a padding of 4 and random horizontal flip.

For ImageNet, we follow [\[15\]](#page-8-7) in that we use SGD optimiser with an initial learning rate of 0.1 and Nesterov momentum of 0.9 and train for 90 epochs. We use a weight decay of 0.0001 and we do not decay affine parameters of normalisation. The learning rate decays with a by a factor of 0.1 every 30 epochs. For all models, we employ the standard transformation of random resized crop to image size of 224×224 with bilinear interpolation and random horizontal flip, before other augmentations.

We choose to train all models from scratch (no finetuning using AFA) so that we can study the effects of AFA without other underlying factors. Therefore, for fair comparison, we retrain PRIME from scratch as well using our setup. For models trained with JSD, we follow [\[42\]](#page-9-6) for the regularising coefficient, mainly: λ = 10 for CIFAR-10 and Tiny ImageNet, λ = 1 for CIFAR-100 and λ = 12 for ImageNet.

We only use the main BN layers during testing, similarly to AugMax for all convolution models.

Compact Convolution Transformer For CIFAR-10/100 and ImageNet we also train a transformer architecture. For all datasets we use CutMix (alpha=1.0) and MixUp (alpha=0.2 for ImageNet and alpha=1.0 for CIFAR-10/100) with an equal chance of applying one of the two. For CIFAR-10/100, we follow [\[11\]](#page-8-28). We train using the AdamW optimiser with max learning rate of 0.0006 and weight decay of 0.06, and we do not decay the affine parameters of the normalisation modules. We train with an effective batch size of 256, and apply learning rate decay following a cosine decay with a warm-up period of 10 epochs and the learning rate scheduler is stepped step-wise. For ImageNet, we use a max learning rate of 0.0005, effective batch size of 1024 and a weight decay of 0.05. The learning rate decay follows a cosine annealing schedule with a warm-up of 25 epochs. The same standard transformations as for convolutional neural networks were applied.

## <span id="page-10-1"></span>B. Evaluation metrics

Mean corruption error (mCE) measures the robustness of models against image corruptions [\[14\]](#page-8-1), computed as:

$$
\text{mCE} = \frac{1}{|C|} \sum_{c \in C} \frac{\sum_{s=1}^{5} E_{s,c}^{f}}{\sum_{s=1}^{5} E_{s,c}^{baseline}},\tag{4}
$$

where the sum of classification error E of five severity s ∈ {1, 2, 3, 4, 5} per corruption c of model f is normalized by that of a baseline model. The normalized classification errors of all corruptions C in the dataset are averaged to obtain mCE. We use AlexNet as baseline in ImageNet experiments and ResNet-18 for Tiny ImageNet. For CIFAR-10/100 there are no baselines advised so we do not report the mCE for these datasets.

Mean flip rate (mFR) evaluates the consistency of model predictions with increasing perturbations [\[14\]](#page-8-1), computed as follows:

$$
\text{mFR} = \frac{1}{|C|} \sum_{c \in C} \text{FR}_c^f = \frac{1}{|C|} \sum_{c \in C} \frac{\text{FP}_c^f}{\text{FP}_c^{baseline}},\quad (5)
$$

with

$$
\text{FP}_c^f = \frac{1}{m(n-1)} \sum_{i=1}^m \sum_{j=2}^n \mathbb{1}(f(x_j^{(i)}) \neq f(x_{j-1}^{(i)})). \tag{6}
$$

1(f(x (i) j ) ̸= f(x (i) j−1 )) measures whether the prediction of the model f on a frame x<sup>j</sup> is the same as its previous perturbed frame in the i th sequence. If the predictions are the same, 1(f(x (i) j ) ̸= f(x (i) j−1 )) equals to zero, and thus the performance of the model is not affected by the considered perturbations. FP<sup>f</sup> <sup>c</sup> measures the consistency of predictions over m perturbed sequences, each with n of frames. For a sequence corrupted by noise, the predictions are compared with those of the first frame, as noise is not temporally related. The mFR is obtained by averaging the normalized FP<sup>f</sup> <sup>c</sup> by that of a baseline model across all the perturbations C. The value of mFR is expected to be close to zero for a robust model.

Mean top-5 distance (mT5D) also measures the consistency of model predictions in terms of increasing perturbations [\[14\]](#page-8-1). For a robust model, the top-5 predictions of frames over a sequence should be relevant to those of the previous frames in the sequence. The top-5 distance thus <span id="page-11-3"></span>measures the inconsistency of top-5 predictions under consecutive perturbations, computed as follows:

$$
T5D_c^f = \frac{1}{m(n-1)} \sum_{i=1}^m \sum_{j=2}^n d(\tau(x_j), \tau(x_{j-1})), \quad (7)
$$

with

$$
d(\tau(x_j), \tau(x_{j-1})) = \sum_{i=1}^{5} \sum_{j=\min\{i, \rho(i)\}+1}^{\max\{i, \rho(i)\}} 1(1 \le j-1 \le 5),
$$
\n(8)

where ρ(τ (x<sup>j</sup> )(k)) = τ (xj−1)(k), τ (x<sup>j</sup> ) is the ranking of predictions for a perturbed frame x<sup>j</sup> and τ (x<sup>j</sup> )(k) indicates the rank of the prediction being k. If τ (x<sup>j</sup> ) and τ (xj−1) are the same, then d(τ (x<sup>j</sup> ), τ (xj−1)) = 0. Averaging the normalized T5D by that of the baseline over all corruptions obtain mT5D = <sup>1</sup> |C| P c∈C T5D<sup>f</sup> c T5Dbaseline c .

Fourier heatmap evaluates model robustness from a Fourier perspective [\[49\]](#page-9-13) exploiting Fourier basis functions to perturb test images and measuring the classification error of models. They are constructed as follows. Let Ui,j ∈ Rd1×d<sup>2</sup> be a real-valued matrix such that its norm equals to 1. The Fourier transform of Ui,j has only two non-zero elements located at (i, j) and the corresponding symmetric coordinate with respect to the image center. Given an image X, a perturbed image with Fourier basis noise can be generated by X˜ i,j = X + rvUi,j , where r is chosen randomly from a uniform distribution ranging from -1 to 1, and v controls the strength of the added noise. Each channel of the images is perturbed independently with different r and v. The model robustness against Fourier basis noise Ui,j is evaluated by the classification error, and the final outcome is in a form of heatmap which records the error of the evaluated model under different Fourier basis noise. Examples are in Fig. [8.](#page-11-4)

## C. Supplementary results

#### <span id="page-11-2"></span>C.1. Results on Tiny ImageNet

In Tab. [6](#page-11-5) we provide the robustness results on Tiny ImageNet (TIN), which are consistent with those presented on other datasets. Models trained with AFA show robustness improvements consistently by significant margin with only negligible reduction of the clean accuracy. We again see that JSD improves robustness slightly, and in AugMix it improves clean accuracy greatly.

#### <span id="page-11-1"></span>C.2. Robustness in the frequency spectrum.

The Fourier heatmaps of CCT trained with standard setting, PRIME, PRIME+AFA, TA and TA+AFA are provided in Fig. [8.](#page-11-4) Our observations are consistent with those in the

<span id="page-11-5"></span>

|          |              |           |       | TIN-C |       |  |  |
|----------|--------------|-----------|-------|-------|-------|--|--|
| -        | Main         | Auxiliary | SA↑   | RA↑   | mCE↓  |  |  |
|          | -            | ✗         | 63.56 | 25.86 | 97.34 |  |  |
|          | AFA          | ✗         | 59.04 | 28.87 | 93.45 |  |  |
|          | -            | AFA       | 62.52 | 33.35 | 87.58 |  |  |
|          | AugMix       | ✗         | 62.95 | 36.26 | 84.05 |  |  |
|          | AugMix       | AFA       | 62.51 | 38.67 | 80.83 |  |  |
|          | AugMix†      | ✗         | 64.65 | 36.30 | 83.90 |  |  |
| ResNet18 | AugMix†      | AFA       | 64.34 | 38.52 | 80.79 |  |  |
|          | PRIME        | ✗         | 63.07 | 39.67 | 79.42 |  |  |
|          | PRIME<br>AFA |           | 62.48 | 41.09 | 77.55 |  |  |
|          | ✗<br>PRIME†  |           | 63.24 | 41.22 | 77.44 |  |  |
|          | PRIME†       | AFA       | 62.65 | 43.00 | 73.11 |  |  |

Table 6. Results for TIN-C with ResNet18. Models with † use loss with JSD.

<span id="page-11-4"></span>![](_page_11_Figure_13.jpeg)

**Caption:** Figure 8 presents Fourier heatmaps for various augmentation techniques, demonstrating that models trained with AFA achieve better robustness against low and middle-high frequency corruptions compared to standard methods.

Figure 8. Fourier heatmaps of CCT trained with standard setting, PRIME, PRIME+AFA, TA and TA+AFA.

main paper. Also CCT models trained with the contribution of AFA have better robustness to low and middle-high frequency corruptions.

#### <span id="page-11-0"></span>C.3. Robustness per corruption severity.

We report the classification error of models tested under corruptions with different severity levels Fig. [9.](#page-12-3) The models trained with AFA have consistently lower error than their counterpart trained without AFA, showing that AFA can further boost the robustness of models against common image corruptions, especially in difficult testing conditions with high severity.

#### C.4. Robustness to each image corruption.

Furthermore, we show the classification error averaged over five corruption severity levels per corruption type in Fig. [11.](#page-14-0) The error points of model trained with visual augmentations only, and with further use of AFA are connected by a line. A downward trend means models trained with AFA have better robustness performance on specific corruption types. We observe that, in general, models with AFA have better corruption robustness than models trained only with visual augmentations. Significant improvements are especially evident on noise corruptions (Gaussian noise, impulse noise, iso noise, plasma noise, shot noise, single frequency grayscale noise and cocentric sine waves). One exception is ResNet50 trained with AugMix and AFA, for which the

<span id="page-12-3"></span><span id="page-12-2"></span>![](_page_12_Figure_0.jpeg)

**Caption:** Figure 9 illustrates the averaged classification error per corruption type for models trained with visual augmentations and AFA. The results indicate that AFA consistently improves robustness across various corruption types.

Figure 9. Corruption error of ResNet50 and CCT trained with PRIME, PRIME+AFA, TA, TA+AFA, AugMix and AugMix+AFA. Models trained with AFA (orange points) have lower error at each severity than their counterpart trained with only visual augmentation (blue points), demonstrating the benefit of AFA to corruption robustness.

model trained without AFA performs better except on few cases. This can be attributed to the less training time (90 epoch vs 180 epochs) than that of ResNet50+AugMix.

## D. Evidence of adversarial nature of AFA

#### <span id="page-12-1"></span>D.1. Main and auxiliary batch normalisation

For the ResNet architecture, which includes Batch Normalisation layers, we had replaced the Batch Normalisation layers with DuBIN layers [\[42\]](#page-9-6) while operating the Auxiliary setting. Assuming that there is no difference in the distribution of images augmented using AFA and a typical visual augmentation technique, there should be no difference in the affine parameters learnt for each individual batch normalisation parameter (the main and the auxiliary).

We show in Fig. [12](#page-15-2) the Mean Absolute Difference of the same parameter between the main and the auxiliary component of the DuBIN layer at different depths of the model. We show the results for for models trained with ACE loss for ResNet-50 where AFA is paired with just standard transforms, AugMix, PRIME and Trivial Augment (TA).

We can see that at earlier depths the parameter differ largely, which is explained by the difference in distribution of a visually augmented and AFA augmented image. This difference converges to a lower value, which is again explained by the model attempting to extract similar features from the differently augmented images.

#### <span id="page-12-0"></span>D.2. Embedding Space Visualization

We compare how diverse are the augmentations of AFA are with respect to other methods. We follow the procedure in [\[33\]](#page-9-8). To reiterate the procedure, we randomly select 3 images from ImageNet, each one belonging to a different class. For each image, we generate 100 transformed instances using Standard Transform, Trivial Augment, PRIME, PGD attack with the following parameters: 5 steps, epsilon of 8/255 and alpha of 2/255, and with AFA. Then, we pass the transformed instances of each method through a ResNet-50 pre-trained on ImageNet using standard transform and training setup, and extract the features of its embedding space from the penultimate layer before the dense layer. On the features extracted for each method, we perform PCA after whitening and then visualize the projection of the features onto the first two principal components. We visualize the projected augmented space in Fig. [13,](#page-16-0) which demonstrates that AFA generates which are more akin to an adversarial attack rather than a standard augmen-

![](_page_13_Figure_0.jpeg)

**Caption:** Figure 10 visualizes the differences in embedding space for models trained with various augmentation methods. AFA-generated embeddings show greater separability and robustness against frequency perturbations compared to standard augmentations.

Figure 10. Averaged classification error per corruption of ResNet50s (orange) and CCTs (green). The error points of model trained with visual augmentations and additionally with AFA are connected. A decreasing line indicates better performance when trained additionally with AFA (a).

<span id="page-14-0"></span>![](_page_14_Figure_0.jpeg)

**Caption:** Figure 11 highlights the norm of convolutional layers' weights for ResNet models trained with different augmentation techniques. AFA demonstrates a strong regularization effect, akin to PRIME, enhancing model stability.

Figure 11. Averaged classification error per corruption of ResNet50s (orange) and CCTs (green). The error points of model trained with visual augmentations and additionally with AFA are connected. A decreasing line indicates better performance when models are trained additionally with AFA (b).

<span id="page-15-2"></span>![](_page_15_Figure_0.jpeg)

**Caption:** Figure 12 compares the mean absolute difference of learned affine parameters in batch normalization layers for models trained with AFA and standard augmentations, indicating significant distribution shifts due to AFA.

Figure 12. Comparison of the mean absolute difference of the learnt affine parameters for the two batch normalisations in the Dual Batch Norm Layers of ResNet50-DuBIN architecture at different depths.

tation. This is clear from a visual similarity of AFA's result in Fig. [13e](#page-16-0) to PGD's result in Fig. [13d](#page-16-0) and dissimilarity to the other Visual Augmentation techniques.

Finally, we also add in Fig. [13f](#page-16-0) the embedding space visualisation for the Auxiliary Trained model with AFA augmentation and standard transform for main, following the same procedure as above. We see that the model learns more separable embeddings for images augmented with AFA using the auxiliary setting, therefore is less sensitive to Frequency perturbation. The embeddings also retain a large variance and hardness, therefore showcasing the diversity of the augmentations of AFA.

# <span id="page-15-1"></span>E. Regularisation Effect

In Fig. [14](#page-16-1) we show the norm of the weights of the convolutional kernels for the ResNet50 models trained with and without AFA at each depth. We see that AFA provides a strong regularisation effect that is akin to the regularisation effect of PRIME. Meanwhile, we see that AugMix does not regularise the weights at all compared to the baseline model with only the standard transforms. The weights are however regularised to when AFA is paired with AugMix. Combined with PRIME, there does not seem to be further regularisation of the weights.

# <span id="page-15-0"></span>F. Proof of Augmenting Fourier Domain

<span id="page-15-3"></span>Lemma 1 (Linearity). *Let* f*,* g *be functions of a real variable and let* F(f) *and* F(g) *be their Fourier transforms. Then for complex numbers* a *and* b

$$
\mathscr{F}(af + bg) = a\mathscr{F}(f) + b\mathscr{F}(g),\tag{9}
$$

*therefore, Fourier transform* F *is a linear transformation.*

<span id="page-15-4"></span>Lemma 2 (Fourier Transform of Plane Wave). *The Fourier transform of the planar wave given by the frequency* f *and the direction* ω*,* Af,ω *has a fourier transform*

$$
\mathcal{F}(A_{f,\omega}) = \mathcal{F}\left(R\cos(2\pi f(u\cos(\omega) + v\sin(\omega))))\right)
$$
 (10)

$$
= \frac{R}{2} \left( \delta(\hat{x}, \hat{y}) + \delta(\bar{x}, \bar{y}) \right), \tag{11}
$$

*where,* xˆ = x − f cos(ω)*,* yˆ = y − f sin(ω) *and* x¯ = x + f cos(ω)*,* y¯ = y + f sin(ω)*.*

Theorem 1 (AFA Augments the Fourier Domain). *Given an image sample* s*, an augmentation using AFA produces as augmentation in the Fourier domain of the image for one specific frequency and orientation of the wave* (f, ω)*.*

*Proof.* Given image s and the randomly sampled planar wave using AFA, σAf,ω, dropping the subscript for the channels for clarity, we have:

$$
\mathcal{F}(AFA(s)) = \mathcal{F}(s + \sigma A_{f,\omega})
$$
  
=  $\mathcal{F}(s) + \sigma \mathcal{F}(A_{f,\omega})$   
(using Lemma 1) (12)

$$
= \mathcal{F}(s) + \frac{\sigma R}{2} (\delta(\hat{x}, \hat{y}) + \delta(\bar{x}, \bar{y})).
$$
 (13)  
(using Lemma 2)

Therefore, we prove augmenting an image s with AFA corresponds to augmenting the amplitude of a specific frequency component (f,ω) in the 2D Fourier transform of the image.

<span id="page-16-0"></span>![](_page_16_Figure_0.jpeg)

**Caption:** Figure 13 showcases the differences in embedding space for models trained with various methods, revealing that AFA leads to more adversarial-like augmentations, enhancing model robustness against perturbations.

Figure 13. Differences in the Embedding Space for Different Methods and PGD Attack. From (a)-(e) the standardly trained model is used, and for (f) the model trained in the auxiliary setting is used.

<span id="page-16-1"></span>![](_page_16_Figure_2.jpeg)

Figure 14. The norm of the Conv2d Layers for ResNet 50 trained with different augmentation techniques with and without AFA. The plot highlights the regularisation effect the methods have on the model weights.