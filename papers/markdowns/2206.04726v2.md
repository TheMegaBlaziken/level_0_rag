# COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive Learning

Yifei Zhang<sup>1</sup> , Hao Zhu2,<sup>3</sup> , Zixing Song<sup>1</sup> , Piotr Koniusz3,<sup>2</sup> , Irwin King<sup>1</sup>

The Chinese University of Hong Kong<sup>1</sup> , Hong Kong SAR, China Australian National University<sup>2</sup> and Data61/CSIRO<sup>3</sup> , Canberra, Australia {yfzhang,zxsong,king}@cse.cuhk.edu.hk allenhaozhu@gmail.com; piotr.koniusz@data61.csiro.au

## Abstract

Graph contrastive learning (GCL) improves graph representation learning, leading to SOTA on various downstream tasks. The graph augmentation step is a vital but scarcely studied step of GCL. In this paper, we show that the node embedding obtained via the graph augmentations is highly biased, somewhat limiting contrastive models from learning discriminative features for downstream tasks.Thus, instead of investigating graph augmentation in the input space, we alternatively propose to perform augmentations on the hidden features (feature augmentation). Inspired by so-called matrix sketching, we propose COSTA, a novel COvariance-preServing feaTure space Augmentation framework for GCL, which generates augmented features by maintaining a "good sketch" of original features. To highlight the superiority of feature augmentation with COSTA, we investigate a single-view setting (in addition to multi-view one) which conserves memory and computations. We show that the feature augmentation with COSTA achieves comparable/better results than graph augmentation based models.

## 1 Introduction

Many Graph Neural Networks (GNNs) [\[17,](#page-12-0) [19,](#page-12-1) [43,](#page-13-0) [30,](#page-13-1) [31,](#page-13-2) [39\]](#page-13-3) focus on (semi-)supervised learning, which requires access to abundant labels. Recent trends in Self-Supervised Learning (SSL) have resulted in several methods that do not require labels [\[18,](#page-12-2) [13\]](#page-12-3). Among SSL methods, Contrastive Learning (CL) already achieved comparable performance with its supervised counterparts on many tasks [\[3,](#page-11-0) [9\]](#page-12-4). Recently, CL has been applied to the graph domain. A typical Graph Contrastive Learning (GCL) method constructs multiple graph views by stochastic augmentation of the input to learn representations by contrasting positive samples with negative samples [\[47,](#page-14-0) [26,](#page-13-4) [48\]](#page-14-1). However, the irregular structure of graphs complicates the adaptation of augmentation techniques used on images and prevents extending of theoretical analysis for vision-based contrastive learning to graphs. Thus, many works focus on the empirical design of hand-crafted graph augmentations (GA) for graph contrastive learning (*i.e*., random edge/node/attribute dropping) [\[41,](#page-13-5) [47,](#page-14-0) [48\]](#page-14-1). Notably, some latest works point out that random data augmentations are problematic as their noise may not be relevant to downstream tasks [\[33,](#page-13-6) [34\]](#page-13-7). In certain scenarios (*i.e*., recommendation systems[\[40,](#page-13-8) [5,](#page-11-1) [4\]](#page-11-2)), GCL achieves the desired performance gain under extremely sparse GAs (with an edge dropout rate 0.9) [\[38\]](#page-13-9) but method [\[42\]](#page-13-10) achieves similar results without GAs. Such observations naturally raise the question: are there better augmentation strategies for GCL other than GA?

To this end, we show that the embeddings obtained with GA are highly biased compared to the embeddings obtained with feature augmentation (FA), that is, the embeddings obtained with FA (*e.g*., an injection of random noise into the embedding) exhibit the so-called weak law of large numbers (WLLN). Specifically, for any error ε ≥ 0, limk→∞ P kE(x˜ (k) ) − xk<sup>1</sup> > ε = 0, where

This paper is accepted by the ACM KDD 2022. See <https://doi.org/10.1145/3534678.3539425>.

<span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)

**Caption:** Figure 1 illustrates the distribution of node embeddings on the Cora dataset, comparing feature augmentation (1a) with graph augmentation (1b). The feature augmentation shows a population mean close to the original embedding, while graph augmentation introduces significant bias, highlighting the effectiveness of feature augmentation in preserving embedding integrity.

Figure 1: The distribution of node embeddings on Cora is generated by 500× graph augmentations. [1a](#page-1-0) corresponds to the feature augmentation (Gaussian noise injection). [1b](#page-1-0) corresponds to the graph augmentation (edge permutation & attribute masking). We use 2D embeddings for visualization.

x˜ (k) denotes the embedding obtained by augmenting x, and x is the original embedding without augmentation. For the i.i.d. random variables, as the sample size k increases, the expectation of embedding after augmentation, E(x˜ (k) ) , tends toward the real mean x (embedding without augmentation). In contrast to FA, GA violates the weak law of large numbers. As shown in Figure [1a,](#page-1-0) the population mean for the embedding obtained by the FA is in the densest region and in the proximity of the embedding of the original sample (without augmentation). In contrast, we cannot see such a trend in the case of the GA in Figure [1b.](#page-1-0) In other words, GA introduces some bias, whereas FA produces unbiased embeddings.

We assert that a successful contrastive objective should promote similarity/dissimilarity between features of encoded attributes by implicitly grouping/separating related/unrelated nodes according to their attribute space, respectively. However, as the GA strategy results in the bias (Figure [1b\)](#page-1-0), attraction/separation of embeddings in the feature space does not necessarily result in an optimal attraction/separation of desired nodes in the attribute space, which may result in suboptimal pretraining for downstream tasks. Figure [2](#page-2-0) and Section [4.1](#page-4-0) further illustrate and motivate the above two scenarios. Furthermore, the adoption of GA in GCL often increases the complexity as GCL compares the node features obtained from multiple views (*e.g*., multiple network streams) to obtain correlated views of the same graph. However, this strategy is prohibitive on large graphs as, in the worst-case scenario, multi-view GCL requires a time and space complexity quadratic w.r.t. the number of views and nodes. Thus, apart from the multi-view setting, we also investigate a single-view GCL setting.

Our Contributions. Instead of the GA, we propose to perform augmentation on the hidden feature vectors (feature augmentation). Inspired by matrix sketching, we propose COSTA, a novel COvariance-preServing feaTure space Augmentation framework for GCL, which produces augmented features by generating a "good sketch" of original features. To highlight the superiority of feature augmentation, apart from the multi-view setting, we show many results in the single-view setting, which conserves the memory usage and computations. We empirically show that COSTA (even the single-view variant, *i.e*., COSTASV ) achieves comparable or better results than other GA strategies.

Our contributions are threefold:

- i. We point out the issue of bias introduced by the topology graph augmentation in the GCL framework, and we advocate feature augmentation strategies to prevent the aforementioned bias.
- ii. Inspired by matrix sketching, we propose COSTA, a simple and effective covariance-preserving feature augmentation framework for GCL, which generates augmented features by generating a "good sketch" (variance is bounded) of original features.
- iii. As an alternative to the multi-view GCL setting, we propose the single-view GCL setting, which produces equivalent or better results than the multi-view GCL while requiring less memory and incurring shorter computations.

<span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)

**Caption:** Figure 2 contrasts unbiased (2a) and biased (2b) augmentation strategies in contrastive learning. The unbiased strategy effectively maintains the proximity of embeddings, while the biased strategy leads to suboptimal separations, emphasizing the importance of unbiased feature augmentation for improved model performance.

(b) CL under biased augmentation.

Figure 2: (Fig. [2a\)](#page-2-0) Our strategy results in an unbiased augmentation strategy. (Fig. [2b\)](#page-2-0) A counterexample illustrates the problem of using a biased augmentation strategy in CL.

To our best knowledge, this is the first work which considers feature augmentation (in the single-view setting) in GCL with the matrix sketching step performing feature augmentation.

## 2 Related Works

#### 2.1 Data Augmentation

Input Space Augmentation, studied across many domains, usually refers to the augmentation performed in the input space. In computer vision, image transformations such as rotation, flipping, color jitters, translation, and noise injection [\[28\]](#page-13-11) as well as more recent cut-off and random erasure [\[6\]](#page-11-3) are very popular. In neural language processing, input space augmentations include token-level random augmentations such as synonym replacement, word swapping, word insertion, and deletion [\[37\]](#page-13-12). In the graph domain, input space augmentation is referred to as graph augmentation. Attribute masking, edge permutation, and node dropout are common graph augmentation strategies [\[41\]](#page-13-5). Adaptive graph augmentations based on node centrality and PageRank centrality were studies by Zhu *et al*. [\[48\]](#page-14-1) and Page *et al*. [\[24\]](#page-12-5) respectively with the goal of masking different edges with varying probability. We discuss the negative effect of graph augmentation (*e.g*., edge and node removal) later in the text.

Feature Augmentation strategies generate augmented samples in the feature space instead of the input space [\[8\]](#page-12-6). Wang *et al*. [\[36\]](#page-13-13) augment features in the hidden feature space, resulting in a feature representation that corresponds to another sample with the same class label but different semantics. So-called instance augmentations add perturbations to original instances [\[36\]](#page-13-13). Many few-shot learning approaches [\[14\]](#page-12-7) estimate the "analogy transformations" between examples of known classes to apply them to examples of novel classes. Finally, feature augmentations are popular in many research domains, *i.e*., semi-supervised learning, one-shot learning, and few-shot learning. However, no prior work has combined FA with contrastive learning in the graph domain the way COSTA performs FA.

### 2.2 Graph Contrastive Learning

Inspired by contrastive methods in vision and NLP [\[16,](#page-12-8) [3,](#page-11-0) [9\]](#page-12-4), CL has also been adapted to the graph domain. By adapting DeepInfoMax [\[1\]](#page-11-4) to graph representation learning, DGI [\[35\]](#page-13-14) learns embedding by maximizing the mutual information to discriminate between nodes of original and corrupted graphs. REFINE [\[44\]](#page-13-15) uses a simple negative sampling term inspired by skip-gram models. Fisher-Bures Adversarial GCN [\[32\]](#page-13-16) uses adversarial perturbations of graph Laplacian. Inspired by SimCLR [\[3\]](#page-11-0), GRACE [\[47\]](#page-14-0) correlates graph views by pushing closer representations of the same node in different views and pushing apart representations of different nodes. Another example of a SimCLR strategy is the recent GraphCL method [\[12\]](#page-12-9). In contrast to GRACE, which learns node

embedding, GraphCL learns embeddings for graph-level tasks. The above multi-view methods suffer from the large memory and computational footprint, respectively. Although COLES [\[46\]](#page-14-2) proposes a robust single-view GCL approach, it works the best with linear GNNs such as S<sup>2</sup>GC [\[45\]](#page-14-3). Thus, apart from multi-view COSTA, we also study the single-view GCL setting with FA.

# <span id="page-3-0"></span>3 Preliminaries

#### 3.1 Notations

In this paper, a graph with node features is denoted as G = (V, E, X), where V is the vertex set, E is the edge set, and X ∈ R n×d is the feature matrix (*i.e*., the i-th row of X is the feature vector x<sup>i</sup> of node vi). Let n = |V| and m = |E| be the numbers of vertices and edges respectively. We use A ∈ {0, 1} <sup>n</sup>×<sup>n</sup> to denote the adjacency matrix of G, *i.e*., the (i, j)-th entry in A is 1 if and only if there is an edge between v<sup>i</sup> and v<sup>j</sup> . The degree of a node v<sup>i</sup> , denoted as d<sup>i</sup> , is the number of edges incident with v<sup>i</sup> . The degree matrix D is a diagonal matrix, and its i-th diagonal entry is di . For a d-dimensional vector, x ∈ R d , kxk<sup>2</sup> is the Euclidean norm of x. We use x<sup>i</sup> to denote the i-th entry of x, and diag(x) ∈ R d×d is a diagonal matrix such that the i-th diagonal entry is x<sup>i</sup> . We use Ai: and A:<sup>i</sup> to denote the i-th row and column of A respectively, and Aij for the (i, j)-th entry of A. The trace of a square matrix A is denoted by Tr(A), which is the sum along the diagonal of A. The singular value decomposition of A is denoted as A = UΣV <sup>&</sup>gt; where U = [u1, . . . ,un] , Σ = diag (σ1, . . . , σd), and V = [v1, . . . , vd]. We use kAk<sup>2</sup> to denote the spectral norm of A, which is the largest singular value σmax. We use kAk<sup>F</sup> for the Frobenius norm, which is kAk<sup>F</sup> = qP i,j |aij | <sup>2</sup> = p Tr (A<sup>&</sup>gt;A) = qP<sup>d</sup> <sup>i</sup>=1 σ 2 i (A).

#### 3.2 Multi-view Graph Contrastive Learning (MV-GCL)

Following the conventions presented in [\[47,](#page-14-0) [48\]](#page-14-1), MV-GCL learns node representations by maximizing the mutual information (MI) between views of the same graph. Below, we introduce components of MV-GCL: (i) graph augmentation, (ii) GNN-based encoders, (iii) projection head, and (iv) a contrastive loss.

Graph Augmentation. TGA generates augmented (A˜, X˜ ) by directly adding random perturbations to the original graph (A, X). Different augmented graphs are constructed given one input (A, X), yielding correlated views (A˜ i , X˜ <sup>i</sup>) that represent the augmented adjacent matrix and node features in the i-th view. In the common GCL setting [\[47,](#page-14-0) [48\]](#page-14-1), the graph structure is augmented via edge permutation. Node features are augmented via attribute masking.

Graph Neural Network Encoders. The GNN encoder f<sup>i</sup> : R <sup>n</sup>×<sup>n</sup> × R <sup>n</sup>×<sup>d</sup> → R n×d extracts hidden node features H<sup>i</sup> ∈ R n×d from the i-th augmented graph (A˜ i , X˜ <sup>i</sup>). Usually, multiple encoders are applied to obtain the hidden node features H<sup>i</sup> of different views as:

$$
\boldsymbol{H}_1 = f_1(\tilde{\boldsymbol{A}}_1, \tilde{\boldsymbol{X}}_1), \cdots, \boldsymbol{H}_k = f_k(\tilde{\boldsymbol{A}}_k, \tilde{\boldsymbol{X}}_k).
$$
(1)

The GNN encoders are implemented as a two-layer Graph Convolution Network (GCN):

$$
GCN_l(\boldsymbol{X}, \boldsymbol{A}) = \sigma\left(\hat{\boldsymbol{D}}^{-\frac{1}{2}}\hat{\boldsymbol{A}}\hat{\boldsymbol{D}}^{-\frac{1}{2}}\boldsymbol{X}\boldsymbol{W}_l\right),
$$
  

$$
f(\boldsymbol{X}, \boldsymbol{A}) = GCN_2(GCN_1(\boldsymbol{X}, \boldsymbol{A}), \boldsymbol{A}),
$$
 (2)

where Aˆ = A+I is the adjacency matrix with self-loops, D is the degree matrix, σ(·) is an activation function, *e.g*., ReLU(·) = max(0, ·), and W<sup>l</sup> is a trainable weight matrix for the l-th layer.

Feature Augmentation TF A. We apply feature augmentation on H. We elaborate on the proposed FA and detail its properties in Section [4.2.](#page-5-0) FA results in the augmented feature maps fed into the projection head describe below.

Projection Head. The projection head θ(·) is a small network that maps representations to the space where contrastive loss is applied. It is implemented as a multi-layer perceptron (MLP) with one hidden layer to obtain Z<sup>i</sup> = θ (Hi) = W(2)σ W(1)H<sup>i</sup> , where σ is the ReLU non-linearity. As described in [\[3\]](#page-11-0), it is beneficial to define the contrastive loss on Z<sup>i</sup> rather than H<sup>i</sup> .

<span id="page-4-1"></span>![](_page_4_Figure_0.jpeg)

**Caption:** Figure 3 depicts the architectures of multi-view graph contrastive learning (3a) and single-view graph contrastive learning (3b). The multi-view approach utilizes multiple augmented views for contrastive learning, while the single-view approach simplifies the process, reducing computational complexity and memory usage.

(b) Single-View GCL.

Figure 3: The illustrations of MV-GCL (standard) in Fig. [3a](#page-4-1) and SV-GCL (simplified) in Fig [3b.](#page-4-1) For simplicity, the architecture of MV-GCL is shown using two views only and SV-GCL is the trivial case where two views are the same. MV-GCL contrast two views while SV-GCL perform self-contrast.

Contrastive Loss. Let two feature matrices U ∈ R n×d and V ∈ R n×d , where U = Z<sup>1</sup> and V = Z<sup>2</sup> are node features obtained from two different views. Then for any node i, its embedding generated in one view, Ui: , is treated as the anchor, its embedding generated in another view, Vi: , forms the positive sample. Remaining node embeddings Uj: and Vj: such that j 6= i (from two views) are naturally regarded as negative samples. The contrastive loss function L for all positive pairs is defined as:

<span id="page-4-2"></span>
$$
\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \left[ (\boldsymbol{U}_{i:}^{\top}, \boldsymbol{V}_{i:})/\tau - \log \left( \sum_{j=1}^{n} e^{(\boldsymbol{U}_{i:}^{\top}, \boldsymbol{V}_{j:})/\tau} + \sum_{j=1}^{n} e^{(\boldsymbol{U}_{i:}^{\top}, \boldsymbol{U}_{j:})/\tau} \right) \right].
$$
 (3)

Note that computing Eq. [\(3\)](#page-4-2) is both memory costly and time consuming as it requires the computation of three large similarity matrices, UV <sup>&</sup>gt;, UU <sup>&</sup>gt;, V U <sup>&</sup>gt; ∈ R <sup>n</sup>×<sup>n</sup> for two views. Therefore, the memory consumption and runtime depend on the number of views multiplied by the number of nodes, making such a multi-view setting challenging to run on large-scale graphs.

Single-view Graph Contrastive Learning (SV-GCL). To validate the effectiveness of graph augmentation and feature augmentation, apart from MV-GCL, we use a special case of multi-view GCL that shares the same augmented graph for two views and is thus equivalent to single-view Graph Contrastive Learning (SV-GCL). We note that SV-GCL has a computational advantage, *i.e*., only the features of a single view are calculated, and distances between features within the view. Distances between views are not needed. SV-GCL also provides a fairer way to compare the effectiveness of graph augmentations and feature augmentations as otherwise the multi-view setting would be the reason for the performance gain rather than the graph augmentation strategy.

## 4 Methodology

Section [4.1](#page-4-0) presents our motivation. Section [4.2](#page-5-0) presents COSTA, COvariance preServing feaTure space Augmentation framework. Section [4.3](#page-5-1) relates COSTA to the problem of matrix sketching, which generates desired augmented samples with theoretical guarantee.

#### <span id="page-4-0"></span>4.1 Motivation

We motivate COSTA with a simple experimental example, which shows that the node embedding obtained under graph augmentation is highly biased compared to feature augmentation. Inspired by WLLN (the weak law of large numbers explained in the introduction), below we quantify the bias introduced by the data augmentation T (·) as follows. Let x<sup>i</sup> be the original embedding of the i-th node and X˜ <sup>i</sup> be its augmentation set where each embedding x˜ (k) <sup>i</sup> <sup>∈</sup> <sup>X</sup>˜ i is obtained by stochastic transformation, *i.e*., T (f(·)) or f(T (·)). Let the transformation distribution of x<sup>i</sup> be T˜(xi), then:

$$
Bias(\mathcal{T}(\boldsymbol{x}_i)) = \left\| \mathbb{E}_{\tilde{\boldsymbol{x}}_i \sim \tilde{\mathcal{T}}(\boldsymbol{x}_i)}(\tilde{\boldsymbol{x}}_i) - \boldsymbol{x}_i \right\|_2 \approx \left\| \frac{1}{|\tilde{\mathcal{X}}_i|} \sum_{k=1}^{|\tilde{\mathcal{X}}_i|} \tilde{x}_i^{(k)} - \boldsymbol{x}_i \right\|_2.
$$
 (4)

Following Eq. [\(4\)](#page-5-2), given a randomly chosen node (Cora dataset), we randomly generate |X | ˜ = 500 augmented samples by the graph augmentation (GA) by edge permutation, and the feature augmentation (FA) by directly adding random noise to x. In both cases, we use the same encoder to obtain the node embeddings. The output layer of the encoder has two dimensions to facilitate visualization. Figure [1](#page-1-0) depicts the distribution of node embeddings for both types of augmentations. Clearly, the expectation of node embeddings E(TFA(x)) (the blue star in Figure [1a\)](#page-1-0) obtained by the FA converges to its original embedding x (the red triangle in Figure [1a\)](#page-1-0), whereas expectation of node embeddings E(TGA(x)) obtained via the GA deviates far from x, indicating the following:

<span id="page-5-2"></span>
$$
Bias(\mathcal{T}_{GA}(\boldsymbol{x})) \gg Bias(\mathcal{T}_{FA}(\boldsymbol{x})). \tag{5}
$$

As shown, GA-based contrastive learning suffers from optimizing the biased setting. A standard contrastive loss aims to maximize the similarity of embeddings within the same augmentation set (*e.g*., x˜ (1) i , x˜ (2) <sup>i</sup> <sup>∈</sup> <sup>X</sup>˜ <sup>i</sup> ) and minimize the similarity of embeddings between different sets (*e.g*., x˜ (1) <sup>i</sup> <sup>∈</sup> <sup>X</sup>˜ i and x˜ (1) <sup>j</sup> <sup>∈</sup> <sup>X</sup>˜ <sup>j</sup> ). To perform well, such an objective requires the augmented embeddings to adhere to the unbiased case described above because as the bias tends to zero, the expectation of augmented embeddings converges to x<sup>i</sup> , *i.e*., E(x˜i) → x<sup>i</sup> . Thus, pushing away x˜ (k) i from x˜ (k) j if i 6= j separates embeddings of different instances, whereas the augmented embeddings x˜ (k) i concentrate around x<sup>i</sup> . In contrast, when the bias is large, *i.e*., kE(x˜i) − xik<sup>2</sup> 0, separating augmented embeddings of different instance (*i.e*., x˜ (k) i , x˜ (k) j , i 6= j) may not increase the discrimination of learned embeddings (*i.e*., x<sup>i</sup> , x<sup>j</sup> ) for downstream tasks. Figure [2](#page-2-0) (bottom) illustrates such a case.

This motivates us to explore new ways of performing augmentations for GCL. Instead of explicitly eliminating the bias in the current GCL, we apply feature augmentations as an alternative.

#### <span id="page-5-0"></span>4.2 Covariance-Preserving Feature Augmentation

Previous section indicates the graph augmentation produces the bias. We adopt the feature augmentation for GCL loss because FA lets us control the variance and reduce the bias.

Thus, we propose the feature augmentation framework in which the augmented feature matrix X˜ ∈ R k×d , given the original feature matrix X ∈ R n×d , is obtained via:

<span id="page-5-3"></span>
$$
\tilde{X} = PX + E,
$$
  
such that 
$$
\|X^{\top}X - \tilde{X}^{\top}\tilde{X}\|_2 \leq \varepsilon \text{Tr}(X^{\top}X).
$$
 (6)

P ∈ R <sup>k</sup>×<sup>n</sup> in Eq. [\(6\)](#page-5-3) denotes an affine transformation, E is the random noise matrix and ε is the error which controls the quality of approximation kX<sup>&</sup>gt;X − X˜ <sup>&</sup>gt;X˜ k2. Note that the affine transformation can be either deterministic or stochastic.

Connection to the Gaussian Noise Injection. One special case of COSTA is the Gaussian noise injection [\[36,](#page-13-13) [42\]](#page-13-10) which produces the augmented feature matrix X˜ by adding random noise sampled as X˜ <sup>i</sup>: ∼ N (Xi: , εI), where ε ≥ 0 controls the strength of noise. This is equivalent to setting P = I and Ei: ∼ N (0, εI) in Eq. [\(6\)](#page-5-3).

#### <span id="page-5-1"></span>4.3 Feature Augmentation via Matrix Sketching

Definition 4.1 (Matrix Sketching [\[21\]](#page-12-10)). Let X ∈ R <sup>n</sup>×<sup>d</sup> be the given feature matrix, P ∈ R <sup>k</sup>×<sup>n</sup> be a sketching matrix, *e.g*., random projection or row selection matrix. The sketch of X is defined as X˜ = P X ∈ R k×d . Usually, X˜ contains fewer rows than P , where k n but X˜ still preserves many properties of P .

Eq. [\(6\)](#page-5-3) performs matrix sketching. Obtaining the augmented feature matrix X˜ requires a good sketch of X such that second-order statistics of the original and sketched matrices are similar. In what follows, we use SVD, random row selection, or random projection to form a sketch of X. We prove that X˜ obtained by sketching satisfies kX>X − X˜ <sup>&</sup>gt;X˜ k<sup>2</sup> ≤ ε Tr(X>X) for small ε ≥ 0.

Matrix Sketching via SVD. One solution for Eq. [\(6\)](#page-5-3) can be obtained through the singular value decomposition (SV D) where:

<span id="page-6-1"></span>
$$
P = U^{\top}, X = U \Sigma V^{\top}. \tag{7}
$$

Lemma 4.2. *Let* X˜ = P X *and* X = UΣV <sup>&</sup>gt; *where* U = [u1, . . . ,un]*,* Σ = diag (σ1, . . . , σd)*,* V = [v1, . . . , vd]*. Then* kX>X − X˜ <sup>&</sup>gt;X˜ k<sup>2</sup> *is bounded as:*

$$
\|\boldsymbol{X}^{\top}\boldsymbol{X} - \tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}}\|_2 \le \frac{\sigma_{k+1}}{\sigma_{\text{max}}} \operatorname{Tr}(\boldsymbol{X}^{\top}\boldsymbol{X}). \tag{8}
$$

*Proof.* See Appendix [A.](#page-14-4)

*Remark* 4.3*.* The upper bound of kX<sup>&</sup>gt;X − X˜ <sup>&</sup>gt;X˜ )k<sup>2</sup> is controlled by the (k + 1)-th largest eigenvalue σk+1. Usually, <sup>σ</sup>k+1 σmax is small as σk+1 σmax even when k is small. However, SVD is computationally intensive and unsuitable for decomposition of large feature matrices.

Random Row Selection (RS). Randomized algorithms trade accuracy for efficiency and strive for high accuracy and low runtime. A sketch of a matrix X˜ can be constructed via randomly stacking the rows of the original matrix X. Random row selection employs a small subset of rows based on a pre-defined probability distribution P(i) to form a sketch. The random assignment matrix P ∈ R k×n stacks one-hot vectors, *i.e*., P = {e<sup>i</sup> ∈ R <sup>n</sup>|P(i) = <sup>k</sup>Xi:k<sup>2</sup> kXk<sup>F</sup> } ∈ R <sup>k</sup>×<sup>n</sup>, where e<sup>i</sup> indicates that the i-th row is selected.

Lemma 4.4. *Let* X ∈ R n×d *. Let* X˜ ∈ R <sup>m</sup>×<sup>d</sup> *be a matrix whose rows are randomly selected from rows of* X ∈ R n×d *. It holds that:*

<span id="page-6-2"></span>
$$
\mathcal{P}\left(\|\boldsymbol{X}^{\top}\boldsymbol{X}-\tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}}\|_{2} \leq \varepsilon \operatorname{Tr}(\boldsymbol{X}^{\top}\boldsymbol{X})\right) \geq 1 - e^{\left(-\frac{(\varepsilon\sqrt{k}-1)^{2}}{8}\right)}.
$$
\n(9)

*Proof.* See Appendix [A.](#page-14-4)

*Remark* 4.5*.* The failure probability δRS = e (− (ε k−1)2 8 ) is exponentially decreasing with the error ε meaning that we can bound kX<sup>&</sup>gt;X − X˜ <sup>&</sup>gt;X˜ k<sup>2</sup> given small ε ≥ 0 with a high probability 1 − δRS.

Random Projection (RP). A sketch of matrix can be RP. The projection matrix is defined as P ∈ R <sup>n</sup>×<sup>k</sup> whose entry pij is sampled from N (0, 1). Ideally, we expect P to provide a stable sketch that approximately preserves the distance between all pairs of columns in the original matrix. As the computation of dense matrix P is time-consuming, a sparse version from Appendix [B\)](#page-16-0) can be used.

<span id="page-6-3"></span>Lemma 4.6. *let* X˜ = <sup>√</sup> 1 k P X *where* pij *is the* (i, j)*-th element of* P *, and* pij ∼ N (0, 1)*. For* ε ∈ (0, 1)*, it holds that:*

<span id="page-6-0"></span>
$$
\mathcal{P}\left(\|\boldsymbol{X}^{\top}\boldsymbol{X}-\tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}}\|_{2} \leq \varepsilon \operatorname{Tr}(\boldsymbol{X}^{\top}\boldsymbol{X})\right) \geq 1 - e^{-\frac{\varepsilon^{2}k}{8}}.
$$
\n(10)

*Proof.* See Appendix [A.](#page-14-5)

*Remark* 4.7*.* Note that the failure probability of the random projection δP R is less than δRS when k > ε<sup>−</sup><sup>2</sup> .

<span id="page-7-1"></span>![](_page_7_Figure_0.jpeg)

**Caption:** Figure 4 presents the relationship between bias and node degree, revealing that lower-degree nodes exhibit higher bias in their embeddings. This trend underscores the challenges posed by graph augmentation methods, particularly for nodes with fewer connections, which are more susceptible to embedding distortion.

Figure 4: The bias *vs*. the node degree. Figure 5: Node degrees obey the power law distribution.

<span id="page-7-0"></span>![](_page_7_Figure_2.jpeg)

**Caption:** Figure 6 shows the distribution of bias across all nodes in the Cora dataset under different augmentation strategies. The results indicate that graph augmentation introduces more significant bias compared to feature augmentation, particularly affecting nodes with lower degrees, thus validating the proposed feature augmentation approach.

Figure 6: The bias distribution of all nodes on the Cora dataset w.r.t. different augmentation strategies and encoder .

## 5 Experiments

Below, we provide details of experimental settings, and we discuss our results. We answer the following research questions (RQ):

- RQ1: What is the bias problem in graph augmentation?
- RQ2: Does the proposed feature augmentation work for problems of practical interest? How does its accuracy/speed compare with MV-GCL models that adopt the graph augmentation strategy, and with other models? Does SV-GCL perform well in comparison to MV-GCL models?
- RQ3: What is the performance of the feature augmentation given different matrix sketching schemes? What are the major factors that contribute to the success of the proposed feature augmentation method?
- RQ4: How is the effectiveness affected by the number of augmented samples?

### 5.1 The Bias Problem of Graph Augmentation with GNNs (RQ1)

In Section [4.1,](#page-4-0) we intuitively point out the pitfall of the topology GA by providing an illustrative example. Based on that observation, we hypothesize that the topology GA introduces a substantial bias into the node embeddings used by the contrastive loss, which deteriorates the quality of features from the pre-training step, thus affecting downstream tasks. In this section, we conduct a quantitative analysis of this problem.

Experimental Protocol. We adopt the edge perturbation and attribute masking, the most commonly used strategies for the graph augmentation [\[47,](#page-14-0) [48,](#page-14-1) [35\]](#page-13-14). To show the difference between the attribute GA and the topology GA, we use MLP and GNN to encode the original features. For a fair comparison, MLP and GNN share the fixed weights. We only change the type of augmentations used to obtain the node embedding. Specifically, We denote GNN with the edge perturbation (EP) as gnn\_e, GNN with

<span id="page-8-1"></span>Table 1: Results (Cora, CiteSeer, and WikiCS) given a common testbed for different feature augmentation strategies realized by Eq. [\(6\)](#page-5-3). The symbol e<sup>i</sup> denotes the one-hot vector and O is the zero matrix.

| Feature Augmentation     | Type          | P ∈ R<br>k×n                          | E ∈ R<br>k×d | Cora   | CiteSeer | WikiCS |
|--------------------------|---------------|---------------------------------------|--------------|--------|----------|--------|
| Gaussian Noise Injection | Stochastic    | P = I                                 | E ∼ N (0, 1) | 0.8271 | 0.7134   | 0.7823 |
| SVD                      | Deterministic | P = U >, X = UΣV<br>>                 | E = O        | 0.8269 | 0.7142   | 0.7814 |
| Random Selection         | Stochastic    | P(i) = kXi:k2<br>P = {ei<br>}<br>kXkF | E = O        | 0.8245 | 0.7121   | 0.7811 |
| Random Projection        | Stochastic    | P ∼ N (0, 1)                          | E = O        | 0.8425 | 0.7247   | 0.7911 |

<span id="page-8-0"></span>Table 2: Node classification in terms of accuracy (%) with standard deviation. The highest performance is highlighted in boldface. COSTAMV and COSTASV denote the variants of multi-view and single-view setting respectively, OOM indicates Out-Of-Memory.

| Method       | Training Data | Wiki-CS      | Amazon-Computers | Amazon-Photo | Coauthor-CS  | Coauthor-Physics |
|--------------|---------------|--------------|------------------|--------------|--------------|------------------|
| Raw features | X             | 71.98 ± 0.00 | 73.81 ± 0.00     | 78.53 ± 0.00 | 90.37 ± 0.00 | 93.58 ± 0.00     |
| Node2vec     | A             | 71.79 ± 0.05 | 84.39 ± 0.08     | 89.67 ± 0.12 | 85.08 ± 0.03 | 91.19 ± 0.04     |
| DeepWalk     | A             | 74.35 ± 0.06 | 85.68 ± 0.06     | 89.44 ± 0.11 | 84.61 ± 0.22 | 91.77 ± 0.15     |
| DeepWalk     | X, A          | 77.21 ± 0.03 | 86.28 ± 0.07     | 90.05 ± 0.08 | 87.70 ± 0.04 | 94.90 ± 0.09     |
| GAE          | X, A          | 70.15 ± 0.01 | 85.27 ± 0.19     | 91.62 ± 0.13 | 90.01 ± 0.71 | 94.92 ± 0.07     |
| VGAE         | X, A          | 75.63 ± 0.19 | 86.37 ± 0.21     | 92.20 ± 0.11 | 92.11 ± 0.09 | 94.52 ± 0.00     |
| DGI          | X, A          | 75.35 ± 0.14 | 83.95 ± 0.47     | 91.61 ± 0.22 | 92.15 ± 0.63 | 94.51 ± 0.52     |
| GMI          | X, A          | 74.85 ± 0.08 | 82.21 ± 0.31     | 90.68 ± 0.17 | OOM          | OOM              |
| MVGRL        | X, A          | 77.52 ± 0.08 | 87.52 ± 0.11     | 91.74 ± 0.07 | 92.11 ± 0.12 | 95.33 ± 0.03     |
| GRACE        | X, A          | 78.31 ± 0.05 | 87.80 ± 0.23     | 92.53 ± 0.16 | 92.95 ± 0.03 | 95.72 ± 0.03     |
| GCA          | X, A          | 78.23 ± 0.04 | 87.54 ± 0.49     | 92.24 ± 0.21 | 92.95 ± 0.13 | 95.73 ± 0.03     |
| G-BT         | X, A          | 76.83 ± 0.73 | 87.93 ± 0.36     | 92.46 ± 0.35 | 92.91 ± 0.25 | 95.25 ± 0.13     |
| COSTASV      | X, A          | 79.03 ± 0.05 | 88.26 ± 0.03     | 92.30 ± 0.25 | 92.95 ± 0.12 | 95.74 ± 0.02     |
| COSTAMV      | X, A          | 79.12 ± 0.02 | 88.32 ± 0.03     | 92.56 ± 0.45 | 92.94 ± 0.10 | 95.60 ± 0.02     |

attribute masking (AM) as gnn\_a, GNN with AM and EP as gnn\_ea, MLP with AM as nn\_a. For each node from Cora, we generate 500 augmented samples in four variants to compute their bias by Eq. [\(4\)](#page-5-2).

Bias Patterns of Graph Augmentations. Figure [6](#page-7-0) shows distributions containing bias. It is obvious that gnn\_ea contains more node embeddings with a larger bias compared to nn\_ea, judging by the distribution shift towards right. This confirms our hypothesis that the graph augmentation introduces the bias. Moreover, the long-tailed trend is mainly caused by the edge perturbation. Figure [4](#page-7-1) shows that nodes with a small degree exhibit more bias, which confirms our insight that for node embeddings of low-degree nodes, obtained via GNN, removing any edges perturbs these embedding significantly. Real-world graphs (citation network, social network, *etc*.) follow the power-law distribution (shown in Figure [5\)](#page-7-1) meaning a few of nodes connect with the majority of the edges, whereas the majority of nodes have only a few of edges (low degree). For example, 54%, 68%, 71%, 53% nodes of Cora, CitSeer, PubMed, and DBLP datasets have less than 3 edges. Thus, applying the graph augmentation in contrastive learning results in a large bias.

#### 5.2 Comparison with the State-of-the-Art Methods (RQ2)

In this section, we compare COSTA to other baseline models to answer RQ2. We use the same experimental setup as the representative MV-GCL method (*i.e*., GCA [\[48\]](#page-14-1), and GRACE [\[47\]](#page-14-0)) to perform a fair comparison to these methods. Unless stated otherwise, the random projection is employed as the default setting of COSTA as it balances well between accuracy and efficiency. A detailed comparison between different feature augmentations given different matrix sketching schemes is shown in Section [5.3.](#page-10-0)

Datasets. To evaluate our method, we adopt nine commonly used benchmark datasets in the previous works [\[47,](#page-14-0) [48,](#page-14-1) [35\]](#page-13-14), including citation networks (Cora, CiteSeer, Pubmed, DBLP, Coauthor-CS, and Coauthor-Physics) and social networks (Wiki-CS, Amazon-Computers, Amazon-Photo) [\[17,](#page-12-0) [29,](#page-13-17) [22,](#page-12-11) [23\]](#page-12-12). Detailed descriptions and statistics are given in Appendix [C.](#page-16-1) Apart from Wiki-CS adopting the public split, other datasets are randomly divided into 10%, 10%, 80% for training, validation, and testing.

<span id="page-9-0"></span>![](_page_9_Figure_0.jpeg)

**Caption:** Figure 7 compares the runtime of single-view COSTA (COSTASV) against multi-view graph contrastive learning models like GCA and GRACE. The results demonstrate that COSTASV is significantly faster, especially as the number of nodes increases, highlighting its efficiency in large-scale graph scenarios.

Figure 7: The runtime comparison of single-view COSTA (COSTASV ) *vs*. MV-GCL models such as GCA and GRACE. Table 3: Results on Cora, CiteSeer, PubMed, and DBLP. We use the same setting as the setting used for experiments in Table [2.](#page-8-0)

Evaluation Protocol. For each experiment, we adopt the same evaluation scheme as in [\[35,](#page-13-14) [47,](#page-14-0) [48\]](#page-14-1), where each model is firstly trained in an unsupervised manner on the whole graph with node features. Then, we transform the raw features into the resulting embeddings with the use of the trained encoder. Next, we train an `2-regularized logistic regression classifier from the Scikit-Learn library [\[25\]](#page-12-13) with the use of embeddings obtained in the previous step. We also perform a grid search over the regularization parameter with the following values {2 −10 , 2 −9 , . . . , 2 <sup>−</sup>1}. We compute the classification accuracy and report the mean and standard deviations for 20 model initializations and splits.

Baselines. To compare COSTA with previous works, we choose the representative baselines from traditional graph self-supervised learning, autoencoder-based model, and contrastive-based graph self-supervised learning. Methods include (i) Random walk based models: Deepwalk [\[27\]](#page-13-18) and node2vec [\[11\]](#page-12-14), (ii) Autoencoder Based models: GAE and VGAE [\[18\]](#page-12-2), (iii) the contrastive-based models including Deep Graph Infomax (DGI) [\[35\]](#page-13-14), Graphical Mutual Information Maximization (GMI) [\[26\]](#page-13-4), Graph Barlow Twins (G-BT) [\[2\]](#page-11-5) and Multi-View Graph Representation Learning (MVGRL) [\[15\]](#page-12-15), GRACE [\[47\]](#page-14-0) and GCA [\[48\]](#page-14-1). Note that all the contrastive models use the topology graph augmentation by default. For all baselines, we report their performance based on the official implementations and we use use default hyper-parameters from original papers.

Main Results. Tables [2](#page-8-0) and [3](#page-9-0) show that COSTA achieves competitive performance compared to the baseline methods, and even surpasses them on most datasets. These results demonstrate that COSTA is an effective framework leveraging the advantage of feature augmentations. Specifically, the superiority of COSTA is confirmed by the fact that both single- and multi-view COSTA variants, COSTASV and COSTAMV , outperform several MV-GCL models that use the topology graph augmentation (*i.e*., GCA, GRACE, MVGRL) on several datasets (Cora, CiteSeer, DBLP, Wiki-CS, Amazon-Computers, AM-Photo and Coauthor-Physics) and achieve comparable results on PubMed, and Coauthor-CS datasets. We note that datasets on which COSTASV does not achieve SOTA have a small number of nodes with low node degrees (*i.e*., only around 10% of nodes in Amazon-photo and Coauthor-Physics have the degree less than 3, meaning less bias is introduced by the topology GA. However, COSTASV requires less runtime and memory to achieve the comparable performance. This is attributed to the single-view design (SV-GCL) of COSTASV (Section [3\)](#page-3-0). We also note that COSTAMV typically outperforms COSTASV by a small margin which suggests that single-view augmentation strategies are a good choice for GA. In addition, we note that most of the contrastive learning models outperform models based on the reconstruction error (*i.e*., GAE, VGAE, DeepWalk, Node2Vec), which reflects the superiority of contrastive learning.

Running time. We measure the runtime to further validate the practicality of the single-view COSTA (COSTASV ) in terms of time complexity. We mainly compare it to GCA and GRACE (Amazon-Computers dataset) because both GCA and GRACE are the representative models for the multi-view contrastive learning framework utilizing graph augmentations. Note that GCA uses adaptive graph augmentations. To form Figure [7,](#page-9-0) we sampled a subgraph with a fixed number of nodes from 1, 000 to 8, 000. Figure [7](#page-9-0) shows the training time for 1,000 epochs given different numbers of nodes. The figure shows that our method is faster than the other two models. What stands out is that the gap between them becomes more apparent as the number of nodes increases. COSTASV becomes 2 times faster than the other models at ≥ 5, 000 nodes. We attribute this to the single-view setting with the

<span id="page-10-0"></span>![](_page_10_Figure_0.jpeg)

**Caption:** Figure 8 illustrates accuracy variations with respect to sketching ratios in the feature augmentation process. The results indicate that optimal sketching ratios enhance model performance, suggesting that careful tuning of augmentation parameters is crucial for achieving the best results.

![](_page_10_Figure_1.jpeg)

**Caption:** Figure 9 examines accuracy trends relative to matrix densities in random projection. The findings reveal that increasing density improves performance up to a certain point, indicating a balance between computational efficiency and model accuracy in feature augmentation strategies.

Figure 8: Accuracy w.r.t. sketching ratios. Figure 9: Accuracy w.r.t. matrix densities.

Figure 10: Ablation study on different variants of random projection.

<span id="page-10-1"></span>Table 4: Ablation study: the topology graph augmentation *vs*. the feature augmentation (single-view COSTA).

| Graph Aug. | Feature Aug. | WikiCS | Cora  | CiteSeer |  |
|------------|--------------|--------|-------|----------|--|
| ×          | ×            | 74.31  | 80.04 | 71.35    |  |
| X          | ×            | 77.67  | 82.13 | 71.93    |  |
| ×          | X            | 79.03  | 84.30 | 72.81    |  |
| X          | X            | 79.07  | 84.33 | 72.90    |  |

feature augmentation. Note that COSTASV computes the node feature matrix once before feeding it into the projection head, and the feature augmentation effectively can be understood as reducing the number of nodes, as our feature augmentation acts on columns of hidden feature matrix. Thus, we form one relatively small similarity matrix for the contrastive loss. In contrast, the MV-GCL framework incurs a higher complexity. Our experiments confirm that COSTASV is efficient in practice. Moreover, COSTASV can be accelerated by employing sketching by a sparse matrix without sacrificing its performance, as shown in Figure [9](#page-10-0) and Appendix [B.](#page-16-0)

#### 5.3 Ablations/Performance Analysis (RQ3 & 4)

Below we use the single-view COSTA (COSTASV ), as multi-view COSTA has a similar performance. We firstly show the superiority of random projection by comparing it with other matrix sketching variants. Subsequently, we ablate and discuss the factors that lead to the success of the random projection. Finally, we show the effect of the number of augmented samples that influence the error bound.

Comparison with Other Feature Augmentations. We compare the random projection with other matrix sketching strategies. Table [1](#page-8-1) uses the common tested based on COSTASV : only the definition of P and E are varied. We note that the random projection works consistently better than all the other strategies as the random projection introduces a lesser error compared to the random selection and random noise strategies. It is somewhat surprising that although the random projection is not the optimal solution to Eq. [\(6\)](#page-5-3), it still outperforms the SVD-based sketching. Such a good performance comes from the following facts: (i) the error bound of random projection is sufficiently small to maintain a good sketch; and (ii) compared to SVD, which is deterministic, random projection adds stochastic perturbations to the model (variance is a source of feature augmentations), which serves as a regularization.

Ablations w.r.t. the Augmentation Type in GCL. Below, we investigate different augmentation types, *i.e*., the topology graph augmentation and the feature augmentation. To minimize other factors other than the augmentation strategy that might affect the results (*i.e*., multi-view setting), we opt for the single-view COSTA. We only replace the augmentation type. Table [4](#page-10-1) shows that without any augmentations, the model performs badly, showing the necessity of data augmentations. Furthermore, we observe that applying either the topology graph augmentation or the feature augmentation can improve results. However, the improvement of feature augmentation is larger compared to the topology graph augmentation, highlighting the effectiveness of COSTA.

Ablations on the Random Projection. To see where the performance improvement comes from, we conduct an ablation study on the COSTASV with the random projection. Firstly, we fix the random projection so that it remains the same in each epoch, denoting this variant as ran\_proj\_f ix, and then we eliminate the random projection completely (none\_ran\_proj). We experiment on Cora, CiteSeer, PubMed, and DBLP. Figure [10](#page-10-0) shows that fixing the random projection throughout the experiment causes the performance drop in all four datasets. In contrast, forming a new random projection matrix per epoch generates a variety of feature augmentations obeying the variance bound of random projection. As projecting is performed along columns of the hidden feature matrix, this is an equivalent of drawing different new nodes (obeying the mean and variance) with each change of random projection. Notably, removing the random projection completely from training decreases the accuracy on three datasets, as expected.

Downsampling *vs*. Upsampling the Node Dimension. According to the error bound derived in Eq. [\(10\)](#page-6-0), the bound is related to the number of augmented samples k. The use of COSTA lets us control the number of augmented features by adjusting the number of rows of P ∈ R <sup>k</sup>×<sup>n</sup> in Eq. [\(6\)](#page-5-3). Downsampling and upsampling are applied by setting k < |V| and k > |V| respectively. We denote the <sup>k</sup> |V| as the reduction ratio. Figure [8](#page-10-0) shows the relation between the performance and the reduction ratio. Specifically, we obtain the best performance for downsampling (the sketched rows are fewer than the number of nodes), which also accelerates computations of the contrastive loss (smaller similarity/dissimilarity matrices).

# 6 Conclusions

We have quantitatively and qualitatively analyzed the problems stemming from the topology graph augmentation of current GCL methods, and we have shown that such a strategy suffers from the bias problem. To overcome this bias, we have proposed the feature augmentation framework COSTA. We theoretically proved that the quality of augmented features obtained via COSTA are guaranteed and COSTA accelerates the speed of GCL by working well in the single-view mode. Our results are equivalent or better than results of the standard contrastive multi-view graph augmentation models that rely on topology-based augmentations.

# 7 Acknowledgments

This work was supported by the National Key Research and Development Program of China (No. 2018AAA0100204) and the Research Grants Council of the Hong Kong Special Administrative Region, China (CUHK 2410021, Research Impact Fund, No. R5034-18).

# References

- <span id="page-11-4"></span>[1] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. *arXiv preprint arXiv:1906.00910*, 2019.
- <span id="page-11-5"></span>[2] Piotr Bielak, Tomasz Kajdanowicz, and Nitesh V Chawla. Graph barlow twins: A selfsupervised representation learning framework for graphs. *arXiv preprint arXiv:2106.02466*, 2021.
- <span id="page-11-0"></span>[3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In *International conference on machine learning*, pages 1597–1607. PMLR, 2020.
- <span id="page-11-2"></span>[4] Yankai Chen, Menglin Yang, Yingxue Zhang, Mengchen Zhao, Ziqiao Meng, Jianye Hao, and Irwin King. Modeling scale-free graphs with hyperbolic geometry for knowledge-aware recommendation. In *WSDM '22: The Fifteenth ACM International Conference on Web Search and Data Mining*. ACM, 2022.
- <span id="page-11-1"></span>[5] Yankai Chen, Yaming Yang, Yujing Wang, Jing Bai, Xiangchen Song, and Irwin King. Attentive knowledge-aware graph convolutional networks with collaborative guidance for personalized recommendation. In *The 38th IEEE International Conference on Data Engineering*, 2022.
- <span id="page-11-3"></span>[6] Terrance DeVries and Graham W Taylor. Dataset augmentation in feature space. *arXiv preprint arXiv:1702.05538*, 2017.
- <span id="page-12-17"></span>[7] Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. *SIAM Journal on Computing*, 36(1):132–157, 2006.
- <span id="page-12-6"></span>[8] Steven Y Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. A survey of data augmentation approaches for nlp. *arXiv preprint arXiv:2105.03075*, 2021.
- <span id="page-12-4"></span>[9] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. *arXiv preprint arXiv:2104.08821*, 2021.
- <span id="page-12-16"></span>[10] Gene H Golub, Alan Hoffman, and Gilbert W Stewart. A generalization of the eckart-youngmirsky matrix approximation theorem. *Linear Algebra and its applications*, 88:317–327, 1987.
- <span id="page-12-14"></span>[11] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In *Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining*, pages 855–864, 2016.
- <span id="page-12-9"></span>[12] Hakim Hafidi, Mounir Ghogho, Philippe Ciblat, and Ananthram Swami. Graphcl: Contrastive self-supervised learning of graph representations. *arXiv preprint arXiv:2007.08025*, 2020.
- <span id="page-12-3"></span>[13] William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In *Proceedings of the 31st International Conference on Neural Information Processing Systems*, pages 1025–1035, 2017.
- <span id="page-12-7"></span>[14] Bharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating features. In *Proceedings of the IEEE International Conference on Computer Vision*, pages 3018–3027, 2017.
- <span id="page-12-15"></span>[15] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In *International Conference on Machine Learning*, pages 4116–4126. PMLR, 2020.
- <span id="page-12-8"></span>[16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 9729–9738, 2020.
- <span id="page-12-0"></span>[17] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. *arXiv preprint arXiv:1609.02907*, 2016.
- <span id="page-12-2"></span>[18] Thomas N Kipf and Max Welling. Variational graph auto-encoders. *arXiv preprint arXiv:1611.07308*, 2016.
- <span id="page-12-1"></span>[19] Piotr Koniusz and Hongguang Zhang. Power normalizations in fine-grained image, fewshot image and graph classification. In *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2020.
- <span id="page-12-18"></span>[20] Ping Li, Trevor J Hastie, and Kenneth W Church. Very sparse random projections. In *Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining*, pages 287–296, 2006.
- <span id="page-12-10"></span>[21] Edo Liberty. Simple and deterministic matrix sketching. In *Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining*, pages 581–588, 2013.
- <span id="page-12-11"></span>[22] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In *Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval*, pages 43–52, 2015.
- <span id="page-12-12"></span>[23] Péter Mernyei and Cat˘ alina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural ˘ networks. *arXiv preprint arXiv:2007.02901*, 2020.
- <span id="page-12-5"></span>[24] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, 1999.
- <span id="page-12-13"></span>[25] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikitlearn: Machine learning in python. *the Journal of machine Learning research*, 12:2825–2830, 2011.
- <span id="page-13-4"></span>[26] Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou Huang. Graph representation learning via graphical mutual information maximization. In *Proceedings of The Web Conference 2020*, 2020.
- <span id="page-13-18"></span>[27] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In *Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining*, pages 701–710, 2014.
- <span id="page-13-11"></span>[28] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. *Journal of Big Data*, 6(1):1–48, 2019.
- <span id="page-13-17"></span>[29] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and Kuansan Wang. An overview of microsoft academic service (mas) and applications. In *Proceedings of the 24th international conference on world wide web*, pages 243–246, 2015.
- <span id="page-13-1"></span>[30] Zixing Song, Ziqiao Meng, Yifei Zhang, and Irwin King. Semi-supervised multi-label learning for graph-structured data. In *CIKM*, pages 1723–1733. ACM, 2021.
- <span id="page-13-2"></span>[31] Zixing Song, Xiangli Yang, Zenglin Xu, and Irwin King. Graph-based semi-supervised learning: A comprehensive review. *IEEE Transactions on Neural Networks and Learning Systems*, pages 1–21, 2022.
- <span id="page-13-16"></span>[32] Ke Sun, Piotr Koniusz, and Zhen Wang. Fisher-bures adversary graph convolutional networks. *Conference on Uncertainty in Artificial Intelligence*, 115:465–475, 2019.
- <span id="page-13-6"></span>[33] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. *CoRR*, abs/2106.05819, 2021.
- <span id="page-13-7"></span>[34] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? In *Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020*, 2020.
- <span id="page-13-14"></span>[35] Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. *ICLR (Poster)*, 2019.
- <span id="page-13-13"></span>[36] Yulin Wang, Xuran Pan, Shiji Song, Hong Zhang, Gao Huang, and Cheng Wu. Implicit semantic data augmentation for deep networks. *Advances in Neural Information Processing Systems*, 32:12635–12644, 2019.
- <span id="page-13-12"></span>[37] Jason Wei and Kai Zou. Eda: Easy data augmentation techniques for boosting performance on text classification tasks. *arXiv preprint arXiv:1901.11196*, 2019.
- <span id="page-13-9"></span>[38] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. Self-supervised graph learning for recommendation. In *Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval*, pages 726–735, 2021.
- <span id="page-13-3"></span>[39] Menglin Yang, Ziqiao Meng, and Irwin King. Featurenorm: L2 feature normalization for dynamic graph embedding. In *2020 IEEE International Conference on Data Mining (ICDM)*, pages 731–740. IEEE, 2020.
- <span id="page-13-8"></span>[40] Menglin Yang, Min Zhou, Jiahong Liu, Defu Lian, and Irwin King. Hrcf: Enhancing collaborative filtering via hyperbolic geometric regularization. In *Proceedings of the ACM Web Conference 2022*, pages 2462–2471, 2022.
- <span id="page-13-5"></span>[41] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. *Advances in Neural Information Processing Systems*, 33:5812–5823, 2020.
- <span id="page-13-10"></span>[42] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Nguyen Quoc Viet Hung. Are graph augmentations necessary? simple graph contrastive learning for recommendation. *arXiv preprint arXiv:2112.08679*, 2022.
- <span id="page-13-0"></span>[43] Yifei Zhang, Hao Zhu, Ziqiao Meng, Piotr Koniusz, and Irwin King. Graph-adaptive rectified linear unit for graph neural networks. In *WWW '22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022*, pages 1331–1339. ACM, 2022.
- <span id="page-13-15"></span>[44] Hao Zhu and Piotr Koniusz. Refine: Random range finder for network embedding. In *ACM Conference on Information and Knowledge Management*, 2021.
- <span id="page-14-3"></span>[45] Hao Zhu and Piotr Koniusz. Simple spectral graph convolution. In *International Conference on Learning Representations*, 2021.
- <span id="page-14-2"></span>[46] Hao Zhu, Ke Sun, and Peter Koniusz. Contrastive laplacian eigenmaps. *Advances in Neural Information Processing Systems*, 34, 2021.
- <span id="page-14-0"></span>[47] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive representation learning. *arXiv preprint arXiv:2006.04131*, 2020.
- <span id="page-14-1"></span>[48] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In *Proceedings of the Web Conference 2021*, pages 2069–2080, 2021.

—— Appendices ——

# <span id="page-14-4"></span>A Error Bounds of Different Feature Augmentations Preserving Second-order Statistics

Proof of Lemma [4.2](#page-6-1) (Matrix Sketching via SVD).

*Proof.* According to the Eckart-Young-Mirsky theorem [\[10\]](#page-12-16), X<sup>k</sup> = P <sup>&</sup>gt;P X is the best k-rank approximation of X and kX − Xkk<sup>2</sup> = σk+1. Thus, we have:

$$
\begin{aligned}\n\| \boldsymbol{X}^\top \boldsymbol{X} - \tilde{\boldsymbol{X}}^\top \tilde{\boldsymbol{X}}) \|_2 &= \| \boldsymbol{X}^\top \boldsymbol{X} - \boldsymbol{X}^\top \boldsymbol{P}^\top \boldsymbol{P} \boldsymbol{X} \|_2, \\
&= \| \boldsymbol{X}^\top (\boldsymbol{X} - \boldsymbol{P}^\top \boldsymbol{P} \boldsymbol{X}) \|_2 = \| \boldsymbol{X}^\top (\boldsymbol{X} - \boldsymbol{X}_k) \|_2, \\
&\le \| \boldsymbol{X}^\top \|_2 \| \boldsymbol{X} - \boldsymbol{X}_k \|_2 = \| \boldsymbol{X} \|_2^2 \frac{\| \boldsymbol{X} - \boldsymbol{X}_k \|_2}{\| \boldsymbol{X} \|_2}, \\
&\le \frac{\sigma_{k+1}}{\sigma_1} \| \boldsymbol{X} \|_F^2 = \frac{\sigma_{k+1}}{\sigma_1} \operatorname{Tr} (\boldsymbol{X}^\top \boldsymbol{X}).\n\end{aligned}
$$

Proof of Lemma [4.4](#page-6-2) (Random Row Selection). To prove this Lemma, we use the following theorem from [\[7\]](#page-12-17).

<span id="page-14-6"></span>Theorem A.1. *Let* A ∈ R <sup>d</sup>×n, B ∈ R <sup>r</sup>×<sup>d</sup> *and* k ∈ Z <sup>+</sup>*such that* 1 ≤ k ≤ n *and* {pi} n <sup>i</sup>=1 *be probability distribution over rows of* A *and columns of* B *such that* p<sup>i</sup> ≥ <sup>P</sup> βkAi:k2kBi:k<sup>2</sup> n <sup>j</sup>=1 kAj:k2kAj:k<sup>2</sup> *for some positive constant* β ≤ 1*. If matrix* C ∈ R d×k *is constructed by sampling columns of* A *according to* {pi} n <sup>i</sup>=1 *and matrix* <sup>D</sup> <sup>∈</sup> <sup>R</sup> k×r *is constructed by picking same rows of* B*, then with probability at least* 1 − δ*:*

$$
\|\bm{A}\bm{B}-\bm{C}\bm{D}\|_F^2 \leq \frac{\mu^2}{\beta c}\|\bm{A}\|_F^2\|\bm{B}\|_F^2,
$$

*where* δ ∈ (0, 1), µ = 1 + p (8/β) log(1/δ)*.*

We set A = X<sup>&</sup>gt; ∈ R <sup>d</sup>×<sup>n</sup>, B = X ∈ R n×d , C = X˜ <sup>&</sup>gt; ∈ R d×k and D = X˜ ∈ R k×d . Note that the distribution p<sup>i</sup> = <sup>P</sup> βkAi:k2kBi:k<sup>2</sup> n <sup>j</sup>=1 kAj:k2kAj:k<sup>2</sup> = kXi:k kXk<sup>F</sup> holds with β = 1. Using theorem [A.1,](#page-14-6) we obtain bound:

$$
\| \boldsymbol{X}^{\top} \boldsymbol{X} - \tilde{\boldsymbol{X}}^{\top} \tilde{\boldsymbol{X}} \|_{F}^{2} \leq \frac{\mu^{2}}{k} \| \boldsymbol{A} \|_{F}^{4},
$$
  
\n
$$
\Rightarrow \| \boldsymbol{X}^{\top} \boldsymbol{X} - \tilde{\boldsymbol{X}}^{\top} \tilde{\boldsymbol{X}} \|_{F} \leq \frac{\mu}{\sqrt{k}} \| \boldsymbol{A} \|_{F}^{2} \text{ (As } \| \boldsymbol{X} \|_{2} \leq \| \boldsymbol{X} \|_{F}),
$$
  
\n
$$
\Rightarrow \| \boldsymbol{X}^{\top} \boldsymbol{X} - \tilde{\boldsymbol{X}}^{\top} \tilde{\boldsymbol{X}} \|_{2} \leq \frac{\mu}{\sqrt{k}} \text{Tr}(\boldsymbol{X}^{\top} \boldsymbol{X}),
$$
\n(11)

where δ ∈ (0, 1), µ = 1 + p 8 log(1/δ). By setting <sup>√</sup> µ k = ε, we have:

<span id="page-14-5"></span>
$$
\mathcal{P}(\|\boldsymbol{X}^{\top}\boldsymbol{X}-\tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}}\|_{F}\leq\varepsilon\operatorname{Tr}(\boldsymbol{X}^{\top}\boldsymbol{X}))\geq1-e^{\left(-\frac{(\varepsilon\sqrt{k}-1)^{2}}{8}\right)},
$$
\n(12)

which completes the proof.

Proof of Lemma [4.6](#page-6-3) (Random Projection). We prove the lemma by showing the following inequality holds for any x ∈ R n:

:

<span id="page-15-0"></span>
$$
\mathcal{P}\left((1-\varepsilon)\|\mathbf{x}\|_{2}^{2} \leq \left\|\frac{1}{\sqrt{k}}\mathbf{P}\mathbf{x}\right\|^{2} \leq (1+\varepsilon)\|\mathbf{x}\|_{2}^{2}\right) \geq 1 - e^{\left(-\frac{k\varepsilon^{2}}{8}\right)}.
$$
\n(13)

We firstly show that E √ 1 k Ax 2 = kxk 2 2

$$
\mathbb{E}\left[\frac{1}{\sqrt{k}}\mathbf{P}\mathbf{x}\right] = \mathbb{E}\left[\sum_{i}^{k}\frac{1}{k}\sum_{j}(P_{ij}x_{j})^{2}\right] = \mathbb{E}\left[\sum_{i}^{k}\frac{1}{k}\sum_{j,j'}(P_{ij}P_{ij'}x_{j}x_{j'})\right]
$$
\n
$$
= \sum_{i}^{k}\frac{1}{k}\mathbb{E}\left[\sum_{j}\mathbf{P}_{ij}^{2}\mathbf{x}_{j}^{2}\right] = \sum_{i}^{k}\frac{1}{k}\sum_{j}x_{j}^{2}
$$
\n
$$
= \|\mathbf{x}\|_{2}^{2}.
$$
\n(14)

This implies X<sup>j</sup> ∼ N (0, 1)) where X<sup>j</sup> = Aj:x kxk<sup>2</sup> . Then we obtain:

$$
\mathcal{P}(\|\frac{1}{\sqrt{k}}\|\mathbf{P}\mathbf{x}\|_{2}^{2} > (1+\varepsilon)\|\mathbf{x}\|_{2}) = \mathcal{P}(\sum_{j=1}^{k} X_{j}^{2} > (1+\varepsilon)k),
$$
\n
$$
= \mathcal{P}\left(e^{t\sum_{i=1}^{k} X_{j}^{2}} > e^{t(1+\varepsilon)k}\right)
$$
\n
$$
\leq \frac{\mathbb{E}\left[e^{t\sum_{j=1}^{k} X_{j}^{2}}\right]}{e^{t(1+\varepsilon)k}} \quad \text{(apply Markov's inequality)}
$$
\n
$$
= \frac{\prod_{j=1}^{k} \mathbb{E}\left[e^{tX_{j}^{2}}\right]}{e^{t(1+\varepsilon)k}} \quad \text{(as } X_{j} \text{ is i.i.d)} \qquad (15)
$$
\n
$$
= \frac{\left(\mathbb{E}\left[e^{tX_{j}^{2}}\right]\right)^{k}}{e^{t(1+\varepsilon)k}} \quad \text{(for } 1 < t < 1/2 \text{, it holds that } \mathbb{E}\left[e^{tX_{i}^{2}}\right] \leq \left(\frac{1}{\sqrt{1-2t}}\right)
$$
\n
$$
\leq \left(\frac{1}{\sqrt{1-2t}}\right)^{k} \left(\frac{1}{e^{t(1+\varepsilon)}}\right)^{k}
$$
\n
$$
= \left(e^{-\varepsilon + \ln(1+\varepsilon)}\right)^{k/2} \quad \text{(using } \ln(1+\varepsilon) \leq \varepsilon - \frac{\varepsilon^{2}}{4})
$$

Thus, we have:

$$
\mathcal{P}(\|\frac{1}{\sqrt{k}}\|\mathbf{P}\mathbf{x}\|_{2}^{2} \le (1+\varepsilon)\|\mathbf{x}\|_{2}^{2})\le 1 - e^{-\varepsilon^{2}k/8}.
$$
 (16)

In the similar way, it is easy to prove that:

$$
\mathcal{P}(\|\frac{1}{\sqrt{k}}\|\mathbf{P}\mathbf{x}\|_{2}^{2} \ge (1-\varepsilon)\|\mathbf{x}\|_{2}^{2})\le 1-e^{-\varepsilon^{2}k/8}.
$$
 (17)

Thus, it holds that:

$$
\mathcal{P}((1-\varepsilon)\|\mathbf{x}\|_{2}^{2} \le \frac{1}{\sqrt{k}}\|\mathbf{P}\mathbf{x}\|_{2}^{2} \le (1+\varepsilon)\|\mathbf{x}\|_{2}^{2}) \ge 1 - e^{-\varepsilon^{2}k/8}.
$$
 (18)

Suppose X<sup>&</sup>gt;X − X˜ <sup>&</sup>gt;X˜ 0 and x is the eigenvector of X<sup>&</sup>gt;X − X˜ <sup>&</sup>gt;X˜ corresponding to its largest eigenvalue σmax, then we have:

$$
\|\boldsymbol{X}^{\top}\boldsymbol{X} - \tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}}\|_{2} = \boldsymbol{x}^{\top}(\boldsymbol{X}^{\top}\boldsymbol{X} - \tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}})\boldsymbol{x} = \sigma_{\max} \tag{19}
$$

$$
= \boldsymbol{x}^T \boldsymbol{X}^T \boldsymbol{X} \boldsymbol{x} - \boldsymbol{x}^T \tilde{\boldsymbol{X}}^T \tilde{\boldsymbol{X}} \boldsymbol{x}
$$
 (20)

$$
= \|Xx\|_2^2 - \|\tilde{X}x\|_2^2 \tag{21}
$$

$$
= \|Xx\|_2^2 - \|PXx\|_2^2. \tag{22}
$$

Applying Eq. [\(13\)](#page-15-0), there is at least probability 1 − e −ε <sup>2</sup>k/8 such that:

$$
\| \boldsymbol{X}^{\top} \boldsymbol{X} - \tilde{\boldsymbol{X}}^{\top} \tilde{\boldsymbol{X}} \|_{2} \le \| \boldsymbol{X} \boldsymbol{x} \|_{2}^{2} + (\varepsilon - 1) \| \boldsymbol{X} \boldsymbol{x} \|_{2}^{2}
$$
  
\n
$$
= \varepsilon \| \boldsymbol{X} \boldsymbol{x} \|_{2}^{2}
$$
  
\n
$$
\le \varepsilon \| \boldsymbol{X} \|_{2}^{2} \| \boldsymbol{x} \|_{2}^{2} (\| \boldsymbol{x} \|_{2}^{2} = 1)
$$
  
\n
$$
\le \varepsilon \operatorname{Tr} (\boldsymbol{X}^{\top} \boldsymbol{X})
$$
\n(23)

In similar way, it is easy to show that above equation also holds for X˜ <sup>&</sup>gt;X˜ − X>X 0, Thus, we have:

$$
\mathcal{P}(\|\boldsymbol{X}^{\top}\boldsymbol{X} - \tilde{\boldsymbol{X}}^{\top}\tilde{\boldsymbol{X}}\|_2 \leq \varepsilon \operatorname{Tr}(\boldsymbol{X}^{\top}\boldsymbol{X})) \geq 1 - e^{-\varepsilon^2 k/8},\tag{24}
$$

which completes the proof.

# B Accelerating the Feature Augmentation with a Very Sparse Random Projection

To accelerate the random projection, approach [\[20\]](#page-12-18) presented a sparse random projection as an improvement over the Gaussian random projection, in which entries of P are i.i.d. sampled from:

<span id="page-16-0"></span>
$$
p_{ij} = \begin{cases} \sqrt{s} & \text{with probability } \frac{1}{2s}, \\ 0 & \text{with probability } 1 - \frac{1}{s}, \\ -\sqrt{s} & \text{with probability } \frac{1}{2s}, \end{cases}
$$
 (25)

where <sup>1</sup> s denotes the density of matirx P .

The computation cost of the random projection is related to the sparsity of the sparse random matrix (SRP). The low density of the SRP reduces the computational cost while it may affect the performance at the same time. To explore the relationship between the density and performance, we vary the density of the random matrix in the range of [0.001, 0.002, 0.004, 0.008, 0.016, 0.32, 0.064]. We plot the relationship between the performance and density on three datasets in Figure [9.](#page-10-0) It is apparent that the performance improves as the density increases. However, the performance reaches a relatively high peak at a low density ( <sup>1</sup> <sup>s</sup> < 0.01). This suggests that one could enjoy the efficiency provided by the SPMM without sacrificing the performance.

# <span id="page-16-2"></span><span id="page-16-1"></span>C Statistics of Datasets

| Dataset               | #Nodes | #Edges  | #Features | #Classes |
|-----------------------|--------|---------|-----------|----------|
| Wiki-CS [23]          | 11,701 | 216,123 | 300       | 10       |
| Amazon-Computers[22]  | 13,752 | 245,861 | 767       | 10       |
| Amazon-Photo[22]      | 7,650  | 119,081 | 745       | 8        |
| Coauthor-CS [29]      | 18,333 | 81,894  | 6,805     | 15       |
| Coauthor-Physics [29] | 34,493 | 247,962 | 8,415     | 5        |
| Cora [17]             | 2,708  | 5,429   | 1,433     | 7        |
| Citeseer [17]         | 3,327  | 4,732   | 3,703     | 6        |
| Pubmed [17]           | 19,717 | 44,338  | 500       | 3        |
| DBLP [17]             | 17,716 | 105,734 | 1,639     | 4        |

Table 5: Statistics of datasets used in our experiments.

Below we describe datasets from Table [5:](#page-16-2)

• Cora, CiteSeer, Pubmeb, DBLP. These are well-known citation network datasets, in which nodes represent publications and edges indicate their citations. All nodes are labeled according to paper subjects [\[17\]](#page-12-0).

- WikiCS. It is a network of Wikipedia pages related to the computer science, with edges showing cross-references. Each article is assigned to one of 10 subfields (classes), with characteristics computed using the content's averaged GloVe embeddings. We make no changes to the 20 train/val/test data splits provided by [\[23\]](#page-12-12).
- Am-Computer, AM-Photo. Both of these networks are based on Amazon's co-purchase data. Nodes represent products, while edges show how frequently they were purchased together. Each product is described using a Bag-of-Words representation based on the reviews (node features). There are ten node classes (product categories) and eight node classes (product categories), respectively [\[22\]](#page-12-11).
- Coauthor-CS, Coauthor-Physics. These are two networks that were extracted from the Microsoft Academic Graph dataset. The edges reflect a collaboration between two authors, while the nodes represent writers. The keywords that each author uses in their articles are utilized to categorize them (Bag-of-Words representation; node features). According to [\[29\]](#page-13-17), there are 15 author research fields (node classes) and 5 author research fields (node classes).