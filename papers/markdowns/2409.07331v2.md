# Learning to Compress Contexts for Efficient Knowledge-based Visual Question Answering

Weixi Weng <sup>1</sup> Jieming Zhu <sup>2</sup> Xiaojun Meng <sup>2</sup> Hao Zhang <sup>2</sup> Rui Zhang <sup>3</sup> Chun Yuan <sup>1</sup>

### Abstract

Multimodal large language models (MLLMs) have demonstrated great performance on visual question answering (VQA). When it comes to knowledge-based Visual Question Answering (KB-VQA), MLLMs may lack the specialized domain knowledge needed to answer questions, necessitating the retrieval of necessary information from external knowledge sources. Previous works like Retrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input information, such as image-based textual descriptions and retrieved knowledge, as possible to improve performance, but they all overlook the issue that with the number of input tokens increasing, inference efficiency significantly decreases, which contradicts the demands of practical applications. To address this issue, we propose Retrieval-Augmented MLLMs with Compressed Contexts (RACC). RACC learns to compress and aggregate retrieved knowledge for a given image-question pair, generating a compact modulation in the form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby achieving effective and efficient inference. RACC achieves a state-of-theart (SOTA) performance of 63.92% on OK-VQA. Moreover, it significantly reduces inference latency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments show RACC's broad applicability. It is compatible with various off-the-shelf MLLMs and can also handle different knowledge sources including textual and multimodal documents.

### 1. Introduction

Multimodal large language models (MLLMs) have attracted wide research attention, demonstrating great zero-shot performances among various visual question answering (VQA) datasets. However, in practical applications, generating accurate answers to specific questions necessitates not just a precise grasp of image content, but also human commonsense or domain-specific knowledge. This category of VQA

tasks is known as knowledge-based VQA (KB-VQA). Given that knowledge parameterized within MLLMs is static and limited, utilizing an external knowledge source to furnish necessary information to MLLMs emerges as a dependable strategy for addressing KB-VQA challenges.

In previous studies of KB-VQA, a line of works [\(Hu et al.,](#page-8-0) [2023a;](#page-8-0) [Khademi et al.,](#page-8-1) [2023;](#page-8-1) [An et al.,](#page-8-2) [2024\)](#page-8-2) utilize knowledge from very large MLLMs (GPT-4) or LLMs (ChatGPT, GPT-3) to aid in answering question. However, the static knowledge in such models may become out-of-date, and the models may generate incorrect content due to hallucinations, particularly in specific domains. What's more, the high cost associated with using these models is also a notable downside. Another line of research which retrieves knowledge from updateable external knowledge sources, such as knowledge graphs [\(Speer et al.,](#page-9-0) [2017\)](#page-9-0), documents [\(Luo et al.,](#page-9-1) [2021\)](#page-9-1), *etc.*, is more reliable and better aligns with the needs of real-world applications.

Retrieval Augmented VQA-v2 (RAVQA-v2) tackles the problem of KB-VQA by performing straightforward retrieval-augmented generation (RAG) on MLLMs. However, it has a notable shortcoming, *i.e.* low efficiency during inference. In the inference process of RAVQA-v2, the K retrieved documents are first concatenated with the imagequestion pair and inputted into the MLLM to obtain K candidate answers, then the final answer is selected from the K candidates by their joint probabilities. The process is undoubtedly very time-consuming and resource-intensive. Moreover, the retrieved documents can be quite long and often contain a lot of redundant information, which can further exacerbate the problem of low inference efficiency.

Inference efficiency is a key concern in practical applications of MLLMs. However, previous work, including RAVQA-v2, has focused on how to use as much knowledge as possible to improve the accuracy of answers while neglecting the fact that inference efficiency significantly declines as the number of input tokens increases.

Therefore, we aim to design an innovative RAG framework based on MLLMs, which can utilize the information of retrieved knowledge in an effective and efficient manner to improve MLLMs' inference efficiency for KB-VQA.

Furthermore, RAVQA-v2 and many previous works [\(Luo](#page-9-1) [et al.,](#page-9-1) [2021;](#page-9-1) [Lin & Byrne,](#page-8-3) [2022;](#page-8-3) [Lin et al.,](#page-8-4) [2022\)](#page-8-4) on KB-VQA primarily focus on using textual documents as external knowledge sources, and there has been relatively less research on using multimodal documents as knowledge sources. However, multimodal documents are an important knowledge resource in real-world applications, which can provide knowledge in handling KB-VQA [\(Raffel et al.,](#page-9-2) [2020;](#page-9-2) [Hu et al.,](#page-8-5) [2023b\)](#page-8-5). What's more, MLLMs inherently have the ability to directly comprehend multimodal knowledge. Therefore, we aim to explore the effects of multimodal documents in RAG applications based on MLLMs.

In this paper, we propose RACC, *i.e.* Retrieval-Augmented MLLMs with Compressed Contexts, an effective and efficient RAG framework for KB-VQA based on MLLMs. RACC consists of three phases, namely compression learning, information aggregation, and modulation generation. Our contributions can be summarized as follows:

- RACC is the first work to integrate prompt compression technology with KB-VQA, proposing an innovative framework for efficient RAG on MLLMs. We identify four key issues in training RACC and propose four corresponding methods to address them.
- RACC achieves excellent performance comparable to many competitive baselines on two KB-VQA datasets at a very low cost, reaching a state-of-the-art (SOTA) performance of 63.92% on the OK-VQA dataset.
- RACC achieves outstanding enhancements in terms of time and space efficiency. For time efficiency, RACC can save 22.0-59.7% of inference latency compared to RAVQA-v2. For space efficiency, RACC supports presaving documents that occupy a large storage footprint in the form of compressed prompts to save disk space.
- RACC can be applied to various off-the-shelf MLLMs, but also can handle different knowledge sources such as textual documents and multimodal documents.

### 2. Related Work

#### 2.1. Multimodal Large Language Models

Multimodal large language models (MLLMs) bridge the gap between text and other modalities and unify understanding of different modalities. Although recent MLLMs [\(Alayrac](#page-8-6) [et al.,](#page-8-6) [2022;](#page-8-6) [Li et al.,](#page-8-7) [2023;](#page-8-7) [Dai et al.,](#page-8-8) [2023;](#page-8-8) [Bai et al.,](#page-8-9) [2023;](#page-8-9) [Liu et al.,](#page-8-10) [2023;](#page-8-10) [Hu et al.,](#page-8-11) [2024\)](#page-8-11) have demonstrated excellent zero-shot performance in VQA tasks, they require the assistance of external knowledge sources for questions that require specialized domain knowledge.

In this work, we investigate two main architectures of MLLMs: encoder-decoder MLLMs including BLIP2- FlanT5XL [\(Li et al.,](#page-8-7) [2023\)](#page-8-7) and InstructBLIP-FlanT5XL [\(Dai et al.,](#page-8-8) [2023\)](#page-8-8), and the decoder-only MLLMs including InstructBLIP-Vicuna7B and miniCPM-v2 [\(Hu et al.,](#page-8-11) [2024\)](#page-8-11).

#### 2.2. Knowledge-based Visual Question Answering

Knowledge-based Visual Question Answering (KB-VQA), anchored by the given images, works to answer questions that require external knowledge. KB-VQA is an important multimodal task and has garnered widespread attention.

Early KB-VQA works [\(Luo et al.,](#page-9-1) [2021;](#page-9-1) [Marino et al.,](#page-9-3) [2021\)](#page-9-3) train specialized VQA models with designated knowledge sources such as ConceptNet KnowledgeGraph [\(Speer et al.,](#page-9-0) [2017\)](#page-9-0), textual document sources like Google Search [\(Luo](#page-9-1) [et al.,](#page-9-1) [2021\)](#page-9-1) or WikiData [\(Vrandeciˇ](#page-9-4) c & Kr ´ otzsch ¨ , [2014\)](#page-9-4), and image sources like Google Image Search *etc.*. However, these methods often demonstrate limited performance.

With the emergence of pretrained large language models (LLMs), they have become a focus of research in this field. A series of works utilize very large LLMs like GPT-3 [\(Brown et al.,](#page-8-12) [2020\)](#page-8-12), chatGPT to generate auxiliary knowledge or directly answer the questions [\(Gui et al.,](#page-8-13) [2021;](#page-8-13) [Lin](#page-8-4) [et al.,](#page-8-4) [2022;](#page-8-4) [Yang et al.,](#page-9-5) [2022;](#page-9-5) [Shao et al.,](#page-9-6) [2023;](#page-9-6) [Hu et al.,](#page-8-0) [2023a\)](#page-8-0), achieving performance breakthroughs. Due to the high costs associated with using very large LLMs like GPT-3, and the fact that the knowledge within them can be outdated and incorrect, another line of work focuses on how to conduct retrieval-augmented generation (RAG) on smaller LLMs with various external knowledge bases for KB-VQA [\(Gui et al.,](#page-8-13) [2021;](#page-8-13) [Gao et al.,](#page-8-14) [2022;](#page-8-14) [Lin & Byrne,](#page-8-3) [2022;](#page-8-3) [Lin](#page-8-15) [et al.,](#page-8-15) [2024a;](#page-8-15) [Hu et al.,](#page-8-5) [2023b\)](#page-8-5). However, the above methods all face the same problem: they require converting images into textual descriptions such as captions, object tags, *etc.,* so that LLMs can understand [\(Gui et al.,](#page-8-13) [2021;](#page-8-13) [Gao et al.,](#page-8-14) [2022;](#page-8-14) [Shao et al.,](#page-9-6) [2023;](#page-9-6) [Lin et al.,](#page-8-15) [2024a\)](#page-8-15), which may result in loss of critical visual information, but also significantly increases the number of input tokens, leading to a notable increase in inference latency.

With the advent of multimodal large language models (MLLMs), the aforementioned problems have been perfectly solved. Recent works have made great progress by utilizing MLLMs. A line of works proposes to combine MLLMs and LLMs together [\(Khademi et al.,](#page-8-1) [2023;](#page-8-1) [Xenos et al.,](#page-9-7) [2023;](#page-9-7) [An et al.,](#page-8-2) [2024;](#page-8-2) [Liang et al.,](#page-8-16) [2024;](#page-8-16) [2025\)](#page-8-17). MM-Reasoner [\(Khademi et al.,](#page-8-1) [2023\)](#page-8-1) leverages vision APIs and rationales generated by GPT-4 to fine-tune MLLMs such as Flamingo. RAVQA-v2 [\(Lin et al.,](#page-8-15) [2024a\)](#page-8-15) is the first work to build a simple RAG framework on top of MLLMs. However, it suffers from low efficiency during the inference stage and doesn't study the impact of multimodal documents on RAG applications of MLLMs.

Title Suppressed Due to Excessive Size

![](_page_2_Figure_1.jpeg)

**Caption:** Figure 1 illustrates the structural framework of Retrieval-Augmented MLLMs with Compressed Contexts (RACC). It details the process of retrieving K relevant documents based on an image-question pair, followed by compression learning, information aggregation, and modulation generation phases, ultimately producing a Key-Value cache for efficient inference in knowledge-based visual question answering.

Figure 1. The structural framework of RACC. An image and a question are first input into the multimodal retriever to retrieve K relevant documents. During the compression learning phase, the K documents, image, and question are input into HyperMLLM to obtain their corresponding compressed prompts. In the information aggregation phase, the obtained compressed prompts are aggregated to form the document-based compressed prompts of vision and question. In the modulation generation phase, the output of the aggregation network is processed by a set of MLPs to obtain the KV cache for each layer of the downstream BaseMLLM. At the same time, the BaseMLLM receives the image and question as input and generates the final answer.

### 2.3. Prompt Compression

Given the inherent redundancy in natural language, prompt compression methods have been extensively studied to improve the efficiency of LLM inference. Prompt compression can be categorized into task-aware and task-agnostic methods. Since the generation of compressed prompts that perform well across diverse tasks is particularly challenging, we focus on the task-aware prompt compression paradigm.

An important line of work [Jiang et al.](#page-8-18) [\(2023a\)](#page-8-18); [Pan et al.](#page-9-8) [\(2024\)](#page-9-8); [Jiang et al.](#page-8-19) [\(2023b\)](#page-8-19) estimates the importance of the tokens within the original prompts by the information-based metrics and removes redundant tokens. [Xu et al.](#page-9-9) [\(2024\)](#page-9-9) trains a compressor model, which cuts redundant tokens in the passage based on the question.

In addition to the above methods, which detect and remove inherent redundant tokens in long contexts at the natural language level, a series of works aims at compressing long contexts into parameters, leveraging the capability of LLMs to implicitly eliminate redundant information within long contexts. [Mu et al.](#page-9-10) [\(2024\)](#page-9-10) supposes that each prompt is <span id="page-2-0"></span>composed of a task instruction part and a content part, and finetunes LLMs to compress the task instruction part into several gist tokens. [Chevalier et al.](#page-8-20) [\(2023\)](#page-8-20); [Wang et al.](#page-9-11) [\(2024\)](#page-9-11); [Tack et al.](#page-9-12) [\(2024\)](#page-9-12) proposed to compress long contexts into compact summary vectors, parameters of a Loramodule and KV Cache, respectively. Such kind of methods have already demonstrated significant research value and strong research potential. In this paper, we extend these methods to the KB-VQA task.

### 3. Methods

In this section, we introduce our proposed RACC, *i.e.* Retrieval-Augmented MLLM with Compress Contexts. RACC can be divided into three phases: compression learning, information aggregation, and modulation generation, which will be detailed in the corresponding subsections. Based on a profound understanding of the task and an analysis of the shortcomings in RAVQA-v2, we identify four key issues that need to be addressed in training RACC and further propose four methods to address them, which is elaborated in Subsection [3.2](#page-3-0) and [3.3.](#page-4-0)

Title Suppressed Due to Excessive Size

| Model                                 | Image-base Textual Description         | Base Model            | knowledge source         | VQA Accuracy |  |
|---------------------------------------|----------------------------------------|-----------------------|--------------------------|--------------|--|
| Specialized baselines                 |                                        |                       |                          |              |  |
| KRISP                                 |                                        |                       | C                        | 38.35        |  |
| VRR                                   | Caption                                |                       | GS                       | 45.08        |  |
| MALI                                  |                                        |                       | miniGPT4 + C             | 56.69        |  |
| REVIVE                                | Caption + Object Tags                  |                       | WD + GPT-3               | 58.00        |  |
| REVEAL                                |                                        | T5-Large              | WIT + CC + WD + V2       | 59.10        |  |
| Baselines on LLMs                     |                                        |                       |                          |              |  |
| KAT                                   | Caption + Object Tags                  | T5-large              | W                        | 44.25        |  |
| KGenVQA                               | Caption                                | UnifiedQA             | PNP                      | 45.40        |  |
| PICa                                  | Caption + Object Tags                  | GPT-3                 |                          | 48.00        |  |
| RA-VQA                                | OCR + Caption + Object Tags            | T5-large              | GS                       | 51.22        |  |
| KAT-Ensemble                          | Caption + Object Tags                  | T5-large              | W + GPT-3                | 54.41        |  |
| RA-VQAv2                              | OCR + Caption + Object Tags            | T5-large              | GS                       | 54.85        |  |
| Prophet                               | Caption                                | GPT-3                 | MCAN                     | 58.27        |  |
| PromptCap                             | Caption                                | GPT-3                 | ICE (16)                 | 60.40        |  |
| Baselines based on MLLMs              |                                        |                       |                          |              |  |
| PaLI                                  |                                        | PaLI-15B              |                          | 56.50        |  |
| Flamingo                              |                                        | Flamingo              |                          | 57.80        |  |
| BLIP2                                 |                                        | BLIP2-FlanT5XL        |                          | 31.76        |  |
| RA-VQAv2                              |                                        | BLIP2-FlanT5XL        | GS                       | 60.40        |  |
| RA-VQAv2                              |                                        | InstructBLIP-FlanT5XL | GS                       | 62.90        |  |
|                                       | Baselines based on both LLMs and MLLMs |                       |                          |              |  |
| MM-Reasoner                           | OCR + Caption + Object Tags            | Flamingo              | GPT-4                    | 59.20        |  |
| ASB                                   | Caption                                | LLAMA-2               | PNP + ICE (14)           | 59.07        |  |
| DKA                                   | Caption                                | LLAMA-2               | PNP + ChatGPT + ICE (14) | 62.10        |  |
| Our proposed framework based on MLLMs |                                        |                       |                          |              |  |
| RACC-homo                             |                                        | BLIP2-FlanT5XL        | WIT                      | 55.07        |  |
| RACC-homo                             |                                        | InstructBLIP-FlanT5XL | WIT                      | 59.17        |  |
| RACC-homo                             |                                        | BLIP2-FlanT5XL        | GS                       | 55.26        |  |
| RACC-homo                             |                                        | InstructBLIP-FlanT5XL | GS                       | 59.49        |  |
| RACC-hetero                           |                                        | BLIP2-Vicuna7B        | GS                       | 61.65        |  |
| RACC-hetero                           |                                        | InstructBLIP-Vicuna7B | GS                       | 63.92        |  |

Table 1. Model Performance on the OK-VQA dataset. Knowledge source abbreviations: C: ConceptNet; CC: CC12M; V2: VQA-2; W: Wikipedia; WD: WikiData; WIT: Wikipedia Image-Text; GS: Google Search; GI: Google Images; ICE: In-context Examples; PNP: Plug-and-Play VQA captioner [\(Tiong et al.,](#page-9-13) [2022\)](#page-9-13). Please refer to Section [3.4](#page-5-0) for the implications for RACC-homo and RACC-hetero. In the last two rows of results of the RACC-hetero, the hyperMLLM used is InstructBLIP-FlanT5XL.

### 3.1. Problem Setup

A typical VQA dataset can be divided into three components: images, questions, and answers, which can be represented using the notation {v, q, a}n. Following [Lin et al.](#page-8-15) [\(2024a\)](#page-8-15), we consider a realistic scenario of KB-VQA: an MLLM that takes an image v<sup>i</sup> and its related question q<sup>i</sup> as input, where the knowledge required to answer the question is supplied by external knowledge sources. In this paper, we study two important knowledge sources in real-world applications: multimodal documents and textual documents.

We utilize an off-the-shelf frozen multimodal retriever to retrieve K documents from the given knowledge source conditioned on the provided image and question. The K retrieved documents is denoted as {di} <sup>K</sup> = {d 1 i , d<sup>2</sup> i , . . . , d<sup>K</sup> i }.

The confidence scores that the multimodal retriever outputs for each of the K documents, *i.e.* the document retrieval scores, are denoted as {pi} <sup>K</sup> = {p 1 i , p<sup>2</sup> i , . . . , p<sup>K</sup> i }. The MLLM needs to leverage these retrieved documents to provide the correct answer to the question q<sup>i</sup> based on v<sup>i</sup> .

### <span id="page-3-1"></span><span id="page-3-0"></span>3.2. Phase 1: Compression Learning

The first phase of RACC is to compress the retrieved document into soft prompts of a specified length. [Phang](#page-9-14) [et al.](#page-9-14) [\(2023\)](#page-9-14) and [Tack et al.](#page-9-12) [\(2024\)](#page-9-12) introduce the idea of amortized-based meta-learning into the online learning task, which brings us inspiration. We utilize MLLMs of encoderdecoder architecture (such as BLIP2-FlanT5XL [\(Li et al.,](#page-8-7) [2023\)](#page-8-7), InstructBLIP-FlanT5XL [\(Dai et al.,](#page-8-8) [2023\)](#page-8-8), *etc.*) with a set of learnable prompts to compress the input information.

We denote the hyperMLLM by Mhyper and represent the encoder and decoder within the hyperMLLM by ENChyper and DEChyper, respectively. Each retrieved document is first fed into ENChyper. Subsequently, DEChyper takes the output from the encoder and the learnable prompts θ<sup>d</sup> as inputs, and the resulting output is the compressed prompts corresponding to that document. After such a compression process, the contextual information of each document is preserved in its compressed prompts. Consequently, in subsequent processing steps, the number of tokens that need to be processed for each document is reduced from the original

token count of the document to the predefined length of θd. The compressing process can be denoted as follows:

$$
\theta_i^k = \mathbf{M}_{hyper}(d_i^k, \theta_d) = \text{DEC}_{hyper}(\text{ENC}_{hyper}(d_i^k), \theta_d)
$$
\n(1)

θ k i denotes the compressed prompts of the document d k i .

We also compress the given image-question pair in a similar manner. Note that the predefined learnable prompts for compressing the image-question pairs are different from θd, which we denote as θvq, the compression process of the given image-question pair is denoted as follows:

$$
\theta_{vq_i} = \mathbf{M}_{hyper}(\text{CONCAT}(v_i, q_i), \theta_{vq})
$$
\n(2)

We name this phase the compression learning phase, as the learnable prompts need to be trained to acquire the ability to compress document context information. We then identify two key issues in this phase and propose corresponding solutions, as described below:

#### Issue 1: How to initialize the learnable prompts?

*Analysis:* First, the lengths of the two sets of learnable prompts, *i.e.* θ<sup>d</sup> and θvq, are critical in the compression learning process. If the prompts are too short, the amount of compressed semantic information that they can retain will be insufficient. In contrast, if the prompts are too long, the difficulty of training increases significantly.

We suggest determining the lengths of two sets of learnable prompts by experiments. To determine the optimal lengths of these prompts, we conducted a series of comparative experiments. We treat the lengths of θ<sup>d</sup> and θvq as hyperparameters and systematically explore their impact on the compression learning process.

Additionally, the initialization weights of the learnable prompts also play a crucial role in the hyperMLLM's ability to compress inputs, especially in the early training stage.

*Solution:* Therefore, we propose a method to initialize learnable prompts called Prompt Initialization with hard Prompt Embeddings (PIPE). We begin by manually designing two sets of hard prompts. For example, the hard prompt corresponding to θ<sup>d</sup> is "Summarize the key information of the given passage in a concise manner." The hard prompts are then processed by the tokenizer and the embedding layer of hyperMLLM to generate their embeddings. Subsequently, these embeddings are used to initialize the weights of the learnable prompts θ<sup>d</sup> and θvq.

#### Issue 2: How to deal with the irrelevant documents?

*Analysis:* The documents retrieved by the multimodal retriever may sometimes be completely irrelevant to the given image and question. Even if they are relevant, they may not provide the model with useful information to give the correct answer.

However, RAVQA-v2 [\(Lin et al.,](#page-8-15) [2024a\)](#page-8-15) does not consider the impact of irrelevant documents and treats both irrelevant documents and useful documents in the same way, *i.e.* concatenating retrieved documents with corresponding image-question pairs and inputting them into MLLMs to calculate the loss based on the correct answer. In clearer terms, RAVQA-v2 forces MLLMs to generate correct answers even based on irrelevant documents, which imposes incorrect supervised signals on the MLLM and can significantly harm the training process.

*Solution:* Therefore, to avoid the negative impact on the compression learning process caused by the involvement of irrelevant documents in training, we propose a method called Pseudo-Relevance-based Backpropagation Dropout (PRDB), which is introduced as follows:

Following [Luo et al.](#page-9-1) [\(2021\)](#page-9-1); [Lin et al.](#page-8-15) [\(2024a\)](#page-8-15), we consider a document to be pseudo-relevant if it contains any of the human-annotated answers. After all K retrieved documents are converted into compressed prompts, we apply a stop gradient operation, *i.e.* STOPGRAD(Mhyper(d k i , θd)) to the compressed prompts of those documents which are considered pseudo-irrelevant. Only the gradients of the compressed prompts of the pseudo-relevant documents will be backpropagated to the learnable prompts. In this way, after loss calculation and gradient backpropagation, PRDB prevents the gradients of the compressed prompts corresponding to irrelevant documents from leading the learnable prompts' weights to update in the wrong direction, ensuring the stability of the compression learning phase.

#### <span id="page-4-0"></span>3.3. Phase 2: Information Aggregation

After the compression learning phase, we now have a set of compressed prompts of the retrieved documents {θi} K, θvi , θq<sup>i</sup> and θvq<sup>i</sup> . In this section, we detail the process of using θvq<sup>i</sup> to aggregate useful semantic information from {θi} <sup>K</sup>. We further identify two key issues in designing the information aggregation process and propose two corresponding methods to address them. We name this phase as the information aggregation phase.

### Issue 3: How to enhance the semantic information in the compressed prompts of documents associated with the given image and question?

*Analysis:* The retrieved documents often contain a large number of redundant tokens, with only a small portion of tokens being relevant to the question and the image, which can be useful for answering the question and should be paid more attention to. Even after compression, the compressed prompts can still contain redundant semantic information. We think that leveraging the information from the image and the question to enhance the semantic information in the compressed prompts of documents is essential.

*Solution:* Based on the above, we propose a method called DCSE, *i.e.* Decoupled Compression for Semantics Enhancement. Our method begins by decoupling the given image-question pair (v<sup>i</sup> , qi) and input them separately into the hyperMLLM with θvq, generating θv<sup>i</sup> and θq<sup>i</sup> , which can be denoted as follows:

$$
\theta_{v_i} = \mathbf{M}_{hyper}(v_i, \theta_{vq})
$$
  
\n
$$
\theta_{q_i} = \mathbf{M}_{hyper}(q_i, \theta_{vq})
$$
\n(3)

Compared with θvq<sup>i</sup> obtained by jointly compressing the image-question pair, θv<sup>i</sup> and θq<sup>i</sup> derived from decoupled compression can better preserve the semantic information contained in both the image and the question. In the retrieved documents, the semantic information related to either the image or the question is of significance for answering the question. Therefore, we employ the concatenated θv<sup>i</sup> and θ<sup>q</sup><sup>i</sup> to enhance the semantic information of {θi} <sup>K</sup> via cross-attention. Here, we adopt a original cross-attention block, where {θi} <sup>K</sup> serves as the query, and the concatenated θv<sup>i</sup> and θq<sup>i</sup> act as both the key and the value. Denoting a original cross-attention block [\(Vaswani et al.,](#page-9-15) [2017\)](#page-9-15) as CA, the computational process of DCSE is denoted as follows:

$$
\{\theta_i^*\}^K = \mathbf{CA}(\{\theta_i\}^K, \text{CONCAT}(\theta_{v_i}, \theta_{q_i}))
$$
 (4)

{θ ∗ i } <sup>K</sup> represents the query-enhanced compressed prompts of documents.

### Issue 4: How to utilize the document retrieval scores to guide the aggregation process?

*Analysis:* In the retrieval process, most retrievers assign a confidence score to each retrieved document based on metrics such as embedding similarity. In the following text, we use "document retrieval score" to represent this score.

While judging whether a document can truly answer a question based on an image is difficult, the retrieval score offers a reliable metric for this purpose. Based on the principles of retrieval mechanisms, documents with higher retrieval scores are generally considered to provide more relevant and useful information for the given image and question. [Lin et al.](#page-8-15) [\(2024a\)](#page-8-15) utilizes the retrieval scores as a reference metric for selecting the final answer in the inference process, but it does not use this crucial metric in the training process.

*Solution:* Based on the above, we propose a method called Retrieval-Guided Cross-Attention (RGCA). RGCA is an improvement of the original cross-attention mechanism, designed to gather the semantic information from {θ ∗ i } <sup>K</sup> that can assist in answering questions into θvq<sup>i</sup> , guided by {pi} K. RGCA not only considers the embedding similarity between compressed prompts but also assigns more attention to the compressed prompts of documents with higher retrieval scores. Refer to the appendix for the pseudo code of RGCA. We denote a retrieval-guided cross-attention block as CAr, whose forward pass is denoted as:

$$
\theta_{vq_i}^* = \mathbf{CA}_r(\theta_{vq_i}, \{\theta_i^*\}^K, \{p_i\}^K)
$$
 (5)

The number of retrieval-guided cross-attention blocks contained in our framework is set to n<sup>r</sup> = 3.

#### <span id="page-5-0"></span>3.4. Phase 3: Modulation Generation

After the information aggregation phase, we have the documents-based compressed prompts of vision and question, which is denoted as θ ∗ vq<sup>i</sup> . Then we convert θ ∗ vq<sup>i</sup> into a P-Tuning v2 modulation for the downstream baseMLLM, which involves adding a small amount of KV cache at each layer of the baseMLLM. Given that different layers of MLLM process information at varying levels of abstraction and complexity, we employ a set of m Multi-Layer Perceptrons (MLPs) for projecting θ ∗ vq<sup>i</sup> into the additional KV cache of each layer in the baseMLLM, where m is the number of layers within the baseMLLM. Here, we denote the generated P-Tuning v2 modulation as Θ<sup>i</sup> and the frozen baseMLLM as Mbase.

Our framework can be optimized in an end-to-end manner using the loss function L, namely the language modeling loss based on the ground truth answer:

$$
\min_{\theta_d, \theta_{vq}, h} \frac{1}{N} \sum_{i=1}^N \mathcal{L}(\mathbf{M}_{base}(v_i, q_i; \Theta_i), a_i)
$$
 (6)

h includes a single CA block, n<sup>r</sup> CA<sup>r</sup> blocks and a set of MLPs. N is the training batch size. RACC offerss two variants: RACC-homogeneous and RACCheterogeneous, abbreviated as RACC-homo and RACC-hetero, respectively. In the setup of RACC-homo, the hyperMLLM and the baseMLLM are identical, which means that the MLLM learns to compress contexts for itself. For RACC-hetero, the hyperMLLM and the baseMLLM differ in either structure or weight.

### 4. Experiments

#### 4.1. Datasets and Knowledge Sources

We evaluate our framework on OK-VQA [\(Marino et al.,](#page-9-16) [2019\)](#page-9-16), which is the most widely studied KB-VQA dataset. We also conduct experiments on AOK-VQA [\(Schwenk et al.,](#page-9-17) [2022\)](#page-9-17), which is the successor of OK-VQA.

In terms of the knowledge source, following [Lin et al.](#page-8-15) [\(2024a\)](#page-8-15), we adopt Google Search [\(Luo et al.,](#page-9-1) [2021\)](#page-9-1) for OK-VQA and AOK-VQA, which is a textual document base comprised of nearly 200 thousand documents. We also carefully curated a multimodal document source from the Wikipedia Image-Text dataset [\(Srinivasan et al.,](#page-9-18) [2021\)](#page-9-18) for OK-VQA to show that our framework also works well with

|             |                       | Direct Answer |      |
|-------------|-----------------------|---------------|------|
| Method      | Base Model            | Val           | Test |
| ClipCap     |                       | 30.9          | 25.9 |
| LXMERT      |                       | 30.7          | 25.9 |
| KRISP       |                       | 33.7          | 27.1 |
| KGenVQA     | UnifiedQA             | 39.1          | -    |
| GPV-2       | T5-Large              | 48.6          | 40.7 |
| REVEAL      | T5-Large              | 52.2          | -    |
| PromptCap   | GPT-3                 | 56.3          | 59.6 |
| MM-Reasoner | Flamingo + i-Code     | -             | 60.2 |
| ASB         | LLAMA-2               | 58.6          | 57.5 |
| RACC-homo   | InstructBLIP-FlanT5XL | 62.1          | 58.1 |

<span id="page-6-0"></span>Table 2. The results on the AOK-VQA dataset. We use the GS knowledge for AOK-VQA here.

multimodal knowledge sources. We use GS and WIT to refer to these two knowledge sources.

We use FLMR [\(Lin et al.,](#page-8-15) [2024a\)](#page-8-15) and PREFLMR [\(Lin et al.,](#page-8-21) [2024b\)](#page-8-21) as retrievers for document retrieval. The FLMR retriever was used to retrieve information from the GS knowledge source, while the PREFLMR retriever was used to retrieve information from the WIT knowledge source.

#### 4.2. Training Setup

Most of the experiments are conducted on a 32G V100 GPU. The chosen optimizer is AdamW. During the first 1000 steps of training, the learning rate linearly increases from 10<sup>−</sup><sup>5</sup> to 10<sup>−</sup><sup>4</sup> . Subsequently, a cosine-decaying scheduler is applied to the learning rate to reduce it from 10<sup>−</sup><sup>4</sup> to 0. The batch size is set to 2. The hyperparameter K, *i.e.* the number of retrieved documents for each image-question pair, is always set to 5. Note that during the training process of RACC, all parameters of the hyperMLLM, baseMLLM, and multimodal retrievers are kept frozen.

### 4.3. Evaluation

We evaluate the performance of our framework using the official VQA Accuracy [\(Marino et al.,](#page-9-16) [2019\)](#page-9-16). Let a<sup>i</sup> be the list of human-annotated answers of the given imagequestion pair (v<sup>i</sup> , qi), and y<sup>i</sup> be the model's outputs. The VQA accuracy for (v<sup>i</sup> , qi) is calculated as follows:

$$
VQAACCURACY(a_i, y_i) = min(\frac{\#S(y_i)}{3}, 1)
$$
 (7)

where #S(yi) is the occurrence of y<sup>i</sup> in a<sup>i</sup> . The VQA accuracy on the entire dataset is obtained by averaging the accuracy of all image-question pairs.

### 4.4. Comparative Study

In this section, we will elaborate on the advantages of RACC compared to previous works from the following three as-

pects: performance, cost, and inference efficiency.

First of all, RACC outperforms many competitive baselines. The performance comparison of RACC with other competitive baselines on the OK-VQA dataset is presented in Table [1.](#page-3-1) Based on InstructBLIP-FlanT5XL, RACChomo with GS as the knowledge source reaches an accuracy of 59.65%. With WIT as the knowledge source, our framework achieves 59.17%. When adopting RACChetero, with InstructBLIP-FlanT5XL as the hyperMLLM and InstructBLIP-Vicuna7B as the baseMLLM, we achieve a state-of-the-art (SOTA) accuracy of 63.92%.

The results of AOK-VQA are shown in Table [2.](#page-6-0) Since the GS knowledge source we use for AOK-VQA is not designed for it, the documents in GS may not provide the required knowledge for all questions in AOK-VQA. However, RACC-homo based on InstructBLIP-FlanT5XL still achieves a state-of-the-art (SOTA) accuracy of 62.1% on the validation set. The performance on the test set is 58.1%.

In terms of cost, our work has notable advantages. First, we do not utilize any image-based textual descriptions provided by external APIs or models [\(Gui et al.,](#page-8-13) [2021;](#page-8-13) [Lin & Byrne,](#page-8-3) [2022;](#page-8-3) [An et al.,](#page-8-2) [2024\)](#page-8-2), such as captions, object tags, OCR *etc.* Second, RACC does not use any very large LLMs (ChatGPT, GPT-3) or MLLMs (GPT-4) but still achieves excellent performance even with small-scale MLLMs.

The inference efficiency is the main concern of this paper. RACC demonstrates significant advantages in inference efficiency compared to RAVQA-v2, which is shown in Table [3.](#page-6-1) In Table [3,](#page-6-1) the inference efficiency and disk usage of RACC are demonstrated under two scenarios: "w/o pre" and "w pre". Here, "w pre" refers to the scenario where the compressed encodings of retrieved documents are presaved before the inference process, which is shown in Figure [1.](#page-2-0) We also present the inference efficiency and disk usage of RAVQA-v2. When pre-saving compressed prompts, we achieve a substantial reduction of 59.7% in inference latency and 91.0% in disk space usage compared to RAVQA-v2. Even without pre-saved compressed prompts, the inference latency can still be reduced by 22.0%.

<span id="page-6-1"></span>

|                |          | RACC-homo |        |
|----------------|----------|-----------|--------|
|                | RAVQA-v2 | w/o pre   | w pre  |
| Eval Time (s)  | 1.1242   | 0.8768    | 0.4576 |
| Disk Usage (M) | 6.9680   | 6.9680    | 0.6280 |

Table 3. Comparison of inference efficiency between RAVQA-v2 and RACC when adopting the WIT knowledge source. "Eval time" and "Disk Usage" are measured for a single image-question pair input. "w pre" indicates pre-saving the compressed prompts of retrieved documents before inference. The MLLM used in both two frameworks is InstructBLIP-FlanT5XL.

Title Suppressed Due to Excessive Size

| No. | PIPE | DCSE | RGCA | PRDB | VQA Accuracy (%) |
|-----|------|------|------|------|------------------|
| 1   |      |      |      |      | 57.60            |
| 2   |      |      |      | ✓    | 58.18 (+0.58)    |
| 3   | ✓    |      |      | ✓    | 58.49 (+0.89)    |
| 4   | ✓    | ✓    |      | ✓    | 59.26 (+1.66)    |
| 5   | ✓    |      | ✓    | ✓    | 58.86 (+1.26)    |
| 6   | ✓    | ✓    | ✓    |      | 58.95 (+1.35)    |
| 7   |      | ✓    | ✓    | ✓    | 59.07 (+1.47)    |
| 8   | ✓    | ✓    | ✓    | ✓    | 59.49 (+1.89)    |

Table 4. The results of ablation studies of RACC. The GS knowledge source is adopted here. The ablation experiments are conducted based on RACC-homo with InstructBLIP-FlanT5XL.

| L(θvq) | L(θd) | VQA Accuracy |
|--------|-------|--------------|
| 8      | 12    | 58.77        |
| 8      | 16    | 58.83        |
| 12     | 12    | 58.96        |
| 12     | 16    | 59.07        |
| 12     | 20    | 58.56        |

Table 5. RACC-homo's results of the comparative experiments on the length of the predefined learnable prompts θvq and θd. The two sets of learnable prompts are randomly initialized here.

#### 4.5. Ablation Studies

We propose four methods to improve the aggregation process of compressed contexts and conduct ablation studies to verify their effectiveness. The settings and results of ablation studies are depicted in Table [8](#page-12-0) and Table [5.](#page-7-0) Note that we adopt RACC-homo with the GS knowledge source in the ablation studies, where the hyperMLLM and baseMLLM are both initialized from InstructBLIP-FlanT5XL. Refer to the appendix for more details of the ablation studies.

Firstly, comparing lines 2 and 3, as well as lines 7 and 8, we can observe that the PIPE method brings improvements of 0.31% and 0.42% under different settings. From the difference between lines 3 and 4 in Table [8,](#page-12-0) we observe that the DCSE method brings an improvement of 0.77%. On the other hand, the RGCA method results in a performance gain of 0.37%, as shown in lines 3 and 5. Last but not least, the performance difference between lines 6 and 8 shows that the PRDB method leads to a performance gain of 0.54%.

We also investigate into the settings of some important hyperparameters. We first explore how to set the length of learnable prompts (*i.e.* L(θvq) *and* L(θd)), and the results are shown in Table [5.](#page-7-0) We select the best configuration, setting L(θvq) and L(θd) to 12 and 16, respectively. All other experiments in this paper are conducted using this configuration. In the appendix, we provide comparative experiments on the hyperparameter K.

#### 4.6. Broad Applicability of RACC

RACC shows broad applicability from multiple aspects.

1. RACC can utilize different types of knowledge sources to aid its efficient RAG process. We evaluate RACC with two knowledge sources, *i.e.* WIT and GS, which represent multimodal documents and textual documents.

2. RACC can leverage any off-the-shelf multimodal retriever for retrieval, and our proposed RGCA method enables RACC to benefit from advancements in multimodal retrieval technology.

<span id="page-7-0"></span>3. RACC can be applied to any off-the-shelf MLLMs. We further conduct experiments under the setup of RACChetero and present the results in Table [6.](#page-7-1) Experiments show that RACC-hetero performs well across different baseMLLMs. The setup of RACC-hetero is also of practical significance: When it is not feasible to directly fine-tune the baseMLLM due to resource constraints, our framework can still work by adopting a much smaller hyperMLLM to adapt the larger frozen baseMLLM.

<span id="page-7-1"></span>

| baseMLLM              | VQA Accuracy |  |
|-----------------------|--------------|--|
| miniCPM-v2            | 48.21        |  |
| BLIP2-FlanT5XL        | 54.91        |  |
| InstructBLIP-FlanT5XL | 59.49        |  |
| BLIP2-Vicuna7B        | 61.65        |  |
| InstructBLIP-Vicuna7B | 63.92        |  |

Table 6. RACC-hetero's experimental results on OK-VQA using different MLLMs as the baseMLLM. The hyperMLLM is fixed as InstructBLIP-FlanT5XL here.

# 5. Conclusion

In this paper, we propose Retrieval-Augmented MLLMs with Compressed Contexts (RACC). RACC has achieved the following accomplishments in the area of KB-VQA:

1. RACC achieves competitive performance at a very low cost on challenging KB-VQA datasets.

2. As the first work to explore how to conduct efficient RAG on MLLMs for KB-VQA tasks, RACC provides a reliable way that not only reduces inference latency but also significantly saves disk space.

3. RACC is applicable to different MLLMs and various kinds of external knowledge sources.

With the rapid development of RAG technology and MLLMs, we believe that inference latency is a key concern in practical applications, which has often been overlooked in previous KB-VQA works. We hope our research will provide some inspiration for future work in this field.

### References

- <span id="page-8-6"></span>Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. *Advances in Neural Information Processing Systems*, 35:23716–23736, 2022.
- <span id="page-8-2"></span>An, W., Tian, F., Nie, J., Shi, W., Lin, H., Chen, Y., Wang, Q., Wu, Y., Dai, G., and Chen, P. Knowledge acquisition disentanglement for knowledge-based visual question answering with large language models. *arXiv preprint arXiv:2407.15346*, 2024.
- <span id="page-8-9"></span>Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A frontier large visionlanguage model with versatile abilities. *arXiv preprint arXiv:2308.12966*, 2023.
- <span id="page-8-12"></span>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. *Advances in neural information processing systems*, 33: 1877–1901, 2020.
- <span id="page-8-20"></span>Chevalier, A., Wettig, A., Ajith, A., and Chen, D. Adapting language models to compress contexts. *arXiv preprint arXiv:2305.14788*, 2023.
- <span id="page-8-8"></span>Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
- <span id="page-8-14"></span>Gao, F., Ping, Q., Thattai, G., Reganti, A., Wu, Y. N., and Natarajan, P. Transform-retrieve-generate: Natural language-centric outside-knowledge visual question answering. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 5067–5077, June 2022.
- <span id="page-8-13"></span>Gui, L., Wang, B., Huang, Q., Hauptmann, A., Bisk, Y., and Gao, J. Kat: A knowledge augmented transformer for vision-and-language. *arXiv preprint arXiv:2112.08614*, 2021.
- <span id="page-8-11"></span>Hu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z., Fang, Y., Huang, Y., Zhao, W., et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. *arXiv preprint arXiv:2404.06395*, 2024.
- <span id="page-8-0"></span>Hu, Y., Hua, H., Yang, Z., Shi, W., Smith, N. A., and Luo, J. Promptcap: Prompt-guided image captioning for vqa with gpt-3. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 2963–2975, 2023a.
- <span id="page-8-5"></span>Hu, Z., Iscen, A., Sun, C., Wang, Z., Chang, K.-W., Sun, Y., Schmid, C., Ross, D. A., and Fathi, A. Reveal:

Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 23369–23379, 2023b.

- <span id="page-8-18"></span>Jiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L. Llmlingua: Compressing prompts for accelerated inference of large language models. *arXiv preprint arXiv:2310.05736*, 2023a.
- <span id="page-8-19"></span>Jiang, H., Wu, Q., Luo, X., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. *arXiv preprint arXiv:2310.06839*, 2023b.
- <span id="page-8-1"></span>Khademi, M., Yang, Z., Frujeri, F., and Zhu, C. Mmreasoner: A multi-modal knowledge-aware framework for knowledge-based visual question answering. In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pp. 6571–6581, 2023.
- <span id="page-8-7"></span>Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. *arXiv preprint arXiv:2301.12597*, 2023.
- <span id="page-8-16"></span>Liang, J., Meng, X., Wang, Y., Liu, C., Liu, Q., and Zhao, D. End-to-end video question answering with frame scoring mechanisms and adaptive sampling. *arXiv preprint arXiv:2407.15047*, 2024.
- <span id="page-8-17"></span>Liang, J., Meng, X., Zhang, H., Wang, Y., Wei, J., and Zhao, D. Reasvqa: Advancing videoqa with imperfect reasoning process. *arXiv preprint arXiv:2501.13536*, 2025.
- <span id="page-8-3"></span>Lin, W. and Byrne, B. Retrieval augmented visual question answering with outside knowledge. *arXiv preprint arXiv:2210.03809*, 2022.
- <span id="page-8-15"></span>Lin, W., Chen, J., Mei, J., Coca, A., and Byrne, B. Finegrained late-interaction multi-modal retrieval for retrieval augmented visual question answering. *Advances in Neural Information Processing Systems*, 36, 2024a.
- <span id="page-8-21"></span>Lin, W., Mei, J., Chen, J., and Byrne, B. Preflmr: Scaling up fine-grained late-interaction multi-modal retrievers. *arXiv preprint arXiv:2402.08327*, 2024b.
- <span id="page-8-4"></span>Lin, Y., Xie, Y., Chen, D., Xu, Y., Zhu, C., and Yuan, L. Revive: Regional visual representation matters in knowledge-based visual question answering. *Advances in Neural Information Processing Systems*, 35:10560– 10571, 2022.
- <span id="page-8-10"></span>Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. *arXiv preprint arXiv:2304.08485*, 2023.
- <span id="page-9-1"></span>Luo, M., Zeng, Y., Banerjee, P., and Baral, C. Weaklysupervised visual-retriever-reader for knowledge-based question answering. *arXiv preprint arXiv:2109.04014*, 2021.
- <span id="page-9-16"></span>Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-vqa: A visual question answering benchmark requiring external knowledge. In *Proceedings of the IEEE/cvf conference on computer vision and pattern recognition*, pp. 3195–3204, 2019.
- <span id="page-9-3"></span>Marino, K., Chen, X., Parikh, D., Gupta, A., and Rohrbach, M. Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 14111–14121, 2021.
- <span id="page-9-10"></span>Mu, J., Li, X., and Goodman, N. Learning to compress prompts with gist tokens. *Advances in Neural Information Processing Systems*, 36, 2024.
- <span id="page-9-8"></span>Pan, Z., Wu, Q., Jiang, H., Xia, M., Luo, X., Zhang, J., Lin, Q., Ruhle, V., Yang, Y., Lin, C.-Y., et al. Llmlingua-2: ¨ Data distillation for efficient and faithful task-agnostic prompt compression. *arXiv preprint arXiv:2403.12968*, 2024.
- <span id="page-9-14"></span>Phang, J., Mao, Y., He, P., and Chen, W. Hypertuning: Toward adapting large language models without backpropagation. In *International Conference on Machine Learning*, pp. 27854–27875. PMLR, 2023.
- <span id="page-9-2"></span>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. *Journal of machine learning research*, 21 (140):1–67, 2020.
- <span id="page-9-17"></span>Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R. A-okvqa: A benchmark for visual question answering using world knowledge. In *European conference on computer vision*, pp. 146–162. Springer, 2022.
- <span id="page-9-6"></span>Shao, Z., Yu, Z., Wang, M., and Yu, J. Prompting large language models with answer heuristics for knowledgebased visual question answering. In *Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition*, pp. 14974–14983, 2023.
- <span id="page-9-0"></span>Speer, R., Chin, J., and Havasi, C. Conceptnet 5.5: An open multilingual graph of general knowledge. In *Proceedings of the AAAI conference on artificial intelligence*, volume 31, 2017.
- <span id="page-9-18"></span>Srinivasan, K., Raman, K., Chen, J., Bendersky, M., and Najork, M. Wit: Wikipedia-based image text dataset for

multimodal multilingual machine learning. In *Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval*, pp. 2443–2449, 2021.

- <span id="page-9-12"></span>Tack, J., Kim, J., Mitchell, E., Shin, J., Teh, Y. W., and Schwarz, J. R. Online adaptation of language models with a memory of amortized contexts. *arXiv preprint arXiv:2403.04317*, 2024.
- <span id="page-9-13"></span>Tiong, A. M. H., Li, J., Li, B., Savarese, S., and Hoi, S. C. Plug-and-play vqa: Zero-shot vqa by conjoining large pretrained models with zero training. *arXiv preprint arXiv:2210.08773*, 2022.
- <span id="page-9-15"></span>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. *Advances in neural information processing systems*, 30, 2017.
- <span id="page-9-4"></span>Vrandeciˇ c, D. and Kr ´ otzsch, M. Wikidata: a free collabo- ¨ rative knowledgebase. *Communications of the ACM*, 57 (10):78–85, 2014.
- <span id="page-9-11"></span>Wang, Y., Ma, D., and Cai, D. With greater text comes greater necessity: Inference-time training helps long text generation. *arXiv preprint arXiv:2401.11504*, 2024.
- <span id="page-9-7"></span>Xenos, A., Stafylakis, T., Patras, I., and Tzimiropoulos, G. A simple baseline for knowledge-based visual question answering. *arXiv preprint arXiv:2310.13570*, 2023.
- <span id="page-9-9"></span>Xu, F., Shi, W., and Choi, E. Recomp: Improving retrievalaugmented lms with context compression and selective augmentation. In *The Twelfth International Conference on Learning Representations*, 2024.
- <span id="page-9-5"></span>Yang, Z., Gan, Z., Wang, J., Hu, X., Lu, Y., Liu, Z., and Wang, L. An empirical study of gpt-3 for few-shot knowledge-based vqa. In *Proceedings of the AAAI conference on artificial intelligence*, volume 36, pp. 3081–3089, 2022.

# A. RAVQA-v2 reproduction

Since RAVQA-v2 does not investigate the effects of multimodal documents in its retrieval-agumented generation process, we replicate RAVQA-v2 using our crafted WIT knowledge source based on two types of MLLMs.

The experimental results are shown in Table [7.](#page-10-0) Specifically, we follow all the experiment setup mentioned by [Lin et al.](#page-8-15) [\(2024a\)](#page-8-15), such as the prompt template "Question: {} Knowledge: {} Answer:", where the Knowledge section includes both text tokens and image tokens transformed by Qformer. It is evident that the WIT knowledge source can, to some extent, provide the necessary knowledge for the OK-VQA dataset.

Additionally, we can observe that when using WIT as the knowledge source, the replication results of RAVQA-v2 across different MLLMs indicate that omitting the image information leads to better fine-tuning performance.

We believe this may be due to the following two reasons: (1) The inclusion of images directly increases the number of tokens in the knowledge section, making it more challenging for the MLLM to identify key information from the provided knowledge. (2) The use of images likely introduces redundant information that is unrelated to the document content, as images themselves contain a wealth of information. Furthermore, incorporating images also increases the time required for both training and inference based on RAVQA-v2.

Overall, although RAVQA-v2 theoretically supports multimodal document-based knowledge sources, experiments show that its performance with multimodal documents is not satisfactory.

<span id="page-10-0"></span>

| MLLM         | Knowledge Source | VQA Accuracy (%) |  |
|--------------|------------------|------------------|--|
| BLIP2        | No Knowledge     | 54.10            |  |
|              | WIT              | 56.26            |  |
|              | WIT (Text Only)  | 56.44            |  |
| InstructBLIP | No Knowledge     | 57.32            |  |
|              | WIT              | 58.77            |  |
|              | WIT (w/o Image)  | 59.12            |  |

Table 7. The performance of two types of MLLMs after finetuned on OK-VQA with the WIT knowledge source, based on the RAVQA-v2 framework.

# B. Complete Ablation Studies

We identify four key issues in training RACC and further propose four methods to solve the issues. Extensive ablation studies are conducted to verify the effectiveness the proposed four methods.

We conduct experiments on two types of MLLMs, *i.e.* BLIP2-FlanT5XL and InstructBLIP-FlanT5XL. Due to page constraints of the paper content, we only present the ablation results for InstructBLIP-FlanT5XL in Table [8.](#page-12-0) We adopt RACC-homo in the experiments of ablation studies, where the hyperMLLM and baseMLLM are both initialized from the models listed in the "MLLM" column of Table [8.](#page-12-0)

For RACC-homo based on BLIP2-FlanT5XL, comparing lines 2 and 3, as well as lines 7 and 8, we can observe that the PIPE method brings improvements of 0.28% and 0.44% under different settings. The PIPE method is also effective for the InstructBLIP-based RACC-homo, leading to performance gains of 0.31% and 0.42% as shown by lines 10 and 11, as well as lines 15 and 16.

From the differences between lines 3 and 4, as well as lines 11 and 12 in Table 5, we can observe the effectiveness of the DCSE method, bringing improvements of 0.55% and 0.77% respectively. The RGCA method, on the other hand, results in improvements of 0.28% and 0.37%, as shown in lines 3 and 5, as well as lines 11 and 13 in Table 5.

Last but not least, the performance differences between lines 6 and 8 and lines 14 and 16 in Table 5 show that the PRDB method leads to performance gains of 0.39% and 0.54% based on the two types of MLLMs.

We also plot the VQA accuracy on the validation set during the training process of the framework using different methods, as shown in Figure [2.](#page-11-0)

<span id="page-11-0"></span>![](_page_11_Figure_1.jpeg)

**Caption:** Figure 2 presents the VQA accuracy on the validation set during the training process of RACC using various methods. The results indicate performance improvements with the implementation of techniques such as Pseudo-Relevance-based Backpropagation Dropout (PRDB) and Decoupled Compression for Semantics Enhancement (DCSE), showcasing the effectiveness of the proposed framework in enhancing accuracy.

Figure 2. The VQA accuracy on the validation set during the training process of the framework using different methods.

# C. Details of Different Knowledge Sources

In this section, we further introduce more details of the two knowledge sources, *i.e.* GS and WIT.

First of all, the word count in knowledge source documents directly affects the efficiency during training and inference in RAG applications. Therefore, we first present some statistics of the document word counts for these two knowledge sources in Table [9.](#page-13-0)

We employ FLMR and PREFLMR to retrieve documents from these knowledge sources. FLMR has been trained on OK-VQA and GS, thus it has a strong capability to retrieve relevant documents from the GS knowledge source. PREFLMR is an upgraded version of FLMR, equipped with powerful capabilities for retrieving multimodal documents that FLMR lacks. The detailed retrieval results for OK-VQA with different knowledge sources are shown in Table [10.](#page-13-1)

Firstly, from Table [10,](#page-13-1) we can tell that there is obviously a number of irrelevant documents among the retrieved documents in different knowledge sources.

Second, Table [11](#page-13-2) shows that the quality of the GS knowledge base is superior to that of the WIT knowledge base. The PPRecall@1 metric for the GS knowledge base reaches 63.59%, indicating that the document with the highest confidence retrieved has a high probability of containing the answer to the question. The PRRecall@5 metric even reaches 88.32%. These data highlight the advantages of GS as a knowledge source specifically designed for OK-VQA.

# D. Investigation of hyperparameter K

The hyperparameter K represents the number of retrieved documents used for each image-question pair. The value of K is typically set to 5 in this paper, consistent with RAVQA-v2. We also investigated the impact of different values of K across various knowledge sources and the results are shown in Table [11.](#page-13-2)

On the GS knowledge base, when K = 1, RACC-homo achieved a performance of 59.65%, surpassing the 59.49% performance when K = 5. This may be due to the high quality of the GS knowledge base, where the first retrieved document is often likely to contain the information needed to answer the question. In such cases, setting K to a larger value might directly introduce documents with weaker relevance, thereby affecting the performance of the whole framework.

On the WIT knowledge source, we conduct experiments with and without images. In both settings, a larger value of K results in better performance. This may be because there are fewer documents related to the question in the WIT knowledge source, so a larger K allows RACC to extract useful information from more documents. Furthermore, for the same value of K, using images results in better performance in both settings, indicating that RACC can obtain valuable information from images. Compared to the results in Table [7,](#page-10-0) we can conclude that RACC has an advantage over RAVQA2 when utilizing multimodal document knowledge sources.

<span id="page-12-0"></span>Title Suppressed Due to Excessive Size

| MLLM                  | No. | PIPE | DCSE | RGCA | PRDB | VQA Accuracy (%) |
|-----------------------|-----|------|------|------|------|------------------|
| BLIP2-FlanT5XL        | 1   |      |      |      |      | 53.77            |
|                       | 2   |      |      |      | ✓    | 54.20(+0.43)     |
|                       | 3   | ✓    |      |      | ✓    | 54.48 (+0.71)    |
|                       | 4   | ✓    | ✓    |      | ✓    | 55.03 (+1.26)    |
|                       | 5   | ✓    |      | ✓    | ✓    | 54.76 (+0.99)    |
|                       | 6   | ✓    | ✓    | ✓    |      | 54.81 (+1.04)    |
|                       | 7   |      | ✓    | ✓    | ✓    | 54.76 (+0.99)    |
|                       | 8   | ✓    | ✓    | ✓    | ✓    | 55.20 (+1.43)    |
|                       | 9   |      |      |      |      | 57.60            |
| InstructBLIP-FlanT5XL | 10  |      |      |      | ✓    | 58.18 (+0.58)    |
|                       | 11  | ✓    |      |      | ✓    | 58.49 (+0.89)    |
|                       | 12  | ✓    | ✓    |      | ✓    | 59.26 (+1.66)    |
|                       | 13  | ✓    |      | ✓    | ✓    | 58.86 (+1.26)    |
|                       | 14  | ✓    | ✓    | ✓    |      | 58.95 (+1.35)    |
|                       | 15  |      | ✓    | ✓    | ✓    | 59.07 (+1.47)    |
|                       | 16  | ✓    | ✓    | ✓    | ✓    | 59.49 (+1.89)    |

Table 8. The results of ablation studies on the design of our proposed framework. The GS knowledge source is adopted here. The ablation experiments are conducted based on RACC-homo with the above two types of MLLMs.

Finally, we can conclude from Table [11](#page-13-2) that the RACC framework achieves excellent performance across various knowledge sources and different settings of K.

# E. Pseudo Code of RGCA method

Our proposed RGCA method is applied during the forward computation process of the cross-attention mechanism, aiming to ensure that compressed prompts corresponding to documents with higher retrieval scores receive more attention in the retrieval-guided cross-attention blocks. The pseudo code for the forward function of the retrieval-guided cross-attention block is as follows:

Listing 1 Pseudo code of RGCA

```
1 def forward(self, x, context, r_scores):
2 # projecting inputs into q, k, v
3 # transform q, k, v into multi-head forms
4 # expanding the retrieval scores to a certain shape
5 r_scores = repeat(r_scores, 'b x y -> (b h) (x m) (y n)', h=self.heads, m=x.shape
          [1], n=context.shape[1])
6
7 # calculating the similarities between queries and keys
8 sim = einsum('b i d, b j d -> b i j', q, k) * self.scale
9 sim = sim * r_scores
10 attn = sim.softmax(dim=-1)
11
12 # calculating the outputs
```
<span id="page-13-0"></span>

| Knowledge Source | Metric | Statistic |
|------------------|--------|-----------|
|                  | Mean   | 59.76     |
|                  | Std    | 46.05     |
|                  | Min    | 10        |
| GS               | 25%    | 29.0      |
|                  | 50%    | 45.0      |
|                  | 75%    | 75.0      |
|                  | Max    | 521       |
|                  | Mean   | 155.71    |
|                  | Std    | 78.86     |
|                  | Min    | 24        |
| WIT              | 25%    | 89        |
|                  | 50%    | 149       |
|                  | 75%    | 214       |
|                  | Max    | 1122      |

Table 9. The statistical data on document word counts of different knowledge sources. The "25%" column indicates that 25% of the documents have a word count below this value, and similarly for other percentiles.

<span id="page-13-1"></span>

|                  |           |    | PRRecall@K (%) |       |
|------------------|-----------|----|----------------|-------|
| Knowledge Source | Retriever | K  | Train          | Val   |
|                  | FLMR      | 1  | 71.56          | 63.59 |
|                  |           | 2  | 83.77          | 77.07 |
| GS               |           | 3  | 88.63          | 82.85 |
|                  |           | 4  | 91.03          | 86.41 |
|                  |           | 5  | 92.82          | 88.32 |
|                  |           | 10 | 95.99          | 93.63 |
|                  | PREFLER   | 1  | 37.09          | 35.85 |
|                  |           | 2  | 48.26          | 46.76 |
|                  |           | 3  | 54.89          | 53.27 |
| WIT              |           | 4  | 59.37          | 57.67 |
|                  |           | 5  | 62.89          | 60.93 |
|                  |           | 10 | 72.16          | 70.41 |

Table 10. The detailed retrieval results of different knowledge sources. PRRecall@K measures whether the retrieved K documents contain at least one pseudo-relevant document..

<span id="page-13-2"></span>

| knowledge source | K | PRRecall@K (%) | VQA Accuracy (%) |
|------------------|---|----------------|------------------|
| WIT              | 1 | 35.85          | 59.11            |
|                  | 3 | 53.27          | 59.16            |
|                  | 5 | 60.93          | 59.17            |
| WIT (Text Only)  | 1 | 35.85          | 58.97            |
|                  | 3 | 53.27          | 59.01            |
|                  | 5 | 60.93          | 59.04            |
| GS               | 1 | 63.59          | 59.65            |
|                  | 3 | 82.85          | 59.35            |
|                  | 5 | 88.32          | 59.49            |

Table 11. RACC-homo's experimental results with varying hyperparameter K across different knowledge sources. The MLLM used here is InstructBLIP-FlanT5XL.