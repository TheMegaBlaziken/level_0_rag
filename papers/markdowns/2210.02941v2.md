# Boosting Text Augmentation via Hybrid Instance Filtering Framework

Heng Yang, Ke Li<sup>∗</sup>

Department of Computer Science, University of Exeter, EX4 4QF, Exeter, UK {hy345, k.li}@exeter.ac.uk

### Abstract

Text augmentation is an effective technique for addressing the problem of insufficient data in natural language processing. However, existing text augmentation methods tend to focus on few-shot scenarios and usually perform poorly on large public datasets. Our research indicates that existing augmentation methods often generate instances with shifted feature spaces, which leads to a drop in performance on the augmented data (for example, EDA generally loses ≈ 2% in aspect-based sentiment classification). To address this problem, we propose a hybrid instance-filtering framework (BO O S TAU G) based on pre-trained language models that can maintain a similar feature space with natural datasets. BO O S TAU G is transferable to existing text augmentation methods (such as synonym substitution and back translation) and significantly improves the augmentation performance by ≈ 2 − 3% in classification accuracy. Our experimental results on three classification tasks and nine public datasets show that BO O S TAU G addresses the performance drop problem and outperforms state-ofthe-art text augmentation methods. Additionally, we release the code to help improve existing augmentation methods on large datasets.

# 1 Introduction

Recent pre-trained language models (PLMs) [\(De](#page-9-0)[vlin et al.,](#page-9-0) [2019;](#page-9-0) [Brown et al.,](#page-9-1) [2020;](#page-9-1) [He et al.,](#page-9-2) [2021;](#page-9-2) [Yoo et al.,](#page-11-0) [2021\)](#page-11-0) have been able to learn from large amounts of text data. However, this also leads to a critical problem of data insufficiency in many lowresource fine-tuning scenarios [\(Chen et al.,](#page-9-3) [2020;](#page-9-3) [Zhou et al.,](#page-12-0) [2022a;](#page-12-0) [Miao et al.,](#page-10-0) [2021;](#page-10-0) [Kim et al.,](#page-10-1) [2022;](#page-10-1) [Wang et al.,](#page-11-1) [2022b;](#page-11-1) [Yang et al.,](#page-11-2) [2022\)](#page-11-2). Despite this, existing augmentation studies still encounter failures on large public datasets. While some studies[\(Ng et al.,](#page-10-2) [2020;](#page-10-2) [Body et al.,](#page-9-4) [2021;](#page-9-4) [Chang et al.,](#page-9-5) [2021;](#page-9-5) [Luo et al.,](#page-10-3) [2021\)](#page-10-3) have attempted

![](_page_0_Figure_8.jpeg)

**Caption:** Figure 1 illustrates the feature space shift of the Laptop14 dataset using t-SNE visualization. The shift metric S quantifies the difference between augmented and natural instances across three augmentation methods: BoostAug, MonoAug, and EDA. Notably, BoostAug exhibits the least feature space shift, indicating its effectiveness in maintaining data integrity during augmentation.

Figure 1: The visualization of feature space shift of the Laptop14 dataset based on t-SNE. We calculate the shift metric S of feature space between augmented and natural instances. The augmentation methods are BoostAug, MonoAug, and EDA augmentation, respectively. Our BoostAug has the least feature space shift.

to leverage the language modeling capabilities of PLMs in text augmentation, these methods still suffer from performance drops on large datasets.

To explore the root cause of this failure mode, we conducted experiments to explain the difference between "good" and "bad" augmentation instances. Our study found that existing augmentation methods [\(Wei and Zou,](#page-11-3) [2019;](#page-11-3) [Coulombe,](#page-9-6) [2018;](#page-9-6) [Li et al.,](#page-10-4) [2019;](#page-10-4) [Kumar et al.,](#page-10-5) [2019;](#page-10-5) [Ng et al.,](#page-10-2) [2020\)](#page-10-2) usually fail to maintain the feature space in augmentation instances, which leads to bad instances. This shift in feature space occurs in both edit-based and PLM-based augmentation methods. For example, edit-based methods can introduce breaking changes that corrupt the meaning of the text, while PLM-based methods can introduce outof-vocabulary words. In particular, for the edit-

<sup>∗</sup>Corresponding author

based methods, the shifted feature space mainly comes from breaking text transformations, such as changing important words (e.g., 'but ' ) in sentiment analysis. As for PLM-based methods, they usually introduce out-of-vocabulary words due to word substitution and insertion, which leads to an adverse meaning in sentiment analysis tasks.

To address the performance drop in existing augmentation methods caused by shifted feature space, we propose a hybrid instance-filtering framework (BO O S TAU G) based on PLMs to guide augmentation instance generation. Unlike other existing methods [\(Kumar et al.,](#page-10-6) [2020\)](#page-10-6), we use PLMs as a powerful instance filter to maintain the feature space, rather than as an augmentor. This is based on our finding that PLMs fine-tuned on natural datasets are familiar with the identical feature space distribution. The proposed framework consists of four instance filtering strategies: perplexity filtering, confidence ranking, predicted label constraint, and a cross-boosting strategy. These strategies are discussed in more detail in section Section [2.3.](#page-3-0) Compared to prominent studies, BO O S TAU G is a pure instance-filtering framework that can improve the performance of existing text augmentation methods by maintaining the feature space.

With the mitigation of feature space shift, BO O S TAU G can generate more valid augmentation instances and improve existing augmentation methods' performance, which more augmentation instances generally trigger performance sacrifice in other studies [\(Coulombe,](#page-9-6) [2018;](#page-9-6) [Wei and Zou,](#page-11-3) [2019;](#page-11-3) [Li et al.,](#page-10-4) [2019;](#page-10-4) [Kumar et al.,](#page-10-6) [2020\)](#page-10-6)). According to our experimental results on three finegrained and coarse-grained text classification tasks, BO O S TAU G[1](#page-1-0) significantly alleviates feature space shifts for existing augmentation methods.

Our main contributions are:

- We propose the feature space shift to explain the performance drop in existing text augmentation methods, which is ubiquitous in full dataset augmentation scenarios.
- We propose a universal augmentation instance filter framework to mitigate feature space shift and significantly improve the performance on the ABSC and TC tasks.
- Our experiments show that the existing text augmentation methods can be easily improved by employing BO O S TAU G.

#### Algorithm 1: The pseudo code of BO O S TAU G <sup>1</sup> Split D into k folds, D := {F<sup>i</sup> } k <sup>i</sup>=1; <sup>2</sup> Daug := ∅; <sup>3</sup> for i ← 1 to k do <sup>4</sup> D i aug := ∅, D i boost := F i ; <sup>5</sup> Randomly pick up k − 2 folds except F i to constitute D i train; <sup>6</sup> D i valid := F \ (F <sup>i</sup> S D i train); <sup>7</sup> Use the DeBERTa on D i train and D i valid to build the surrogate language model; <sup>8</sup> forall dorg ∈ D<sup>i</sup> boost do <sup>9</sup> D i aug := F(d i org, N, ˜ Θ); <sup>10</sup> forall daug ∈ D<sup>i</sup> aug do <sup>11</sup> Use the surrogate language model to predict P(daug), C(daug), and the ˜ℓaug of daug; <sup>12</sup> if P(daug) ≥ α ∥ C(daug) ≤ β ∥ ˜ℓ<sup>d</sup>aug ̸= ˜ℓ<sup>d</sup>org then <sup>13</sup> D i aug := D i aug \ {daug}; <sup>14</sup> Daug := Daug S D i aug; <sup>15</sup> Daug := Daug S D i boost; <sup>16</sup> return Daug

# <span id="page-1-1"></span>2 Proposed Method

The workflow of BO O S TAU G is shown in Figure [2](#page-2-0) and the pseudo code is given in Algorithm [1.](#page-1-1) Different from most existing studies, which focus on unsupervised instance generation, BO O S TAU G serves as an instance filter to improve existing augmentation methods. The framework consists of two main phases: 1) Phase #1: the training of surrogate language models; 2) Phase #2: surrogate language models guided augmentation instance filtering. The following paragraphs will provide a detailed explanation of each step of the implementation.

# <span id="page-1-3"></span>2.1 Surrogate Language Model Training

At the beginning of Phase #1, the original training dataset is divided into k ≥ 3 folds where the k−2 ones are used for training (denoted as the training fold) while the other two are used for the validation and augmentation purposes, denoted as the validation and boosting fold, respectively[2](#page-1-2) (lines 4- 6). Note that the generated augmentation instances, which will be introduced in Section [2.2,](#page-2-1) can be

<span id="page-1-0"></span><sup>1</sup>We release the source code and experiment scripts of BO O S TAU G at: [https://github.com/yangheng95/](https://github.com/yangheng95/BoostTextAugmentation) [BoostTextAugmentation](https://github.com/yangheng95/BoostTextAugmentation).

<span id="page-1-2"></span><sup>2</sup>We iteratively select the i-th fold, i ∈ 1, · · · , k, as the boosting fold (line 3 in Algorithm [1\)](#page-1-1). The validation fold is used to select the best checkpoint of the surrogate language model to filter the augmented instances. This process is repeated k times to ensure that all the folds have been used for validation and boosting at least once, thus avoiding data overlapping between the training and validation folds.

<span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)

**Caption:** Figure 2 depicts the workflow of the BO O S TAU G framework, divided into two phases. Phase #1 involves training a DeBERTa-based surrogate language model, while Phase #2 focuses on filtering low-quality augmentation instances generated by various backends. The k-fold cross-boosting strategy ensures robust instance filtering and minimizes feature space shifts.

Figure 2: The workflow of BO O S TAU G can be divided into two phases: Phase #1 and Phase #2. In Phase #1, we fine-tune a DeBERTa-based classification model using re-split training and validation sets and extract the fine-tuned DeBERTa to build a surrogate language model. In Phase #2, BO O S TAU G employs a text augmentation backend to generate raw augmentations and filters out low-quality instances identified by the surrogate language model. To avoid data overlapping between the training folds and validation fold, BO O S TAU G performs k-fold cross-boosting, meaning that Phase #1 and #2 are repeated k times.

identical to the instances in training folds the surrogate language model. This data overlapping problem will lead to a shifted feature space. We argue that the proposed k-fold augmentation approach, a.k.a. "cross-boosting", can alleviate the feature space shift of the augmentation instances, which will be validated and discussed in detail in Section [4.3.](#page-6-0) The main crux of Phase #1 is to build a surrogate language model as a filter to guide the elimination of harmful and poor augmentation instances.

We construct a temporary classification model using the DeBERTa [\(He et al.,](#page-9-2) [2021\)](#page-9-2) architecture. This model is then fine-tuned using the data in the k − 2 training folds and the validation fold to capture the semantic features present in the data (line 7). It is important to note that we do not use the original training dataset for this fine-tuning process. Once the fine-tuning is complete, the language model constructed from the DeBERTa classification model is then utilized as the surrogate language model in the instance filtering step in Phase #2 of BO O S TAU G.

This is different from existing works that use a pre-trained language model to directly generate augmentation instances. We clarify our motivation for this from the following two aspects.

• In addition to modeling the semantic feature, the surrogate language model can provide more information that can be useful for the quality control of the augmentation instances, such as text perplexity, classification confidence, and predicted label.

• Compared to the instance generation, we argue that the instance filtering approach can be readily integrated with any existing text augmentation approach.

#### <span id="page-2-1"></span>2.2 Augmentation Instance Generation

As a building block of Phase #2, we apply some prevalent data augmentation approaches as the back end to generate the augmentation instances in BO O S TAU G (line 9). More specifically, let Dorg := {d i org} N <sup>i</sup>=1 be the original training dataset. d i org := ⟨s i , ℓi ⟩ is a data instance where s i indicates a sentence and ℓ i is the corresponding label, i ∈ 1, · · · , N. By applying the transformation function F(·, ·, ·) upon d i org as follows, we expect to obtain a set of augmentation instances D<sup>i</sup> aug for d i org:

$$
\mathcal{D}_{\text{aug}}^i := F(d_{\text{org}}^i, \tilde{N}, \Theta), \tag{1}
$$

where N˜ ≥ 1 is used to control the maximum number of generated augmentation instances. In the end, the final augmentation set is constituted as Daug := S<sup>N</sup> <sup>i</sup>=1 D<sup>i</sup> aug (line 14). Note that depending on the specific augmentation back end, there can be more than one strategy to constitute the transformation function. For example, EDA [\(Wei and](#page-11-3) [Zou,](#page-11-3) [2019\)](#page-11-3) has four transformation strategies, including synonym replacement, random insertion, random swap, and random deletion. Θ consists

of the parameters associated with the transformation strategies of the augmentation back end, e.g., the percentage of words to be modified and the mutation probability of a word.

#### <span id="page-3-0"></span>2.3 Instance Filtering

Our preliminary experiments have shown that merely using data augmentation can be detrimental to the modeling performance, no matter how many augmentation instances are applied in the training process. In addition, our experiments in Section [4.3](#page-6-0) have shown a surprising feature space shift between the original data and the augmented instances in the feature space. To mitigate this issue, BO O S TAU G proposes an instance filtering approach to control the quality of the augmentation instances. It consists of three filtering strategies, including perplexity filtering, confidence ranking, and predicted label constraint, which will be delineated in the following paragraphs, respectively. Note that all these filtering strategies are built on the surrogate language model developed in Phase #1 of BO O S TAU G (lines 12 and 13).

#### 2.3.1 Perplexity Filtering

Text perplexity is a widely used metric to evaluate the modeling capability of a language model [\(Chen](#page-9-7) [and Goodman,](#page-9-7) [1999;](#page-9-7) [Sennrich,](#page-11-4) [2012\)](#page-11-4). Our preliminary experiments have shown that low-quality instances have a relatively high perplexity. This indicates that perplexity information can be used to evaluate the quality of an augmentation instance. Since the surrogate language model built in Phase #1 is bidirectional, the text perplexity of an augmentation instance daug is calculated as:

$$
\mathbb{P}(d_{\text{aug}}) = \prod_{i=1}^{s} p(w_i \mid w_1, \cdots, w_{i-1}, w_{i+1}, \cdots, w_s),
$$
\n(2)

where w<sup>i</sup> represents the token in the context. s is the number of tokens in daug and p (w<sup>i</sup> | w1, · · · , wi−1, wi+1, · · · , ws) is the probability of w<sup>i</sup> conditioned on the preceding tokens, according to the surrogate language model, i ∈ 1, · · · , s. Note that daug is treated as a lowquality instance and is discarded if P(daug) ≥ α while α ≥ 0 is a predefined threshold.

#### 2.3.2 Confidence Ranking

We observe a significant feature space shift in the augmentation instances. These instances will be allocated with low confidences by the surrogate language model. In this case, we can leverage the classification confidence as a driver to control the quality of the augmentation instances. However, it is natural that long texts can have way more augmentation instances than short texts, thus leading to the so-called unbalanced distribution. Besides, the confidence of most augmentation instances is ≥ 95%, which is not selective as the criterion for instance filtering. To mitigate the unbalanced distribution in augmentation instances and make use of confidence, we develop a confidence ranking strategy to eliminate the redundant augmentation instances generated from long texts while retaining the rare instances having a relatively low confidence. More specifically, we apply a softmax operation on the output hidden state learned by the surrogate language model, denoted as H(daug), to evaluate the confidence of daug as:

$$
\mathbb{C}(d_{\text{aug}}) = \operatorname{argmax}\left(\frac{\exp(\mathbb{H}_{d\text{aug}})}{\sum_{1}^{c} \exp(\mathbb{H}_{d\text{aug}})}\right), \quad (3)
$$

where c is the number of classes in the original training dataset. To conduct the confidence ranking, 2 × N˜ instances are generated at first, while only the top N˜ instances are selected to carry out the confidence ranking. By doing so, we expect to obtain a balanced augmentation dataset even when there is a large variance in the confidence predicted by the surrogate language model. After the confidence ranking, the augmentation instances with Cdaug ≤ β are discarded while β ≥ 0 is a fixed threshold.

#### 2.3.3 Predicted Label Constraint

Due to some breaking text transformation, text augmentation can lead to noisy data, e.g., changing a word "greatest" to "worst" in a sentence leads to an adverse label in a sentiment analysis task. Since the surrogate language model can predict the label of an augmentation instance based on its confidence distribution, we develop another filtering strategy that eliminates the augmentation instances whose predicted label ˜ℓdaug is different from the ground truth. By doing so, we expect to mitigate the feature space bias.

#### 2.4 Feature Space Shift Metric

To quantify the shift of the feature space, we propose an ensemble metric based on the overlapping ratio and distribution skewness of the t-SNE-based augmented instances' feature space.

The feature space overlapping ratio measures the diversity of the augmented instances. A larger overlapping ratio indicates that more natural instances have corresponding augmented instances. On the other hand, the distribution skewness measure describes the uniformity of the distribution of the augmented instances. A smaller distribution skewness indicates that the natural instances have approximately equal numbers of corresponding augmented instances. To calculate the feature space shift, we first calculate the overlapping ratio and distribution skewness of the natural instances and their corresponding augmented instances. The feature space shift is calculated as follows:

$$
S = 1 - \mathcal{O} + sk,\tag{4}
$$

where O and sk are the feature space convex hull overlapping ratio and feature space distribution skewness, which will be introduced in the following subsections.

#### 2.4.1 Convex hull overlapping calculation

To calculate the convex hull overlapping rate, we use the Graham Scan algorithm[3](#page-4-0) [\(Graham,](#page-9-8) [1972\)](#page-9-8) to find the convex hulls for the test set and target dataset in the t-SNE visualization, respectively.

Let P<sup>1</sup> and P<sup>2</sup> represent the convex hulls of two datasets in the t-SNE visualization; we calculate the overlapping rate as follows:

$$
\mathcal{O} = \frac{\mathcal{P}_1 \cap \mathcal{P}_2}{\mathcal{P}_1 \cup \mathcal{P}_2},\tag{5}
$$

where ∩ and ∪ denote convex hull intersection and union operations, respectively. O is the overlapping rate between P<sup>1</sup> and P2.

#### 2.4.2 Distribution skewness calculation

The skewness of an example distribution is computed as follows:

$$
sk = \frac{m_3}{m_2^{3/2}},
$$
 (6)

$$
m_i = \frac{1}{N} \sum_{n=1}^{N} (x_n - \bar{x})^i,
$$
 (7)

where N is the number of instances in the distribution; sk is the skewness of an example distribution. m<sup>i</sup> and x¯ are the i-th central moment and mean of the example distribution, respectively. Because the t-SNE has two dimensions (namely x and y

axes), we measure the global skewness of the target dataset (e.g., training set, augmentation set) by summarizing the absolute value of skewness on the x and y axes in t-SNE:

$$
sk^g = |sk^x| + |sk^y|,\tag{8}
$$

where sk<sup>g</sup> is the global skewness of the target dataset; sk<sup>x</sup> and sk<sup>y</sup> are the skewness on the x and y axes, respectively.

By combining the convex hull overlapping ratio and distribution skewness, the proposed feature space shift metric offers a comprehensive view of how well the augmented instances align with the original data distribution. This metric can be used to evaluate the effectiveness of different data augmentation approaches, as well as to inform the fine-tuning process for better model performance.

# 3 Experimental Setup

# 3.1 Datasets

Our experiments are conducted on three classification tasks: the sentence-level text classification (TC), the aspect-based sentiment classification (ABSC), and natural language inference (NLI). The datasets used for the TC task include SST2, SST5 [\(Socher et al.,](#page-11-5) [2013\)](#page-11-5) from the Stanford Sentiment Treebank, and AGNews10K[4](#page-4-1) [\(Zhang et al.,](#page-12-1) [2015\)](#page-12-1). Meanwhile, the datasets used for the ABSC task are Laptop14, Restaurant14[\(Pontiki et al.,](#page-11-6) [2014\)](#page-11-6), Restaurant15 [\(Pontiki et al.,](#page-10-7) [2015\)](#page-10-7), Restaurant16 [\(Pontiki et al.,](#page-10-8) [2016\)](#page-10-8), and MAMS [\(Jiang](#page-9-9) [et al.,](#page-9-9) [2019\)](#page-9-9). The datasets[5](#page-4-2) used for the NLI task are the SNLI [\(Bowman et al.,](#page-9-10) [2015\)](#page-9-10) and MNLI [\(Williams et al.,](#page-11-7) [2018\)](#page-11-7) datasets, respectively. The split of these datasets is summarized in Table [1.](#page-5-0) The commonly used Accuracy (i.e., Acc) and macro F1 are used as the metrics for evaluating the performance of different algorithms following existing research [\(Wang et al.,](#page-11-8) [2016;](#page-11-8) ?). Additionally, all experiments are repeated five times with different random seeds. Detailed information on the hyper-parameter settings and sensitivity tests of α and β can be found in Appendix [A.](#page-12-2)

<span id="page-4-0"></span><sup>3</sup><https://github.com/shapely/shapely>.

<span id="page-4-1"></span><sup>4</sup>We use the first 10, 000 examples to build the AG-News10K dataset (7, 000 for training, 1, 000 for validation, and 2, 000 for testing), which is large enough compared to other datasets.

<span id="page-4-2"></span><sup>5</sup>We select the first 1000 training examples as the training set and keep the original validation/testing sets for experimental efficiency.

| Dataset      | Training Set | Validation Set | Testing Set |
|--------------|--------------|----------------|-------------|
| Laptop14     | 2328         | 0              | 638         |
| Restaurant14 | 3608         | 0              | 1120        |
| Restaurant15 | 1120         | 0              | 540         |
| Restaurant16 | 1746         | 0              | 615         |
| MAMS         | 11186        | 1332           | 1336        |
| SST2         | 6920         | 872            | 1821        |
| SST5         | 8544         | 1101           | 2210        |
| AGNews10K    | 7000         | 1000           | 2000        |
| SNLI         | 1000         | 10000          | 10000       |
| MNLI         | 1000         | 20000          | 0           |

<span id="page-5-0"></span>Table 1: The summary of experimental datasets for the text classification, aspect-based sentiment analysis and natural language inference tasks.

#### 3.2 Augment Backends

We use BO O S TAU G to improve five state-of-theart baseline text augmentation methods, all of which are used as the text augmentation backend of BO O S TAU G. Please find the introductions of these baselines in Appendix [B](#page-12-3) and refer to Table [6](#page-14-0) for detailed performance of BO O S TAU G based on different backends.

We also compare BO O S TAU G enhanced EDA with the following text augmentation methods:

- EDA (TextAttack[6](#page-5-1) ) [\(Wei and Zou,](#page-11-3) [2019\)](#page-11-3) performs text augmentation via random word insertions, substitutions, and deletions.
- SynonymAug (NLPAug[7](#page-5-2) ) [\(Niu and Bansal,](#page-10-9) [2018\)](#page-10-9) replaces words in the original text with their synonyms. This method has been shown to be effective in improving the robustness of models on certain tasks.
- TAA [\(Ren et al.,](#page-11-9) [2021\)](#page-11-9) is a Bayesian optimizationbased text augmentation method. It searches augmentation policies and automatically finds the best augmentation instances.
- AEDA [\(Karimi et al.,](#page-10-10) [2021\)](#page-10-10) is based on the EDA, which attempts to maintain the order of the words while changing their positions in the context. Besides, it alleviates breaking changes such as critical deletions and improves the robustness.
- AMDA [\(Si et al.,](#page-11-10) [2021\)](#page-11-10) linearly interpolates the representations of pairs of training instances, which has a diversified augmentation set compared to discrete text adversarial augmentation.

In our experiments, LSTM, BERT-BASE[\(Devlin](#page-9-0) [et al.,](#page-9-0) [2019\)](#page-9-0), and DeBERTa-BASE[\(He et al.,](#page-9-2) [2021\)](#page-9-2) are used as the objective models for the TC task. FastLCF is an objective model available for the

ABSC task.

# 4 Experimental Results

### 4.1 Main Results

From the results shown in Table [2,](#page-6-1) it is clear that BO O S TAU G consistently improves the performance of the text augmentation method EDA across all datasets and models. It is also worth noting that some traditional text augmentation methods can actually harm the performance of the classification models. Additionally, the performance improvement is relatively small for larger datasets like SST-2, SST-5, and MAMS. Furthermore, the performance of LSTM is more affected by text augmentation, as it lacks the knowledge gained from the large-scale corpus that is available in PLMs.

When comparing the different text augmentation methods, it is apparent that EDA performs the best, despite being the simplest method. On the other hand, SplitAug performs the worst for LSTM because its augmentation instances are heavily biased in the feature space due to the word splitting transformation. The performance of SpellingAug is similar to EDA. This can be attributed to the fact that PLMs have already captured some common misspellings during pretraining. Additionally, PLM-based augmentation methods like WordsEmbsAug tend to generate instances with unknown words, further exacerbating the feature space shift of the augmented texts.

We also compare the performance of BO O S TAU G with several state-of-the-art text augmentation methods. The results of these comparisons can be found in Table [3.](#page-6-2) From the results, it can be seen that even when using EDA [\(Wei and Zou,](#page-11-3) [2019\)](#page-11-3) as the backend, BO O S TAU G outperforms other state-of-the-art methods such as AEDA [\(Karimi et al.,](#page-10-10) [2021\)](#page-10-10), AMDA [\(Si et al.,](#page-11-10) [2021\)](#page-11-10), and Bayesian optimization-based TAA [\(Ren et al.,](#page-11-9) [2021\)](#page-11-9) on the full SST2 dataset.

### 4.2 Ablation Study

To gain a deeper understanding of the working mechanism of BO O S TAU G, we conduct experiments to evaluate the effectiveness of crossboosting, predicted label constraint, confidence ranking, and perplexity filtering. The results, which can be found in Table [4,](#page-7-0) show that the performance of the variant MonoAug is significantly lower than that of BO O S TAU G. This is because MonoAug trains the surrogate language model using the entire

<span id="page-5-1"></span><sup>6</sup><https://github.com/QData/TextAttack>

<span id="page-5-2"></span><sup>7</sup><https://github.com/makcedward/nlpaug>

<span id="page-6-1"></span>Table 2: The performance comparison between BO O S TAU G-enhanced EDA and baseline augmentation methods. The best and second-best metric values are highlighted in bold and underlined faces, respectively. None indicates the vanilla version without using text augmentation. † indicates that BO O S TAU G is significantly better than the backend according to the Wilcoxon's rank sum test at a 0.05 significance level. "-" indicates that FastLCF is not available for text classification or the results are not considered due to resource limitation.

| Augmentation<br>Model |                                                                               |       | Laptop14<br>Restaurant14                                                   |       | Restaurant15<br>Restaurant16 |       |       | MAMS  |       | SST2  |       | SST5  |                                         | AGNews10K |    |                   |    |
|-----------------------|-------------------------------------------------------------------------------|-------|----------------------------------------------------------------------------|-------|------------------------------|-------|-------|-------|-------|-------|-------|-------|-----------------------------------------|-----------|----|-------------------|----|
|                       |                                                                               | Acc   | F1                                                                         | Acc   | F1                           | Acc   | F1    | Acc   | F1    | Acc   | F1    | Acc   | F1                                      | Acc       | F1 | Acc               | F1 |
|                       | LSTM                                                                          | 71.32 | 65.45                                                                      | 77.54 | 66.89                        | 78.61 | 58.54 | 87.40 | 64.41 | 56.96 | 56.18 | 84.36 | 84.36                                   | 45.29     |    | 44.61 87.60 87.36 |    |
|                       | BERT                                                                          | 79.47 | 75.70                                                                      | 85.18 | 78.31                        | 83.61 | 69.73 | 91.3  | 77.16 | 82.78 | 82.04 | 90.88 | 90.88                                   | 53.53     |    | 52.06 92.47 92.26 |    |
| None                  | DeBERTa 83.31                                                                 |       | 80.02                                                                      | 87.72 | 81.73                        | 86.58 | 74.22 | 93.01 | 81.93 | 83.31 | 82.87 | 95.07 | 95.07                                   | 56.47     |    | 55.58 92.30 92.13 |    |
|                       | FastLCF                                                                       | 83.23 | 79.68                                                                      | 88.5  | 82.7                         | 87.74 | 73.69 | 93.69 | 81.66 | 83.46 | 82.88 | -     | -                                       | -         | -  | -                 | -  |
|                       | LSTM                                                                          | 68.65 | 62.09                                                                      | 76.18 | 62.41                        | 76.30 | 56.88 | 85.59 | 61.78 | 56.59 | 55.33 | 84.79 | 84.79                                   | 43.85     |    | 43.85 87.72 87.46 |    |
|                       | BERT                                                                          | 78.37 | 74.23                                                                      | 83.75 | 75.38                        | 81.85 | 65.63 | 91.38 | 77.27 | 81.81 | 81.10 | 91.16 | 91.16                                   | 51.58     |    | 50.49 92.50 92.28 |    |
| EDA                   | DeBERTa 80.96                                                                 |       | 78.65                                                                      | 86.79 | 79.82                        | 84.44 | 70.40 | 93.01 | 77.59 | 81.96 | 81.96 | 94.07 | 94.07                                   | 56.43     |    | 53.88 92.55 92.33 |    |
|                       | FastLCF                                                                       | 81.97 | 79.57                                                                      | 87.68 | 81.52                        | 86.39 | 72.51 | 93.17 | 78.96 | 82.19 | 81.63 | -     | -                                       | -         | -  | -                 | -  |
|                       | LSTM                                                                          | 67.24 | 60.30                                                                      | 75.36 | 63.01                        | 73.52 | 49.04 | 84.72 | 53.92 | 55.99 | 55.16 | 83.14 | 83.14                                   | 41.45     |    | 40.40 87.25 86.96 |    |
|                       | BERT                                                                          | 73.59 | 69.11                                                                      | 82.54 | 73.18                        | 79.63 | 62.32 | 89.76 | 74.74 | 81.89 | 81.42 | 91.00 | 91.00                                   | 52.26     |    | 50.90 92.42 92.22 |    |
| SpellingAug           | DeBERTa 80.17                                                                 |       | 76.01                                                                      | 85.13 | 76.67                        | 85.83 | 71.54 | 92.76 | 78.33 | 81.89 | 81.24 | 93.68 | 93.68                                   | 55.95     |    | 53.78 92.68 92.50 |    |
|                       | FastLCF                                                                       | 79.62 | 74.81                                                                      | 86.03 | 78.73                        | 87.41 | 75.14 | 92.60 | 75.27 | 82.19 | 81.66 | -     | -                                       | -         | -  | -                 | -  |
|                       | LSTM                                                                          | 62.98 | 56.53                                                                      | 73.43 | 58.57                        | 70.19 | 45.71 | 83.93 | 54.41 | 56.74 | 55.34 | 84.29 | 84.29                                   | 44.00     |    | 42.10 87.23 87.01 |    |
|                       | BERT                                                                          | 75.47 | 70.56                                                                      | 82.86 | 74.48                        | 82.87 | 65.19 | 90.98 | 77.51 | 81.74 | 81.35 | 90.88 | 90.88                                   | 51.99     |    | 50.95 92.45 92.16 |    |
| SplitAug              | DeBERTa 79.15                                                                 |       | 75.72                                                                      | 86.03 | 79.28                        | 85.46 | 70.43 | 92.76 | 79.79 | 81.59 | 81.09 | 94.29 | 94.29                                   | 55.51     |    | 49.77 92.52 92.29 |    |
|                       | FastLCF                                                                       | 81.82 | 78.46                                                                      | 86.34 | 78.36                        | 86.67 | 70.87 | 93.09 | 76.50 | 82.07 | 81.53 | -     | -                                       | -         | -  | -                 | -  |
|                       | LSTM                                                                          | 67.40 | 61.57                                                                      | 75.62 | 62.13                        | 74.44 | 51.67 | 84.98 | 58.67 | 56.06 | 55.10 | 83.14 | 83.14                                   | 44.07     |    | 42.03 87.53 87.24 |    |
|                       | BERT                                                                          | 75.63 | 70.79                                                                      | 83.26 | 75.11                        | 78.61 | 61.48 | 90.24 | 72.37 | 81.29 | 80.50 | 91.02 | 91.02                                   | 51.27     |    | 50.27 92.10 91.86 |    |
| ContextualWordEmbsAug | DeBERTa 76.88                                                                 |       | 71.98                                                                      | 85.49 | 77.22                        | 84.63 | 70.50 | 92.28 | 77.42 | 81.66 | 81.32 | 94.12 | 94.12                                   | 55.48     |    | 53.60 92.80 92.62 |    |
|                       | FastLCF                                                                       | 79.08 | 74.61                                                                      | 85.62 | 76.88                        | 84.91 | 70.06 | 91.38 | 76.27 | 81.89 | 81.09 | -     | -                                       | -         | -  | -                 | -  |
|                       | LSTM                                                                          | 68.50 | 62.22                                                                      | 78.12 | 66.70                        | 78.85 | 59.08 | 86.97 | 63.47 | -     | -     | -     | -                                       | -         | -  | -                 | -  |
|                       | BERT                                                                          | 79.94 | 76.19                                                                      | 85.54 | 78.51                        | 84.42 | 72.05 | 92.02 | 85.78 | -     | -     | -     | -                                       | -         | -  | -                 | -  |
| BackTranslationAug    | DeBERTa 84.17                                                                 |       | 81.15                                                                      | 88.93 | 83.54                        | 89.42 | 78.67 | 93.97 | 80.52 | -     | -     | -     | -                                       | -         | -  | -                 | -  |
|                       | FastLCF                                                                       | 82.76 | 79.82                                                                      | 89.46 | 84.94                        | 88.13 | 75.70 | 94.14 | 81.82 | -     | -     | -     | -                                       | -         | -  | -                 | -  |
|                       |                                                                               |       | LSTM 73.20† 67.46† 79.12† 68.07† 80.06† 59.61† 87.80† 65.33† 59.21† 59.58† |       |                              |       |       |       |       |       |       |       | 85.83† 85.83† 45.93† 43.59† 88.45 88.16 |           |    |                   |    |
|                       | BERT                                                                          |       | 80.10† 76.48† 86.34† 79.99† 86.12† 73.79† 91.95† 79.12† 84.01† 83.44†      |       |                              |       |       |       |       |       |       |       | 92.33† 92.33† 53.94† 52.80† 92.48 92.25 |           |    |                   |    |
| BO O S TAU G (EDA)    | DeBERTa 84.56† 81.77† 89.02† 83.35† 88.33† 76.77† 93.58† 81.93† 84.51† 83.97† |       |                                                                            |       |                              |       |       |       |       |       |       |       | 96.09† 96.09† 57.78† 56.15† 92.95 92.76 |           |    |                   |    |
|                       | FastLCF 85.11† 82.18† 90.38† 85.04† 89.81† 77.92† 94.37† 82.67† 84.13† 82.97† |       |                                                                            |       |                              |       |       |       |       |       |       | -     | -                                       | -         | -  | -                 | -  |

<span id="page-6-2"></span>Table 3: The performance comparison on augmented SST2 dataset between different augmentation methods. We list the standard deviations for each method, while "-" indicates the standard deviation is not available. <sup>∗</sup> is derived from our experiments.

| Augmentation      | Model | Acc          | F1           |
|-------------------|-------|--------------|--------------|
| None∗             | BERT  | 90.88 (0.31) | 90.87 (0.31) |
| EDA∗              | BERT  | 90.99 (0.46) | 90.99 (0.46) |
| SynonymAug∗       | BERT  | 91.32 (0.55) | 91.31 (0.55) |
| TAA∗              | BERT  | 90.94 (0.31) | 90.94 (0.31) |
| AEDA              | BERT  | 91.76 ( - )  | —            |
| AMDA              | BERT  | 91.54 ( - )  | —            |
| BO O S TAU G(EDA) | BERT  | 92.33 (0.29) | 92.33 (0.29) |

training set, leading to a high degree of similarity between the original and augmentation instances. This data overlapping problem, as discussed in Section [2.1,](#page-1-3) results in biased instance filtering and overfitting of the instances to the training fold data distribution. Additionally, the variant without the perplexity filtering strategy performs the worst, indicating that the perplexity filtering strategy is crucial in removing instances with syntactical and grammatical errors. The performance of the variants without the predicted label constraint and confidence ranking is similar, with the label constraint helping to prevent the mutation of features into an adverse meaning and the confidence ranking helping to eliminate out-of-domain words and reduce feature space shift.

# <span id="page-6-0"></span>4.3 Feature Space Shift Investigation

In this subsection, we explore the feature space shift problem in more detail by using visualizations and the feature space shift metric. We use t-SNE to visualize the distribution of the features of the testing set and compare it to different augmented variants. The full results of feature space shift metrics are available in Figure [6.](#page-14-1) The results of feature space shift metrics in our experiment show that the augmentation instances generated by BO O S TAU G have the least shift of feature space. Specifically, the overlapping ratio and skewness in relation to the testing set are consistently better than those of the training set. This explains the performance improvement seen when using BO O S TAU G in previous experiments. In contrast, the augmentation instances generated by EDA, which was the best peer text augmentation method, have a worse overlapping rate compared to even the training set. This explains the performance degradation when using EDA on the baseline classification models. It is also noteworthy that the quality of the augmentation instances generated by MonoAug is better than EDA.

# 4.4 Effect of Augmentation Instances Number

To further understand the effectiveness of BO O S TAU G, we conduct an experiment to analyze the relationship between the number of augmentation instances generated and the performance of

<span id="page-7-0"></span>![](_page_7_Figure_0.jpeg)

**Caption:** Table 4 presents a performance comparison of various ablated variants of the BO O S TAU G framework. The results highlight the significant performance improvements achieved through the proposed instance filtering strategies, including perplexity filtering and predicted label constraints, demonstrating their importance in enhancing augmentation quality.

Table 4: The performance comparison between different ablated variants of BO O S TAU G.

<span id="page-7-1"></span>![](_page_7_Figure_2.jpeg)

**Caption:** Figure 3 shows the trajectories of accuracy and F1 scores as the number of augmentation instances generated by BO O S TAU G increases. The results indicate a consistent performance improvement, reaching a saturation point, and confirm that BO O S TAU G outperforms traditional methods like MonoAug and EDA in mitigating feature space shifts.

Figure 3: Trajectories of the Acc and the F1 values with error bars versus the number of augmentation instances generated for an example by using BO O S TAU G(EDA). The trajectory visualization plot of MonoAug and EDA can be found in Figure [7](#page-15-0)

the classification models. We use Acc and F1 as the evaluation metrics and plot the trajectories of these metrics with error bars against the number of augmentation instances generated for an example by using BO O S TAU G. The results are shown in Figure [3.](#page-7-1) For comparison, the trajectory visualization plots of MonoAug and EDA can also be found in Figure [7.](#page-15-0) From the results, it is clear to see that the performance of the classification models improves as the number of augmentation instances increases, but eventually reaches a saturation point. Furthermore, it is observed that the performance improvement achieved by BO O S TAU G is consistently better than that of MonoAug and EDA. This further confirms the effectiveness of BO O S TAU G in mitigating the feature space shift problem and improving the performance of the classification models.

However, it is also important to consider the computational budgets required to generate a large number of augmentation instances, as this can impact the overall efficiency of the text augmentation method being used.

#### 4.5 Hyper-parameter Sensitivity Analysis

We find that there is no single best setting for the two hyper-parameters, α and β, in different situations such as different datasets and backend augmentation methods. To explore the sensitivity of these hyper-parameters, we conducted experiments on the Laptop14 and Restaurant14 datasets and show the Scott-Knott rank test [\(Mittas and An](#page-10-11)[gelis,](#page-10-11) [2013\)](#page-10-11) plots and performance box plots in Figure [4](#page-8-0) and Figure [5,](#page-13-0) respectively. We found that the best value of α highly depends on the dataset. For the Laptop14 and Restaurant14 datasets, a value of α = 0.5 was found to be the best choice according to Figure [4.](#page-8-0) However, it's worth noting that the smaller the value of α, the higher the computation complexity due to the need for more

augmentation instances. To balance efficiency and performance, we recommend a value of α = 0.99 (α = 1 means no augmentation instances survive) in BO O S TAU G, which reduces computation complexity. Additionally, we found that β is relatively easy to determine, with a value of β = 4 being commonly used.

<span id="page-8-0"></span>![](_page_8_Figure_1.jpeg)

**Caption:** Figure 4 illustrates the Scott-Knott rank test results for different hyperparameter settings (α and β) in the BO O S TAU G framework. The plot indicates that optimal values for these parameters vary by dataset, with α = 0.99 recommended for balancing performance and computational efficiency across various augmentation scenarios.

Figure 4: The Scott-knott rank test plots under different α and β in BO O S TAU G(EDA). The bigger rank means better performance.

### 5 Related Works

As pretraining has advanced, text augmentation techniques have become an increasingly popular area of research [\(Sennrich et al.,](#page-11-11) [2016;](#page-11-11) [Coulombe,](#page-9-6) [2018;](#page-9-6) [Li et al.,](#page-10-4) [2019;](#page-10-4) [Wei and Zou,](#page-11-3) [2019;](#page-11-3) [Kumar](#page-10-6) [et al.,](#page-10-6) [2020;](#page-10-6) [Lewis et al.,](#page-10-12) [2020;](#page-10-12) [Xie et al.,](#page-11-12) [2020;](#page-11-12) [Bi](#page-9-11) [et al.,](#page-9-11) [2021;](#page-9-11) [Ren et al.,](#page-11-9) [2021;](#page-11-9) [Haralabopoulos et al.,](#page-9-12) [2021;](#page-9-12) [Wang et al.,](#page-11-13) [2022c;](#page-11-13) [Yue et al.,](#page-11-14) [2022;](#page-11-14) [Zhou](#page-12-0) [et al.,](#page-12-0) [2022a;](#page-12-0) [Kamalloo et al.,](#page-10-13) [2022;](#page-10-13) [Wang et al.,](#page-11-15) [2022a\)](#page-11-15). Many of these techniques focus on lowresource scenarios [\(Chen et al.,](#page-9-3) [2020;](#page-9-3) [Zhou et al.,](#page-12-0) [2022a;](#page-12-0) [Kim et al.,](#page-10-1) [2022;](#page-10-1) [Zhou et al.,](#page-12-4) [2022b;](#page-12-4) [Wu](#page-11-16) [et al.,](#page-11-16) [2022;](#page-11-16) [Yang et al.,](#page-11-2) [2022;](#page-11-2) [Wang et al.,](#page-11-1) [2022b;](#page-11-1) [Yang et al.,](#page-11-2) [2022\)](#page-11-2). However, they tend to fail when applied to large public datasets [\(Zhou et al.,](#page-12-0) [2022a\)](#page-12-0). Recent prominent works [\(Sennrich et al.,](#page-11-11) [2016;](#page-11-11) [Ku](#page-10-6)[mar et al.,](#page-10-6) [2020;](#page-10-6) [Lewis et al.,](#page-10-12) [2020;](#page-10-12) [Ng et al.,](#page-10-2) [2020;](#page-10-2) [Body et al.,](#page-9-4) [2021;](#page-9-4) [Chang et al.,](#page-9-5) [2021;](#page-9-5) [Luo et al.,](#page-10-3) [2021;](#page-10-3) [Wang et al.,](#page-11-1) [2022b\)](#page-11-1) recognize the significance of pre-trained language models (PLMs) for text augmentation and propose PLM-based methods to improve text augmentation. However, the quality of augmentation instances generated by unsupervised PLMs cannot be guaranteed. Some re-

search [\(Dong et al.,](#page-9-13) [2021\)](#page-9-13) has attempted to use adversarial training in text augmentation, which can improve robustness, but these methods are more suitable for low-sample augmentation scenarios and cause shifted feature spaces in large datasets.

While recent studies have emphasized the importance of quality control for augmentation instances [\(Lewis et al.,](#page-10-14) [2021;](#page-10-14) [Kamalloo et al.,](#page-10-13) [2022;](#page-10-13) [Wang](#page-11-1) [et al.,](#page-11-1) [2022b\)](#page-11-1), there remains a need for a transferable augmentation instance-filtering framework that can serve as an external quality controller to improve existing text augmentation methods.

Our work aims to address the failure mode of large dataset augmentation and improve existing augmentation methods more widely. Specifically, BO O S TAU G is a simple but effective framework that can work with a variety of existing augmentation backends, including EDA [\(Wei and Zou,](#page-11-3) [2019\)](#page-11-3) and PLM-based augmentation [\(Kumar et al.,](#page-10-6) [2020\)](#page-10-6).

### 6 Conclusion

Existing text augmentation methods usually lead to performance degeneration in large datasets due to numerous low-quality augmentation instances, while the reason for performance degeneration has not been well explained. We find low-quality augmentation instances usually have shifted feature space compare to natural instances. Therefore, we propose a universal augmentation instance filter framework BO O S TAU G to widely enhance existing text augmentation methods. BO O S TAU G is an external and flexible framework, all the existing text augmentation methods can be seamless improved. Experimental results on three TC datasets and five ABSC datasets show that BO O S TAU G is able to alleviate feature space shift in augmentation instances and significantly improve existing augmentation methods.

### Acknowledgements

This work was supported by UKRI Future Leaders Fellowship (MR/X011135/1, MR/S017062/1), NSFC (62076056), Alan Turing Fellowship, EP-SRC (2404317), Royal Society (IES/R2/212077) and Amazon Research Award.

### 7 Limitations

We propose and solve the feature space shift problem in text augmentation. However, there is a limitation that remains. BO O S TAU G cannot preserve

the grammar and syntax to a certain extent. We apply the perplexity filtering strategy, but it is an implicit constraint and cannot ensure the syntax quality of the augmentation instances due to some breaking transformations, such as keyword deletions and modifications. However, we do not need precise grammar and syntax information in most classification tasks, especially in PLM-based classification. For some syntax-sensitive tasks, e.g., syntax parsing and the syntax-based ABSC [\(Zhang](#page-12-5) [et al.,](#page-12-5) [2019;](#page-12-5) [Phan and Ogunbona,](#page-10-15) [2020;](#page-10-15) [Dai et al.,](#page-9-14) [2021\)](#page-9-14), ensuring the syntax quality of the augmented instances is an urgent problem. Therefore, BO O S TAU G may not be an best choice for some tasks or models requiring syntax as an essential modeling objective [\(Zhang et al.,](#page-12-5) [2019\)](#page-12-5). In other words, the syntax quality of BO O S TAU G depends on the backend.

### References

- <span id="page-9-11"></span>Wei Bi, Huayang Li, and Jiacheng Huang. 2021. [Data](https://doi.org/10.18653/v1/2021.acl-long.173) [augmentation for text generation without any aug](https://doi.org/10.18653/v1/2021.acl-long.173)[mented data.](https://doi.org/10.18653/v1/2021.acl-long.173) In *ACL/IJCNLP'21: Proc. of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing*, pages 2223– 2237. Association for Computational Linguistics.
- <span id="page-9-4"></span>Thomas Body, Xiaohui Tao, Yuefeng Li, Lin Li, and Ning Zhong. 2021. [Using back-and-forth trans](https://doi.org/10.1016/j.eswa.2021.115033)[lation to create artificial augmented textual data](https://doi.org/10.1016/j.eswa.2021.115033) [for sentiment analysis models.](https://doi.org/10.1016/j.eswa.2021.115033) *Expert Syst. Appl.*, 178:115033.
- <span id="page-9-10"></span>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. [A large anno](https://doi.org/10.18653/v1/d15-1075)[tated corpus for learning natural language inference.](https://doi.org/10.18653/v1/d15-1075) In *EMNLP'15: Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing*, pages 632–642. The Association for Computational Linguistics.
- <span id="page-9-1"></span>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. [Language models are few-shot learners.](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html) In *NeurlIPS'20: Advances in Neural Information Processing Systems*.
- <span id="page-9-5"></span>Ernie Chang, Xiaoyu Shen, Dawei Zhu, Vera Demberg, and Hui Su. 2021. [Neural data-to-text generation](https://doi.org/10.18653/v1/2021.eacl-main.64)

[with lm-based text augmentation.](https://doi.org/10.18653/v1/2021.eacl-main.64) In *EACL'21: Proc. of the 16th Conference of the European Chapter of the Association for Computational Linguistics*, pages 758–768. Association for Computational Linguistics.

- <span id="page-9-3"></span>Jiaao Chen, Zichao Yang, and Diyi Yang. 2020. [Mixtext:](https://doi.org/10.18653/v1/2020.acl-main.194) [Linguistically-informed interpolation of hidden space](https://doi.org/10.18653/v1/2020.acl-main.194) [for semi-supervised text classification.](https://doi.org/10.18653/v1/2020.acl-main.194) In *ACL'20: Proc. of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 2147–2157. Association for Computational Linguistics.
- <span id="page-9-7"></span>Stanley F. Chen and Joshua Goodman. 1999. [An em](https://doi.org/10.1006/csla.1999.0128)[pirical study of smoothing techniques for language](https://doi.org/10.1006/csla.1999.0128) [modeling.](https://doi.org/10.1006/csla.1999.0128) *Comput. Speech Lang.*, 13(4):359–393.
- <span id="page-9-6"></span>Claude Coulombe. 2018. [Text data augmentation](http://arxiv.org/abs/1812.04718) [made simple by leveraging NLP cloud apis.](http://arxiv.org/abs/1812.04718) *CoRR*, abs/1812.04718.
- <span id="page-9-14"></span>Junqi Dai, Hang Yan, Tianxiang Sun, Pengfei Liu, and Xipeng Qiu. 2021. [Does syntax matter? A strong](https://doi.org/10.18653/v1/2021.naacl-main.146) [baseline for aspect-based sentiment analysis with](https://doi.org/10.18653/v1/2021.naacl-main.146) [roberta.](https://doi.org/10.18653/v1/2021.naacl-main.146) In *NAACL-HLT'21: Proc. of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 1816–1829. Association for Computational Linguistics.
- <span id="page-9-0"></span>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. [BERT: pre-training of](https://doi.org/10.18653/v1/n19-1423) [deep bidirectional transformers for language under](https://doi.org/10.18653/v1/n19-1423)[standing.](https://doi.org/10.18653/v1/n19-1423) In *NAACL-HLT'19: Proc. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 4171–4186. Association for Computational Linguistics.
- <span id="page-9-13"></span>Xin Dong, Yaxin Zhu, Zuohui Fu, Dongkuan Xu, and Gerard de Melo. 2021. [Data augmentation](https://doi.org/10.18653/v1/2021.acl-long.401) [with adversarial training for cross-lingual NLI.](https://doi.org/10.18653/v1/2021.acl-long.401) In *ACL/IJCNLP'21: Proc. of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing*, pages 5158–5167. Association for Computational Linguistics.
- <span id="page-9-8"></span>Ronald L. Graham. 1972. [An efficient algorithm for](https://doi.org/10.1016/0020-0190(72)90045-2) [determining the convex hull of a finite planar set.](https://doi.org/10.1016/0020-0190(72)90045-2) *Inf. Process. Lett.*, 1(4):132–133.
- <span id="page-9-12"></span>Giannis Haralabopoulos, Mercedes Torres Torres, Ioannis Anagnostopoulos, and Derek McAuley. 2021. [Text data augmentations: Permutation, antonyms and](https://doi.org/10.1016/j.eswa.2021.114769) [negation.](https://doi.org/10.1016/j.eswa.2021.114769) *Expert Syst. Appl.*, 177:114769.
- <span id="page-9-2"></span>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. [Deberta: decoding-enhanced](https://openreview.net/forum?id=XPZIaotutsD) [bert with disentangled attention.](https://openreview.net/forum?id=XPZIaotutsD) In *ICLR'21: 9th International Conference on Learning Representations*. OpenReview.net.
- <span id="page-9-9"></span>Qingnan Jiang, Lei Chen, Ruifeng Xu, Xiang Ao, and Min Yang. 2019. [A challenge dataset and effec](https://doi.org/10.18653/v1/D19-1654)[tive models for aspect-based sentiment analysis.](https://doi.org/10.18653/v1/D19-1654) In *EMNLP-IJCNLP'19: Proc. of the 2019 Conference*

*on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing*, pages 6279–6284. Association for Computational Linguistics.

- <span id="page-10-13"></span>Ehsan Kamalloo, Mehdi Rezagholizadeh, and Ali Ghodsi. 2022. [When chosen wisely, more data is what](https://doi.org/10.18653/v1/2022.findings-acl.84) [you need: A universal sample-efficient strategy for](https://doi.org/10.18653/v1/2022.findings-acl.84) [data augmentation.](https://doi.org/10.18653/v1/2022.findings-acl.84) In *ACL'22: Findings of the Association for Computational Linguistics*, pages 1048– 1062. Association for Computational Linguistics.
- <span id="page-10-10"></span>Akbar Karimi, Leonardo Rossi, and Andrea Prati. 2021. [AEDA: an easier data augmentation technique for](https://doi.org/10.18653/v1/2021.findings-emnlp.234) [text classification.](https://doi.org/10.18653/v1/2021.findings-emnlp.234) In *EMNLP'21: Findings of the Association for Computational Linguistics*, pages 2748– 2754. Association for Computational Linguistics.
- <span id="page-10-1"></span>Hazel H. Kim, Daecheol Woo, Seong Joon Oh, Jeong-Won Cha, and Yo-Sub Han. 2022. [ALP: data augmen](https://ojs.aaai.org/index.php/AAAI/article/view/21336)[tation using lexicalized pcfgs for few-shot text classi](https://ojs.aaai.org/index.php/AAAI/article/view/21336)[fication.](https://ojs.aaai.org/index.php/AAAI/article/view/21336) In *AAAI'22: Thirty-Sixth AAAI Conference on Artificial Intelligence*, pages 10894–10902. AAAI Press.
- <span id="page-10-5"></span>Ashutosh Kumar, Satwik Bhattamishra, Manik Bhandari, and Partha P. Talukdar. 2019. [Submodular](https://doi.org/10.18653/v1/n19-1363) [optimization-based diverse paraphrasing and its effec](https://doi.org/10.18653/v1/n19-1363)[tiveness in data augmentation.](https://doi.org/10.18653/v1/n19-1363) In *NAACL-HLT'19: Proc. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 3609–3619. Association for Computational Linguistics.
- <span id="page-10-6"></span>Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2020. [Data augmentation using pre-trained trans](http://arxiv.org/abs/2003.02245)[former models.](http://arxiv.org/abs/2003.02245) *CoRR*, abs/2003.02245.
- <span id="page-10-12"></span>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. [BART: denoising sequence-to-sequence pre-training](https://doi.org/10.18653/v1/2020.acl-main.703) [for natural language generation, translation, and com](https://doi.org/10.18653/v1/2020.acl-main.703)[prehension.](https://doi.org/10.18653/v1/2020.acl-main.703) In *ACL'20: Proc. of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 7871–7880. Association for Computational Linguistics.
- <span id="page-10-14"></span>Patrick S. H. Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021. [PAQ: 65](https://doi.org/10.1162/tacl_a_00415) [million probably-asked questions and what you can](https://doi.org/10.1162/tacl_a_00415) [do with them.](https://doi.org/10.1162/tacl_a_00415) *Trans. Assoc. Comput. Linguistics*, 9:1098–1115.
- <span id="page-10-4"></span>Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2019. [Textbugger: Generating adversarial](https://www.ndss-symposium.org/ndss-paper/textbugger-generating-adversarial-text-against-real-world-applications/) [text against real-world applications.](https://www.ndss-symposium.org/ndss-paper/textbugger-generating-adversarial-text-against-real-world-applications/) In *NDSS'19: 26th Annual Network and Distributed System Security Symposium*. The Internet Society.
- <span id="page-10-16"></span>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

[Roberta: A robustly optimized BERT pretraining](http://arxiv.org/abs/1907.11692) [approach.](http://arxiv.org/abs/1907.11692) *CoRR*, abs/1907.11692.

- <span id="page-10-3"></span>Qiaoyang Luo, Lingqiao Liu, Yuhao Lin, and Wei Zhang. 2021. [Don't miss the labels: Label-semantic](https://doi.org/10.18653/v1/2021.findings-acl.245) [augmented meta-learner for few-shot text classifi](https://doi.org/10.18653/v1/2021.findings-acl.245)[cation.](https://doi.org/10.18653/v1/2021.findings-acl.245) In *ACL/IJCNLP'21: Findings of the Association for Computational Linguistics*, volume ACL/IJCNLP 2021, pages 2773–2782. Association for Computational Linguistics.
- <span id="page-10-0"></span>Zhengjie Miao, Yuliang Li, and Xiaolan Wang. 2021. [Rotom: A meta-learned data augmentation frame](https://doi.org/10.1145/3448016.3457258)[work for entity matching, data cleaning, text classifi](https://doi.org/10.1145/3448016.3457258)[cation, and beyond.](https://doi.org/10.1145/3448016.3457258) In *SIGMOD'21: International Conference on Management of Data, Virtual Event, China, June 20-25, 2021*, pages 1303–1316. ACM.
- <span id="page-10-11"></span>Nikolaos Mittas and Lefteris Angelis. 2013. Ranking and clustering software cost estimation models through a multiple comparisons algorithm. *IEEE Trans. Software Eng.*, 39(4):537–551.
- <span id="page-10-2"></span>Nathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi. 2020. [SSMBA: self-supervised manifold based data](https://doi.org/10.18653/v1/2020.emnlp-main.97) [augmentation for improving out-of-domain robust](https://doi.org/10.18653/v1/2020.emnlp-main.97)[ness.](https://doi.org/10.18653/v1/2020.emnlp-main.97) In *EMNLP'20: Proc. of the 2020 Conference on Empirical Methods in Natural Language Processing*, pages 1268–1283. Association for Computational Linguistics.
- <span id="page-10-9"></span>Tong Niu and Mohit Bansal. 2018. [Adversarial over](https://doi.org/10.18653/v1/k18-1047)[sensitivity and over-stability strategies for dialogue](https://doi.org/10.18653/v1/k18-1047) [models.](https://doi.org/10.18653/v1/k18-1047) In *CoNLL'18: Proc. of the 22nd Conference on Computational Natural Language Learning*, pages 486–496. Association for Computational Linguistics.
- <span id="page-10-15"></span>Minh Hieu Phan and Philip O. Ogunbona. 2020. [Mod](https://doi.org/10.18653/v1/2020.acl-main.293)[elling context and syntactical features for aspect](https://doi.org/10.18653/v1/2020.acl-main.293)[based sentiment analysis.](https://doi.org/10.18653/v1/2020.acl-main.293) In *ACL'20: Proc. of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 3211–3220. Association for Computational Linguistics.
- <span id="page-10-8"></span>Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad Al-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphée De Clercq, Véronique Hoste, Marianna Apidianaki, Xavier Tannier, Natalia V. Loukachevitch, Evgeniy V. Kotelnikov, Núria Bel, Salud María Jiménez Zafra, and Gülsen Eryigit. 2016. [Semeval-2016 task 5: Aspect based sentiment anal](https://doi.org/10.18653/v1/s16-1002)[ysis.](https://doi.org/10.18653/v1/s16-1002) In *NAACL-HLT'16: Proc. of the 10th International Workshop on Semantic Evaluation*, pages 19–30. The Association for Computer Linguistics.
- <span id="page-10-7"></span>Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. [Semeval-2015 task 12: Aspect based sentiment anal](https://doi.org/10.18653/v1/s15-2082)[ysis.](https://doi.org/10.18653/v1/s15-2082) In *NAACL-HLT'15: Proc. of the 9th International Workshop on Semantic Evaluation*, pages 486– 495. The Association for Computer Linguistics.
- <span id="page-11-6"></span>Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. [Semeval-2014 task 4: Aspect](https://doi.org/10.3115/v1/s14-2004) [based sentiment analysis.](https://doi.org/10.3115/v1/s14-2004) In *ACL'14: Proc. of the 8th International Workshop on Semantic Evaluation*, pages 27–35. The Association for Computer Linguistics.
- <span id="page-11-9"></span>Shuhuai Ren, Jinchao Zhang, Lei Li, Xu Sun, and Jie Zhou. 2021. [Text autoaugment: Learning composi](https://doi.org/10.18653/v1/2021.emnlp-main.711)[tional augmentation policy for text classification.](https://doi.org/10.18653/v1/2021.emnlp-main.711) In *EMNLP'21: Proc. of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 9029–9043. Association for Computational Linguistics.
- <span id="page-11-4"></span>Rico Sennrich. 2012. [Perplexity minimization for trans](https://aclanthology.org/E12-1055/)[lation model domain adaptation in statistical machine](https://aclanthology.org/E12-1055/) [translation.](https://aclanthology.org/E12-1055/) In *EACL'12: 13th Conference of the European Chapter of the Association for Computational Linguistics*, pages 539–549. The Association for Computer Linguistics.
- <span id="page-11-11"></span>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. [Improving neural machine translation mod](https://doi.org/10.18653/v1/p16-1009)[els with monolingual data.](https://doi.org/10.18653/v1/p16-1009) In *ACL'16: Proc. of the 54th Annual Meeting of the Association for Computational Linguistics*. The Association for Computer Linguistics.
- <span id="page-11-10"></span>Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, and Maosong Sun. 2021. [Better robustness by more coverage: Adversar](https://doi.org/10.18653/v1/2021.findings-acl.137)[ial and mixup data augmentation for robust finetun](https://doi.org/10.18653/v1/2021.findings-acl.137)[ing.](https://doi.org/10.18653/v1/2021.findings-acl.137) In *ACL/IJCNLP'21: Findings of the Association for Computational Linguistics*, volume ACL/IJCNLP 2021 of *Findings of ACL*, pages 1569–1576. Association for Computational Linguistics.
- <span id="page-11-5"></span>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. [Recursive deep mod](https://aclanthology.org/D13-1170/)[els for semantic compositionality over a sentiment](https://aclanthology.org/D13-1170/) [treebank.](https://aclanthology.org/D13-1170/) In *EMNLP'13: Proc. of the 2013 Conference on Empirical Methods in Natural Language Processing*, pages 1631–1642. ACL.
- <span id="page-11-15"></span>Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, and Nan Duan. 2022a. [Logic-driven context extension and](https://doi.org/10.18653/v1/2022.findings-acl.127) [data augmentation for logical reasoning of text.](https://doi.org/10.18653/v1/2022.findings-acl.127) In *ACL'22: Findings of the Association for Computational Linguistics*, pages 1619–1629. Association for Computational Linguistics.
- <span id="page-11-8"></span>Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. 2016. [Attention-based LSTM for aspect](https://doi.org/10.18653/v1/d16-1058)[level sentiment classification.](https://doi.org/10.18653/v1/d16-1058) In *EMNLP'16: Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing*, pages 606–615. The Association for Computational Linguistics.
- <span id="page-11-1"></span>Yufei Wang, Can Xu, Qingfeng Sun, Huang Hu, Chongyang Tao, Xiubo Geng, and Daxin Jiang. 2022b. [Promda: Prompt-based data augmentation](https://doi.org/10.18653/v1/2022.acl-long.292)

[for low-resource NLU tasks.](https://doi.org/10.18653/v1/2022.acl-long.292) In *ACL'22: Proc. of the 60th Annual Meeting of the Association for Computational Linguistics*, pages 4242–4255. Association for Computational Linguistics.

- <span id="page-11-13"></span>Yufei Wang, Can Xu, Qingfeng Sun, Huang Hu, Chongyang Tao, Xiubo Geng, and Daxin Jiang. 2022c. [Promda: Prompt-based data augmentation](https://doi.org/10.18653/v1/2022.acl-long.292) [for low-resource NLU tasks.](https://doi.org/10.18653/v1/2022.acl-long.292) In *ACL'22: Proc. of the 60th Annual Meeting of the Association for Computational Linguistics*, pages 4242–4255. Association for Computational Linguistics.
- <span id="page-11-3"></span>Jason W. Wei and Kai Zou. 2019. [EDA: easy data](https://doi.org/10.18653/v1/D19-1670) [augmentation techniques for boosting performance](https://doi.org/10.18653/v1/D19-1670) [on text classification tasks.](https://doi.org/10.18653/v1/D19-1670) In *EMNLP-IJCNLP'19: Proc. of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing*, pages 6381–6387. Association for Computational Linguistics.
- <span id="page-11-7"></span>Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018. [A broad-coverage challenge corpus](https://doi.org/10.18653/v1/n18-1101) [for sentence understanding through inference.](https://doi.org/10.18653/v1/n18-1101) In *NAACL'18: Proc. of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics*, pages 1112–1122. Association for Computational Linguistics.
- <span id="page-11-16"></span>Xing Wu, Chaochen Gao, Meng Lin, Liangjun Zang, and Songlin Hu. 2022. [Text smoothing: Enhance var](https://doi.org/10.18653/v1/2022.acl-short.97)[ious data augmentation methods on text classification](https://doi.org/10.18653/v1/2022.acl-short.97) [tasks.](https://doi.org/10.18653/v1/2022.acl-short.97) In *ACL'22: Proc. of the 60th Annual Meeting of the Association for Computational Linguistics*, pages 871–875. Association for Computational Linguistics.
- <span id="page-11-12"></span>Qizhe Xie, Zihang Dai, Eduard H. Hovy, Thang Luong, and Quoc Le. 2020. [Unsupervised data augmentation](https://proceedings.neurips.cc/paper/2020/hash/44feb0096faa8326192570788b38c1d1-Abstract.html) [for consistency training.](https://proceedings.neurips.cc/paper/2020/hash/44feb0096faa8326192570788b38c1d1-Abstract.html) In *NeurIPS'20: Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems*.
- <span id="page-11-2"></span>Kevin Yang, Olivia Deng, Charles Chen, Richard Shin, Subhro Roy, and Benjamin Van Durme. 2022. [Ad](https://doi.org/10.18653/v1/2022.findings-acl.291)[dressing resource and privacy constraints in semantic](https://doi.org/10.18653/v1/2022.findings-acl.291) [parsing through data augmentation.](https://doi.org/10.18653/v1/2022.findings-acl.291) In *ACL'22: Findings of the Association for Computational Linguistics*, pages 3685–3695. Association for Computational Linguistics.
- <span id="page-11-0"></span>Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woo-Myoung Park. 2021. [Gpt3mix: Lever](https://doi.org/10.18653/v1/2021.findings-emnlp.192)[aging large-scale language models for text augmenta](https://doi.org/10.18653/v1/2021.findings-emnlp.192)[tion.](https://doi.org/10.18653/v1/2021.findings-emnlp.192) In *EMNLP'21: Findings of the Association for Computational Linguistics*, pages 2225–2239. Association for Computational Linguistics.
- <span id="page-11-14"></span>Tianchi Yue, Shulin Liu, Huihui Cai, Tao Yang, Shengkang Song, and Tinghao Yu. 2022. [Improving](https://doi.org/10.18653/v1/2022.findings-acl.233) [chinese grammatical error detection via data augmen](https://doi.org/10.18653/v1/2022.findings-acl.233)[tation by conditional error generation.](https://doi.org/10.18653/v1/2022.findings-acl.233) In *ACL'22:*

*Findings of the Association for Computational Linguistics*, pages 2966–2975. Association for Computational Linguistics.

- <span id="page-12-5"></span>Chen Zhang, Qiuchi Li, and Dawei Song. 2019. [Aspect-based sentiment classification with aspect](https://doi.org/10.18653/v1/D19-1464)[specific graph convolutional networks.](https://doi.org/10.18653/v1/D19-1464) In *EMNLP-IJCNLP'19: Proc. of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing*, pages 4567–4577. Association for Computational Linguistics.
- <span id="page-12-1"></span>Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. [Character-level convolutional networks for text clas](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html)[sification.](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) In *NeurlIPS'15: Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems*, pages 649–657.
- <span id="page-12-0"></span>Jing Zhou, Yanan Zheng, Jie Tang, Li Jian, and Zhilin Yang. 2022a. [Flipda: Effective and robust data aug](https://doi.org/10.18653/v1/2022.acl-long.592)[mentation for few-shot learning.](https://doi.org/10.18653/v1/2022.acl-long.592) In *Proc. of the 60th Annual Meeting of the Association for Computational Linguistics*, pages 8646–8665. Association for Computational Linguistics.
- <span id="page-12-4"></span>Ran Zhou, Xin Li, Ruidan He, Lidong Bing, Erik Cambria, Luo Si, and Chunyan Miao. 2022b. [MELM:](https://doi.org/10.18653/v1/2022.acl-long.160) [data augmentation with masked entity language mod](https://doi.org/10.18653/v1/2022.acl-long.160)[eling for low-resource NER.](https://doi.org/10.18653/v1/2022.acl-long.160) In *ACL'22: Proc. of the 60th Annual Meeting of the Association for Computational Linguistics*, pages 2251–2262. Association for Computational Linguistics.

### <span id="page-12-2"></span>A Hyperparameter Settings

### A.1 Hyperparameter Settings for **BO O S TAU G**

Some important parameters are set as follows.

- k is set to 5 for the k-fold cross-boosting on all datasets.
- The number of augmentation instances per example N˜ is 8.
- The transformation probability of each token in a sentence is set to 0.1 for all augmentation methods.
- The fixed confidence and perplexity thresholds are set as α = 0.99 and β = 5 based on grid search. We provide sensitivity test of α and β in Appendix [C.2.](#page-13-1)
- The learning rates of base models LSTM and DeBERTa-BASE are set as 10−<sup>3</sup> and 10−<sup>5</sup> , respectively.
- The batch size and maximum sequence modeling length are 16 and 80, respectively.
- The L<sup>2</sup> regularization parameter λ is 10−<sup>8</sup> ; we use Adam as the optimizer for all models during the training process.

### <span id="page-12-3"></span>B Baseline Backends

We use BO O S TAU G to improve five state-of-the-art baseline text augmentation methods, all of which are used as the text augmentation back end of BO O S TAU G. Please refer to Table [6](#page-14-0) for detailed experimental results.

- EDA(TextAttack[8](#page-12-6) ) [\(Wei and Zou,](#page-11-3) [2019\)](#page-11-3) performs text augmentation via random word insertions, substitutions and deletions.
- SynonymAug(NLPAug[9](#page-12-7) ) [\(Niu and Bansal,](#page-10-9) [2018\)](#page-10-9) replaces words in the original text with their synonyms. This method has been shown to be effective in improving the robustness of models on certain tasks.
- SpellingAug [\(Coulombe,](#page-9-6) [2018\)](#page-9-6): it substitutes words according to spelling mistake dictionary.
- SplitAug [\(Li et al.,](#page-10-4) [2019\)](#page-10-4) (NLPAug): it splits some words in the sentence into two words randomly.
- BackTranslationAug [\(Sennrich et al.,](#page-11-11) [2016\)](#page-11-11) (NLPAug): it is a sentence level augmentation method based on sequence translation.
- ContextualWordEmbsAug [\(Kumar et al.,](#page-10-6) [2020\)](#page-10-6) (NLPAug): it substitutes similar words ac-

<span id="page-12-7"></span><span id="page-12-6"></span><sup>8</sup><https://github.com/QData/TextAttack> <sup>9</sup><https://github.com/makcedward/nlpaug>

cording to the PLM (i.e., Roberta-base [\(Liu](#page-10-16) [et al.,](#page-10-16) [2019\)](#page-10-16)) given the context.

# C Additional Experiments

# C.1 Natural Language Inference Experiments

The experimental results in Table [5](#page-13-2) show that the performance of both BERT and DeBERTa models can be improved by applying BO O S TAU G. With BO O S TAU G, the accuracy of the BERT model on SNLI improves from 70.72% to 73.08%, and on MNLI from 51.11% to 52.49%. The DeBERTa model also shows significant improvement with EDA, achieving 86.39% accuracy on SNLI and 78.04% on MNLI. These results demonstrate the effectiveness of BO O S TAU G in improving the generalizability of natural language inference models, and its compatibility with different state-of-the-art pre-trained models such as BERT and DeBERTa.

<span id="page-13-2"></span>Table 5: The additional experimental results on the SNLI and MNLI datasets for natural language inference. The back end of BO O S TAU G is EDA.

| Augmentation | Model   |       | SNLI  | MNLI  |       |  |
|--------------|---------|-------|-------|-------|-------|--|
|              |         | Acc   | F1    | Acc   | F1    |  |
| None         | BERT    | 70.72 | 72.8  | 51.11 | 50.47 |  |
|              | DeBERTa | 83.50 | 83.47 | 74.75 | 74.62 |  |
|              | BERT    | 73.08 | 71.57 | 52.49 | 50.91 |  |
| BO O S TAU G | DeBERTa | 86.39 | 86.16 | 78.04 | 77.04 |  |

<span id="page-13-0"></span>![](_page_13_Figure_6.jpeg)

**Caption:** Figure 5 presents box plots illustrating the performance of BO O S TAU G under varying hyperparameter settings (α and β). The results emphasize the framework's adaptability and effectiveness in enhancing text classification performance across different datasets, showcasing its potential for broader applications in NLP tasks.

Figure 5: The performance box plots under different α and β in BO O S TAU G(EDA).

# <span id="page-13-1"></span>C.2 Hyper-parameter Sensitivity Experiment

We provide the experimental results of BO O S TAU G on the Laptop14 and Restaurant14 datasets in Figure [5.](#page-13-0)

# C.3 Performance of **BO O S TAU G** on Different Backends

To investigate the generalization ability of BO O S TAU G, we evaluate its performance based on the existing augmentation backends. From the results shown in Table [6,](#page-14-0) we find that the performance of these text augmentation back ends can be improved by using our proposed BO O S TAU G. Especially by cross-referencing the results shown in Table [2,](#page-6-1) we find that the conventional text augmentation methods can be enhanced if appropriate instance filtering strategies are applied.

Another interesting observation is that PLMs are not effective for text augmentation, e.g., WordEmdsAug is outperformed by EDA in most comparisons[10](#page-13-3). Moreover, PLMs are resourceintense and usually cause a biased feature space. This is because PLMs can generate some unknown words, which are outside the testing set, during the pre-training stage. Our experiments indicate that using PLM as an augmentation instance filter, instead of a text augmentation tool directly, can help alleviate the feature space shift.

# C.4 Visualization of feature space

Figure [6](#page-14-1) shows the feature space shift of the ABSC datasets, where the augmentation back end of BO O S TAU G is EDA.

# C.5 Trajectory Visualization of RQ4

Figure [7](#page-15-0) shows the performance trajectory visualization of MonoAug and EDA. Compared to BO O S TAU G, MonoAug and existing augmentation methods usually trigger performance sacrifice while augmentation instances for each example are more than 3.

<span id="page-13-3"></span><sup>10</sup>In fact, we also tried some other PLM-based augmentation back ends, e.g., BackTranslationAug, and we come up with same observation.

|             |         | MAMS  |       | SST2  |       | SST5  |       | AGNews10K |       |  |
|-------------|---------|-------|-------|-------|-------|-------|-------|-----------|-------|--|
| Backend     | Model   | Acc   | F1    | Acc   | F1    | Acc   | F1    | Acc       | F1    |  |
| None        | LSTM    | 56.96 | 56.18 | 82.37 | 82.37 | 44.39 | 43.60 | 87.60     | 87.36 |  |
|             | BERT    | 82.78 | 82.04 | 90.77 | 90.76 | 52.90 | 53.02 | 92.47     | 92.26 |  |
|             | DeBERTa | 83.31 | 82.87 | 95.28 | 95.28 | 56.47 | 55.58 | 92.30     | 92.13 |  |
|             | LSTM    | 59.21 | 59.58 | 85.83 | 85.83 | 45.93 | 43.59 | 88.45     | 88.16 |  |
| EDA         | BERT    | 84.01 | 83.44 | 92.33 | 92.33 | 53.94 | 52.80 | 92.48     | 92.25 |  |
|             | DeBERTa | 84.51 | 83.97 | 96.09 | 96.09 | 57.78 | 56.15 | 92.95     | 92.76 |  |
|             | LSTM    | 58.50 | 57.65 | 85.23 | 85.23 | 43.39 | 42.45 | 87.93     | 87.63 |  |
| SpellingAug | BERT    | 83.23 | 82.70 | 92.01 | 92.01 | 52.26 | 51.03 | 91.82     | 91.59 |  |
|             | DeBERTa | 83.98 | 83.44 | 95.22 | 95.22 | 57.91 | 55.88 | 92.77     | 92.54 |  |
| SplitAug    | LSTM    | 58.65 | 57.23 | 85.64 | 85.64 | 46.04 | 43.97 | 87.65     | 87.42 |  |
|             | BERT    | 83.05 | 82.49 | 92.20 | 92.20 | 51.86 | 51.39 | 91.92     | 91.69 |  |
|             | DeBERTa | 82.67 | 82.26 | 94.76 | 94.76 | 57.67 | 55.90 | 92.70     | 92.51 |  |
| WordEmdsAug | LSTM    | 59.54 | 57.58 | 86.30 | 86.30 | 46.47 | 44.15 | 88.38     | 88.10 |  |
|             | BERT    | 83.31 | 82.72 | 91.76 | 91.76 | 52.49 | 50.27 | 92.43     | 92.24 |  |
|             | DeBERTa | 83.35 | 82.87 | 95.33 | 95.33 | 57.22 | 56.08 | 93.88     | 93.70 |  |

<span id="page-14-0"></span>Table 6: Performance comparison of BO O S TAU G based on different augment back ends.

<span id="page-14-1"></span>![](_page_14_Figure_2.jpeg)

**Caption:** Figure 6 visualizes the feature space shift (S) across four ABSC datasets using t-SNE. The results demonstrate that BO O S TAU G achieves the least feature space shift compared to other augmentation methods like MonoAug and EDA, highlighting its effectiveness in preserving data integrity during augmentation.

Figure 6: This figure shows the feature space shift (S) of four ABSC datasets as visualized by t-SNE. The results demonstrate that BO O S TAU G has the least feature space shifts in comparison to other augmentation methods, such as MonoAug and EDA.

<span id="page-15-0"></span>![](_page_15_Figure_0.jpeg)

**Caption:** Figure 7 visualizes the relationship between the number of augmentation instances generated and the performance metrics (accuracy and F1 score) for BO O S TAU G. The results indicate that performance improves with more instances, but eventually saturates, confirming the framework's effectiveness in enhancing classification outcomes.

Figure 7: The performance (i.e., classification accuracy and F1 score) visualization of how BO O S TAU G perform as the number of augmentation instances per example increases.