# Adaptive Augmentation Policy Optimization with LLM Feedback

Ant Duru1 [0009−0002−9898−015X] and Alptekin Temizel1[0000−0001−6082−2573]

Graduate School of Informatics, METU ant.duru@metu.edu.tr, atemizel@metu.edu.tr

Abstract. Data augmentation is a critical component of deep learning pipelines, enhancing model generalization by increasing dataset diversity. Traditional augmentation strategies rely on manually designed transformations, stochastic sampling, or automated search-based approaches. Although automated methods improve performance, they often require extensive computational resources and are specifically designed for certain datasets. In this work, we propose a Large Language Model (LLM) guided augmentation optimization strategy that refines augmentation policies based on model performance feedback.

We propose two approaches: (1) LLM-Guided Augmentation Policy Optimization, where augmentation policies are selected by LLM prior to training and refined iteratively across multiple training cycles, and (2) Adaptive LLM-Guided Augmentation Policy Optimization, where policies adapt in real-time based on performance metrics. This in-training approach eliminates the need for full model retraining before getting LLM feedback, reducing computational costs while increasing performance.

Our methodology employs an LLM to dynamically select augmentation transformations based on dataset characteristics, model architecture, and prior training performance. Unlike traditional search-based methods, our approach leverages LLMs' contextual knowledge, particularly in specialized domains like medical imaging, to recommend augmentation strategies tailored to domain-specific data.

We evaluate our approach on multiple domain-specific image classification datasets where augmentation is key to model robustness. Results show that LLM-guided augmentation optimization outperforms traditional methods, enhancing model accuracy. These findings highlight the potential of LLMs in automating and adapting deep learning training workflows. The code for the adaptive approach will be publicly available.

Keywords: Medical Image Classification · Automated Augmentation · Large Language Models.

### 1 Introduction

Deep learning models have achieved remarkable success in various image classification tasks, but they often require large labeled datasets for robust generalization. In specialized fields, such as medical imaging, acquiring extensive labeled datasets is challenging due to data scarcity, ethical concerns, and high annotation costs. To address this limitation, data augmentation has become an essential technique, enhancing model performance by increasing the diversity of training data through transformations such as rotation, flipping, cropping, and contrast adjustments. The choice of augmentation strategy is critical, as suboptimal augmentations may fail to improve generalization or even degrade model performance.

Traditional augmentation relies on manually designed transformations, requiring domain expertise to tailor policies to specific datasets. However, this approach is time-consuming and does not scale across different tasks. As the complexity of deep learning applications grows, manual augmentation tuning becomes impractical, necessitating the development of systematic, automated augmentation strategies. Search-based optimization techniques such as AutoAugment [\[1\]](#page-12-0), RandAugment [\[2\]](#page-12-1), and TrivialAugment [\[3\]](#page-12-2) systematically explore augmentation search spaces to identify policies that maximize model performance. While effective, they often require extensive computational resources and are sensitive to dataset characteristics, limiting their applicability to various realworld scenarios. Additionally, the augmentation strategies derived from these methods frequently lack flexibility, as they rely on pre-computed augmentation configurations that may not be optimal across different training stages or evolving model architectures.

Recently, large language models (LLMs) have emerged as powerful tools capable of optimizing various aspects of deep learning workflows, including hyperparameter tuning and automated machine learning. Prior studies have shown that LLMs can effectively suggest hyperparameters such as learning rates, batch sizes, and optimizers based on model performance feedback [\[4\]](#page-13-0). Their capacity to process large amounts of unstructured knowledge and synthesize contextual insights makes them particularly well-suited for data-driven optimization tasks. Inspired by these advancements, we explore the use of LLMs to optimize data augmentation strategies for image classification. Unlike conventional search-based methods, LLMs can incorporate contextual understanding of dataset properties, augmentation effects, and domain-specific constraints, making them well-suited for guiding augmentation policy selection. Furthermore, their ability to iteratively refine augmentation choices based on real-time feedback presents a significant advantage over traditional augmentation search techniques.

In this paper, we propose two automated augmentation strategies leveraging large language models (LLMs) to generate dataset-specific augmentation policies.

1. LLM-Guided Augmentation Policy Optimization: Augmentation policies are proposed by the LLM before training, and they are then refined iteratively across multiple training runs. While this approach improves performance, it requires repeated full-model retraining and evaluation, making it computationally expensive for large-scale datasets or time-sensitive applications.

2. Adaptive Augmentation Policy Optimization with LLM Feedback: In this method, augmentation policies proposed by LLM are dynamically updated during training through communication with the LLM. This approach significantly reduces computational overhead while maintaining performance improvements by adapting policies based on model feedback. This adaptive capability allows the augmentation policy to evolve alongside model learning dynamics, leading to potentially better convergence and model generalization.

Both processes allow adaptation to the unique features of each dataset, making them versatile and scalable solutions for any data type. They also enable domain experts to train their models more effectively with less reliance on technical experts.

We evaluated our approach through extensive experiments on multiple domainspecific medical imaging datasets, where augmentation is critical for model generalization. Our results show that LLM-guided augmentation optimization outperforms state-of-the-art search-based methods by improving model accuracy while reducing the need for manual tuning and exhaustive searches.

### 2 Related Work

Early augmentation methods use combinations of basic transformations like flipping, rotating, and cropping to manipulate an image to mitigate overfitting. However, finding the optimal combination of augmentation techniques for a specific task is challenging. To address this issue, several automated augmentation techniques have been developed. AutoAugment [\[1\]](#page-12-0) optimizes the augmentation policy through a reinforcement-learning-based approach, where the augmentation policy is selected from a search space in every iteration. RandAugment [\[2\]](#page-12-1) and TrivialAugment [\[3\]](#page-12-2) use randomness when selecting from a predefined set of augmentation techniques. In contrast, AugMix [\[5\]](#page-13-1) mixes differently augmented images to improve robustness.

Recently, Large Language Models (LLMs) have started to be used for decisionmaking problems. [\[6\]](#page-13-2) and [\[7\]](#page-13-3) show how LLMs can be used in evolutionary algorithm optimization. In AI applications, LLMs have been used in various decisionmaking problems as well. In [\[4\]](#page-13-0), ChatGPT is utilized to tune fundamental hyperparameters such as batch size, learning rate, and optimizer type iteratively to improve model training. LLMs are also used for neural architecture search as a black-box optimizer, as shown in [\[8\]](#page-13-4) and [\[9\]](#page-13-5). Lastly, in [\[10\]](#page-13-6), a fully LLM-driven training pipeline idea is presented. All these aforementioned research indicates how LLMs are entered into the AI model training process as decision-makers by utilizing their wide and contextual knowledge of different types of challenges in model training.

Large language models that can generate textual data are also used in generative data augmentation. In [\[11\]](#page-13-7) and [\[12\]](#page-13-8), it has been shown that these models can be used to create synthetic data to enhance the diversity and size of existing datasets. Unlike our work, this approach creates textual data and applies

data augmentation on a dataset by generating new examples using related embeddings, while our work focuses on obtaining optimal augmentation policies without adding any new examples to existing datasets.

### 3 Methodology

### 3.1 Overview of the Approach

The overall system consists of the following core components: (1) an LLM responsible for suggesting augmentation policies based on dataset characteristics and prior model performance, (2) a deep learning model trained using the recommended augmentation policies, and (3) a feedback mechanism that relays model performance back to the LLM for policy refinement. The primary distinction between the two methodologies lies in the timing and frequency of augmentation policy updates, which significantly impact computational efficiency and generalization capability.

### 3.2 LLM-Guided Augmentation Policy Optimization

As shown in Fig. [1,](#page-4-0) this strategy follows an iterative process where the policies are continuously updated based on validation performance from previous training cycles. The process can be summarized as follows:

- 1. The LLM receives an initial prompt containing the dataset description, model architecture details, performance objectives, and the desired number of augmentation types.
- 2. Based on its prior knowledge and dataset characteristics, the LLM generates an initial augmentation policy.
- 3. The deep learning model is trained for a full cycle using the suggested augmentation policy.
- 4. Validation accuracy is calculated and returned back to the LLM.
- 5. Using this feedback, the LLM refines the augmentation policy and suggests an improved version for the next training iteration.
- 6. This process repeats for a fixed number of iterations.

### 3.3 Adaptive LLM-Guided Augmentation Policy Optimization

In addition to the previous approach, where the augmentation policy is updated only after a full-training cycle, we propose a dynamic in-training augmentation optimization method that refines the policy throughout the training process. As shown in Fig. [2,](#page-4-1) this method continuously updates augmentation policies at periodic intervals based on model performance feedback, rather than relying on a fixed augmentation strategy for each training session. The workflow for this method is as follows:

![](_page_4_Figure_1.jpeg)

**Caption:** Figure 1 illustrates the workflow of LLM-guided augmentation policy optimization. The LLM initializes the augmentation policy before training, and after each training cycle, the model's evaluation results are fed back to the LLM, which updates the policy based on this feedback, enhancing model performance iteratively.

<span id="page-4-0"></span>Fig. 1. The workflow of LLM-guided augmentation policy optimization. The LLM initializes the augmentation policy before training begins. After a full training cycle, the model's evaluation results are fed back to the LLM, which then updates the augmentation policy based on this feedback.

- 1. The LLM receives a query containing dataset attributes, model architecture details, performance objectives, and the number of augmentations wanted in the policy.
- 2. Every N epochs, validation accuracy and other performance metrics are computed.
- 3. These performance metrics, along with details of previously used augmentation policies, are sent to the LLM.
- 4. The LLM processes this feedback and generates an updated augmentation policy tailored to the model's current training stage.
- 5. The updated augmentation policy is applied immediately, allowing training to continue without interruption.
- 6. This process repeats until training is complete.

![](_page_4_Figure_9.jpeg)

**Caption:** Figure 2 depicts the in-training augmentation optimization process. Here, the LLM selects the augmentation policy before training and updates it after every N epochs based on performance metrics, allowing for real-time adjustments that improve model generalization and reduce computational costs.

<span id="page-4-1"></span>Fig. 2. The process of in-train augmentation optimization, where augmentation policy is selected before training and updated after each N epoch through training.

This adaptive approach offers several distinct advantages:

- Adaptive Policy Evolution: Augmentations evolve dynamically, allowing realtime modifications tailored to the model's needs at different training stages.
- Lower Computational Cost: Unlike before-training optimization, this method eliminates the need for full retraining cycles for each augmentation refinement since the adaptive approach does not require iterative interaction with the LLM after each training session. It requires only one training session where the augmentation policies change after every N epochs.
- Improved Generalization: Dynamic augmentation policies prevent overfitting by continuously adapting, leading to better model performance across diverse datasets.
- Enhanced Exploration: By modifying augmentations throughout training, a broader range of augmentations is evaluated.

### 3.4 LLM Model and API Integration

The augmentation policy selection relies on a pretrained LLM, such as Chat-GPT [\[13\]](#page-13-9), Gemini [\[14\]](#page-13-10), or DeepSeek [\[15\]](#page-13-11), accessed via an API. The query format includes dataset descriptions, model specifications, and past validation performance. The LLM interprets these inputs and generates augmentation policies by leveraging its internal knowledge of augmentation techniques and their effects on model performance. Additionally, LLMs possess contextual knowledge about different dataset domains, including medical imaging, where augmentation strategies must account for unique visual characteristics, such as variations in contrast, anatomical structures, and imaging artifacts. This inherent knowledge allows the LLM to suggest augmentation strategies that align with the structural properties of medical datasets, potentially improving model robustness in these specialized applications. The API-based nature of this approach ensures scalability, enabling augmentation refinement in real time with minimal manual intervention.

#### 3.5 Augmentation Search Space

The augmentation search space is not explicitly restricted; instead, the LLM is allowed to select augmentation strategies dynamically from the available transformations within torchvision.transforms [\[16\]](#page-13-12). This flexibility ensures that the LLM can explore a wide range of augmentation techniques without predefined constraints, enabling it to adapt its selections based on dataset characteristics and model performance.

### 3.6 Training Setup

To evaluate our method, we conduct experiments on multiple image classification datasets, including challenging medical imaging datasets that require specialized augmentations. We use standard deep learning architectures, such as ResNet-18

<sup>6</sup> A. Duru et al.

[\[17\]](#page-13-13), MobileNetV2 [\[18\]](#page-13-14), DenseNet [\[19\]](#page-13-15), and ViT [\[20\]](#page-13-16), to ensure the general applicability of our findings. The training process is implemented using PyTorch and TensorFlow, with fixed hyperparameters such as learning rate, batch size, and optimizer settings to isolate the effects of augmentation policies. All models were trained for 100 epochs, with an early stopping of 10. All LLMs used deterministic temperature=0 prompting for reproducibility.

### 4 Experimental Evaluation

We experimentally evaluated our proposed LLM-guided augmentation optimization methods with traditional augmentation strategies and search-based methods: RandAugment, TrivialAugment, and AugMix. TrivialAugment applies a single random augmentation per image from a predefined set of transformations. By default, AugMix applies three augmentation chains, each consisting of one to three randomly selected augmentation operations. For RandAugment and the proposed methods, LLM-Guided and Adaptive LLM-Guided (integrated into the training loop), we report for the case when 2 and 3 augmentations (N=2 and N=3) are allowed. The comparative analysis provides insights into the effectiveness of dynamic augmentation strategies associated with real-time policy refinement. Fig. [3](#page-7-0) displays an example dialogue with DeepSeek to show how our before-training optimization works.

#### 4.1 Used Datasets

For experimental evaluation, we focused on medical image datasets with different characteristics (various image sizes, color characteristics, and feature representations), and complexity:

- The APTOS 2019 Blindness Detection Dataset [\[21\]](#page-13-17) comprises 3,662 retinal fundus images sourced from rural regions across India. These images were systematically reviewed and annotated by experienced ophthalmologists in accordance with the International Clinical Diabetic Retinopathy Disease Severity Scale (ICDRSS). Each retinal image is categorized into one of five distinct stages of diabetic retinopathy (DR): no DR, mild DR, moderate DR, severe DR, and proliferative DR.
- The Melanoma Cancer Image Dataset [\[22\]](#page-14-0) contains 13,900 curated, uniformlysized images (224 × 224 pixels) that support the development of machine learning models to distinguish between benign and malignant lesions. It captures Melanoma's diverse presentations, aiding early detection and diagnostic tool development.
- The Alzheimer Parkinson Diseases 3 Class Dataset [\[23\]](#page-14-1) contains uniformlysized (176×208) RGB brain MRI images for classifying into Healthy, Alzheimer's Disease (AD), and Parkinson's Disease (PD).
- The LIMUC Dataset [\[24,](#page-14-2)[25\]](#page-14-3) contains 11,276 images collected from 564 patients during 1,043 colonoscopy procedures. Each image is labeled by medical doctors according to the severity of Ulcerative Colitis using the Mayo Endoscopic Score (MES).

![](_page_7_Figure_1.jpeg)

**Caption:** Figure 3 presents an example dialogue with DeepSeek for before-training augmentation optimization using the Alzheimer Dataset with MobileNetV2. The LLM selects an augmentation policy prior to training, which is then refined iteratively based on model performance feedback, showcasing the adaptive nature of the approach.

<span id="page-7-0"></span>Fig. 3. An example dialogue with Deepseek for before-train augmentation optimization for Alzheimer Dataset with MobileNetV2, where augmentation policy is selected before training and updated iteratively after each training process.

### 5 Results and Discussion

The results in Table [1](#page-9-0) highlight the advantages of LLM-driven optimization over traditional augmentation methods. Across all ResNet18, MobileNetV2, and ViT32, models trained with LLM-optimized augmentation strategies consistently achieve higher validation accuracy, demonstrating the potential of data-aware augmentation policies. While TrivialAugment, AugMix, and RandAugment also improve performance compared to training without augmentation, their effectiveness varies by model architecture. TrivialAugment provides moderate gains, reaching 0.8497 (ResNet18), 0.8706 (MobileNetV2), and 0.8051 (ViT32), while AugMix achieves 0.8525, 0.8469, and 0.8005, respectively. RandAugment (N=2, N=3) yields inconsistent results, with notable variations across different architectures, suggesting that fixed augmentation magnitudes may not be optimal for all datasets.

The results show that the proposed LLM-Guided Augmentation optimization approach outperforms the competing methods, with ChatGPT, Gemini, and DeepSeek achieving 0.8743 (ResNet18), 0.8716 (MobileNetV2), and 0.8777 (MobileNetV2), respectively, surpassing conventional augmentation strategies. The most significant performance gains are obtained with Adaptive LLM-Guided Augmentation Policy Optimization, where augmentation policies evolve dynamically based on model feedback. The highest validation accuracy (0.8798 for both architectures) is achieved when N=3 and augmentations are updated every epoch, emphasizing the importance of frequent augmentation adjustments through model feedback. When updates occur every five epochs, performance remains strong (0.8756), suggesting that a slightly less frequent augmentation update schedule can still yield competitive results while reducing computational overhead.

These findings confirm that LLM-driven augmentation policies offer superior generalization, making them a valuable tool for improving deep learning models in challenging domains such as medical imaging.

Table [2](#page-10-0) summarizes the results on the Melanoma Cancer Image Dataset. Unlike the previous dataset, where traditional augmentation methods offered moderate gains, this particular dataset highlights their limitations. TrivialAugment and AugMix result in lower accuracy than the no-augmentation baseline, suggesting that these generic augmentation strategies may introduce distortions that hinder performance. Similarly, RandAugment (N=2, N=3) fails to provide consistent improvements, with performance varying across different architectures. These results indicate that predefined augmentation policies may not be well-suited for the unique characteristics of medical imaging datasets, where subtle visual features are critical for classification accuracy.

On the other hand, the proposed LLM-Guided Augmentation optimization further improves over the baseline and standard methods, with ChatGPT, Gemini, and DeepSeek reaching 0.9125, 0.9087, and 0.9529, respectively, for MobileNetV2. However, the most significant accuracy gains are obtained Adaptive LLM-Guided Augmentation. Updating augmentation every epoch boosts the accuracy to 0.9738 (ResNet18), 0.9 (MobileNetV2), and 0.8512 (ViT32), while

<span id="page-9-0"></span>Table 1. Validation Accuracy Comparison on APTOS2019 Dataset. TrivialAugment applies a single random augmentation. AugMix uses three augmentation chains; each combining one to three augmentations.

| Augmentation Method                     |        | ResNet18 MobileNetV2 ViT32 |        |  |
|-----------------------------------------|--------|----------------------------|--------|--|
| No Augmentation                         | 0.8388 | 0.8415                     | 0.7978 |  |
| TrivialAugment                          | 0.8497 | 0.8706                     | 0.8051 |  |
| AugMix                                  | 0.8525 | 0.8469                     | 0.8005 |  |
| N=2 (Two Augmentations Allowed)         |        |                            |        |  |
| RandAugment                             | 0.8607 | 0.8388                     | 0.8197 |  |
| LLM-Guided (ChatGPT)                    | 0.8689 | 0.8716                     | 0.8212 |  |
| LLM-Guided (Gemini)                     | 0.8743 | 0.8661                     | 0.8205 |  |
| LLM-Guided (DeepSeek)                   | 0.8770 | 0.8777                     | 0.8226 |  |
| LLM-Guided (Adaptive, @1 Epoch)         | 0.8743 | 0.8743                     | 0.8319 |  |
| LLM-Guided (Adaptive, @5 Epochs) 0.8743 |        | 0.8743                     | 0.8305 |  |
| N=3 (Three Augmentations Allowed)       |        |                            |        |  |
| RandAugment                             | 0.8470 | 0.8607                     | 0.8169 |  |
| LLM-Guided (ChatGPT)                    | 0.8743 | 0.8716                     | 0.8240 |  |
| LLM-Guided (Gemini)                     | 0.8661 | 0.8743                     | 0.8219 |  |
| LLM-Guided (DeepSeek)                   | 0.8675 | 0.8750                     | 0.8275 |  |
| LLM-Guided (Adaptive, @1 Epochs) 0.8798 |        | 0.8798                     | 0.8368 |  |
| LLM-Guided (Adaptive, @5 Epochs) 0.8756 |        | 0.8756                     | 0.8771 |  |

updates at every five epochs still maintain a substantial advantage. These results confirm that dynamic augmentation optimization enhances model robustness in high-variance medical imaging datasets.

Overall, our findings suggest that static augmentation strategies are suboptimal for datasets with complex visual patterns. In contrast, LLM-driven intraining augmentation enables models to adapt dynamically, improving generalization and stability.

The results in Table [3](#page-11-0) provide further evidence of the benefits of LLM-driven augmentation optimization, particularly in datasets where distinguishing between subtle visual differences is critical. Unlike the melanoma dataset, where most traditional augmentation strategies failed to improve accuracy, RandAugment (N=2) performs well, achieving 0.9684 (ResNet18), 0.9444 (MobileNetV2), and 0.8234 (ViT32), indicating that certain predefined augmentation strategies can be beneficial in this context. However, TrivialAugment and AugMix underperform, with TrivialAugment showing a substantial drop to 0.8981 (ResNet18), 0.8629 (MobileNetV2), and 0.7787 (ViT32), suggesting that uncontrolled augmentation variations may not align well with the dataset's inherent structure.

Our before-training augmentation optimization approach provides consistently strong results, with ChatGPT (N=2) achieving 0.9684 (ResNet18), 0.9614 (MobileNetV2), and 0.8454 (ViT32), outperforming all baseline augmentation strategies. Gemini performs slightly lower but still surpasses traditional methods. DeepSeek, however, delivers the highest validation accuracy, even better than our adaptive approach for ResNet18 and MobileNetV2. The adaptive ap-

<span id="page-10-0"></span>

| Augmentation Method                     |        | ResNet18 MobileNetV2 ViT32 |        |
|-----------------------------------------|--------|----------------------------|--------|
| No Augmentation                         | 0.9061 | 0.8998                     | 0.8161 |
| TrivialAugment                          | 0.8838 | 0.8910                     | 0.7963 |
| AugMix                                  | 0.8864 | 0.8981                     | 0.7471 |
| N=2 (Two Augmentations Allowed)         |        |                            |        |
| RandAugment                             | 0.8902 | 0.8973                     | 0.7627 |
| LLM-Guided (ChatGPT)                    | 0.9078 | 0.9125                     | 0.8352 |
| LLM-Guided (Gemini)                     | 0.9087 | 0.9028                     | 0.8244 |
| LLM-Guided (DeepSeek)                   | 0.9091 | 0.9091                     | 0.8401 |
| LLM-Guided (Adaptive, @1 Epoch)         | 0.9506 | 0.9352                     | 0.8512 |
| LLM-Guided (Adaptive, @5 Epochs) 0.9468 |        | 0.9336                     | 0.8433 |
| N=3 (Three Augmentations Allowed)       |        |                            |        |
| RandAugment                             | 0.8918 | 0.8960                     | 0.7726 |
| LLM-Guided (ChatGPT)                    | 0.9070 | 0.9078                     | 0.8198 |
| LLM-Guided (Gemini)                     | 0.8965 | 0.8897                     | 0.8202 |
| LLM-Guided (DeepSeek)                   | 0.9529 | 0.9367                     | 0.8310 |
| LLM-Guided (Adaptive, @1 Epoch)         | 0.9738 | 0.9668                     | 0.8417 |
| LLM-Guided (Adaptive, @5 Epochs) 0.9506 |        | 0.9344                     | 0.8456 |

Table 2. Validation Accuracy Performance Comparison of Augmentation Strategies on Melanoma Cancer Image Dataset.

proach, with N=3 and every-epoch updates, achieves 0.9701 (ResNet18), 0.9405 (MobileNetV2), and 0.8728 (ViT32). Even though this is the best result with ViT32, the before-training optimization approach outperforms the adaptive approach for other models. These findings suggest that although dynamic augmentation policies provide the most significant performance gains most of the time, some datasets may be fragile to frequent augmentation changes throughout the training.

Lastly, Table [4](#page-11-1) displays the results on the LIMUC dataset. Traditional automated augmentation methods give moderate gains or losses, failing to stabilize improvement over training the models without augmentation. Traditional approaches reach 0.7660 (ResNet18), 0.7660 (DenseNet121), and 0.7265 (ViT32) with TrivialAugment for the first RandAugment (N=3) for the latter two. This shows the unreliability of the traditional methods for domain-specific datasets.

There is a considerable increase in validation accuracy as a result of our before-training augmentation optimization approach. ChatGPT reached 0.7847 on DenseNet121 and 0.7748 on ResNet18, while Gemini surpasses traditional methods by forming an augmentation policy resulting in 0.7468 validation accuracy on ViT32. The best results came with our adaptive augmentation optimization approach for all model types. The adaptive approach, with N=3 and every-epoch updates achieving 0.7919 (ResNet18), 0.7852 (DenseNet121), and 0.7605 (ViT32), demonstrates a clear improvement over traditional methods.

<span id="page-11-0"></span>

| Augmentation Method                    |                                 | ResNet18 MobileNetV2 ViT32 |        |  |
|----------------------------------------|---------------------------------|----------------------------|--------|--|
| No Augmentation                        | 0.9422                          | 0.9037                     | 0.7809 |  |
| TrivialAugment                         | 0.8981                          | 0.8629                     | 0.7787 |  |
| AugMix                                 | 0.9267                          | 0.9352                     | 0.7815 |  |
|                                        | N=2 (Two Augmentations Allowed) |                            |        |  |
| RandAugment                            | 0.9684                          | 0.9444                     | 0.8234 |  |
| LLM-Guided (ChatGPT)                   | 0.9684                          | 0.9614                     | 0.8454 |  |
| LLM-Guided (Gemini)                    | 0.9622                          | 0.9534                     | 0.8333 |  |
| LLM-Guided (DeepSeek)                  | 0.9738                          | 0.9668                     | 0.8404 |  |
| LLM-Guided (Adaptive, @1 Epoch) 0.9526 |                                 | 0.9460                     | 0.8666 |  |
| LLM-Guided (Adaptive, @5 Epoch) 0.9468 |                                 | 0.9452                     | 0.8660 |  |
| N=3 (Three Augmentations Allowed)      |                                 |                            |        |  |
| RandAugment                            | 0.9630                          | 0.9174                     | 0.8311 |  |
| LLM-Guided (ChatGPT)                   | 0.9676                          | 0.9483                     | 0.8354 |  |
| LLM-Guided (Gemini)                    | 0.9614                          | 0.9550                     | 0.8391 |  |
| LLM-Guided (DeepSeek)                  | 0.9707                          | 0.9707                     | 0.8660 |  |
| LLM-Guided (Adaptive, @1 Epoch) 0.9701 |                                 | 0.9405                     | 0.8728 |  |
| LLM-Guided (Adaptive, @5 Epoch) 0.9684 |                                 | 0.9475                     | 0.8712 |  |

Table 3. Validation Accuracy Performance Comparison of Augmentation Strategies on Alzheimer-Parkinson Dataset.

Table 4. Validation Accuracy Performance Comparison of Augmentation Strategies on LIMUC Dataset.

<span id="page-11-1"></span>

| Augmentation Method                     |        | ResNet18 DenseNet121 ViT32 |        |  |
|-----------------------------------------|--------|----------------------------|--------|--|
| No Augmentation                         | 0.7599 | 0.7648                     | 0.6910 |  |
| TrivialAugment                          | 0.7660 | 0.7660                     | 0.7008 |  |
| AugMix                                  | 0.7413 | 0.7413                     | 0.7018 |  |
| N=2 (Two Augmentations Allowed)         |        |                            |        |  |
| RandAugment                             | 0.7636 | 0.7512                     | 0.7172 |  |
| LLM-Guided (ChatGPT)                    | 0.7784 | 0.7673                     | 0.7373 |  |
| LLM-Guided (Gemini)                     | 0.7611 | 0.7748                     | 0.7359 |  |
| LLM-Guided (DeepSeek)                   | 0.7611 | 0.7748                     | 0.7373 |  |
| LLM-Guided (Adaptive, @1 Epoch)         | 0.7883 | 0.7772                     | 0.7491 |  |
| LLM-Guided (Adaptive, @5 Epochs) 0.7676 |        | 0.7712                     | 0.7491 |  |
| N=3 (Three Augmentations Allowed)       |        |                            |        |  |
| RandAugment                             | 0.7561 | 0.766                      | 0.7265 |  |
| LLM-Guided (ChatGPT)                    | 0.7748 | 0.7847                     | 0.7454 |  |
| LLM-Guided (Gemini)                     | 0.7587 | 0.7834                     | 0.7468 |  |
| LLM-Guided (DeepSeek)                   | 0.7611 | 0.7748                     | 0.7447 |  |
| LLM-Guided (Adaptive, @1 Epoch)         | 0.7919 | 0.7852                     | 0.7605 |  |
| LLM-Guided (Adaptive, @5 Epochs) 0.7892 |        | 0.7834                     | 0.7576 |  |

### 6 Conclusion

In this work, we introduced LLM-driven augmentation optimization strategies that dynamically refine data augmentation policies based on model performance feedback. Unlike traditional methods that rely on static or search-based approaches, our framework leverages the reasoning capabilities of large language models to iteratively enhance augmentation strategies.

Experimental evaluations across multiple datasets demonstrate that LLMdriven augmentation consistently outperforms conventional augmentation techniques, including state-of-the-art methods such as RandAugment, TrivialAugment, and AugMix. Our results highlight that adaptive augmentation, integrated into the training, improves generalization and model robustness across diverse medical imaging datasets. Furthermore, our adaptive approach reduces computational costs compared to search based methods making it a viable alternative.

Our findings suggest that LLM-driven augmentation selection represents a promising step toward more adaptive, efficient, and automated deep learning workflows. By leveraging the contextual knowledge and iterative reasoning of LLMs, we move beyond rigid augmentation policies and toward intelligent, dataaware augmentation strategies that evolve alongside training.

Beyond performance improvements, our work underscores the broader potential of LLMs in optimizing various aspects of deep learning pipelines. Effectiveness of LLMs in augmentation policy optimization show the potential of using LLMs for further training optimization tasks. Future work could explore integrating of vision-language models (VLMs) to provide deeper semantic insights into dataset properties, potentially leading to even more effective augmentation strategies.

## 7 Acknowledgments

This work has been supported by Middle East Technical University Scientific Research Projects Coordination Unit under grant number ADEP-704-2024-11486.

# References

- <span id="page-12-0"></span>1. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: AutoAugment: Learning augmentation strategies from data. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 113–123 (2019). <https://doi.org/10.1109/CVPR.2019.00020>
- <span id="page-12-1"></span>2. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: RandAugment: Practical automated data augmentation with a reduced search space. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 3008–3017. IEEE (2020) <https://doi.org/10.1109/CVPRW50498.2020.00359>
- <span id="page-12-2"></span>3. Müller, S.G., Hutter, F.: TrivialAugment: Tuning-free yet state-of-theart data augmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 754–762. IEEE (2021) <https://doi.org/10.1109/ICCV48922.2021.00081>
- <span id="page-13-0"></span>14 A. Duru et al.
- 4. Zhang, M., Desai, N., Bae, J., Lorraine, J., Ba, J.: Using large language models for hyperparameter optimization. In: NeurIPS 2023 Workshop on Foundation Models for Decision Making (2023)
- <span id="page-13-1"></span>5. Hendrycks, D., Mu, N., Cubuk, E.D., Zoph, B., Gilmer, J., Lakshminarayanan, B.: AugMix: A simple method to improve robustness and uncertainty under data shift. In: International Conference on Learning Representations (ICLR) (2020)
- <span id="page-13-2"></span>6. Liu, S., Chen, C., Qu, X., Tang, K., Ong, Y.S.: Large Language Models as Evolutionary Optimizers. arXiv preprint [arXiv:2310.19046](http://arxiv.org/abs/2310.19046) (2023). <https://doi.org/10.48550/arXiv.2310.19046>
- <span id="page-13-3"></span>7. Liu, F., Tong, X., Yuan, M., Zhang, Q.: Algorithm Evolution Using Large Language Model. arXiv preprint [arXiv:2311.15249](http://arxiv.org/abs/2311.15249) (2023). <https://doi.org/10.48550/arXiv.2311.15249>
- <span id="page-13-4"></span>8. Zheng, M., Su, X., You, S., Wang, F., Qian, C., Xu, C., Albanie, S.: Can GPT-4 Perform Neural Architecture Search? arXiv preprint [arXiv:2304.10970](http://arxiv.org/abs/2304.10970) (2023). <https://doi.org/10.48550/arXiv.2304.10970>
- <span id="page-13-5"></span>9. Chen, A., Dohan, D., So, D.: EvoPrompting: Language Models for Code-Level Neural Architecture Search. In: Advances in Neural Information Processing Systems (NeurIPS), vol. 36, pp. 7787–7817 (2023)
- <span id="page-13-6"></span>10. Zhang, S., Gong, C., Wu, L., Liu, X., Zhou, M.: AutoML-GPT: Automatic Machine Learning with GPT. arXiv preprint [arXiv:2305.02499](http://arxiv.org/abs/2305.02499) (2023). <https://doi.org/10.48550/arXiv.2305.02499>
- <span id="page-13-7"></span>11. Ding, B. et al. (2024) 'Data augmentation using LLMS: Data Perspectives, learning paradigms and challenges', Findings of the Association for Computational Linguistics ACL 2024, pp. 1679–1705. doi:10.18653/v1/2024.findings-acl.97.
- <span id="page-13-8"></span>12. Li, Y., Bonatti, R., Abdali, S., Wagle, J., Koishida, K.: Data Generation Using Large Language Models for Text Classification: An Empirical Case Study. arXiv preprint [arXiv:2407.12813](http://arxiv.org/abs/2407.12813) (2024). <https://doi.org/10.48550/arXiv.2407.12813>
- <span id="page-13-10"></span><span id="page-13-9"></span>13. OpenAI: ChatGPT (GPT-4o). https://chat.openai.com/ (accessed: Apr 2025)
- 14. Google: Gemini 1.5 via Google AI Studio. https://gemini.google.com/ (accessed: Apr 2025)
- <span id="page-13-12"></span><span id="page-13-11"></span>15. DeepSeek: DeepSeek AI. https://deepseek.ai/ (accessed: Apr 2025)
- 16. Torchvision: Image and video datasets and models for torch deep learning. https://github.com/pytorch/vision (accessed: Apr 2025)
- <span id="page-13-13"></span>17. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778. IEEE (2016)
- <span id="page-13-14"></span>18. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: MobileNetV2: Inverted residuals and linear bottlenecks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4510–4520. IEEE (2018)
- <span id="page-13-15"></span>19. Huang, G., Liu, Z., Maaten, L.v.d., Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2261–2269. IEEE (2017)
- <span id="page-13-16"></span>20. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16×16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (ICLR) (2021)
- <span id="page-13-17"></span>21. Karthik, M., Dane, S.: APTOS 2019 Blindness Detection. Kaggle. https://www.kaggle.com/competitions/aptos2019-blindness-detection/data (accessed: Apr 2025)
- <span id="page-14-0"></span>22. Mittal, B.: Melanoma Cancer Image Dataset. Kaggle. https://www.kaggle.com/datasets/bhaveshmittal/melanoma-cancer-dataset (accessed: Apr 2025)
- <span id="page-14-1"></span>23. Kabir, F.: Alzheimer Parkinson Diseases 3 Class. Kaggle. https://www.kaggle.com/datasets/farjanakabirsamanta/alzheimer-diseases-3-class (accessed: Apr 2025)
- <span id="page-14-2"></span>24. Polat, G., Kani, H.T., Ergenc, I., Ozen Alahdab, Y., Temizel, A., Atug, O.: Improving the computer-aided estimation of ulcerative colitis severity according to Mayo endoscopic score by using regression-based deep learning. In: Inflammatory Bowel Diseases, vol. 29(9), pp. 1431–1439 (2023)
- <span id="page-14-3"></span>25. Polat, G., Kani, H.T., Ergenc, I., Ozen Alahdab, Y., Temizel, A., Atug, O.: Labeled Images for Ulcerative Colitis (LIMUC) Dataset. Version 1. Zenodo (2022). <https://doi.org/https://doi.org/10.5281/zenodo.5827695>