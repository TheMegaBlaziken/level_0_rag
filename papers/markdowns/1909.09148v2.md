# Data Augmentation Revisited: Rethinking the Distribution Gap between Clean and Augmented Data

Zhuoxun He <sup>1</sup> Lingxi Xie <sup>2</sup> Xin Chen <sup>3</sup> Ya Zhang <sup>1</sup> Yanfeng Wang <sup>1</sup> Qi Tian <sup>2</sup>

<sup>1</sup> Shanghai Jiao Tong University <sup>2</sup>Huawei Noahs Ark Labc <sup>3</sup>Tongji University {zhuoxun, ya zhang, wangyanfeng}@sjtu.edu.cn 198808xc@gmail.com

chenxin061@hotmail.com tian.qi1@huawei.com

# Abstract

*Data augmentation has been widely applied as an effective methodology to improve generalization in particular when training deep neural networks. Recently, researchers proposed a few intensive data augmentation techniques, which indeed improved accuracy, yet we notice that these methods augment data have also caused a considerable gap between clean and augmented data. In this paper, we revisit this problem from an analytical perspective, for which we estimate the upper-bound of expected risk using two terms, namely, empirical risk and generalization error, respectively. We develop an understanding of data augmentation as regularization, which highlights the major features. As a result, data augmentation significantly reduces the generalization error, but meanwhile leads to a slightly higher empirical risk. On the assumption that data augmentation helps models converge to a better region, the model can benefit from a lower empirical risk achieved by a simple method, i.e., using less-augmented data to refine the model trained on fully-augmented data. Our approach achieves consistent accuracy gain on a few standard image classification benchmarks, and the gain transfers to object detection.*

# 1. Introduction

With the availability of powerful computational resources nowadays, it is possible to train very deep neural networks that have been verified to achieve good performance in a wide range of computer vision tasks including image classification [\[26,](#page-8-0) [37,](#page-9-0) [19,](#page-8-1) [21\]](#page-8-2), object detection [\[33,](#page-9-1) [29\]](#page-8-3), semantic segmentation [\[2,](#page-8-4) [5\]](#page-8-5), *etc*. On the other hand, complicated models with tens of millions of parameters are often difficult to train with limited training data, and many techniques [\[36,](#page-9-2) [23\]](#page-8-6) have been proposed to stable training and improve generalization.

As a common training trick, data augmentation is de-

 - - - - 

<span id="page-0-0"></span>Figure 1. Cross-entropy loss curves in training *PreActResNet-18* on CIFAR100, with two popular data augmentation methods, namely, Mixup and AutoAugment. Mind the gap between clean and augmented *training* data, while the model is trained on augmented data during whole conventional training process. As indicated by ?, our approach is to train the model on augmented data in the first 400 epochs, and on original data in the final 50 epochs, which helps to achieve lower testing losses, *i.e.*, higher accuracy.

signed to increase the diversity of training data without actually collecting new data. Essentially, it is initially driven by the fact that slight modification on an image will not change the high-level semantics of an image. Standard data augmentation methods used in deep network training involves scale and aspect ratio distortions, random crops, and horizontal flips. However, several sophisticated methods changing the original image a lot, such as Mixup [\[46\]](#page-9-3), Cutout [\[10\]](#page-8-7), and Cutmix [\[43\]](#page-9-4), were proposed to improve the generalization of models. Recently, researchers also used AutoML to learn an optimal set of parameters for augmentation [\[8,](#page-8-8) [28\]](#page-8-9), and achieved state-of-the-art performance in a few image classification benchmarks.

With these complicated data augmentation methods being proposed, quite high training loss, even higher than the testing loss, can be got at the end of the training stage. The question of why models with such high training loss can generalize well remains mostly uncovered. This paper delves deep into this issue by reformulating the upperbound of expected risk for models trained with data augmentation, which implies it is that the empirical risk on original data, but not augmented data, and the generalization error impact the performance of models. Then the effectiveness of intensive data augmentation is equivalent to how to guarantee the convergence of the empirical risks when a large distribution gap between clean and augmented data. By partitioning features into major and minor features, we develop an understanding of intensive data augmentation as regularization on the minor features to guarantee the convergence of the empirical risk on clean data. Even so, the model trained on augmented data has a slightly higher empirical risk in general while generalizes better.

Motivated by this observation, we propose an effective training strategy to reduce the empirical risk and improve performance further on the assumption that models trained with data augmentation arrive in a better region. Compared to conventional training strategy in which augmented data are used during the whole process, our approach is to switch off these intensive data augmentation methods at the last few training epochs. This is to confirm that there is no intensive distribution gap between clean and augmented data at the end of the training stage, which we believe is a safe opportunity of transferring the trained model to the testing scenario. As shown in Figure [1,](#page-0-0) by plotting the curves during the training process, we observe that refining the models without intensive data augmentation decreases the empirical risk further, and meanwhile, a lower testing loss is achieved.

We evaluate our training strategy in a few popular image classification benchmarks including CIFAR10, CIFAR100, Mini-ImageNet, and ImageNet. In all these datasets, the refined model achieves consistent, sometimes significant, accuracy gain, with computational costs in training and testing remain unchanged. Besides, we fine-tune the pre-trained models on ImageNet on PascalVOC following the standard strategy. The pre-trained models with our approach show a significant improvement in the detection task, while it was believed these intensive data augmentation methods were inappropriate for detection. Beyond image recognition experiments, our research sheds light on a new optimization method, which involves using augmented data followed by clean data to optimize global and local properties, and is worth being further studied in the future.

# 2. Related Work

The statistical learning theory [\[40\]](#page-9-5) suggested that the generalization of the learning model was characterized by the capacity of the model and the amount of training data. Data augmentation methods have been commonly adopted in deep learning by creating artificial data to improve model robustness, especially in image classification [\[26,](#page-8-0) [37\]](#page-9-0). Basic image augmentation methods mainly include elastic distortions, scale, translation, and rotation [\[7,](#page-8-10) [35\]](#page-9-6). For natural image datasets, random cropping and horizontal flipping are common. Besides, scaling hue, saturation, and brightness also improve performance [\[21\]](#page-8-2). These methods were usually designed by retaining semantic information unchanged while generating various images.

Recently, some methods to synthesize training data were proposed. Mixup [\[46,](#page-9-3) [38\]](#page-9-7) combined two samples linearly in pixel level, where the target of the synthetic image was the linear combination of one-hot label encodings. There are a few variants of Mixup [\[17,](#page-8-11) [41\]](#page-9-8), as well as a recent effort named Cutmix [\[43\]](#page-9-4) which combined Mixup and Cutout [\[10\]](#page-8-7) by cutting and pasting patches. Compared to moderate data augmentation, images generated by Mixup and its variants are much more different from original data and unreal to human perception.

For different datasets, the best augmentation strategies can be different. Learning more sensible data augmentations for specific datasets has been explored [\[27,](#page-8-12) [32,](#page-9-9) [39\]](#page-9-10). AutoAugment [\[8\]](#page-8-8) used a search algorithm based on reinforcement learning to find the best data augmentation strategies from a discrete search space that is composed of augmentation operations. Population Based Augmentation [\[24,](#page-8-13) [22\]](#page-8-14) proposed to search for augmentation policy schedules instead of fixed augmentation policies. Furthermore, Fast AutoAugment [\[28\]](#page-8-9) largely accelerated AutoAugment by avoiding training on each policy. More recently, augmentation in the deep feature space by intra-class covariance matrices was efficiently implemented with a new loss function [\[42\]](#page-9-11).

Compared to numerous works in the empirical aspect, relatively few theoretical works explained the effectiveness of data augmentation, especially in deep learning. Bishop [\[3\]](#page-8-15) showed that training with noise had the effect of regularization in expectation. Rajput *et al*. analyzed the performance of data augmentation through the lens of margin [\[31\]](#page-8-16). Dao *et al*. [\[9\]](#page-8-17) connected data augmentation with kernels, and show that data augmentation as feature averaging and variance regularization. In this paper, we take a different path to analyze the impacts of data augmentation from the upper bound of expected risk.

The generalization ability of DNN is also greatly affected by optimization methods [\[18,](#page-8-18) [45,](#page-9-12) [25\]](#page-8-19). Even with some regularization methods [\[36,](#page-9-2) [23\]](#page-8-6), the objective function can not converge to the global minimum. The DNN tends to learn low-level features firstly which can be easier to learn for machines such as size, color, and texture [\[13,](#page-8-20) [14,](#page-8-21) [15\]](#page-8-22). Brendel and Bethge [\[4\]](#page-8-23) found that the decision-making behavior of current neural network architectures was mainly based on relatively weak and local features, and high-level features, *e.g.*, shape, that can better improve model robustness were not sufficiently learned. Recent works [\[1,](#page-8-24) [16\]](#page-8-25) found that regularization does not need to exist in the whole training process, which is compatible with our work.

# 3. Data Augmentation Revisited

In this section, we revisit data augmentation by identifying the loss terms that compose the upper-bound of the expected risk. Then, we provide an explanation on how data augmentation works as regularization, followed by a simple and practical approach to improve the performance of models trained by data augmentation.

#### 3.1. Statistical Learning with Data Augmentation

Let X and Y be the data and label spaces, respectively. Each sample is denoted by (x, y) ∼ P, where P is the joint distribution of data and label. Consider a regular statistical learning process. The goal of learning is to find a function f : X 7→ Y which minimizes the expected value of a pre-defined loss term, `(f(x), y), over the distribution of P. This is known as the *expected risk*:

$$
R(f|\mathcal{P}) = \int \ell(f(x), y) \mathrm{d}P(x, y).
$$

However, the data distribution is unknown in practical situations. Therefore, a common solution is the *empirical risk minimization* (ERM) principle [\[40\]](#page-9-5), which optimizes an *empirical risk* in a training dataset {(xn, yn)} N <sup>n</sup>=1 that mimics the data distribution:

$$
\hat{R}(f|\mathcal{P}) = \frac{1}{N} \sum_{n=1}^{N} \ell(f(x_n), y_n).
$$

The accuracy of estimating the expected risk goes up with N, the amount of training data. In practice, especially when there is limited data, increasing N with data augmentation is a popular and effective solution. It defines a function g ∈ G : X 7→ X , which generates 'new' data x˜<sup>n</sup> with a combination of operations on x<sup>n</sup> – since these operations do not change the semantics, x˜<sup>n</sup> naturally shares the same label with xn, *i.e.*, y˜<sup>i</sup> = y<sup>n</sup> Note that data augmentation actually changes the distribution of P and we denote the new distribution by Paug, which is to say, the goal has been changed from minimizing Rˆ(f|P)) to minimizing Rˆ(f|Paug):

$$
\hat{R}(f|\mathcal{P}_{\text{aug}}) = \frac{1}{N} \sum_{n=1}^{N} \ell(f(\tilde{x}_n), \tilde{y}_n)).
$$

The strategy of data augmentation can be conservative, in which only a small number of 'safe' operations such as horizontal flip and cropping are considered, or aggressive, in which 'dangerous' or a series of operations can be used to cause significant changes on image appearance. Here we briefly introduce several aggressive data augmentation methods, all of which were proposed recently and verified effectiveness in image classification tasks.

For (x, y) and (x 0 , y<sup>0</sup> ) ∼ P, the generated data (˜x, y˜) by Mixup [\[46\]](#page-9-3) is obtained as follows:

$$
\tilde{x} = \lambda \cdot x + (1 - \lambda) \cdot x', \ \tilde{y} = \lambda \cdot y + (1 - \lambda) \cdot y', \ \ (1)
$$

where λ ∼ Beta(γ, γ) and γ is the combination ratio (a hyperparameter). Manifold Mixup [\[41\]](#page-9-8) randomly performs the linear combination at an eligible layer that can be input layer and some hidden layer.

Cutmix [\[43\]](#page-9-4) combines CutOut [\[10\]](#page-8-7) and Mixup, providing a patch-wise, weighted overlay by:

$$
\tilde{x} = \mathbf{M} \odot x + (\mathbf{1} - \mathbf{M}) \odot x', \ \ \tilde{y} = \lambda \cdot y + (1 - \lambda) \cdot y', \ \ (2)
$$

where M is a binary mask indicating the positions of dropout and fill-in,  denotes element-wise multiplication and λ ∼ Beta(1, 1) is the combination ratio.

Instead of manually designing data augmentation tricks, AutoAugment [\[8\]](#page-8-8) applied an automated way of learning parameters for augmentation. A large space with different kinds of operations was pre-defined, and the policy of augmentation was optimized with reinforcement learning. This is to say, the function g applied to each sample for augmentation can be even more complicated compared to those in conventional approaches.

#### 3.2. Rethinking the Mechanism of Augmentation

According to VC theory [\[40\]](#page-9-5), the consistency and generalization of ERM principle have been justified in theoretical aspect. Consider a binary classifier f ∈ F, which has finite VC-Dimension |F|VC. With probability 1 − δ, a upper bound of the *expected* risk is formulated by

<span id="page-2-1"></span>
$$
R(f) \leq \hat{R}(f) + O\left(\left(\frac{|\mathcal{F}|_{\text{VC}} - \log \delta}{N}\right)^{\alpha}\right),\qquad(3)
$$

where <sup>1</sup> <sup>2</sup> <sup>6</sup> <sup>α</sup> <sup>6</sup> <sup>1</sup>. In the simply (separable) case, <sup>α</sup> = 1, and in the complex (non-separable) case, α = 1 2 .

Based on this theory, data augmentation creates sensible data to increase the training data size. Assume there is a finite number of augmented training data. For the model trained over the augmented data distribution Paug, we have

<span id="page-2-0"></span>
$$
R(f_{\text{aug}}|\mathcal{P}_{\text{aug}}) \leq \hat{R}(f_{\text{aug}}|\mathcal{P}_{\text{aug}}) + O\left(\left(\frac{|\mathcal{F}|_{\text{VC}} - \log \delta}{M \times N}\right)_{(4)}^{\alpha}\right),
$$

where M is a finite constant.

Although the generalization error in Equation [\(4\)](#page-2-0) is smaller than that in Equation [\(3\)](#page-2-1), there is a difference between other risk terms. Note P ⊆ Paug, and Paug can be more difficult to learned. Suppose that

$$
R(f_{\text{aug}}|\mathcal{P}_{\text{aug}}) - R(f_{\text{aug}}|\mathcal{P}) = \varepsilon_1 \geq 0,
$$
  

$$
\hat{R}(f_{\text{aug}}|\mathcal{P}_{\text{aug}}) - \hat{R}(f_{\text{aug}}|\mathcal{P}) = \varepsilon_2 \geq 0.
$$

Thus, the inequality is reformulated by

<span id="page-3-0"></span>
$$
R(f_{\text{aug}}|\mathcal{P}) \leq \hat{R}(f_{\text{aug}}|\mathcal{P}) + O\left(\left(\frac{|\mathcal{F}|_{\text{VC}} - \log \delta}{M \times N}\right)^{\alpha}\right) + \varepsilon,
$$
\n(5)

where ε = ε<sup>2</sup> − ε1. Since both ε<sup>1</sup> and ε<sup>2</sup> are caused by the distribution gap between P and Paug, it is reasonable to assume ε is sufficiently small.

As a result, Equation [\(5\)](#page-3-0) highlights that the benefits of learning with data augmentation mainly arise due to two factors:

# 1) *the empirical risk* Rˆ(faug|P) *being small,*

2) *the amount of augmented data being large,*

The conclusions provide a deep insight, *i.e.*, it is the value of Rˆ(faug|P) but not Rˆ(faug|Paug) impacts the effectiveness of data augmentation.

Since the model is trained on Rˆ(faug|Paug), factor 1) requires the consistency between Rˆ(faug|P) and Rˆ(faug|Paug). If no distribution gap exists, *i.e.*, P = Paug, the consistency is guaranteed. Unfortunately, there is some kind of trade-off between the amount of augmented data and the distribution gap. Recent augmentation methods that generate many augmented images by changing images much in appearance can lead to an intensive distribution gap, such as Mixup, AutoAugment and so on. Intuitively, such an intensive distribution gap would greatly impact the convergence of Rˆ(faug|P). However, previous empirical results demonstrate their effectiveness and verify the convergence of Rˆ(faug|P).

#### <span id="page-3-2"></span>3.3. Convergence of the Empirical Risk

The effectiveness of *intensive* data augmentation can be described as the following question:

*How can one guarantee the consistency between* Rˆ(faug|P) *and* Rˆ(faug|Paug)*, when a large distribution gap between clean and augmented data exists?*

Let H ⊆ R <sup>D</sup> and h(x) = (h1(x), h2(x), . . . , hD(x))<sup>&</sup>gt; denote latent space (feature space) and the feature vector of x respectively. Suppose a perfect classifier is given by the true conditional distribution is Q(y|h). We partition the features into major features and minor features by information gain. For major features, the possibility density q(y|hd) concentrates on some point mass. For minor features, the possibility density q(y|hd) is relatively uniform.

To simplify the question, we explore it in conjunction with a linear softmax classifier W = [w1, w2, ..., w<sup>C</sup> ] ∈ R <sup>D</sup>×<sup>C</sup> , where C is the number of categories. The predicted label is given by yˆ = [p1, p2, ..., p<sup>C</sup> ]:

$$
p_i = \frac{\exp(\mathbf{w}_i^{\top} \mathbf{h})}{\sum_{j=1}^{C} \exp(\mathbf{w}_j^{\top} \mathbf{h})} = \frac{1}{\sum_{j=1}^{C} \exp((\mathbf{w}_j - \mathbf{w}_i)^{\top} \mathbf{h})}
$$
(6)

In the objective function on original data Rˆ = 1 N P<sup>N</sup> <sup>n</sup>=1 ` W>h (xn), y<sup>n</sup> , the related terms with some feature h<sup>d</sup> are (wi,d − wj,d)h<sup>d</sup> for 1 6 i, j 6 C. When h<sup>d</sup> is a minor feature, the variation of h<sup>d</sup> should not change the results much, which requires the weights |wi,d − wj,d| → 0. Further, this can be reformulated as wi,d ≈ wj,d for 1 6 i, j 6 C.

For intensive data augmentation g ∼ G which brings a large distribution gap between clean and augmented data, we have ||h(g(x)) − h(x)||<sup>2</sup> > 0, where <sup>0</sup> > 0 is a relative large value. Then, objective function with data augmentation is given as follow:

$$
\hat{R}_{\text{aug}} = \frac{1}{N} \sum_{n=1}^{N} \mathbb{E}_{g \sim \mathcal{G}} \left[ \ell \left( \mathbf{W}^{\top} \mathbf{h} \left( g(x_n) \right), y_n \right) \right].
$$

As the analysis in [\[9\]](#page-8-17), we expand each term of the objective function with data augmentation using Taylor expansion:

<span id="page-3-1"></span>
$$
\mathbb{E}_{g \sim \mathcal{G}}[\ell(\mathbf{W}^{\top}\mathbf{h}(g(x)), y)] =
$$

$$
\ell(\mathbf{W}^{\top}\bar{\mathbf{h}}, y) + \frac{1}{2} \mathbb{E}_{g \sim \mathcal{G}}[\Delta^{\top}\mathbf{H}(\tau, y)\Delta]
$$
(7)

where h¯ = Eg∼G [h(g(x))], ∆ = W<sup>&</sup>gt; h¯ − h(g(x)) and H is the Hessian matrix. Dao *et al*. [\[9\]](#page-8-17) proposed data augmentation as feature averaging and variance regularization. Here we further discuss the effects of intensive data augmentation to emphasise the regularization on the corresponding weights of minor features.

For cross-entropy loss with softmax, the H is positive semi-definite and independent of y. Then, the second-order term in Equation [\(7\)](#page-3-1) requires that wi,d → 0 for all i, if the variance of hd(g(x)) is large. Since the intensive data augmentation must cause large variances of some features and such regularization on wi,k is not appropriate for major features, a reasonable solution is given:

1) *for major features,* |hd(g(x)) − hd(x)| < ζ1, 2) *for minor features,* |hd(g(x)) − hd(x)| > 1, *where* ζ<sup>1</sup> > 0 *is a small value, and* <sup>1</sup> > ζ1*.*

These two inequalities highlight that the major features that are important to classify should be preserved as much and the minor features can be changed a lot after data augmentation. Comparing that Rˆ restricts wi,d ≈ wj,d for 1 6 i, j 6 C, Rˆ aug directly restricts wi,d → 0 for minor features hd. While maintaining the optimized objective consistent, the intensive data augmentation also regularizes the corresponding weights of minor features.

This is consistent with the empirical results [\[8\]](#page-8-8), whose augmentation policies are selected by Rˆ on a validation set. For numeral recognition, the transformation invert is successful to be used, even though the numeral specific color is changed to that not involved in the original dataset. On

![](_page_4_Figure_0.jpeg)

**Caption:** Contour maps illustrating empirical risks Rˆ(f|P) and Rˆ(f|Paug) in function space F. The left map shows Rˆ(f|P) with a global minimum marked by '×', while the right map depicts Rˆ(f|Paug). The comparison highlights how data augmentation influences convergence and risk minimization in model training.

Figure 2. The contour maps of the empirical risks on the function space F. Left: Rˆ(f|P). Right:Rˆ(f|Paug), where the "×" denotes the global minimum of Rˆ(f|P).

the other hand, the transformation horizontal flipping used commonly in natural images is never used in numeral recognition. It is consistent with prior knowledge that the relative color of the numeral and its background and the direction of the numeral are major features, but the specific color of numeral is a minor feature.

#### 3.4. Refined Data Augmentation

As discussed in the last subsection, intensive data augmentation methods highlight the major features, while losing some minor features. In such way, data augmentation keeps the consistency of Rˆ(f|P) and Rˆ(f|Paug) and regularize the corresponding weights of minor features. From this perspective, data augmentation as a regularization scheme imposing some constraints on the function space F by prior knowledge, which forces the model to focus on the major features.

Figure [2](#page-4-0) depicts the effect of data augmentation on changing the empirical risk Rˆ(f) on the function space F. Data augmentation helps the neural network learn major features, which reduces the number of local minimums and keeps the direction of convergence relatively consistent. Compared with Rˆ(f|Paug), Rˆ(f|P) can easily converge to a local minimum that is far away from the global minimum.

To go a step further, since the analysis based on Equation [\(7\)](#page-3-1) has introduced some approximation, the optimal function f † of Rˆ(f|Paug) is not guaranteed to be a minimum of Rˆ(f|P). Associated the empirical results, Rˆ(f ∗ |P) 6 Rˆ(f † |P) in general, where f ∗ is optimized on Rˆ(f|P), while f † generalizes better. Please refer to Table [1](#page-5-0) and Table [2](#page-5-1) for experimental verifications. Therefore, it is reasonable to believe that f † converges to a better region close to the global minimum.

Motivated by this, we propose a method called refined data augmentation, *i.e.*, refining the models without intensive data augmentation at the end of the training stage. On one hand, the distribution gap between clean and augmented data obstructs Rˆ(f|P) to converge further, and a small empirical risk can benefit the performance of models. On the other hand, the relatively minor features ignored by intensive data augmentation are also informative for classifying.

# 4. Experiments

#### 4.1. Results on CIFAR10 and CIFAR100

#### • Dataset and Settings

<span id="page-4-0"></span>The CIFAR10 both consist of 60,000 32 × 32 color images in 10 classes, where 5,000 training images per class as well as 1,000 testing images per class. The CI-FAR100 contains 500 training images and 100 testing images per class in a total of 100 classes. On CIFAR10 and CIFAR100, we train both two variants of residual networks [\[19\]](#page-8-1), PreActResNet-18 [\[20\]](#page-8-26) and WideResNet-28- 10 [\[44\]](#page-9-13), and a stronger backbones: Shake-Shake [\[12\]](#page-8-27). We partition the training procedure into two stages: training with intensive data augmentation and refinement.

For the stage with intensive data augmentation, we train PreActResNet-18 and WideResNet-28-10 on a single GPU using PyTorch for 400 epochs on training set with a minibatch size of 128. For PreActResNet-18, the learning rate starts at 0.1 and is divided by 10 after 150, 275 epochs, and weight decay is set to be 10<sup>−</sup><sup>4</sup> . For WideResNet-28- 10, the learning rate starts at 0.1 and is divided by 5 after 120, 240, 320 epochs except for using a Cosine learning rate decay [\[30\]](#page-8-28) for AutoAugment, and weight decay is set to be 5 × 10<sup>−</sup><sup>4</sup> . Following the settings in Zhang *et al*. [\[46\]](#page-9-3) and Cubuk *et al*. [\[8\]](#page-8-8), we set dropout rate to be 0.3 for WideResNet-28-10 with AutoAugment, and 0 in other experiments. For Shake-Shake model, we train the model on 2 GPUs for 1800 epochs with a mini-batch size of 128. The learning rate starts at 0.01 with Cosine decay, and weight decay is set to be 10<sup>−</sup><sup>3</sup> .

All intensive data augmentation methods are incorporated with standard data augmentation: random crops, horizontal flips with a probability of 50%. For the coefficient λ ∼ Beta(γ, γ) in Mixup and Manifold Mixup, γ = 1. Following the paper [\[43\]](#page-9-4), Cutmix is implemented with a probability of 50% during training. For AutoAugment, we first apply the standard data augmentation methods, then apply the AutoAugment policy, then apply Cutout with 16 × 16 pixels [\[10\]](#page-8-7) following Cubuk *et al*. [\[8\]](#page-8-8). Note that we directly use the AutoAugment policies reported in [\[8\]](#page-8-8).

We refine the models without these intensive data augmentation methods. Since the standard data augmentation methods bring a small distribution gap between clean and augmented data, we preserve the standard data augmentation when refining, which will be discussed in detail later. For PreActResNet-18 and WideResNet-28-10, the models are refined for 50 epochs, and for Shake-Shake, the models are refined for 200 epochs. During refinement, the learning rate keeps a small value. For the models trained with the step-wise learning rate decay, the learning rate is set to be the same as that in the final epoch of the last stage, and for the models trained with the Cosine learning rate decay, the learning is adjusted to a reasonably small value.

| Model                  | Augmentation |       | w/ Refinement |       |
|------------------------|--------------|-------|---------------|-------|
|                        | C10          | C100  | C10           | C100  |
| PreActResNet-18        |              |       |               |       |
| Standard               | 94.63        | 75.79 | 94.48         | 76.06 |
| Mixup                  | 95.92        | 78.95 | 96.26         | 80.85 |
| Manifold Mixup         | 95.81        | 80.19 | 96.07         | 81.39 |
| Cutmix                 | 96.21        | 79.75 | 96.38         | 80.46 |
| AutoAugment            | 96.02        | 79.33 | 96.20         | 80.09 |
| WideResNet-28-10       |              |       |               |       |
| Standard               | 96.11        | 81.15 | -             | -     |
| Mixup                  | 97.05        | 82.11 | 97.52         | 84.25 |
| Manifold Mixup         | 97.13        | 83.02 | 97.32         | 84.77 |
| Cutmix                 | 97.11        | 82.75 | 97.24         | 83.40 |
| AutoAugment            | 97.59        | 83.85 | 97.73         | 85.24 |
| Shake-Shake (26 2x96d) |              |       |               |       |
| Standard               | 97.00        | 82.87 | -             | -     |
| Mixup                  | 97.80        | 84.22 | 98.02         | 85.19 |
| Cutmix                 | 97.77        | 84.51 | 97.76         | 84.64 |
| AutoAugment            | 98.02        | 85.62 | 98.05         | 86.13 |

Table 1. Classification accuracy (%) on CIFAR10 and CIFAR100.

#### • Quantitative Results

In Table [1,](#page-5-0) the mean values are calculated in three independent experiments by the median of the last 10 epochs in each experiment for PreActResNet18 and WideResNet-28-10. Our methods show a consistent improvement with different data augmentation methods on various backbones. Especially for Mixup, refining the models with standard data augmentation improves the accuracy significantly on CIFAR100. While the networks searched by P-DARTS [\[6\]](#page-8-29) achieves 97.81% test accuracy after being trained with AutoAugment for 600 epochs on CIFAR10, our method achieves a 97.97% accuracy with 550 epochs using AutoAugment and 50 epochs for refinement.

Specially, we also conduct the experiments that refine the models trained with the standard data augmentation without any data augmentation methods for PreActResNet-18. There is a 0.27% accuracy gain on CIFAR100, while a 0.15% drop on CIFAR10. On the other hand, for other intensive augmentation methods, removing all data augmentations during refinement shows no significant difference with the experiments that preserve the standard data augmentations on CIFAR100. Therefore, we suggest to preserve the standard data augmentation during refinement.

#### • Qualitative Analysis

In Table [2,](#page-5-1) cross-entropy (CE) losses on clean and augmented data are calculated to quantify the distribution gap between clean and augmented data to some extent. During refinement, the Rˆ aug is calculated with standard augmented data, which reflects a small distribution gap with clean data. Mixup brings the most significant difference between clean and augmented data, which can explain the

<span id="page-5-1"></span>

| Method         | Augmentation |     | w/ Refinement |     |
|----------------|--------------|-----|---------------|-----|
|                | Rˆaug        | Rˆ  | Rˆaug         | Rˆ  |
| Standard       | 3.3          | 1.1 | -             | 1.0 |
| Mixup          | 1356         | 98  | 4.7           | 2.4 |
| Manifold Mixup | 1253         | 67  | 4.2           | 2.2 |
| Cutmix         | 785          | 14  | 1.9           | 0.8 |
| AutoAugment    | 245          | 0.9 | 1.5           | 0.8 |

<span id="page-5-0"></span>Table 2. Cross-entropy losses (×10<sup>−</sup><sup>3</sup> ) of augmented and clean training data on CIFAR100 for *PreActResNet-18*. Rˆaug and Rˆ repectly refer to Rˆ(f|Paug) and Rˆ(f|P).

significant improvement with refinement for Mixup. Interestingly, AutoAugment achieves a low CE loss on cleaning training data, yet refinement still works well to achieve a lower CE loss on clean data. These results suggest that data augmentation indeed helps the model converge to a better region, which can be hard to be arrived for directly training on clean data, and the gap between clean and augmented data obstructs the further convergence.

Since certain directions in the deep feature space imply meaningful semantic information [\[41,](#page-9-8) [42\]](#page-9-11), we compute a singular value decomposition for the intra-class covariance matrix and the covariance matrix of all classes in the penultimate layer of PreActResNet-18 on CIFAR100 to analyze the variation of representations learned with different augmentation methods and refinement. The singular values are plotted and ordered form largest to smallest in Figure [3.](#page-5-2) Here we only present the results of Mixup, since Manifold Mixup and Cutmix perform very similarly to Mixup.

![](_page_5_Figure_11.jpeg)

**Caption:** Singular value decomposition (SVD) results for representations in the penultimate layer of PreActResNet-18 on CIFAR100. The plot shows the distribution of singular values, indicating how refinement adjusts the representation space learned with different augmentation methods, particularly Mixup and AutoAugment.

<span id="page-5-2"></span>Figure 3. SVD on the representations in the penultimate layer of PreActResNet18 (512 neurons) on CIFAR100.

We find that refinement tends to draw the distribution of singular values back to the original model to some extent for the intra-class covariance matrix. The singular values of models trained with Mixup-based methods and AutoAugment respectively are smaller and larger than the original model, while the singular values become closer to original

![](_page_6_Figure_0.jpeg)

**Caption:** Test error rates of PreActResNet-18 on CIFAR100, averaged over five runs, as Mixup is removed at various training epochs. The bars represent the range of test errors, demonstrating the impact of removing augmentation on model performance and indicating optimal timing for refinement.

Figure 4. Test error (averaged over 5 runs) of PreActResNet-18 trained on CIFAR100 when Mixup is removed at different epochs (total of 400 training epochs). The bars represent the range of test errors for each number.

values after refinement. It is interesting to find the different effects brought by Mixup-based methods and AutoAugment, which have not been noticed and researched as so far. A possible interpretation is that Mixup-based methods mainly affect the inter-class distances while AutoAugment introduces more features and induces invariance.

Especially, for the covariance matrix of all classes, the curves for intensive data augmentation become steep near the 100th largest singular value. Notably, CIFAR100 has 100 categories and the last layer is a linear softmax classifier. The representation space will be more discriminative if the rank is larger than 100. However, compared with those larger singular values, the 100th largest singular value is almost 0 for the original model, which can be difficult for the classifier to learn. Such relative enlargement of singular values near the 100th largest can explain the effectiveness of these intensive data augmentation methods. After refinement, this characteristic is still preserved while the distribution of singular values is drawn back to some extent.

#### • Ablation Studies

In previous experiments, we train models for a few more epochs to refine. Here we keep the total training epochs constant to verify the effectiveness of our method. We train PreActResNet-18 on CIFAR100 with Mixup, and the learning rate is divided by 10 at epochs 150 and 275. Figure [4](#page-6-0) shows the test error curve with different epochs, at which Mixup is removed, when a total of 400 training epochs. Especially, Mixup removal at 400 epoch means no refinement is performed. Besides, if we train models on CIFAR with intensive data augmentation for a fixed number of epochs, refining epochs will not influence results once convergence.

We also find that increasing the number of training epochs with intensive data augmentation benefits the performances after refinement, even though AutoAugment causes an obvious over-fitting on augmented data with long training epochs. Table [3](#page-6-1) shows refinement improves accuracy significantly, and suppresses the over-fitting on augmented data. For PreActResNet-18 of T<sup>1</sup> = 1000, the learning rates divided by 10 at epochs 400 and 800. For WideResNet-28-

<span id="page-6-1"></span>

| Model                     | AutoAugment | w/ Refinement |
|---------------------------|-------------|---------------|
| PreActResNet-18           |             |               |
| T1<br>= 400, T2<br>= 50   | 79.33       | 80.09         |
| T1<br>= 1000, T2<br>= 200 | 78.16       | 80.38         |
| WideResNet28-10           |             |               |
| T1<br>= 200, T2<br>= 50   | 83.73       | 84.06         |
| T1<br>= 400, T2<br>= 50   | 83.85       | 85.24         |

<span id="page-6-0"></span>Table 3. Classification accuracy (%) for different training epochs with AutoAugment on CIFAR100. T<sup>1</sup> and T<sup>2</sup> refer to the number of epochs with AutoAugment and the number of refinement epochs, respectively.

10, Cosine learning rate decay is implemented.

Moreover, we try to weaken the intensity of data augmentation gradually so that refining models from augmented data to clean data gradually. For Mixup, γ is decreased to 0 gradually. For Cutmix and AutoAugment, we decrease the probability to implement data augmentation gradually. The results show no significant difference with the earlier experiments.

#### 4.2. Results on Tiny-ImageNet

Tiny-ImageNet consists of 200 64 × 64 image classes with 500 training and 50 validation per class. We train PreActResNet-18 for 400 epochs with intensive data augmentation, and refining the models for 50 epochs. Other hyper-parameters about model training are the same as the settings in previous experiments.

The augmentation policies found by AutoAugment is searched on CIFAR10 and ImageNet. Here we implement both CIFAR-policy and ImageNet-policy on Tiny-ImageNet. Following the setting in the paper [\[8\]](#page-8-8), we apply Cutout with 32 × 32 pixels after CIFAR-policy. The results are listed in Table [4,](#page-7-0) including in different distributions λ ∼ Beta(γ, γ) for Mixup and Manifold Mixup and different probabilities p to implement Cutmix. Consistent gains are achieved over various data augmentation methods. It is worth emphasizing that greater improvements on the last results than the best results in most cases, which means refinement alleviates the over-fitting on augmented data.

#### <span id="page-6-2"></span>4.3. Results on ImageNet (ILSVRC2012)

On the ILSVRC2012 classification dataset [\[34\]](#page-9-14), we train models with initial learning rate 0.1 and a mini-batch size of 256 on 8 GPUs and follow the standard data augmentation: scale and aspect ratio distortions, random crops, and horizontal flips. For Mixup, the models are trained for 200 epochs, and the learning rate is divided by 10 at epochs 60, 120, 180. For AutoAugment, ResNet-50 is trained for 300 epochs, and the learning rate is divided by 10 at epochs 75, 150, and 225, while ResNet-101 is trained for 200 epochs.

| PreActResNet-18 | Augmentation |       | w/ Refinement |       |
|-----------------|--------------|-------|---------------|-------|
|                 | Best         | Last  | Best          | Last  |
| Standard        | 60.84        | 60.68 | -             | -     |
| Mixup           |              |       |               |       |
| γ = 0.2         | 63.18        | 63.08 | 64.54         | 64.20 |
| γ = 0.5         | 63.95        | 63.34 | 65.45         | 65.08 |
| Manifold Mixup  |              |       |               |       |
| γ = 0.2         | 63.66        | 63.28 | 64.54         | 64.33 |
| γ = 0.5         | 64.88        | 64.43 | 65.98         | 65.80 |
| Cutmix          |              |       |               |       |
| p = 0.5         | 64.90        | 64.61 | 65.84         | 65.59 |
| p = 1           | 65.97        | 65.23 | 66.29         | 65.87 |
| AutoAugment     |              |       |               |       |
| CIFAR-Policy    | 65.08        | 64.29 | 65.31         | 65.12 |
| ImageNet-Policy | 61.06        | 60.65 | 61.82         | 61.75 |

Table 4. Classification accuracy (%) on the validation set of Tiny-ImageNet. Both best and last results are reported.

We refine all models with standard data augmentation for 20 epochs by a learning rate of 10<sup>−</sup><sup>4</sup> . In Table [5,](#page-7-1) ResNet-50 for Mixup of γ = 0.5 performs worse than Mixup of γ = 0.2, however, they achieve similar accuracy after being refined. Since γ is to control the strength of interpolation, which can be understood to control the distribution gap, such results reflect the refinement also helps to weaken the negative impacts of the distribution gap.

#### 4.4. Transferring to Object Detection

To verify the effectiveness of our approach further, we conduct experiments on object detection in the PascalVOC 2007 dataset [\[11\]](#page-8-30) by the pre-trained ResNet-50 that are trained in section [4.3.](#page-6-2) The all detection models are trained following the standard strategy with Faster R-CNN [\[33\]](#page-9-1) whose backbone is initialized with the pre-trained ResNet-50. The models are trained for 12 epochs with a mini-batch size of 16, and the learning rate starts at 0.01 and is divided by 10 after 9 epochs.

In Table [6,](#page-7-2) the pre-trained backbone models of Mixup and AutoAugment fail to improve the performance on object detection task over the original model, although they perform well on the classification task in ILSVRC2012. Interestingly, the pre-trained models with refinement achieve higher mAP than not only the models without refinement but also the original model. Here, we try to give a reasonable explanation for this. Different from classification, detection needs additive features about location. However, these intensive augmentation designed only for classification could treat the features about location as minor features and ignore them, as mentioned in section [3.3.](#page-3-2) After refinement, the features about location are learned more precisely, which can help models transfer to detection. This

| Model           | Augmentation |       |       | w/ Refinement |
|-----------------|--------------|-------|-------|---------------|
|                 | Top-1        | Top-5 | Top-1 | Top-5         |
| ResNet-50       |              |       |       |               |
| Standard        | 76.39        | 93.19 | -     | -             |
| Mixup (γ = 0.2) | 77.47        | 93.75 | 77.69 | 93.83         |
| Mixup (γ = 0.5) | 77.26        | 93.78 | 77.65 | 93.93         |
| AutoAugment     | 77.83        | 93.70 | 77.98 | 93.86         |
| ResNet-101      |              |       |       |               |
| Standard        | 78.13        | 93.71 | -     | -             |
| Mixup (γ = 0.5) | 79.41        | 94.70 | 79.61 | 94.73         |
| AutoAugment     | 79.20        | 94.45 | 79.33 | 94.46         |

<span id="page-7-2"></span><span id="page-7-1"></span><span id="page-7-0"></span>Table 5. Classification accuracy (%) on ImageNet.

| Method      | Augmentation | w/ Refinement |
|-------------|--------------|---------------|
| Standard    | 72.0         | -             |
| Mixup       | 67.4         | 73.3          |
| AutoAugment | 70.5         | 73.1          |

Table 6. mAP (%) on PascalVOC 2007 object detection, obtained by training the pre-trained *ResNet-50* on ILSVRC2012 with Faster R-CNN. The augmentation and refinement refer to different training methods on ILSVRC2012.

means the complicated data augmentation methods, which were applied in classification and viewed as inappropriate for detection before, can also benefit detection models with our approach.

# 5. Conclusions

This paper presents a simple yet effective approach for network optimization, which adopts (usually complicated) augmentation for generating abundant training data, but switch off these intensive data augmentation to refine the model in the last training epochs. In this way, the model often arrives at a reduced testing loss, with the generalization error and empirical loss balanced. We also show intuitively that augmented training enables the model to traverse over a large range in the feature space, while refinement assists it to get close to a local minimum. Consequently, models trained in this manner achieve higher accuracy in a wide range of visual recognition tasks, including in a transfer scenario.

Our work sheds light on another direction of data augmentation which is complementary to the currently popular trend that keeps designing more complicated manners for data generation. It is also interesting to combine refined augmentation with other algorithms, *e.g.*, a cosineannealing schedule for refinement, or add this option to the large space explored in automated machine learning.

# References

- <span id="page-8-24"></span>[1] Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep neural networks. In *International Conference on Learning Representations*, 2018.
- <span id="page-8-4"></span>[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 39(12):2481–2495, 2017.
- <span id="page-8-15"></span>[3] Chris M Bishop. Training with noise is equivalent to tikhonov regularization. *Neural computation*, 7(1):108–116, 1995.
- <span id="page-8-23"></span>[4] Wieland Brendel and Matthias Bethge. Approximating CNNs with bag-of-local-features models works surprisingly well on imagenet. In *International Conference on Learning Representations*, 2019.
- <span id="page-8-5"></span>[5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In *European Conference on Computer Vision*, 2018.
- <span id="page-8-29"></span>[6] Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. In *Internation Conference on Computer Vision*, 2019.
- <span id="page-8-10"></span>[7] Dan Ciregan, Ueli Meier, and Jurgen Schmidhuber. Multicolumn deep neural networks for image classification. In *the IEEE Conference on Computer Vision and Pattern Recognition*, 2012.
- <span id="page-8-8"></span>[8] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation strategies from data. In *the IEEE Conference on Computer Vision and Pattern Recognition*, June 2019.
- <span id="page-8-17"></span>[9] Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher Re. A kernel theory of modern data augmentation. In *International Conference on Machine Learning*, 2019.
- <span id="page-8-7"></span>[10] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. *arXiv preprint arXiv:1708.04552*, 2017.
- <span id="page-8-30"></span>[11] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. *International Journal of Computer Vision*, 88(2):303–338, 2010.
- <span id="page-8-27"></span>[12] Xavier Gastaldi. Shake-shake regularization. *arXiv preprint arXiv:1705.07485*, 2017.
- <span id="page-8-20"></span>[13] Leon Gatys, Alexander S Ecker, and Matthias Bethge. Texture synthesis using convolutional neural networks. In *Advances in Neural Information Processing Systems*, 2015.
- <span id="page-8-21"></span>[14] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Texture and art with deep neural networks. *Current Opinion in Neurobiology*, 46:178 – 186, 2017. Computational Neuroscience.
- <span id="page-8-22"></span>[15] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In *International Conference on Learning Representations*, 2019.
- <span id="page-8-25"></span>[16] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence. In *Advances in Neural Information Processing Systems*, 2019.
- <span id="page-8-11"></span>[17] Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regularization. In *the AAAI Conference on Artificial Intelligence,*, 2019.
- <span id="page-8-18"></span>[18] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In *International Conference on Machine Learning*, 2016.
- <span id="page-8-1"></span>[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *the IEEE Conference on Computer Vision and Pattern Recognition*, 2016.
- <span id="page-8-26"></span>[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In *European Conference on Computer Vision*. Springer, 2016.
- <span id="page-8-2"></span>[21] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classification with convolutional neural networks. In *the IEEE Conference on Computer Vision and Pattern Recognition*, 2019.
- <span id="page-8-14"></span>[22] Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi Chen. Population based augmentation: Efficient learning of augmentation policy schedules. In *International Conference on Machine Learning*, 2019.
- <span id="page-8-6"></span>[23] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In *International Conference on Machine Learning*, 2015.
- <span id="page-8-13"></span>[24] Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based training of neural networks. *arXiv preprint arXiv:1711.09846*, 2017.
- <span id="page-8-19"></span>[25] Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam to sgd. In *International Conference on Learning Representations*, 2017.
- <span id="page-8-0"></span>[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In *Advances in Neural Information Processing Systems*, 2012.
- <span id="page-8-12"></span>[27] Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation learning an optimal data augmentation strategy. *IEEE Access*, 5:5858–5869, 2017.
- <span id="page-8-9"></span>[28] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. In *Advances in Neural Information Processing Systems*, 2019.
- <span id="page-8-3"></span>[29] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, ´ Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In *the IEEE Conference on Computer Vision and Pattern Recognition*, 2017.
- <span id="page-8-28"></span>[30] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In *International Conference on Learning Representations*, 2017.
- <span id="page-8-16"></span>[31] Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris Papailiopoulos. Does data augmentation lead

to positive margin? In *International Conference on Machine Learning*, 2019.

- <span id="page-9-9"></span>[32] Alexander J Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher Re. Learning to compose ´ domain-specific transformations for data augmentation. In *Advances in Neural Information Processing Systems*, 2017.
- <span id="page-9-1"></span>[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In *Advances in Neural Information Processing Systems*, 2015.
- <span id="page-9-14"></span>[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. *International Journal of Computer Vision*, 115(3):211–252, 2015.
- <span id="page-9-6"></span>[35] Ikuro Sato, Hiroki Nishimura, and Kensuke Yokoi. Apac: Augmented pattern classification with neural networks. *arXiv preprint arXiv:1505.03229*, 2015.
- <span id="page-9-2"></span>[36] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. *the Journal of Machine Learning Research*, 15(1):1929–1958, 2014.
- <span id="page-9-0"></span>[37] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In *the IEEE Conference on Computer Vision and Pattern Recognition*, June 2015.
- <span id="page-9-7"></span>[38] Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Between-class learning for image classification. In *the IEEE Conference on Computer Vision and Pattern Recognition*, 2018.
- <span id="page-9-10"></span>[39] Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, and Ian Reid. A bayesian data augmentation approach for learning deep models. In *Advances in Neural Information Processing Systems*, 2017.
- <span id="page-9-5"></span>[40] Vladimir Vapnik. *Statistical learning theory*. Wiley New York, 1998.
- <span id="page-9-8"></span>[41] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In *International Conference on Machine Learning*, 2019.
- <span id="page-9-11"></span>[42] Yulin Wang, Xuran Pan, Shiji Song, Hong Zhang, Cheng Wu, and Gao Huang. Implicit semantic data augmentation for deep networks. In *Advances in Neural Information Processing Systems*, 2019.
- <span id="page-9-4"></span>[43] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In *International Conference on Computer Vision*, 2019.
- <span id="page-9-13"></span>[44] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In *British Machine Vision Conference*, September 2016.
- <span id="page-9-12"></span>[45] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In *International Conference on Learning Representations*, 2017.

<span id="page-9-3"></span>[46] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In *International Conference on Learning Representations*, 2017.