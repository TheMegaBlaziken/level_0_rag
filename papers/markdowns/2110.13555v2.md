# <span id="page-0-1"></span>Directional Self-supervised Learning for Heavy Image Augmentations

Yalong Bai1\* Yifan Yang2\* Wei Zhang1† Tao Mei<sup>1</sup>

1

JD AI Research <sup>2</sup>Peking University

ylbai@outlook.com, yif yang@pku.edu.cn, wzhang.cu@gmail.com, tmei@live.com

# Abstract

*Despite the large augmentation family, only a few cherry-picked robust augmentation policies are beneficial to self-supervised image representation learning. In this paper, we propose a directional self-supervised learning paradigm (DSSL), which is compatible with significantly more augmentations. Specifically, we adapt heavy augmentation policies after the views lightly augmented by standard augmentations, to generate harder view (HV). HV usually has a higher deviation from the original image than the lightly augmented standard view (SV). Unlike previous methods equally pairing all augmented views to symmetrically maximize their similarities, DSSL treats augmented views of the same instance as a partially ordered set (with directions as SV*↔*SV, SV*←*HV), and then equips a directional objective function respecting to the derived relationships among views. DSSL can be easily implemented with a few lines of codes and is highly flexible to popular selfsupervised learning frameworks, including SimCLR, Sim-Siam, BYOL. Extensive experimental results on CIFAR and ImageNet demonstrated that DSSL can stably improve various baselines with compatibility to a wider range of augmentations.*

# 1. Introduction

Unsupervised visual representation learning aims at learning image features without using manual semantic annotations. Recently, self-supervised learning driven by instance discrimination tasks [\[2,](#page-8-0) [11,](#page-8-1) [20,](#page-8-2) [24,](#page-8-3) [26\]](#page-8-4) or Siamese architecture [\[3,](#page-8-5) [10\]](#page-8-6) has achieved great success in learning high-quality visual features and closing the performance gap with supervised pretraining on various computer tasks. The visual embedding space of self-supervised learning method is constructed by minimizing the dissimilarity among representations of variations derived from the same image, and/or increasing the distance between the representations of augmented view from different images (negative

<span id="page-0-0"></span>

|               |         |         | 95<br>95                                            |
|---------------|---------|---------|-----------------------------------------------------|
| Augment       | SimSiam | w/ DSSL | 90<br>90                                            |
| Standard Aug. | 92.17   |         | 85<br>85                                            |
| w/ JigSaw(2)  | 92.79   | 93.56   | 80                                                  |
| w/ JigSaw(4)  | 89.72   | 91.95   | 80<br>SimSiam<br>0<br>200<br>400<br>600<br>800      |
| w/ RA(1,1)    | 88.00   | 92.29   | SimSiam w/ RA(2,5)<br>50<br>50                      |
| w/ RA(2,1)    | 82.80   | 93.09   | DSSL+SimSiam w/ RA(2,5)                             |
| w/ RA(2,5)    | 9.78    | 94.17   | 35<br>35                                            |
| w/ UA         | 91.11   | 93.27   | 20<br>20                                            |
|               |         |         | 5<br>5                                              |
|               |         |         | epochs<br>0<br>800<br>0<br>200<br>400<br>600<br>800 |

Figure 1. Left: Linear evaluation accuracy of SimSiam [\[3\]](#page-8-5) on CIFAR-10 by adding extra *heavy augmentations* besides the original *standard augmentations*. The number of grids for *JigSaw*(n) is n × n. *RA*(m, n) is the RandAugment [\[6\]](#page-8-7) with n augmentation transformation of m magnitude. *UA* denotes the UniformAumgent [\[17\]](#page-8-8). Right: validation accuracy of kNN classification during pre-training. Incorporating heavy augmentations on SimSiam results in unstable performance even *collapsing* (linear evaluation on collapsed model tends to random guess). Our DSSL consistently benefits from heavy augmentations for higher performances.

pair). Image transformation, which aims to generate variations from the same image [\[2,](#page-8-0) [3,](#page-8-5) [10,](#page-8-6) [11\]](#page-8-1), plays a crucial role in the self-supervised visual representations learning.

However, these self-supervised learning methods share a common fundamental weakness: only a few carefully selected augmentation policies with proper settings are beneficial to the model training. The combination of *random crop, color distortion, Gaussian blur and grayscale* is crucial to achieving good performance, applied as the basic augmentation setting for many popular instance-wise selfsupervised learning methods [\[1,](#page-8-9) [3,](#page-8-5) [10,](#page-8-6) [11,](#page-8-1) [15\]](#page-8-10). Here we define these augmentations as the standard augmentations. We denote the images augmented by standard augmentations as the standard views. Recent works find that a few other augmentations (e.g., *RandAugment*, *JigSaw*) can further improve the performance of self-supervised learning methods rely on negative pairs [\[21\]](#page-8-11). However, for negative pair free self-supervised learning methods like Sim-Siam [\[3\]](#page-8-5), introducing such augmentations usually results in a lousy performance, even model collapsing during train-

<sup>\*</sup>Equal contribution.

<sup>†</sup>Corresponding author.

<span id="page-1-1"></span><span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)

**Caption:** Figure 2 illustrates the DSSL framework, contrasting standard self-supervised learning with DSSL. It shows how standard views (SV) are generated from original images and how harder views (HV) are derived through heavy augmentations. DSSL employs asymmetric loss to maintain the integrity of feature clusters, enhancing representation learning stability.

Figure 2. Overview of standard self-supervised learning and our DSSL. Original standard image transformations generate the standard views, and the harder view is derived from the the standard view by applying heavy augmentation *RandAugment*. (a) Standard instancewise learning with standard views. (b) Instance-wise self-supervised learning after introducing heavily augmented (harder) views. Applying symmetric loss to maximize the similarity between standard and heavily augmented views roughly expands the feature cluster in the visual embedding space. The model may confuse the instance-level identity. (c) DSSL: To prevent the adverse effect from *missing information* of heavily augmented views, DSSL avoids arbitrarily maximizing their visual agreement. To tighten the feature cluster, DSSL applies an asymmetric loss for only gathering each heavily augmented view to its relevant standard view.

ing, as shown in Fig. [1.](#page-0-0) We name these unstable and risky data augmentation policies as heavy augmentations, since they usually largely alters the image appearance.

Inspired by previous works [\[3,](#page-8-5) [15\]](#page-8-10) that formulating the instance-wise self-supervised learning as K-means clustering of all augmented views from the same instance, we hypothesize a gold standard feature cluster for all views of one given image instance existing in the visual feature embedding space, and define d as the deviation of augmented view's feature from the core-point of its relevant gold standard feature cluster. Standard self-supervised learning methods treat all augmented views of the same image fairly to construct training pairs. As shown in Fig. [2](#page-1-0) (a), such a strategy works well, and the model can converge stably for the standard image transformations. However, after incorporating the views augmented from heavy image transformations (Fig. [2](#page-1-0) (b)), two obvious risks arise. 1) Closing the representation of standard views to heavily augmented views would roughly expand the feature cluster in the embedding space. This would increase the difficulty of constructing an embedding space where all instances are well-separated and also may cause unexpected confusion for instance-wise discriminating, especially for those negative pairs free methods. 2) Maximizing the visual agreement among heavily augmented views disaccords with the "InfoMin principle" [\[21\]](#page-8-11). Since the mutual information among views with large d is usually low, contrasting these views leads to *missing information* and results in poor performance in downstream tasks.

To address this, we propose Directional Self-supervised Learning (DSSL), a new training method for unsupervised representation learning that could stably improve the performance of instance-wise self-supervised learning by completely applying more heavy image transformations. Fig. [2](#page-1-0) (c) shows an illustration of DSSL. For each standard view (SV) augmented from the original robust image transformations, we can generate various harder views (HV) derived from it by applying additional heavy augmentation policies. These heavily augmented view has a larger d than its relevant standard view. In this way, we can treat all augmented views of the same image as a partially ordered set (SV↔SV, SV←HV) in terms of d. An asymmetric loss is introduced to encourage the representation of each heavily augmented view (HV) to be close to its relevant source standard view (SV). In this way, the feature cluster for all augmented views can be presented as non-convex, rather than the K-means convex clustering, the whole cluster is tightened. Moreover, DSSL discards the instance-wise selfsupervised learning among RVs to bypass the issue of low mutual information among HVs. As a result, more augmentation policies can be introduced to enrich the information of the whole embedding space but keep the instances still well-separated.

DSSL is a straightforward algorithm that can be easily implemented in a few line of Pseudo code. Also, there are no additional hyper-parameter needs to be adjusted in DSSL. We validate the effectiveness of DSSL by evaluating it on several self-supervised benchmarks. In particular, on the ImageNet linear evaluation protocol, DSSL achieved stable performance improvements. All DSSL based pretraining models surpass the supervised pre-training model on the CIFAR-10 linear evaluation. Moreover, the trans<span id="page-2-0"></span>fer performance on detection and segmentation task further demonstrate the efficiency of DSSL with heavy augmentations.

The main contributions are summarized as follows:

- A novel Directional Self-supervised Learning (DSSL) paradigm is proposed for unsupervised visual representation learning. We introduce a partially ordered set to organize the augmented views, and introduce an asymmetric loss for harnessing rich information from heavy augmented views.
- DSSL is easy-to-implement and applicable to various standard instance-wise self-supervised learning frameworks by introducing minor modifications without any hyper-parameters.
- DSSL stably improves over various self-supervised learning methods on standard benchmarks, even when thoroughly applying heavy image transformations that show adverse effect to previous methods.

# 2. Related Work

Our work is related to studies in the instance-wise selfsupervised learning and data augmentation policies.

Instance-wise self-supervised learning. Instance-level classification task treats each image and its variants as one specific class. It aims to construct visual embedding space by pull all samples in the same class close while pushing samples from other classes away. Since it is hard to directly categorize all training samples into a large number of classes [\[8\]](#page-8-12), the early instance-wise contrastive learning method replaces the classifier with a memory bank [\[24\]](#page-8-3) to store the previous features of all samples calculated in the previous stage and sample positive and negative pairs from the memory bank. Several other technologies have also been adopted and extended based on this method, such as introducing local similarity [\[28\]](#page-9-0) and neighborhood discovery [\[13\]](#page-8-13) for further improving the quality of feature embedding. He *et al*. [\[11\]](#page-8-1) enhance the training of memory bank based contrastive learning model by storing representations from a momentum encoder instead of the trained network. Instead of storing previously computed representations, some other methods explore different instances' features within the current batch for negative sampling, and it requires a large batch size to work well [\[2,](#page-8-0) [20,](#page-8-2) [26\]](#page-8-4).

All of the above methods require either a large batch, memory bank, or queue to provide enough negative samples for clustering or discriminating. More recently, some works have proposed advancing self-supervised pretraining without using negative samples, e.g., BYOL [\[10\]](#page-8-6), SimSiam [\[3\]](#page-8-5). These negative pair free self-supervised learning methods are more resilient to the changes in the batch size and more friendly to low-resource implementations.

Data augmentation policies. The composition of multi-

ple data augmentation operations is crucial in defining the contrastive prediction tasks that yield effective representations [\[2,](#page-8-0) [3,](#page-8-5) [10\]](#page-8-6). Until now, most of the high-performance contrastive learning framework are designed to learn representations by maximizing agreement between differently augmented views of the same image via a contrastive loss in the feature embedding space. However, different with the supervised learning methods which can benefit from various complex data augmentation polices [\[4–](#page-8-14)[7,](#page-8-15) [27\]](#page-8-16), there are only a few light augmentation policies playing as key contributors of the good performance of instance-wise selfsupervised learning [\[1](#page-8-9)[–3,](#page-8-5) [10\]](#page-8-6). The composition of the random crop, optional left-right flip, color distortion, Gaussian blur is treated as a standard and robust augmentation setting for generating augmented views of training images in unsupervised visual representation learning methods [\[1,](#page-8-9) [3\]](#page-8-5).

Our experimental study also shows that directly applying complex/heavy data augmentation policies leads to damaging performance drop or even model collapsing for negative pair free instance-wise self-supervised learning methods. These heavy data augmentations construct views with small mutual information among them. According to the infoMin principle [\[21,](#page-8-11) [22\]](#page-8-17), unsupervised learning methods trained on such views would result in the "missing information" regime of performance. Even such heavily augmented views have been demonstrated containing rich information [\[23\]](#page-8-18) but they still may mislead the feature clustering in the embedding space.

Different with another concept of directional selfsupervision loss proposed in [\[25\]](#page-8-19) for exploiting the output consistency across different resolutions in 3D human pose estimation task, in this paper, we propose a general self-supervised learning framework DSSL for introducing various image transformations for instance-wise selfsupervised learning with better theoretical justification. The contrast among heavily augmented views with strong probabilistic of missing information is disabled. Moreover, DSSL regards instance-wise self-supervised learning as optimizing a non-convex clustering task. An asymmetric loss is proposed for tightening feature clusters. As a result, DSSL can achieve stable performance improvements on various instance-wise self-supervised learning methods owing to the rich information from heavy image transformations and the data characteristic-based learning strategies.

# 3. Method

Instance-wise self-supervised learning methods aim to learn representation by maximizing agreement among differently augmented views of the same data example in the latent visual feature space. To ease the discussion, we start by briefly summarizing the standard instance-wise selfsupervised learning with a unified formulation.

#### <span id="page-3-4"></span>3.1. A unified formulation

Following the basic settings of recent works, the standard instance-wise self-supervised learning framework has four main components:

- A *data augmentation module* consisting with augmentation policies set T for generating augmented view for given image.
- Deep neural network *encoder* f(·) for projecting the input images to the latent space.
- *Projection head* g(·) for mapping the outputs of encoder networks to space where instance-wise selfsupervised loss is applied.
- A *self-supervised loss* function defined for an instancewise discrimination task or a feature prediction task.

Given an input image I without annotation, the data augmentation module produces augmented view pair set as

<span id="page-3-2"></span>
$$
\mathbf{V}_{\mathcal{T}} = \{ (t(I), t'(I)) \mid t, t' \sim \mathcal{T} \}.
$$
 (1)

where t and t 0 are random augmentation sampled from T . During training, augmented view pair (v, v<sup>0</sup> ) is sampled from V<sup>T</sup> . One of the augmented views v is feed into the encoders to get their visual representations f(v). The projection head transforms the feature of augmented view into a vector as z , g(f(v)). The objective functions of instancewise self-supervised learning is maximizing the agreement between augmented view pairs in V<sup>T</sup> : S(z, y(v 0 )), where y works as an operation to generate the label for neural network training.

For different instance-wise self-supervised learning methods, the function of S and operation y are implemented in different ways:

- In SimCLR [\[2\]](#page-8-0), y(v 0 ) = g(f(v 0 )), and S is formulated softmax-style function across v, v 0 and the negative views v <sup>−</sup> augmented from other images in the minbatch.
- In BYOL [\[10\]](#page-8-6), y(v 0 ) = fξ(v 0 ), where fξ(·) is a siamese encoder with the exponential moving average weights of f. S measures the mean squared error between the normalized z and y(v 0 ).
- In SimSiam [\[3\]](#page-8-5), y(v 0 ) = f(v 0 ), but there is no gradient backward through the neural pathway computing y(v 0 ). S measures the negative cosine similarity of given feature vector pair.

Since the augmented view is generated randomly, above objective function is implemented as symmetrical form:

<span id="page-3-1"></span>
$$
\mathcal{L}_S = \mathcal{S}(z, y(v')) + \mathcal{S}(z', y(v)),\tag{2}
$$

where z <sup>0</sup> , g(f(v 0 )).

#### 3.2. Partially-ordered views construction

In this paper, we introduce additional heavy data augmentation policies <sup>T</sup>b, besides the previous carefully selected standard data augmentation polices T . We advance

<span id="page-3-0"></span>![](_page_3_Figure_18.jpeg)

**Caption:** Figure 3 depicts the construction of partially ordered views in the DSSL framework. The left side illustrates how augmented views are generated, while the right side contrasts symmetric loss for standard views with asymmetric loss for heavily augmented views, emphasizing the importance of directional relationships in optimizing representation learning.

Figure 3. Illustration of our directional self-supervised learning (DSSL) framework. Left: Construction of partially ordered views. Right: Symmetric loss L<sup>S</sup> for bidirectionally maximizing the agreement among augmented view pairs sampled from V<sup>T</sup> remains same. Asymmetric loss L<sup>A</sup> is proposed for encouraging the representations of the heavily augmented views to be close to their source standard views, respecting the partially ordered relationship in <sup>V</sup>T →T<sup>b</sup> .

the data augmentation module for generating augmented views from <sup>T</sup><sup>b</sup> and <sup>T</sup> jointly. In particular, a new augmented view <sup>v</sup><sup>b</sup> , <sup>b</sup>t(v) can be produced based on <sup>v</sup> , <sup>t</sup>(I) by applying image augmentation <sup>b</sup><sup>t</sup> <sup>∼</sup> <sup>T</sup>b, <sup>t</sup> ∼ T . It means each light augmented view v would have various relevant harder augmented view <sup>v</sup>b, and <sup>v</sup><sup>b</sup> is derived from <sup>v</sup>.

For a well-trained unsupervised learning model, all views of the same image should be clustered closely in the visual embedding space [\[1,](#page-8-9) [3,](#page-8-5) [15\]](#page-8-10). We define the deviation of augmented view's feature f(v) from the core-point (original view I) of its relevant cluster as d(v). It is not easy to measure the deviation degree quantitatively. But, in general, larger distortion magnitudes of generating v leads to larger d(v). Applying additional heavy augmentation policies on a standard augmented view can generate a new view with a higher deviation from the original view. As there is no policy overlap between <sup>T</sup> and <sup>T</sup>b, and the operation of <sup>b</sup><sup>t</sup> is non-identity. Thus, we can compare the relative magnitudes of <sup>d</sup> between two views that <sup>d</sup>(vb) > d(v), since <sup>v</sup><sup>b</sup> is produced by applying additional different heavy augmentation operations <sup>b</sup><sup>t</sup> on <sup>v</sup>, as shown in Fig. [3](#page-3-0) (Left).

In this way, we can construct a directed training pair collection <sup>V</sup>T ←T<sup>b</sup> from a partially ordered augmented view set as

<span id="page-3-3"></span>
$$
\mathbf{V}_{\mathcal{T}\leftarrow\widehat{\mathcal{T}}} = \left\{ \left( t(I), \widehat{t}(t(I)) \right) \mid t \sim \mathcal{T}, \widehat{t} \sim \widehat{\mathcal{T}} \right\}.
$$
 (3)

After combining the original training pairs generated from T , we get the final augmented view pairs collection for training as <sup>V</sup><sup>T</sup> <sup>∪</sup>VT ←T<sup>b</sup> . In particular, <sup>V</sup><sup>T</sup> can be regarded as the collection of edges in a complete undirected graph whose vertices are the augmented views. While <sup>V</sup>T ←T<sup>b</sup> can be regarded as a collection of the directed edges in a directed acyclic graph, where the relations among vertices are measured by the magnitude of d(·).

<span id="page-4-3"></span><span id="page-4-0"></span>Algorithm 1 DSSL for SimSiam Pseudocode, PyTorch-like

```
# f: feature encoder; g: prediction head;
for I in dataloader:
    v, v
        0 = t(I), t
                   0(I) # standard data augmentation
    vb, vb0 = bt(I), tb0(I) # heavy data augmentation
    y, y
        0, yb, yb0 = f(v), f(v
                             0), f(vb), f(vb0)
    z, z
        0, zb, zb0 = g(y), g(y
                             0), g(yb), g(yb0)
    # loss computation
    L = D(z, y
               0)/4 + D(z
                         0, y)/4 + D(zb, y)/4 + D(zb0, y
                                                       0)/4
    L.backward() # back-propagate
    update(f, h) # SGD update
def D(z, y): # negative cosine similarity
    y = y.detach() # stop gradient
    # l2-normalize
    z = normalize(z, dim=1)
    y = normalize(y, dim=1)
    return -(z * y).sum(dim=1).mean()
```
#### 3.3. Directional self-supervised learning

For every two views v and v 0 sampled from V<sup>T</sup> , we sample two directed augmented view pairs (v, <sup>v</sup>b), (<sup>v</sup> 0 , <sup>v</sup>b<sup>0</sup>) from <sup>V</sup>T ←T<sup>b</sup> . For the partially ordered relationship between <sup>v</sup><sup>b</sup> and <sup>v</sup>, we introduce a <sup>v</sup> <sup>←</sup> <sup>v</sup><sup>b</sup> directional asymmetric loss as

$$
\mathcal{L}_A = \mathcal{D}(\widehat{z}, y(v)) + \mathcal{D}(\widehat{z'}, y(v')), \tag{4}
$$

where <sup>z</sup><sup>b</sup> , <sup>g</sup>(f(vb)) and <sup>z</sup>b<sup>0</sup> , <sup>g</sup>(f(vb<sup>0</sup>)). The operation of y(·) stays the same with Eq. [\(2\)](#page-3-1) following the settings in standard instance-wise self-supervised learning methods, but y(·) is only computed over the standard views in LA. This makes the self-supervised learning to be directional and asymmetrical. It means the optimization objective of L<sup>A</sup> is to force the representation of heavily augmented view close to the representation of its relevant source view. D simply measure the negative cosine similarity among two vectors:

<span id="page-4-2"></span>
$$
\mathcal{D}(z,y) = -\frac{\langle z,y\rangle}{\|z\|_2 \|y\|_2}.
$$

As shown in Fig. [3](#page-3-0) (Right), the objective function of our proposed directional self-supervised learning for given two standard views and their relevant harder views can be expressed as:

$$
\mathcal{L}_{DSSL} = \mathcal{L}_S + \mathcal{L}_A. \tag{5}
$$

The total loss of DSSL can be measured by P <sup>V</sup><sup>T</sup> <sup>P</sup> L<sup>S</sup> + <sup>V</sup>T ←T<sup>b</sup> L<sup>A</sup> averaged over all images. DSSL can be easily implemented on various instance-wise self-supervised learning frameworks in only a few lines of Pseudocode. Algorithm [1](#page-4-0) shows the Pseudocode applying DSSL for unsupervised learning framework SimSiam [\[3\]](#page-8-5).

Relation to previous self-supervised learning methods. We formulate the general self-supervised learning that considering all combinations of views augmented by <sup>T</sup> and <sup>T</sup><sup>b</sup> jointly as:

<span id="page-4-1"></span>
$$
\alpha \sum_{\mathbf{V}_{\mathcal{T}}} \mathcal{L}_{S} + \beta \sum_{\mathbf{V}_{\hat{\mathcal{T}}}} \mathcal{L}_{S} + \gamma \sum_{\mathbf{V}_{\mathcal{T}\leftarrow \hat{\mathcal{T}}}} \mathcal{L}_{A} + \delta \sum_{\mathbf{V}_{\hat{\mathcal{T}}\leftarrow \mathcal{T}}} \mathcal{L}_{A}, \quad (6)
$$

where <sup>V</sup>T<sup>b</sup> and <sup>V</sup>T ←T <sup>b</sup> are the augmented view collections constructed according to Eq. [\(1\)](#page-3-2) and Eq. [\(3\)](#page-3-3) respectively. ← indicates the direction of predicting or contrasting. The α, β, γ and δ are the weights for each part of the loss. Previous self-supervised learning methods fairly treat all augmented views, thus the objective functions of these methods can be regards as the situation that α = γ = β = δ = 1. While the loss function of DSSL (LDSSL) can be regarded as the situation with α = γ = 1, β = δ = 0.

Considering that the view augmented from additional heavy augmentation policies <sup>T</sup><sup>b</sup> would tend to lose more information than its source view augmented from T , a natural solution is to fit a lower confidence score for heavily augmented views when they play as the targets for optimizing the self-supervised loss. Thus, an ideal settings of the loss weights should ensure α, γ > β, δ. Further, we designed various self-supervised learning paradigms in Sec. [4.3](#page-6-0) to analyze the positive/negative impact of each components in Eq. [\(6\)](#page-4-1) and demonstrate the necessity of asymmetric loss for heavy augmentations in Eq. [\(5\)](#page-4-2).

# 4. Experimental Results

In this section, we empirically study the DSSL behaviors by introducing various heavy data augmentation policies.

### 4.1. Implementation details

Standard augmentations. We use the same sequence of image augmentations as in previous instance-wise selfsupervised learning methods [\[2,](#page-8-0) [3,](#page-8-5) [10\]](#page-8-6), including random *cropping and resizing*, *horizontal flip*, *color distortion*, *grayscale* and *Gaussian blur*. Each of the above augmentations has been proved effective in at least one typical instance-wise self-supervised learning method. The composition of all above image augmentations are treated as the standard augmentation T .

Heavy augmentations. Inspired by the settings in InfoMin [\[21\]](#page-8-11), we use *RandAugment* [\[6\]](#page-8-7) and *Jigsaw* [\[4,](#page-8-14) [18\]](#page-8-20) as heavy augmentations <sup>T</sup>b. These two augmentations have proven effective for supervised representation learning, and negative pairs required instance-wise contrastive learning. But we find that these augmentations result in lousy performance or even model collapsing for the negative-pairsfree unsupervised learning. We denote *RandAugment* as *RA*(n, m), where n is the number of augmentation policies randomly selected from 14 predefined augmentations, m is the magnitude for all the transformation. Unless otherwise specified, we equip a sequential combination of *RA*(2,5) and *Jigsaw* with 4×4 grids as heavy augmentation for experimental results reported in this paper.

Compared methods. We compare three typical selfsupervised learning frameworks, including SimSiam, BYOL, and SimCLR, as shown in Table [1.](#page-5-0) SimCLR uses the normalized temperature-scaled cross entropy as S, and

<span id="page-5-2"></span><span id="page-5-0"></span>

| Method      | negative<br>pairs | stop<br>gradient | momentum<br>encoder | reported | repro. |
|-------------|-------------------|------------------|---------------------|----------|--------|
| SimCLR [2]  | X                 |                  |                     | 60.1     | 59.5   |
| SimSiam [3] |                   | X                |                     | 67.7     | 67.4   |
| BYOL [10]   |                   | X                | X                   | 66.5     | 67.8   |

Table 1. Three instance-wise self-supervised learning methods used for comparisons and analysis in this paper.

the comparisons among positive and negative views are required for model training. Both BYOL and SimSiam are negative sample free methods, and they stop the gradient of the neural branch for computing the label y, except BYOL that applies a momentum encoder for updating y.

For a fair comparison, we generated two augmented view pairs per image each time for negative sample free methods (SimSiam and BYOL) to keep the number of feature prediction pairs the same with their corresponding *w/ DSSL* versions. Moreover, for SimCLR that considers negative samples in the whole min-batch (batch size of n), its *w/ DSSL* version only adds one additional heavily augmented view with asymmetric loss item for each sample. The computation slightly increases from 2n 2 to 2n <sup>2</sup> + 2n for each min-batch after applying DSSL on SimCLR. The number of feedforward and backward passes remains the same. More implementation details of compared methods and DSSL can be found in the Appendix.

Training details. Following the practice of the previous self-supervised learning methods, we use ResNet-50 and ResNet-18 as the basic feature encoder f for the experiments on ImageNet ILSVRC-2012 [\[19\]](#page-8-21) and CIFAR-10 [\[14\]](#page-8-22) dataset, respectively. We strictly follow the network architecture of projection head g, initialization, optimizer represented in these methods' original paper. Except that we apply the same SGD optimizer and learning rate schedule of SimSiam to BYOL since it can slightly improve the performance of BYOL. Existing works differ considerably in batch size and training epochs, which could significantly influence performance. We, therefore, compare all models of the same batch size of 512 and training epochs of 100 for unsupervised training on ImageNet, which is a resourcefriendly implementation.

We elaborate the implementation details of the compared methods below.

- SimCLR: We used the PyTorch repo[1](#page-5-1) officially recommended by the authors.
- BYOL: Our re-implemented BYOL has higher linear evaluation accuracy than the BYOL (100ep) results of ImageNet linear evaluation reported in the SimSiam paper (67.8% vs. 66.5%).
- SimSiam: To align settings of all compared methods

in this paper, we used *lr* with cosine decay but did not fix the *lr* of prediction MLP during linear evaluation. The result of our reproduction is 67.4% on ImageNet (vs. 67.7% reported in the original paper). This performance gap is acceptable.

#### 4.2. Main results and discussions

Linear evaluation on ImageNet. We apply linear evaluation to measure the quality of the representations of DSSL based models after self-supervised pretraining on the unlabelled training images of the ImageNet dataset. Specifically, we train a linear classifier on top of the pre-trained representation. During training, the parameters of the backbone network (feature encoder) are frozen, while only the last fully connected layer is updated via backpropagation.

Tab. [2](#page-6-1) reports the top-1 accuracy of compared methods and their DSSL versions. Same with the experimental conclusion of InfoMin [\[21\]](#page-8-11), the evident advantages of variant information in the harder views augmented by <sup>T</sup><sup>b</sup> can further benefit SimCLR, which is a contrastive learning method relying on negative pairs. Maximizing the dissimilarity among different instances is a direct and effective way to learn a well-separated visual embedding space. Such a manner is also robust to the missing-information hard views. However, for the negative pairs free method SimSiam, the heavy augmentations usually result in the model collapsing. Momentum encoder can increase model training stability since momentum would neutralize the misleading information from inconsistent representations. BYOL is more robust to the hard views than SimSiam. We further analyse the robustness of the momentum encoder in the later subsections.

Our DSSL prevents closing the similarity among heavily augmented views since the commonality among these views is usually scarce, and forcing assimilating them would lead to the model collapsing. As a result, DSSL can make these views no longer risky for negative pairs free self-supervised learning methods and make them play a bigger role in contrastive learning methods which relying on negative pairs.

Although the improvement of DSSL on BYOL is limited under this setting, such limitation is easy to break after introducing more heavy augmentations, as we listed in Tab. [3.](#page-6-2) After equipping *RA*(8,16) and *UA* [\[17\]](#page-8-8), DSSL can significantly boost the accuracy of BYOL linear evaluation +7.2%, +2.2% on ImageNet, respectively. DSSL further taps potentials in various heavy augmentations and results in stable performance improvement.

Linear evaluation on CIFAR-10. Similar to the implementation in the ImageNet dataset, we use the unlabeled training images in CIFAR-10 for self-supervised learning based on the ResNet-18 backbone. We follow the standard settings used in the CIFAR experiments of SimSiam [\[3\]](#page-8-5) that SGD with a learning rate 0.03, cosine lr decay schedule for

<span id="page-5-1"></span><sup>1</sup>https://github.com/AndrewAtanov/simclr-pytorch

<span id="page-6-4"></span><span id="page-6-1"></span>

|         |        | CIFAR-10 |          | ImageNet |      |          |  |
|---------|--------|----------|----------|----------|------|----------|--|
| Method  | SimCLR | BYOL     | SimSiam  | SimCLR   | BYOL | SimSiam  |  |
| repro.  | 91.5   | 92.2     | 92.1     | 59.5     | 67.8 | 67.4     |  |
| w/ Tb   | 92.2   | 94.4     | collapse | 60.0     | 68.3 | collapse |  |
| w/ DSSL | 93.2   | 94.7     | 94.5     | 60.3     | 68.3 | 68.6     |  |

0 20 40 60 80 100 1 3 5 7 9 11 13 15 Accuracy (%) Strength of augmentation SimSiam SimSiam w/ DSSL

Table 2. Comparisons on linear evaluation accuracies (%). *repro*: our reproduction of each method. *collapse*: model collapsed during training. *w/* <sup>T</sup>b: training views are jointly augmented from standard and heavy augmentations. Heavy augmentations perform unstably even model collapsing, while DSSL *consistently* benefits from <sup>T</sup>b.

Figure 4. Performance comparisons on CIFAR-10 linear evaluation accuracy for SimSiam by applying heavy augmentation *RA*(n = 2) across varying distortion magnitudes m.

<span id="page-6-2"></span>

| Method        |          | CIFAR-10 | ImageNet |      |  |
|---------------|----------|----------|----------|------|--|
| BYOL (repro.) |          | 92.2     | 67.8     |      |  |
| +Tb:          | RA(4,10) | RA(8,16) | RA(8,16) | UA   |  |
| BYOL w/ Tb    | 84.1     | collapse | 60.6     | 67.1 |  |
| BYOL w/ DSSL  | 94.4     | 94.0     | 67.8     | 69.3 |  |

Table 3. Linear evaluation accuracies (%) of BYOL by applying more heavy augmentations (+Tb).

800 epochs, image size of 32×32, and a batch size of 512. We first train the ResNet-18 feature encoder on unlabeled CIFAR-10 images and then freeze the backbone to train a linear tasks-specific head on CIFAR-10 dataset with annotations.

Tab. [2](#page-6-1) reports the top-1 accuracy of linear classifier trained on CIFAR-10. Our reproductions of SimSiam (92.1) and SimCLR (91.5) have higher performance than that reported in the paper of SimSiam under the same settings (91.8, 91.1). Similar to the experimental results in ImageNet, after applying DSSL, all three unsupervised learning methods outperform the supervised ResNet-18, whose top-1 accuracy is 93.02%.

Impact of distortion magnitudes of <sup>T</sup>b. In Fig. [4,](#page-6-1) we compared the results from the standard SimSiam and its DSSL version by introducing heavy augmentation with various distortion magnitudes. We use the hyperparameter m of RandAugment to control the distortion magnitudes of the randomly selected policies. It can be found that, after increasing m ≥ 5, the self-supervised models result in divergence. In contrast, SimSiam *w/ DSSL* can converge stably in a wide range of [1, 9]. It further demonstrates the robustness of our proposed directional self-supervised learning on partially ordered views.

Robustness analysis of momentum encoder. As shown in Tab. [2,](#page-6-1) the momentum encoder based method BYOL represents better robustness to the heavy augmentations than SimSiam. To study the limitation of negative sample free self-supervised methods and look into the boundary of momentum updating for the encoder, we further strengthen the magnitudes of heavy augmentations for BYOL. In particular, *UA* and *RA* with more augmentation policies and

<span id="page-6-3"></span>

|                  | COCO detection |      |      | COCO segmentation |      |           |
|------------------|----------------|------|------|-------------------|------|-----------|
| pre-train        | AP50           | AP   | AP75 | APm<br>50         | APm  | APm<br>75 |
| ImageNet sup.    | 59.2           | 37.7 | 40.9 | 55.8              | 33.9 | 35.8      |
| BYOL (repro.)    | 56.9           | 36.9 | 40.2 | 54.1              | 34.0 | 36.5      |
| BYOL w/ DSSL     | 57.8           | 37.6 | 40.8 | 55.0              | 34.6 | 37.0      |
| SimSiam†         | 57.5           | 37.9 | 40.9 | 54.2              | 33.2 | 35.2      |
| SimSiam (repro.) | 58.0           | 37.7 | 40.9 | 55.0              | 34.7 | 37.1      |
| SimSiam w/ DSSL  | 58.2           | 38.1 | 41.5 | 55.5              | 35.1 | 37.6      |

Table 4. Results of object detection and instance segmentation fine-tuned on COCO. We adopt Mask R-CNN R50-FPN with a 1x schedule and report the bounding box AP and mask AP on COCO 2017 val. Our reproduction and DSSL versions are based on 100 epoch pre-training in ImageNet. † : the reported 200-epoch result.

higher distortion magnitudes are selected. The results presented in Tab. [3](#page-6-2) support our hypothesis: the heavy augmentations play as a "double-edged sword" for instance-wise self-supervised learning; without using negative pairs, the model would be easily misled by the heavily augmented views. Although momentum encoder can ease heavily augmented views' side effects up to a point, DSSL is a more fundamental schema for handling various image transformations during instance-wise self-supervised learning.

Transfer to other vision tasks. We evaluate the representations benefited from DSSL on different tasks relevant to computer vision practitioners, including COCO [\[16\]](#page-8-23) object detection and instance segmentation. Unlike linear evaluation, we fine-tune the 100-epoch pre-trained BYOL and SimSiam models end-to-end in the COCO dataset. We apply the public codebase Detectron [\[9\]](#page-8-24) to implement Mask R-CNN [\[12\]](#page-8-25) detector and evaluate the COCO 2017 val. Tab. [4](#page-6-3) shows that DSSL can consistently improve the standard self-supervised learning method on downstream object detection and instance segmentation tasks.

### <span id="page-6-0"></span>4.3. Justification and ablation study

To understand how the asymmetric loss and partiallyordered views impact the representations learning, we design experiments with various settings of instance-wise selfsupervised learning on the CIFAR-10 dataset. We apply the

<span id="page-7-2"></span><span id="page-7-0"></span>![](_page_7_Figure_0.jpeg)

**Caption:** Figure 5 compares various view constructions and self-supervised learning mechanisms, presenting top-1 accuracies for CIFAR-10 linear classifiers. It highlights the detrimental effects of treating all views equally, demonstrating that distinguishing between standard and heavily augmented views significantly improves performance.

Figure 5. Comparisons of different views construction and instance-wise self-supervised learning mechanisms. Top-1 accuracies (%) of CIFAR-10 linear classifiers trained on the freeze representations are listed in the box below.

random resize-crop with color distortion as the standard image transformation, and apply *RA*(2,5) as heavy augmentation. Fig. [5](#page-7-0) shows the *augmentation, loss function design* v.s. *linear evaluation accuracy*.

Obviously, directly making all views to be similar without distinguishing standard and heavily augmented views hurts the performances (c:*collapse*, d:59.7↓, e:20.4↓). According to the hypothesis proposed in InfoMin [\[21\]](#page-8-11), it is the information about the task-relevant variable that is discarded by the hard view, degrading performance. Further, we study the impact of each components in Eq. [\(6\)](#page-4-1) and find below four phenomenons effectively illustrate the rationality of our proposed DSSL (Eq. [5\)](#page-4-2).

(i) Introducing symmetric loss P <sup>V</sup>T<sup>b</sup> L<sup>S</sup> among heavily augmented views results in collapsing during model training (c, f).

(ii) *Fairly contrasting between the standard view and heavily augmented view has negative effects*. After comparing the results of (h:*collapse*) and (j:94.2↑), the misleading impact of P <sup>V</sup>T ←T <sup>b</sup> L<sup>A</sup> can be observed obviously.

(iii) *Equipping directional feature prediction between heavily augmented view and standard view results in stable performance improvement* (d:59.7 vs. g:93.1).

(iv) *Partially-ordered views construction mechanism results in better performance*, as we compared among (g:93.1, i:93.7, j:94.2:↑). It mainly due to that the mutual information between <sup>v</sup> and its derived harder view <sup>v</sup><sup>b</sup> is guaranteed up to a point. Such a mechanism can prevent the issue of unexpected missing information.

P For more critical analysis on the influence of <sup>V</sup>T ←T <sup>b</sup> LA, we trained a SimSiam model according to Eq. [\(6\)](#page-4-1) by setting λ + δ = 1, α = 1, β = 0. As shown in Fig. [6,](#page-7-1) the negative impact of mapping heavily augmented view's feature to standard view only emerges after the value of δ raised to a threshold. At the early stage, the linear

<span id="page-7-1"></span>![](_page_7_Figure_9.jpeg)

**Caption:** Figure 6 shows the linear evaluation accuracy of SimSiam optimized under different settings. It illustrates how varying the loss weights impacts performance, revealing that mapping heavily augmented views to standard views can lead to model collapse, while maintaining a balance ensures stable accuracy.

Figure 6. CIFAR-10 linear evaluation accuracy of SimSiam optimized according to Eq. [\(6\)](#page-4-1) by setting α = 1, β = 0, and λ = 1−δ across varying δ. *RA* is applied to construct hard views.

evaluation accuracy remains stable. This phenomenon further reveals mapping standard view's feature to heavily augmented view would be high-risk and low-return.

Moreover, Eq. [\(6\)](#page-4-1) is the theoretical formulation of the standard instance-wise self-supervised learning method while fairly treating all views. Our DSSL only activates two training view pairs (comparing Eq. [\(6\)](#page-4-1) and Eq. [\(5\)](#page-4-2)). Thus DSSL's computation complexity can be thought to be only half of the previous methods when considering all possible view pairs during training.

# 5. Conclusion and Discussion

We propose a directional self-supervised learning (DSSL) framework for unsupervised visual representation learning. Compared to standard self-supervised learning methods, our proposed framework benefits from more heavy image transformations and results in stable performance improvement on various vision tasks. Moreover, DSSL is easy to implement and compatible with most of the typical instance-wise self-supervised learning methods. The core concepts of DSSL can further guide the loss design of self-supervised learning. According to our formulation in Eq. [\(6\)](#page-4-1), the soft weighted version of DSSL respecting to view characteristics is also worthy of further exploration.

# References

- <span id="page-8-9"></span>[1] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. *arXiv preprint arXiv:2006.09882*, 2020. [1,](#page-0-1) [3,](#page-2-0) [4,](#page-3-4) [10](#page-9-1)
- <span id="page-8-0"></span>[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In *International conference on machine learning*, pages 1597–1607. PMLR, 2020. [1,](#page-0-1) [3,](#page-2-0) [4,](#page-3-4) [5,](#page-4-3) [6](#page-5-2)
- <span id="page-8-5"></span>[3] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. *arXiv preprint arXiv:2011.10566*, 2020. [1,](#page-0-1) [2,](#page-1-1) [3,](#page-2-0) [4,](#page-3-4) [5,](#page-4-3) [6](#page-5-2)
- <span id="page-8-14"></span>[4] Yue Chen, Yalong Bai, Wei Zhang, and Tao Mei. Destruction and construction learning for fine-grained image recognition. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 5157–5166, 2019. [3,](#page-2-0) [5](#page-4-3)
- [5] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 113–123, 2019. [3](#page-2-0)
- <span id="page-8-7"></span>[6] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops*, pages 702–703, 2020. [1,](#page-0-1) [3,](#page-2-0) [5](#page-4-3)
- <span id="page-8-15"></span>[7] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. *arXiv preprint arXiv:1708.04552*, 2017. [3](#page-2-0)
- <span id="page-8-12"></span>[8] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks. *IEEE transactions on pattern analysis and machine intelligence*, 38(9):1734–1747, 2015. [3](#page-2-0)
- <span id="page-8-24"></span>[9] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollar, and Kaiming He. Detectron. ´ [https://github.](https://github.com/facebookresearch/detectron) [com/facebookresearch/detectron](https://github.com/facebookresearch/detectron), 2018. [7](#page-6-4)
- <span id="page-8-6"></span>[10] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin ´ Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. *arXiv preprint arXiv:2006.07733*, 2020. [1,](#page-0-1) [3,](#page-2-0) [4,](#page-3-4) [5,](#page-4-3) [6](#page-5-2)
- <span id="page-8-1"></span>[11] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 9729–9738, 2020. [1,](#page-0-1) [3](#page-2-0)
- <span id="page-8-25"></span>[12] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir- ´ shick. Mask r-cnn. In *Proceedings of the IEEE international conference on computer vision*, pages 2961–2969, 2017. [7](#page-6-4)
- <span id="page-8-13"></span>[13] Jiabo Huang, Qi Dong, Shaogang Gong, and Xiatian Zhu. Unsupervised deep learning by neighbourhood discovery. In *International Conference on Machine Learning*, pages 2849–2858. PMLR, 2019. [3](#page-2-0)
- <span id="page-8-22"></span>[14] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [6](#page-5-2)
- <span id="page-8-10"></span>[15] Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. *arXiv preprint arXiv:2005.04966*, 2020. [1,](#page-0-1) [2,](#page-1-1) [4](#page-3-4)
- <span id="page-8-23"></span>[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence ´ Zitnick. Microsoft coco: Common objects in context. In *European conference on computer vision*, pages 740–755. Springer, 2014. [7](#page-6-4)
- <span id="page-8-8"></span>[17] Tom Ching LingChen, Ava Khonsari, Amirreza Lashkari, Mina Rafi Nazari, Jaspreet Singh Sambee, and Mario A Nascimento. Uniformaugment: A search-free probabilistic data augmentation approach. *arXiv preprint arXiv:2003.14348*, 2020. [1,](#page-0-1) [6](#page-5-2)
- <span id="page-8-20"></span>[18] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In *European conference on computer vision*, pages 69–84. Springer, 2016. [5](#page-4-3)
- <span id="page-8-21"></span>[19] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. *International journal of computer vision*, 115(3):211–252, 2015. [6](#page-5-2)
- <span id="page-8-2"></span>[20] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. *arXiv preprint arXiv:1906.05849*, 2019. [1,](#page-0-1) [3](#page-2-0)
- <span id="page-8-11"></span>[21] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning. *arXiv preprint arXiv:2005.10243*, 2020. [1,](#page-0-1) [2,](#page-1-1) [3,](#page-2-0) [5,](#page-4-3) [6,](#page-5-2) [8](#page-7-2)
- <span id="page-8-17"></span>[22] Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. *arXiv preprint arXiv:1907.13625*, 2019. [3](#page-2-0)
- <span id="page-8-18"></span>[23] Xiao Wang and Guo-Jun Qi. Contrastive learning with stronger augmentations. *arXiv preprint arXiv:2104.07713*, 2021. [3](#page-2-0)
- <span id="page-8-3"></span>[24] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 3733– 3742, 2018. [1,](#page-0-1) [3](#page-2-0)
- <span id="page-8-19"></span>[25] Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszl ´ o A´ Jeni, and Fernando De la Torre. 3d human shape and pose from a single low-resolution image with self-supervised learning. In *European Conference on Computer Vision*, pages 284–300. Springer, 2020. [3](#page-2-0)
- <span id="page-8-4"></span>[26] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via invariant and spreading instance feature. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 6210–6219, 2019. [1,](#page-0-1) [3](#page-2-0)
- <span id="page-8-16"></span>[27] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable fea-

<span id="page-9-1"></span>tures. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 6023–6032, 2019. [3](#page-2-0)

<span id="page-9-0"></span>[28] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning of visual embeddings. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 6002–6012, 2019. [3](#page-2-0)

# A. Implementation details

#### A.1. Standard augmentations

First, an 224×224 image patch is randomly *cropped* from the image of *scale* in [0,2, 1.0] with random *horizontal flip*, followed by a *color distortion* with brightness, contrast, saturation, hue strength of {0.4, 0.4, 0.4, 0.1} with an applying probability of 0.8, and an optional *grayscale* conversion with applying probability of 0.2. At last, *Gaussian blur* with kernel std in [0.1, 2.0] processes the image patch with applying probability of 0.5. Each of above image augmentations has been proved to be effective in at least one typical instance-wise self-supervised learning method. The composition of all above image augmentations are treated as standard augmentation T .

#### A.2. Heavy augmentations

*RandAugment* covers various of image transformations and has been demonstrated with significant performance improvement on supervised representation learning. It has two tunable hyperparameters m, n, where n is the number of augmentation policies randomly selected from 14 predefined augmentations, m is the magnitude for all the transformation. Larger m and n results in stronger image transformation. In this paper, we denote *RandAugment* as *RA*(n, m). Similarity, *Jigsaw* has also been used in pretext task based self-supervised representation learning and finegrained visual patterns. The hyperparameter n for *Jigsaw* is the number of grid n × n for each images. Our heavy augmentations <sup>T</sup><sup>b</sup> consists of *RA*(2,5) and *Jigsaw*(4) with probability of 0.9 and 0.1 respectively. Except these two augmentation policies, we applied *RA*(8,16) and *UniformAugment* as the additional heavy augmentation (+Tb) for robustness analysis experiments in Table 3. There is no hyperparameter for *UniformAugment*. It is a search-free DA method consists of 13 different image transformations by assuming that the augmentation space is approximately distribution invariant.

We show some example images augmented by the standard augmentations and various heavy augmentations in Figure [7.](#page-10-0) All heavy augmented images are derived from the standard augmentation results.

#### A.3. ImageNet linear evaluation

For linear evaluation on ImageNet, we follow a similar procedure of SimCLR, BYOL, and SimSiam. The frozen

#### <span id="page-9-2"></span>Algorithm 2 The Pseudocode of SimSiam baseline with 2 view-pairs

```
# f: feature encoder; g: prediction head;
# aug: data augmentations;
for I in dataloader:
    # random augmentation
    v1, v
         0
         1, v2, v
                  0
                  2 = aug(I), aug(I), aug(I), aug(I)
    # projections
    y1, y
         0
         1, y2, y
                  0
                  2 = f(v1), f(v
                                 0
                                 1), f(v2), f(v
                                                0
                                                2)
    # predictions
    z1, z
         0
         1, z2, z
                  0
                  2 = g(y1), g(y
                                 0
                                 1), g(y2), g(y
                                                0
                                                2)
    # loss
    L = D(z1,y
               0
               1)/4 + D(z
                         0
                         1,y1)/4 + D(z2,y
                                           0
                                           2)/4 + D(z
                                                     0
                                                     2,y2)/4
    L.backward() # back-propagate
    update(f, h) # SGD update
def D(z, y): # negative cosine similarity
    y = y.detach() # stop gradient
    # l2-normalize
    z = normalize(z, dim=1)
    y = normalize(y, dim=1)
    return -(z * y).sum(dim=1).mean()
```
ResNet's global average pooling layer outputs are used to train a supervised linear classifier. For SimCLR, we use the SGD optimizer as the original paper recommended. For SimSiam and BYOL, we used a LARS optimizer. We trained all linear classifier with base lr = 1.6 (following lr = 0.1 × BatchSize/256) and batch size of 4096 for 90 epochs. For the SGD optimizer, we decay the learning rate with the linear decay schedule. For the LARS optimizer, we decay the learning rate with the cosine decay schedule. After training the linear classifiers, we evaluate them on the center cropped 224×224 resolution inputs in validation set.

# B. Compared methods

Algorithm [2](#page-9-2) shows the Pseudocode of our reimplemented SimSiam with four views. We list the SimSiam results of its original version (with two standard views), our re-implemented version with four standard views, and re-implemented versions with four heavily augmented views in Table [5.](#page-10-1) Although previous work SwAV [\[1\]](#page-8-9) shows that contrasting more image crops with smaller size (e.g., 96×96) with regular image crops (224×224) results in performance improvement owing to more visual details in low-resolution crops are highlighted, introducing more views with the same resolution in each min-batch has little impact on performance. Even though the number of views is increased, the frequency of model updating during training and the number of raw samples in min-batch keep the same and results in similar results (one view-pair vs. two view-pairs).

For SimCLR that considers negative samples in the whole min-batch (batch size of n), DSSL only add two heavily augmented views and two <sup>t</sup>(I) <sup>←</sup> <sup>b</sup>t(I) asymmetric loss items for each image. The computation complexity slightly increases from 2n 2 to 2n <sup>2</sup>+2n for each min-batch after applying DSSL.

<span id="page-10-0"></span>![](_page_10_Figure_0.jpeg)

**Caption:** Figure 7 presents examples of image augmentations applied to the ImageNet dataset. It showcases original images alongside their standard views and heavily augmented views derived from them, demonstrating the effects of various augmentation strategies, including RandAugment and Jigsaw, on visual representation.

Figure 7. Example views augmented by various data augmentation on ImageNet dataset. For each original image (in the first row), we show their standard views (in the second row), the heavily augmented views derived from the standard views by applying <sup>T</sup><sup>b</sup> as *RA*(2,5) and *Jigsaw*(4) (in the third row), and the results of the additional heavy augmentation *RA*(8,16) and *UA* (in the last two rows).

<span id="page-10-1"></span>

| Baseline Implementation        | CIFAR-10 | ImageNet |  |
|--------------------------------|----------|----------|--|
| 1 standard view-pair           | 92.14    | 67.7     |  |
| 2 standard view-pairs          | 91.68    | 67.4     |  |
| 2 heavily augmented view-pairs | collapse | collapse |  |

Table 5. Linear evaluation Top-1 accuracy of SimSiam on ImageNet and CIFAR-10 with same training epochs but different training pair settings.

As shown in Algorithm 1 and Eq. (5), DSSL has no additional hyperparameters comparing to the standard instancewise self-supervised learning methods.