# Joint Data and Feature Augmentation for Self-Supervised Representation Learning on Point Clouds *⋆*

Zhuheng Lu *a* , Yuewei Dai *a* , Weiqing Li *b* and Zhiyong Su *a*

*<sup>a</sup>School of Automation, Nanjing University of Science and Technology, China*

*<sup>b</sup>School of Computer Science and Engineering, Nanjing University of Science and Technology, China*

#### A R T I C L E I N F O

*Keywords* : Contrastive Learning Self-supervised Learning Point Cloud Representation Learning

#### A B S T R A C T

To deal with the exhausting annotations, self-supervised representation learning from unlabeled point clouds has drawn much attention, especially centered on augmentation-based contrastive methods. However, specific augmentations hardly produce sufficient transferability to high-level tasks on different datasets. Besides, augmentations on point clouds may also change underlying semantics. To address the issues, we propose a simple but efficient augmentation fusion contrastive learning framework to combine data augmentations in Euclidean space and feature augmentations in feature space. In particular, we propose a data augmentation method based on sampling and graph generation. Meanwhile, we design a data augmentation network to enable a correspondence of representations by maximizing consistency between augmented graph pairs. We further design a feature augmentation network that encourages the model to learn representations invariant to the perturbations using an encoder perturbation. We comprehensively conduct extensive object classification experiments and object part segmentation experiments to validate the transferability of the proposed framework. Experimental results demonstrate that the proposed framework is effective to learn the point cloud representation in a self-supervised manner, and yields state-of-the-art results in the community. The source code is publicly available at: https://zhiyongsu.github.io/Project/AFSRL.html.

### **1. Introduction**

Representation learning on point clouds is a challenging task due to the irregular structure and also the need for permutation invariance when processing each point. With the rapid development of deep learning-based approaches [\[29](#page-7-0) , [41](#page-8-0) , [43](#page-8-1)], many works have been proposed for 3D representation problems such as 3D object classification [\[42](#page-8-2), [51,](#page-8-3) [58\]](#page-8-4), detection [\[37](#page-8-5) , [40](#page-8-6) , [73\]](#page-9-0) and segmentation [ [5](#page-7-1) , [6](#page-7-2) , [21](#page-7-3) , [50](#page-8-7) , [57\]](#page-8-8). Most existing deep neural networks are usually trained in a supervised manner. However it is often very expensive and time-consuming to collect precise point annotations. T o remedy this issue, referring to self-supervised learning from unlabeled images [ [9](#page-7-4) , [12](#page-7-5) , [19](#page-7-6) , [33](#page-8-9) , [77\]](#page-9-1) and videos [\[47](#page-8-10) , [55](#page-8-11) , [56](#page-8-12) , [76](#page-9-2)], self-supervised representation learning from unlabele d point clouds has drawn much attention in the community.

In recent years, numerous self-supervised learning methods have been proposed for point cloud representation learning, which can be divided into generative approaches and discriminative approaches. Generative approaches generate labels with the attributes of the data, and are based on reconstruction [\[11](#page-7-7) , [18](#page-7-8) , [32](#page-8-13) , [46](#page-8-14) , [54](#page-8-15)], generative models [ [3](#page-7-9) , [59\]](#page-8-16), and other pretext tasks [\[39](#page-8-17) , [44](#page-8-18)]. However, generative methods are computationally expensive, and limit the generality of the learned representations. Discriminative approaches [\[13](#page-7-10) , [22](#page-7-11) , [45](#page-8-19) , [63](#page-8-20) , [71](#page-9-3) , [72\]](#page-9-4) relying on augmentations of point clouds perform discrimination between positive and negative pairs, achieving state-of-the-art results. Typical point cloud data augmentations in Euclidean space include geometric transformation, point coordinate jittering, subcloud sampling, point dropout, and point-level invariant mapping. Despite significant progress on contrastive learn ing approaches based on augmentations, many challenges remain. First, specific augmentations of point clouds are no t suitable for all scenarios because the structural information of the point clouds varies significantly across scenarios, making it difficult to learn the transferable point cloud representations. Second, even with minimal disruption, it is challenging to maintain semantics adequately throughout typical augmentations. Therefore, it is highly desirable t o seek a transferable and effective learning framework to further improve the performance of point cloud representations.

This paper proposes an Augmentation Fusion Self-Supervised Representation Learning (AFSRL) framework by combining data augmentations in Euclidean space and feature augmentations in feature space to construct a stabl e and invariant point cloud representation. The AFSRL is composed of three modules: data augmentation module, data augmentation network, and feature augmentation network. The data augmentation module first generates augmented graph pairs as the input of the data augmentation network. The goal of the data augmentation network is to capture the correspondence between the augmented pairs. The feature augmentation network is inspired by the observation [\[62](#page-8-21) ] that graph data can maintain their semantics effectively during encoder perturbations without necessitating manua l trial-and-error, time-consuming search, or expensive domain expertise. Specifically, we take one generated graph as input and a perturbed GNN model as the encoder to obtain the augmented feature representation. Moreover, we consider maximizing the consistency of the corresponding representations. With the encoder perturbation as noise, w e can obtain two different feature embeddings for the same

<sup>∗</sup>Corresponding author. E-mail address: su@njust.edu.cn (Z.Su) ORCID(s):

input as positive pairs. The perturbation cues boost the performance of learned representations.

To evaluate the proposed self-supervised representation learning framework, we adopt the learned representation for object classification and part segmentation tasks. We confirm that the learned representations are easily transferable to downstream tasks directly by pre-training on large datasets. According to the experimental results, we attain new stateof-the-art performances among all these tasks.

In summary, our main contributions in this paper are as follows:

- We introduce an augmentation fusion framework that imposes invariance to data augmentation, and simultaneously feature augmentation encourages the model to learn representations invariant to the perturbations.
- We demonstrate that the transferability of learned representations, which can be readily adapted to downstream tasks on different datasets.
- Experimental results show that compared with other unsupervised methods, our AFSRL achieves competitive results and narrows the gap between unsupervised methods and supervised methods on a series of datasets.

The rest of the paper is organized as follows. Section [2](#page-1-0) briefly reviews the related work on supervised representation learning on point clouds and self-supervised learning on point clouds. In Section [3,](#page-2-0) an overview of the framework is provided. Then, the details of the architecture are introduced. After that, experimental results are presented in Section [4.](#page-3-0) Finally, conclusions and recommendations for future research are given in Section [5.](#page-6-0)

# <span id="page-1-0"></span>**2. Related Work**

In this section, we review a number of previous works on supervised representation learning and self-supervised learning on point clouds.

### **2.1. Supervised Representation Learning on Point Clouds**

Many approaches have been proposed to address various tasks on unordered point cloud representations, such as object classification [\[1](#page-6-1), [29,](#page-7-0) [35](#page-8-22), [42](#page-8-2), [43,](#page-8-1) [51](#page-8-3), [58\]](#page-8-4), object detection [\[37,](#page-8-5) [38](#page-8-23), [40,](#page-8-6) [48](#page-8-24), [69,](#page-9-5) [73\]](#page-9-0), and segmentation [\[2](#page-6-2), [5,](#page-7-1) [21](#page-7-3), [24](#page-7-12), [50,](#page-8-7) [57,](#page-8-8) [64](#page-8-25), [66\]](#page-8-26). Approaches on point cloud supervised representation learning can be mainly classified into three categories: point-based, view-based, and voxel-based.

Among the point-based approaches, one pioneer method PointNet [\[41\]](#page-8-0) was designed to directly feed raw point clouds into neural networks, and obtain the global point cloud feature. The framework can be used for point cloud segmentation, part segmentation, semantic classification, and other tasks. Since then, numerous advancements have been made in point-based tasks. Qi et al. [\[43](#page-8-1)] proposed PointNet++ to hierarchically aggregate the local features of the point cloud. Li et al. [\[29\]](#page-7-0) proposed PointCNN which uses - Conv operator on the input point cloud, and then applies a typical convolution operator on the transformed features. Another branch of point-based approaches relies on graph convolutions on 3D point clouds. Wang et al. [\[58\]](#page-8-4) proposed DGCNN using an edge-convolution network (EdgeConv) to specifically model local neighborhood information. Landrieu et al. [\[26](#page-7-13)] generate a superpoint graph of a point cloud and learn the 3D point geometric organization.

View-based methods generate a shape descriptor from multiple views of each point cloud. Su et al. [\[49](#page-8-27)] proposed MVCNN to apply multi-view convolutional neural network for 3D recognition. The 3D shapes were rendered in different views, each of which is passed through a unified CNN architecture. Xie et al. [\[65](#page-8-28)] generated 2.5D depth images and proposed an extreme learning machine to achieve projective feature learning for 3D shapes. He et al. [\[20\]](#page-7-14) proposed a combination scheme of group-view similarity learning and adaptive margin-based triplet-center loss to improve MVCNN for 3D shape retrieval

Voxel-based methods use convolutional neural networks to extract 3D shape descriptors from regular and dense 3D shape voxel. Wu et al. [\[60](#page-8-29)] proposed 3D ShapeNets to learn the hierarchical compositional part representation of complex 3D shapes from 3D shape volumetric data. Maturana et al. [\[36](#page-8-30)] proposed VoxNet to integrate a volumetric representation with a supervised 3D Convolutional Neural Network. Le et al. [\[27\]](#page-7-15) proposed PointGrid which incorporates a constant number of points in each grid cell thus allowing the network to better capture local geometric details.

### **2.2. Self-supervised Learning on Point Clouds**

Recently, many approaches have sought to explore semantic information in an unsupervised manner with designed pretext tasks. Existing self-supervised learning on point clouds can be classified as either generative or discriminative.

Generative approaches build a distribution over data and latent embedding and use the learned embeddings as point cloud representations. These generative models use generative adversarial networks [\[3,](#page-7-9) [17](#page-7-16), [59\]](#page-8-16) or auto-encoders [\[14,](#page-7-17) [18,](#page-7-8) [28,](#page-7-18) [67,](#page-8-31) [75](#page-9-6)]. However, generative approaches are computationally expensive, and the learning of generalizable representation unnecessarily relies on recovering such highlevel details.

Among discriminative methods, contrastive methods [\[13,](#page-7-10) [22,](#page-7-11) [45](#page-8-19), [63,](#page-8-20) [71](#page-9-3), [72\]](#page-9-4) currently achieve state-of-the-art performance in self-supervised learning. Different from generative approaches that maximize the data likelihood, the contrastive learning-based methods typically learn representation by maximizing feature similarity between differently augmented pairs. Typical point cloud data augmentations include geometric transformation; point coordinate jittering; subcloud sampling; point dropout; and point-level invariant mapping. Zhang et al. [\[71](#page-9-3)] proposed the part contrast by segmenting one object into two parts to learn features from

Short Title of the Article

![](_page_2_Figure_1.jpeg)

**Caption:** Figure 1 illustrates the architecture of the proposed Augmentation Fusion Self-Supervised Representation Learning (AFSRL) framework. It consists of a data augmentation module generating graph pairs, a data augmentation network maximizing representation similarity, and a feature augmentation network utilizing a perturbed GNN encoder to enhance representation consistency, facilitating effective self-supervised learning from point clouds.

<span id="page-2-1"></span>Figure 1: The architecture of the proposed AFSRL for self-supervised point cloud representation learning. The input 3D point clouds are firstly generated into graph pairs. Data augmentation network inputs augmented graph pairs to a shared GNN encoder and maximizes the similarity of the representation pairs. Moreover, feature augmentation network takes one generated graph as input and the perturbed GNN as encoder to obtain the correlated representation. The AFSRL trains the model by combining the learning objectives of the networks.

unlabeled point clouds. Afham et al. [\[4](#page-7-19)] proposed a crossmodal contrastive learning approach to encourage the 2D image feature to be embedded close to the corresponding 3D point cloud. Xie et al. [\[63\]](#page-8-20) proposed PointContrast on two transformed views of the given point cloud with a contrastive loss. Despite the success, choosing an augmentation still requires the time-consuming manual trial-and-error, laborious search, or expensive domain knowledge. Instead, our AFSRL breaks through typical data augmentation methods as prerequisite and narrow the augmentation gap mentioned above.

# <span id="page-2-0"></span>**3. Method**

# **3.1. Overview**

Figure [1](#page-2-1) illustrates our AFSRL architecture, which consists data augmentation module, data augmentation network, and feature augmentation network. The augmentation module first generates graphs via sampling and *퐾*-nearest neighbor as the data augmentation. The data augmentation network inputs augmented graph pairs *푢* and *푣* to a shared GNN encoder and maximizes the similarity of the representation pairs. Meanwhile, the feature augmentation network takes one graph from the augmented graph pairs as input, and employs the perturbed GNN as the encoder to obtain the augmented feature representation. The feature augmentation network maximizes the consistency of correlated representations using contrastive learning loss functions. The AFSRL

trains the model by combining the learning objectives of two networks.

# <span id="page-2-2"></span>**3.2. Data Augmentation Module**

The data augmentation module obtains positive pairs of the point clouds as the input of the following networks in an unsupervised setting. This can be thought of as a form of data augmentation, which is an essential component of our framework. First, given a point cloud , we randomly sample two versions of point cloud *푃 푢* , *푃 <sup>푣</sup>* with a uniform scale. Then, we generate the *퐾*-nearest neighbor adjacency graphs *푢* , *<sup>푣</sup>* of the sampled point clouds as the augmented pairs of the input point cloud . Our method differs from traditional data augmentations by explicitly preserving the semantic structure to create better embeddings.

# **3.3. Data Augmentation Network**

The data augmentation network consists of a shared encoder and a shared projection head. Inspired by recent contrastive learning algorithms, the data augmentation network learns representations by maximizing consistency between differently augmented graphs of the same point cloud via a contrastive loss in the latent space.

First, following the architecture in [\[15,](#page-7-20) [62\]](#page-8-21), a backbone GNN encoder *푓*(⋅; *휃*) is used to extract representation vectors, which are denoted as **퐘***<sup>푢</sup>* and **퐘***<sup>푣</sup>* , from augmented graph examples,

$$
\mathbf{Y}^{u} = f(\mathcal{G}^{u}; \theta), \ \mathbf{Y}^{v} = f(\mathcal{G}^{v}; \theta). \tag{1}
$$

Any deep GNN can be used as the feature extractor, such as GCN [\[23](#page-7-21)], GAT [\[53](#page-8-32)], and GRAPHSAGE [\[16](#page-7-22)].

Then, a neural network projection head *푔*(⋅) is provided to map representations to the space where contrastive loss is applied. We denote the projected vectors of **퐘***<sup>푢</sup>* and **퐘***<sup>푣</sup>* as **퐙** *푢* and **퐙** *푣* , respectively. As proved by several previous approaches [\[8,](#page-7-23) [62\]](#page-8-21), a projection head can enhance performance. By leveraging the nonlinear transformation *푔*(⋅), more information can be formed and maintained in the representation to prevent information loss induced by the contrastive loss. Besides, a nonlinear projectionis better than a linear projection. Therefore, in this framework, we adopt a multi-layer perceptron (MLP) to obtain the projected vectors **퐙** *푢* and **퐙** *푣* ,

$$
\mathbf{Z}^u = g(\mathbf{Y}^u), \mathbf{Z}^v = g(\mathbf{Y}^v). \tag{2}
$$

The goal is to maximize the similarity of **퐙** *<sup>푢</sup>* with **퐙** *푣* while minimizing the similarity with all the other projected vectors in the minibatch of graphs. The AFSRL is then trained by minimizing normalized temperature-scaled cross entropy loss (NT-Xent) [\[61,](#page-8-33) [62,](#page-8-21) [70\]](#page-9-7) to maximize the consistency of **퐙** *푢* and **퐙** *푣* . We compute the cosine similarity between **퐙** *푢* and **퐙** *푣* as follows:

$$
s(\mathbf{Z}^u, \mathbf{Z}^v) = \frac{\mathbf{Z}^u \mathbf{Z}^v}{\parallel \mathbf{Z}^u \parallel \parallel \mathbf{Z}^v \parallel}. \tag{3}
$$

We compute the loss function *푙* for the *푛*-th augmented pair of examples as:

$$
l(n, \mathbf{Z}^u, \mathbf{Z}^v) = -\log \frac{\exp(s(\mathbf{Z}_n^u, \mathbf{Z}_n^v)/\tau)}{\sum\limits_{\substack{k=1 \ k \neq n}}^N \exp(s(\mathbf{Z}_n^u, \mathbf{Z}_k^u)/\tau) + \sum\limits_{\substack{k=1 \ k \neq n}}^N \exp(s(\mathbf{Z}_n^u, \mathbf{Z}_k^v)/\tau)},
$$
(4)

where *휏* is the temperature parameter, *푁* is the minibatch size. The final loss *퐷퐴* is computed across all positive pairs in the minibatch:

$$
\mathcal{L}_{DA} = \frac{1}{2N} \sum_{n=1}^{N} [l(n, \mathbf{Z}^u, \mathbf{Z}^v) + l(n, \mathbf{Z}^v, \mathbf{Z}^u)].
$$
 (5)

#### **3.4. Feature Augmentation Network**

We introduce feature augmentation network to learn the representations through the interactions of two branches, thus yielding better representation learning capability of point clouds. The feature augmentation network has a similar architecture as the data augmentation network, but uses a different set of weights.

More precisely, we first obtain a corresponding perturbed encoder *푓*(⋅; *휃* ′ ) of the backbone GNN encoder *푓*(⋅; *휃*). The perturbed encoder *푓*(⋅; *휃* ′ ) computes the representations **퐑** for each input graph , which can be described as

$$
\mathbf{R} = f(\mathcal{G}; \theta'). \tag{6}
$$

The feature augmentation network perturbs the encoder with a random Gaussian noise instead of momentum updating [\[15\]](#page-7-20). Therefore, the perturbed encoder *푓*(*휃* ′ ) can be mathematically described as

$$
\theta'_{l} = \theta_{l} + \epsilon \cdot \hat{\theta}_{l},\tag{7}
$$

where *휃<sup>푙</sup>* is the weight tensor of the *푙*-th layer of the GNN encoder, *휃* ′ *푙* is the weight tensor of the corresponding perturbed version, *휖* is a parameter that controls the scale of the perturbation, and *휃̂ <sup>푙</sup>* denotes the perturbation term that samples from the Gaussian distribution (0*, 휎*<sup>2</sup> *푙* ).

Afterwards, the representation **퐑** is projected to a latent space by a non-linear transformation *푔*(⋅),

$$
\mathbf{H} = g(\mathbf{R}).\tag{8}
$$

The AFSRL is then trained by minimizing normalized temperature-scaled cross entropy loss to maximize the consistency of **퐙** and **퐇**. Then the loss function for a perturbed pair of examples is defined as

$$
l(n, \mathbf{Z}, \mathbf{H}) = -\log \frac{\exp(s(\mathbf{Z}_n, \mathbf{H}_n)/\tau)}{\sum_{\substack{k=1 \ k \neq n}}^{N} \exp(s(\mathbf{Z}_n, \mathbf{Z}_k)/\tau) + \sum_{k=1}^{N} \exp(s(\mathbf{Z}_n, \mathbf{H}_k)/\tau)},
$$
(9)

where *휏* is the temperature parameter. The final loss function *퐹 퐴* is computed across all positive pairs in the minibatch,

$$
\mathcal{L}_{FA} = \frac{1}{2N} \sum_{n=1}^{N} [l(n, \mathbf{Z}, \mathbf{H}) + l(n, \mathbf{H}, \mathbf{Z})].
$$
 (10)

#### **3.5. Overall Objective**

Finally, we obtain the loss function during training with the combination of *퐷퐴* and *퐹 퐴*, where *퐷퐴* imposes invariance to data augmentation while *퐹 퐴* enforces the model to learn representations invariant to the perturbations,

$$
\mathcal{L} = \alpha \mathcal{L}_{DA} + \beta \mathcal{L}_{FA},\tag{11}
$$

where the constant *훼* and *훽* balance the contrastive loss *퐷퐴* and *퐹 퐴*, respectively. Algorithm [1](#page-4-0) details the proposed AFSRL.

### <span id="page-3-0"></span>**4. Experimental Results**

In this section, we evaluate the feature representations of point clouds learned with the proposed AFSRL for object classification and part segmentation task. We first introduce the datasets and implementation details, and then present the experimental results.

<span id="page-4-0"></span>

| Algorithm 1: Main learning algorithm of AFSRL                                                      |  |  |  |
|----------------------------------------------------------------------------------------------------|--|--|--|
| Input: Point cloud set <br>= {1<br>, 2<br>, ⋯ , 퐿}, batch                                      |  |  |  |
| size 푁, initial encoder weights 휃                                                                  |  |  |  |
| 1 for each point cloud do                                                                          |  |  |  |
| 푢<br>푣<br>Sample two augmentation <br>, <br>// data<br>2                                         |  |  |  |
| augmentation                                                                                       |  |  |  |
| for each mini-batch do<br>3                                                                        |  |  |  |
| 퐘푢 =<br>; 휃), 퐘푣 =<br>푢<br>푣<br>푓(<br>푓(<br>; 휃) // encode<br>4                                  |  |  |  |
| 퐙<br>), 퐙<br>푢 =<br>푢<br>푣<br>푔(푦<br>= 푔(푦<br>) // project<br>5                                    |  |  |  |
| 1<br>∑푁<br>푛=1[푙(푛, 퐙<br>, 퐙<br>) + 푙(푛, 퐙<br>, 퐙<br>푢<br>푣<br>푣<br>푢<br>퐷퐴<br>=<br>)]<br>6<br>2푁 |  |  |  |
| // compute loss                                                                                    |  |  |  |
| 퐑<br>′<br>= 푓(;<br>휃<br>) // encode<br>7                                                          |  |  |  |
| 퐇<br>= 푔(퐑)<br>// project<br>8                                                                     |  |  |  |
| ∑푁<br>1<br>푛=1[푙(푛, 퐙,<br>퐇) +<br>푙(푛, 퐇,<br>퐙)]<br>퐹 퐴<br>=<br>//<br>9<br>2푁                     |  |  |  |
| compute loss                                                                                       |  |  |  |
| <br>= 훼퐷퐴<br>+ 훽퐹 퐴<br>10                                                                       |  |  |  |
| end<br>11                                                                                          |  |  |  |
| Update weights<br>12                                                                               |  |  |  |
| 13 end                                                                                             |  |  |  |
|                                                                                                    |  |  |  |

#### **4.1. Pre-training**

**Dataset.** We use the ShapeNet [\[7](#page-7-24)] for learning the selfsupervised representation, which consists of 57448 objects from 55 categories. By augmenting each point cloud into two relevant graphs defined in Section [3.2,](#page-2-2) we generate positive pairs of point clouds as the input.

**Implementation Details.** We employ the GCN proposed in [\[23\]](#page-7-21) as the feature extractor. The encoder architecture is defined as:

$$
\mathbf{X}^{(l)} = \sigma(\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}\mathbf{X}\mathbf{W}^{(l)}),\tag{12}
$$

where **퐗**(*푙*) is the node embedding matrix of the *푙*-th layer, **퐃** is the degree matrix, **퐀** is the adjacency matrix, *휎*(⋅) is a nonlinear activation function such as ReLU, and **퐖** is the trainable weight matrix for the *푙*-th layer. We employ a 2 layer MLP as the projection heads. *훼* and *훽* are set as equal weight in the training phase.

### **4.2. Object Classification**

We evaluate our classification experiments on Model-Net40 [\[60\]](#page-8-29) and ScanObjectNN [\[52](#page-8-34)]. ModelNet40 is a synthetic dataset obtained by sampling 3D CAD models. It includes 12331 objects from 40 categories, and the dataset is split into 9843 examples for training and 2468 for testing. ScanObjectNN is a real scanned 3D point cloud classification dataset. It contains 2880 objects from 15 categories, 2304 for training and 576 for testing.

We follow the same protocols in [\[4,](#page-7-19) [22\]](#page-7-11) to train a linear Support Vector Machine (SVM) using the encoded feature representations.We randomly sample 1024 points from each object for both training and testing the classification results.

Table [1](#page-4-1) tabulates the linear classification results on ModelNet40 benchmarks. Our method achieves an object classification accuracy of 91.4%. We compare our method with the results of state-of-the-art methods in terms of class average

### Table 1

<span id="page-4-1"></span>Comparison of classification results with existing methods on ModelNet40.

| Method                           | Accuracy (%) |
|----------------------------------|--------------|
| 3D-GAN [59]                      | 83.3         |
| Latent-GAN [3]                   | 85.7         |
| SO-Net [28]                      | 87.3         |
| MRTNet [18]                      | 86.4         |
| 3D-PointCapsNet [75]             | 88.9         |
| FoldingNet [67]                  | 88.4         |
| ClusterNet [71]                  | 86.8         |
| DepthContrast [72]               | 85.4         |
| Sauder et al. + PointNet [46]    | 87.3         |
| Sauder et al. + DGCNN [46]       | 89.1         |
| Poursaeed et al. + PointNet [39] | 88.6         |
| Poursaeed et al. + DGCNN [39]    | 90.7         |
| CrossPoint + PointNet [4]        | 89.1         |
| CrossPoint + DGCNN [4]           | 91.2         |
| STRL + PointNet [22]             | 88.3         |
| STRL + DGCNN [22]                | 90.9         |
| AFSRL( Ours )                    | 91.5         |

#### Table 2

<span id="page-4-2"></span>Comparison of classification results with existing methods on ScanObjectNN.

| Method                        | Accuracy (%) |
|-------------------------------|--------------|
| Sauder et al. + PointNet [46] | 55.2         |
| Sauder et al. + DGCNN [46]    | 59.5         |
| OcCo + PointNet [54]          | 69.5         |
| OcCo + DGCNN [54]             | 78.3         |
| CrossPoint + PointNet [4]     | 75.6         |
| CrossPoint + DGCNN [4]        | 81.7         |
| STRL + PointNet [22]          | 74.2         |
| STRL + DGCNN [22]             | 77.9         |
| AFSRL( Ours )                 | 82.3         |

accuracy. The proposed method outperforms previous stateof-the-art unsupervised methods on the ModelNet40 dataset, which justifies the effectiveness of our method.

Table [2](#page-4-2) reports the linear classification results on ScanObjectNN benchmarks. Our method achieves an object classification accuracy of 82.1%. The proposed AFSRL also

| Table 3 |  |
|---------|--|
|---------|--|

<span id="page-5-0"></span>Comparison of part segmentation results with existing methods on ShapeNetPart dataset.

| Method                      | Supervision     | Mean IoU (%) |
|-----------------------------|-----------------|--------------|
| PointNet [41]               |                 | 83.7         |
| PointNet++ [43]             |                 | 85.1         |
| PointCNN [29]               |                 | 86.1         |
| DGCNN [58]                  |                 | 85.1         |
| KD-Net [24]                 | Supervised      | 82.3         |
| Point2Sequence [31]         |                 | 85.2         |
| DensePoint [34]             |                 | 86.4         |
| PointTransformer [74]       |                 | 86.6         |
| Stratified Transformer [25] |                 | 86.6         |
| Self-Contrast [13]          |                 | 82.3         |
| Sauder et al. [46]          |                 | 85.3         |
| OcCo [54]                   |                 | 85.0         |
| PointContrast [63]          | Self-Supervised | 85.1         |
| Liu et al. [30]             |                 | 85.3         |
| CrossPoint [4]              |                 | 85.5         |
| AFSRL                       |                 | 85.7         |

outperforms all the state-of-the-art unsupervised and selfsupervised methods on ScanObjectNN. Our method is extremely effective in extracting discriminant features from the results.

### **4.3. Object Part Segmentation**

We transfer the pre-trained model to the object part segmentation experiments to further validate that our approach is qualified for point cloud representation learning. The goal of object part segmentation is to predict the semantic part label for each point of input point clouds. For this task, the ShapeNetPart dataset [\[68\]](#page-8-36) is used as the benchmark dataset. The ShapeNetPart benchmark dataset consists of 16881 3D objects from 16 categories. We employ 12137 models for training, and 2874 models for testing following PointNet++. We use the mean Intersection-over-Union (IoU) as the evaluation metric. IoU is computed between ground truth and the prediction for each part of an object. The mean IoU (mIoU) is then calculated by averaging the IoUs of all test objects.

We compare our approach with both unsupervised approaches and supervised approaches, as tabulated in Table [3.](#page-5-0) Compared with the stated-of-the-art unsupervised methods, the AFSRL acquires the best mean IoU of 85.6%. This is mainly due to the fact that the proposed augmentation fusion method preserves semantic information better. Figure [2](#page-6-3) visualizes some examples of our segmentation results, where the segmentation results are highly consistent with the ground truth.

### **4.4. Ablations and Analysis**

**Impact of the fusion learning objective.** We first evaluate the importance of the two networks proposed in this framework. Figure [3](#page-6-4) shows the impact of the data augmentation network and feature augmentation network for the task of object classification on ModelNet40 and ScanObjectNN datasets. We find that the combination of data augmentation network and feature augmentation network contribute to better learned representations than they do separately. The data augmentation network network enforces the model to capture the consistency between the graph pairs, while the feature augmentation network can preserve data semantics with the encoder perturbation. Therefore, it is critical to combine data augmentation with feature augmentation in order to learn generalizable features.

**Magnitude of the perturbation.** We then explore the magnitude of the perturbation which influences the learned representations. As can be observed from Figure [4,](#page-6-5) if we set the magnitude of the perturbation as zero (*휖*= 0), the performance is the lowest compared with other setting of perturbation on both ModelNet40 and ScanObjectNN datasets. Without perturbation, the AFSRL simply compares two original samples as a negative pair and ignores the positive pair loss. Instead, appropriate perturbations encourage the model to learn representations invariant to the perturbations through maximizing the consistency between a graph and the perturbed version. It has come to our attention that increasing the amplitude of the perturbation while remaining within an adequate range can bring about a consistent performance improvement. On the other hand, overly significant perturbations will result in a decrease in performance since the semantics of the graph data will not be maintained.

**Transferability.** Next, we aim to verify that our network has the ability to transfer the pre-trained models to other data domains using different pre-trained datasets. We pre-train the model on the existing largest natural dataset ScanNet [\[10\]](#page-7-28) and synthetic data ShapeNet [\[7\]](#page-7-24), and test their generalizability to different domains. Table [4](#page-7-29) reports the cross-domain experimental results, demonstrating the successful transfer from models pre-trained on natural scenes to the synthetic shape domain. The classification task achieves better performance when using ShapeNet as the pre-trained datasets compared with using ScanNet as pre-trained datasets for both ModelNet40 and ScanObjectNN datasets. This is because the ShapeNet dataset provides point clouds with clean spatial structures and fewer noises, which is beneficial to the pretrained model to learn effective representations.

**Generalizability.** We validate the practical use of AF-SRL using GCN [\[23\]](#page-7-21), GAT [\[53\]](#page-8-32), and GRAPHSAGE [\[16\]](#page-7-22) as the encoders, respectively. The flexibility of the proposed architecture allows us to seamlessly integrate it into different kind of GNN architectures. Table [5](#page-7-30) reports the classification results on ModelNet40 and ScanObjectNN datasets. Note that the GCN outperforms the other two backbones as the encoder.

Short Title of the Article

![](_page_6_Figure_1.jpeg)

**Caption:** Figure 2 presents qualitative results of part segmentation on the ShapeNetPart dataset. Each pair shows the ground truth (GT) on the left and the segmentation results from our method on the right. The consistent color coding indicates accurate semantic part labeling across various object categories, demonstrating the effectiveness of the proposed AFSRL framework.

<span id="page-6-3"></span>Figure 2: Qualitative results of part segmentation on the ShapeNetPart dataset. In each object pair, the left column is the ground truth (GT), and right column is our segmentation result, where parts with the same color have a consistent meaning. Four categories of objects are table, lamp, chair, and airplane.

![](_page_6_Figure_3.jpeg)

**Caption:** Figure 3 depicts the impact of the data and feature augmentation networks on object classification accuracy across ModelNet40 and ScanObjectNN datasets. The results indicate that the combined approach significantly enhances representation learning, outperforming individual network contributions, thus validating the effectiveness of the AFSRL framework.

<span id="page-6-4"></span>Figure 3: Effect of each network of the proposed framework for the task of object classification on ModelNet40 and ScanObjectNN datasets in terms of accuracy.

# <span id="page-6-0"></span>**5. Conclusion**

In this paper, we propose a self-supervised representation learning framework on point clouds. Unlike prior methods that rely on heuristics for data augmentation, our method learns the representation by the combination of data augmentation and feature augmentation. We validate our representation learning method on object classification and part segmentation tasks, and make extensive comparisons with the state-of-the-art approaches. The experimental results show that AFSRL can achieve superior performance compared to existing self-supervised methods. Although in this paper we only implement AFSRL on two tasks, AFSRL can also be applied to other problems with appropriate transformations on the data sets. We intend to investigate these issues in future works.

![](_page_6_Figure_7.jpeg)

**Caption:** Figure 4 shows the effect of perturbation magnitude on classification accuracy for ModelNet40 and ScanObjectNN datasets. The results highlight that optimal perturbation levels improve representation learning, while excessive perturbations degrade performance, underscoring the importance of maintaining semantic integrity during augmentation.

<span id="page-6-5"></span>Figure 4: Effect of magnitude of the perturbation for the task of object classification on ModelNet40 and ScanObjectNN datasets in terms of accuracy.

# **Acknowledgement**

The authors sincerely acknowledge the anonymous reviewers for their insights and comments to further improve the quality of the manuscript. They also would like to thank the participants in the study for their valuable time.

### **References**

- <span id="page-6-1"></span>[1] , 2022. A deep architecture for log-euclidean fisher vector end-to-end learning with application to 3d point cloud classification. Graphical Models 123, 101164.
- <span id="page-6-2"></span>[2] , 2022. Graph-pbn: Graph-based parallel branch network for efficient point cloud learning. Graphical Models 119, 101120.

#### Table 4

<span id="page-7-29"></span>Linear evaluation for shape classification across different pretrain datasets.

| Dataset      | Pre-train Dataset | Accuracy (%) |
|--------------|-------------------|--------------|
| ModelNet40   | ScanNet           | 90.7         |
| ModelNet40   | ShapeNet          | 91.5         |
| ScanObjectNN | ScanNet           | 81.7         |
| ScanObjectNN | ShapeNet          | 82.3         |

#### Table 5

<span id="page-7-30"></span>Linear evaluation for shape classification with different GNN encoders.

| Model          | ModelNet40 | ScanObjectNN |
|----------------|------------|--------------|
| GRAPHSAGE [16] | 89.7       | 80.5         |
| GAT [53]       | 90.7       | 81.1         |
| GCN [23]       | 91.5       | 82.3         |

- <span id="page-7-9"></span>[3] Achlioptas, P., Diamanti, O., Mitliagkas, I., Guibas, L., 2018. Learning representations and generative models for 3d point clouds, in: International conference on machine learning, PMLR. pp. 40–49.
- <span id="page-7-19"></span>[4] Afham, M., Dissanayake, I., Dissanayake, D., Dharmasiri, A., Thilakarathna, K., Rodrigo, R., 2022. Crosspoint: Self-supervised crossmodal contrastive learning for 3d point cloud understanding, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9902–9912.
- <span id="page-7-1"></span>[5] Atzmon, M., Maron, H., Lipman, Y., 2018. Point convolutional neural networks by extension operators. arXiv preprint arXiv:1803.10091 .
- <span id="page-7-2"></span>[6] Cai, J.X., Mu, T.J., Lai, Y.K., Hu, S.M., 2019. Deep pointbased scene labeling with depth mapping and geometric patch feature encoding. Graphical Models 104, 101033. URL: <https://www.sciencedirect.com/science/article/pii/S1524070319300244>, doi:[https://doi.org/10.1016/j.gmod.2019.101033](http://dx.doi.org/https://doi.org/10.1016/j.gmod.2019.101033).
- <span id="page-7-24"></span>[7] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., et al., 2015. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012 .
- <span id="page-7-23"></span>[8] Chen, T., Kornblith, S., Norouzi, M., Hinton, G., 2020. A simple framework for contrastive learning of visual representations, in: International conference on machine learning, PMLR. pp. 1597–1607.
- <span id="page-7-4"></span>[9] Chen, X., He, K., 2021. Exploring simple siamese representation learning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750–15758.
- <span id="page-7-28"></span>[10] Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nießner, M., 2017. Scannet: Richly-annotated 3d reconstructions of indoor scenes, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5828–5839.
- <span id="page-7-7"></span>[11] Deng, H., Birdal, T., Ilic, S., 2018. Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors, in: Proceedings of the European Conference on Computer Vision (ECCV), pp. 602–618.
- <span id="page-7-5"></span>[12] Doersch, C., Gupta, A., Efros, A.A., 2015. Unsupervised visual representation learning by context prediction, in: Proceedings of the IEEE international conference on computer vision, pp. 1422–1430.
- <span id="page-7-10"></span>[13] Du, B., Gao, X., Hu, W., Li, X., 2021. Self-contrastive learning with hard negative sampling for self-supervised point cloud learning, in:

Proceedings of the 29th ACM International Conference on Multimedia, pp. 3133–3142.

- <span id="page-7-17"></span>[14] Eckart, B., Yuan, W., Liu, C., Kautz, J., 2021. Self-supervised learning on 3d point clouds by learning discrete generative models, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8248–8257.
- <span id="page-7-20"></span>[15] Grill, J.B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al., 2020. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems 33, 21271–21284.
- <span id="page-7-22"></span>[16] Hamilton, W., Ying, Z., Leskovec, J., 2017. Inductive representation learning on large graphs. Advances in neural information processing systems 30.
- <span id="page-7-16"></span>[17] Han, Z., Shang, M., Liu, Y.S., Zwicker, M., 2019a. View interprediction gan: Unsupervised representation learning for 3d shapes by learning global shape memories to support local view predictions, in: Proceedings of the AAAI conference on artificial intelligence, pp. 8376–8384.
- <span id="page-7-8"></span>[18] Han, Z., Wang, X., Liu, Y.S., Zwicker, M., 2019b. Multi-angle point cloud-vae: Unsupervised feature learning for 3d point clouds from multiple angles by joint self-reconstruction and half-to-half prediction, in: 2019 IEEE/CVF International Conference on Computer Vision (ICCV), IEEE. pp. 10441–10450.
- <span id="page-7-6"></span>[19] He, K., Fan, H., Wu, Y., Xie, S., Girshick, R., 2020a. Momentum contrast for unsupervised visual representation learning, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729–9738.
- <span id="page-7-14"></span>[20] He, X., Bai, S., Chu, J., Bai, X., 2020b. An improved multiview convolutional neural network for 3d object retrieval. IEEE Transactions on Image Processing 29, 7917–7930.
- <span id="page-7-3"></span>[21] Hua, B.S., Tran, M.K., Yeung, S.K., 2018. Pointwise convolutional neural networks, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 984–993.
- <span id="page-7-11"></span>[22] Huang, S., Xie, Y., Zhu, S.C., Zhu, Y., 2021. Spatio-temporal selfsupervised representation learning for 3d point clouds, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6535–6545.
- <span id="page-7-21"></span>[23] Kipf, T.N., Welling, M., 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 .
- <span id="page-7-12"></span>[24] Klokov, R., Lempitsky, V., 2017. Escape from cells: Deep kdnetworks for the recognition of 3d point cloud models, in: Proceedings of the IEEE international conference on computer vision, pp. 863– 872.
- <span id="page-7-26"></span>[25] Lai, X., Liu, J., Jiang, L., Wang, L., Zhao, H., Liu, S., Qi, X., Jia, J., 2022. Stratified transformer for 3d point cloud segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8500–8509.
- <span id="page-7-13"></span>[26] Landrieu, L., Simonovsky, M., 2018. Large-scale point cloud semantic segmentation with superpoint graphs, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4558– 4567.
- <span id="page-7-15"></span>[27] Le, T., Duan, Y., 2018. Pointgrid: A deep network for 3d shape understanding, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9204–9214.
- <span id="page-7-18"></span>[28] Li, J., Chen, B.M., Lee, G.H., 2018a. So-net: Self-organizing network for point cloud analysis, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9397–9406.
- <span id="page-7-0"></span>[29] Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., 2018b. Pointcnn: Convolution on x-transformed points. Advances in neural information processing systems 31.
- <span id="page-7-27"></span>[30] Liu, F., Lin, G., Foo, C.S., 2021. Point discriminative learning for unsupervised representation learning on 3d point clouds. arXiv preprint arXiv:2108.02104 .
- <span id="page-7-25"></span>[31] Liu, X., Han, Z., Liu, Y.S., Zwicker, M., 2019a. Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 8778–8785.
- <span id="page-8-13"></span>[32] Liu, X., Han, Z., Wen, X., Liu, Y.S., Zwicker, M., 2019b. L2g autoencoder: Understanding point clouds by local-to-global reconstruction with hierarchical self-attention, in: Proceedings of the 27th ACM International Conference on Multimedia, pp. 989–997.
- <span id="page-8-9"></span>[33] Liu, X., Van De Weijer, J., Bagdanov, A.D., 2019c. Exploiting unlabeled data in cnns by self-supervised learning to rank. IEEE transactions on pattern analysis and machine intelligence 41, 1862– 1878.
- <span id="page-8-35"></span>[34] Liu, Y., Fan, B., Meng, G., Lu, J., Xiang, S., Pan, C., 2019d. Densepoint: Learning densely contextual representation for efficient point cloud processing, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 5239–5248.
- <span id="page-8-22"></span>[35] Liu, Y., Fan, B., Xiang, S., Pan, C., 2019e. Relation-shape convolutional neural network for point cloud analysis, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8895–8904.
- <span id="page-8-30"></span>[36] Maturana, D., Scherer, S., 2015. Voxnet: A 3d convolutional neural network for real-time object recognition, in: 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS), IEEE. pp. 922–928.
- <span id="page-8-5"></span>[37] Misra, I., Girdhar, R., Joulin, A., 2021. An end-to-end transformer model for 3d object detection, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2906–2917.
- <span id="page-8-23"></span>[38] Pham, Q.H., Nguyen, T., Hua, B.S., Roig, G., Yeung, S.K., 2019. Jsis3d: Joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8827–8836.
- <span id="page-8-17"></span>[39] Poursaeed, O., Jiang, T., Qiao, H., Xu, N., Kim, V.G., 2020. Selfsupervised learning of point clouds via orientation estimation, in: 2020 International Conference on 3D Vision (3DV), IEEE. pp. 1018– 1028.
- <span id="page-8-6"></span>[40] Qi, C.R., Litany, O., He, K., Guibas, L.J., 2019. Deep hough voting for 3d object detection in point clouds, in: proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9277–9286.
- <span id="page-8-0"></span>[41] Qi, C.R., Su, H., Mo, K., Guibas, L.J., 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652–660.
- <span id="page-8-2"></span>[42] Qi, C.R., Su, H., Nießner, M., Dai, A., Yan, M., Guibas, L.J., 2016. Volumetric and multi-view cnns for object classification on 3d data, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5648–5656.
- <span id="page-8-1"></span>[43] Qi, C.R., Yi, L., Su, H., Guibas, L.J., 2017b. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems 30.
- <span id="page-8-18"></span>[44] Rao, Y., Lu, J., Zhou, J., 2020. Global-local bidirectional reasoning for unsupervised representation learning of 3d point clouds, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5376–5385.
- <span id="page-8-19"></span>[45] Sanghi, A., 2020. Info3d: Representation learning on 3d objects using mutual information maximization and contrastive learning, in: European Conference on Computer Vision, Springer. pp. 626–642.
- <span id="page-8-14"></span>[46] Sauder, J., Sievers, B., 2019. Self-supervised deep learning on point clouds by reconstructing space. Advances in Neural Information Processing Systems 32.
- <span id="page-8-10"></span>[47] Sermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal, S., Levine, S., Brain, G., 2018. Time-contrastive networks: Selfsupervised learning from video, in: 2018 IEEE international conference on robotics and automation (ICRA), IEEE. pp. 1134–1141.
- <span id="page-8-24"></span>[48] Shi, S., Wang, X., Li, H., 2019. Pointrcnn: 3d object proposal generation and detection from point cloud, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 770–779.
- <span id="page-8-27"></span>[49] Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., 2015. Multiview convolutional neural networks for 3d shape recognition, in: Proceedings of the IEEE international conference on computer vision, pp. 945–953.
- <span id="page-8-7"></span>[50] Tatarchenko, M., Park, J., Koltun, V., Zhou, Q.Y., 2018. Tangent convolutions for dense prediction in 3d, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3887– 3896.
- <span id="page-8-3"></span>[51] Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B., Goulette, F., Guibas, L.J., 2019. Kpconv: Flexible and deformable convolution for point clouds, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 6411–6420.
- <span id="page-8-34"></span>[52] Uy, M.A., Pham, Q.H., Hua, B.S., Nguyen, T., Yeung, S.K., 2019. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 1588–1597.
- <span id="page-8-32"></span>[53] Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y., 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 .
- <span id="page-8-15"></span>[54] Wang, H., Liu, Q., Yue, X., Lasenby, J., Kusner, M.J., 2021. Unsupervised point cloud pre-training via occlusion completion, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 9782–9792.
- <span id="page-8-11"></span>[55] Wang, J., Jiao, J., Bao, L., He, S., Liu, Y., Liu, W., 2019a. Selfsupervised spatio-temporal representation learning for videos by predicting motion and appearance statistics, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4006–4015.
- <span id="page-8-12"></span>[56] Wang, J., Jiao, J., Liu, Y.H., 2020. Self-supervised video representation learning by pace prediction, in: European conference on computer vision, Springer. pp. 504–521.
- <span id="page-8-8"></span>[57] Wang, X., Liu, S., Shen, X., Shen, C., Jia, J., 2019b. Associatively segmenting instances and semantics in point clouds, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4096–4105.
- <span id="page-8-4"></span>[58] Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M., 2019c. Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog) 38, 1–12.
- <span id="page-8-16"></span>[59] Wu, J., Zhang, C., Xue, T., Freeman, B., Tenenbaum, J., 2016. Learning a probabilistic latent space of object shapes via 3d generativeadversarial modeling. Advances in neural information processing systems 29.
- <span id="page-8-29"></span>[60] Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J., 2015. 3d shapenets: A deep representation for volumetric shapes, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1912–1920.
- <span id="page-8-33"></span>[61] Wu, Z., Xiong, Y., Yu, S.X., Lin, D., 2018. Unsupervised feature learning via non-parametric instance discrimination, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3733–3742.
- <span id="page-8-21"></span>[62] Xia, J., Wu, L., Chen, J., Hu, B., Li, S.Z., 2022. Simgrace: A simple framework for graph contrastive learning without data augmentation, in: Proceedings of the ACM Web Conference 2022, pp. 1070–1079.
- <span id="page-8-20"></span>[63] Xie, S., Gu, J., Guo, D., Qi, C.R., Guibas, L., Litany, O., 2020. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding, in: European conference on computer vision, Springer. pp. 574–591.
- <span id="page-8-25"></span>[64] Xie, S., Liu, S., Chen, Z., Tu, Z., 2018. Attentional shapecontextnet for point cloud recognition, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4606–4615.
- <span id="page-8-28"></span>[65] Xie, Z., Xu, K., Shan, W., Liu, L., Xiong, Y., Huang, H., 2015. Projective feature learning for 3d shapes with multi-view depth images, in: Computer Graphics Forum, Wiley Online Library. pp. 1–11.
- <span id="page-8-26"></span>[66] Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y., 2018. Spidercnn: Deep learning on point sets with parameterized convolutional filters, in: Proceedings of the European Conference on Computer Vision (ECCV), pp. 87–102.
- <span id="page-8-31"></span>[67] Yang, Y., Feng, C., Shen, Y., Tian, D., 2018. Foldingnet: Point cloud auto-encoder via deep grid deformation, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 206–215.
- <span id="page-8-36"></span>[68] Yi, L., Kim, V.G., Ceylan, D., Shen, I.C., Yan, M., Su, H., Lu, C., Huang, Q., Sheffer, A., Guibas, L., 2016. A scalable active framework for region annotation in 3d shape collections. ACM Transactions on

Graphics (ToG) 35, 1–12.

- <span id="page-9-5"></span>[69] Yi, L., Zhao, W., Wang, H., Sung, M., Guibas, L.J., 2019. Gspn: Generative shape proposal network for 3d instance segmentation in point cloud, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3947–3956.
- <span id="page-9-7"></span>[70] You, Y., Chen, T., Sui, Y., Chen, T., Wang, Z., Shen, Y., 2020. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems 33, 5812–5823.
- <span id="page-9-3"></span>[71] Zhang, L., Zhu, Z., 2019. Unsupervised feature learning for point cloud understanding by contrasting and clustering using graph convolutional neural networks, in: 2019 international conference on 3D vision (3DV), IEEE. pp. 395–404.
- <span id="page-9-4"></span>[72] Zhang, Z., Girdhar, R., Joulin, A., Misra, I., 2021. Self-supervised pretraining of 3d features on any point-cloud, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10252– 10263.
- <span id="page-9-0"></span>[73] Zhang, Z., Sun, B., Yang, H., Huang, Q., 2020. H3dnet: 3d object detection using hybrid geometric primitives, in: European Conference on Computer Vision, Springer. pp. 311–329.
- <span id="page-9-8"></span>[74] Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V., 2021. Point transformer, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16259–16268.
- <span id="page-9-6"></span>[75] Zhao, Y., Birdal, T., Deng, H., Tombari, F., 2019. 3d point capsule networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1009–1018.
- <span id="page-9-2"></span>[76] Zhuang, C., She, T., Andonian, A., Mark, M.S., Yamins, D., 2020. Unsupervised learning from video with deep neural embeddings, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9563–9572.
- <span id="page-9-1"></span>[77] Ziegler, A., Asano, Y.M., 2022. Self-supervised learning of object parts for semantic segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14502– 14511.