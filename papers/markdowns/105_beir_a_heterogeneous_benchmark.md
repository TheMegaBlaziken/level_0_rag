# <span id="page-0-0"></span>BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models

Nandan Thakur, Nils Reimers, Andreas Rücklé<sup>∗</sup> , Abhishek Srivastava, Iryna Gurevych Ubiquitous Knowledge Processing Lab (UKP Lab) Department of Computer Science, Technical University of Darmstadt <http://www.ukp.tu-darmstadt.de>

## Abstract

Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-theart retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction based models on average achieve the best zeroshot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards more robust and generalizable systems in the future. BEIR is publicly available at <https://github.com/UKPLab/beir>.

## 1 Introduction

Major natural language processing (NLP) problems rely on a practical and efficient retrieval component as a first step to find relevant information. Challenging problems include open-domain question-answering [\[8\]](#page-10-0), claim-verification [\[58\]](#page-14-0), duplicate question detection [\[77\]](#page-15-0), and many more. Traditionally, retrieval has been dominated by lexical approaches like TF-IDF or BM25 [\[53\]](#page-13-0). However, these approaches suffer from lexical gap [\[5\]](#page-10-1) and are able to only retrieve documents containing keywords present within the query. Further, lexical approaches treat queries and documents as bag-of-words by not taking word ordering into consideration.

Recently, deep learning and in particular pre-trained Transformer models like BERT [\[12\]](#page-10-2) have become popular in information retrieval [\[75\]](#page-15-1). These neural retrieval systems can be used in many fundamentally different ways to improve retrieval performance. We provide an brief overview of the systems in Section [2.1.](#page-2-0) Many prior work train neural retrieval systems on large datasets like Natural Questions (NQ) [\[32\]](#page-12-0) (133k training examples) or MS MARCO [\[42\]](#page-12-1) (533k training examples), which both focus on passage retrieval given a question or short keyword-based query. In most prior work, approaches are afterward evaluated on the same dataset, where significant performance gains over lexical approaches like BM25 are demonstrated [\[48,](#page-13-1) [29,](#page-12-2) [43\]](#page-12-3).

However, creating a large training corpus is often time-consuming and expensive and hence many retrieval systems are applied in a zero-shot setup, with no available training data to train the system.

35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.

<sup>∗</sup>Contributions made prior to joining Amazon.

<span id="page-1-1"></span><span id="page-1-0"></span>![](_page_1_Picture_0.jpeg)

**Caption:** Figure 1 illustrates the diverse tasks and datasets included in the BEIR benchmark, showcasing 18 retrieval datasets across nine different tasks such as fact-checking and biomedical information retrieval. This diversity aims to enhance the evaluation of model generalization in various text domains.

**Caption:** Figure 1 illustrates the diverse tasks and datasets included in the BEIR benchmark, showcasing 18 retrieval datasets across nine different tasks such as fact-checking and biomedical information retrieval. This diversity aims to enhance the evaluation of model generalization in various text domains.

Figure 1: An overview of the diverse tasks and datasets in BEIR benchmark.

So far, it is unclear how well existing trained neural models will perform for other text domains or textual retrieval tasks. Even more important, it is unclear how well different approaches, like sparse embeddings vs. dense embeddings, generalize to out-of-distribution data.

In this work, we present a novel robust and heterogeneous benchmark called BEIR (Benchmarking IR), comprising of 18 retrieval datasets for comparison and evaluation of model generalization. Prior retrieval benchmarks [\[17,](#page-11-0) [47\]](#page-13-2) have issues of a comparatively narrow evaluation focusing either only on a single task, like question-answering, or on a certain domain. In BEIR, we focus on Diversity, we include nine different retrieval tasks: Fact checking, citation prediction, duplicate question retrieval, argument retrieval, news retrieval, question answering, tweet retrieval, bio-medical IR, and entity retrieval. Further, we include datasets from diverse text domains, datasets that cover broad topics (like Wikipedia) and specialized topics (like COVID-19 publications), different text types (news articles vs. Tweets), datasets of various sizes (3.6k - 15M documents), and datasets with different query lengths (average query length between 3 and 192 words) and document lengths (average document length between 11 and 635 words).

We use BEIR to evaluate ten diverse retrieval methods from five broad architectures: lexical, sparse, dense, late interaction, and re-ranking. From our analysis, we find that no single approach consistently outperforms other approaches on all datasets. Further, we notice that the in-domain performance of a model does not correlate well with its generalization capabilities: models fine-tuned with identical training data might generalize differently. In terms of efficiency, we find a trade-off between the performances and the computational cost: computationally expensive models, like re-ranking models and late interaction model perform the best. More efficient approaches e.g. based on dense or sparse embeddings can substantially underperform traditional lexical models like BM25. Overall, BM25 remains a strong baseline for zero-shot text retrieval.

Finally, we notice that there can be a strong lexical bias present in datasets included within the benchmark, likely as lexical models are pre-dominantly used during the annotation or creation of datasets. This can give an unfair disadvantage to non-lexical approaches. We analyze this for the TREC-COVID [\[63\]](#page-14-1) dataset: We manually annotate the missing relevance judgements for the tested systems and see a significant performance improvement for non-lexical approaches. Hence, future work requires better unbiased datasets that allow a fair comparison for all types of retrieval systems.

With BEIR, we take an important step towards a single and unified benchmark to evaluate the zero-shot capabilities of retrieval systems. It allows to study when and why certain approaches perform well, and hopefully steers innovation to more robust retrieval systems. We release BEIR and an integration of diverse retrieval systems and datasets in a well-documented, easy to use and extensible open-source package. BEIR is model-agnostic, welcomes methods of all kinds, and also allows easy integration of new tasks and datasets. More details are available at <https://github.com/UKPLab/beir>.

# 2 Related Work and Background

To our knowledge, BEIR is the first broad, zero-shot information retrieval benchmark. Existing works [\[17,](#page-11-0) [47\]](#page-13-2) do not evaluate retrieval in a zero-shot setting in depth, they either focus over a single task, small corpora or on a certain domain. This setting hinders for investigation of model generalization across diverse set of domains and task types. MultiReQA [\[17\]](#page-11-0) consists of eight Question-Answering (QA) datasets and evaluates sentence-level answer retrieval given a question. It only tests a single task and five out of eight datasets are from Wikipedia. Further, MultiReQA evaluates retrieval over rather small corpora: six out of eight tasks have less than 100k candidate sentences, which benefits dense retrieval over lexical as previously shown [\[52\]](#page-13-3). KILT [\[47\]](#page-13-2) consists of five knowledge-intensive

<span id="page-2-1"></span>tasks including a total of eleven datasets. The tasks involve retrieval, but it is not the primary task. Further, KILT retrieves documents only from Wikipedia.

#### <span id="page-2-0"></span>2.1 Neural Retrieval

Information retrieval is the process of searching and returning relevant documents for a query from a collection. In our paper, we focus on text retrieval and use *document* as a cover term for text of any length in the given collection and *query* for the user input, which can be of any length as well. Traditionally, lexical approaches like TF-IDF and BM25 [\[53\]](#page-13-0) have dominated textual information retrieval. Recently, there is a strong interest in using neural networks to improve or replace these lexical approaches. In this section, we highlight a few neural-based approaches and we refer the reader to Lin et al. [\[75\]](#page-15-1) for a recent survey in neural retrieval.

Retriever-based Lexical approaches suffer from the lexical gap [\[5\]](#page-10-1). To overcome this, earlier techniques proposed to improve lexical retrieval systems with neural networks. Sparse methods such as docT5query [\[45\]](#page-13-4) identified document expansion terms using a sequence-to-sequence model that generated possible queries for which the given document would be relevant. DeepCT [\[11\]](#page-10-3) on the other hand used a BERT [\[12\]](#page-10-2) model to learn relevant term weights in a document and generated a pseudo-document representation. Both methods still rely on BM25 for the remaining parts. Similarly, SPARTA [\[78\]](#page-15-2) learned token-level contextualized representations with BERT and converted the document into an efficient inverse index. More recently, dense retrieval approaches were proposed. They are capable of capturing semantic matches and try to overcome the (potential) lexical gap. Dense retrievers map queries and documents in a shared, dense vector space [\[16\]](#page-11-1). This allowed the document representation to be pre-computed and indexed. A bi-encoder neural architecture based on pre-trained Transformers has shown strong performance for various open-domain question-answering tasks [\[17,](#page-11-0) [29,](#page-12-2) [33,](#page-12-4) [40\]](#page-12-5). This dense approach was recently extended by hybrid lexical-dense approaches which aims to combine the strengths of both approaches [\[15,](#page-11-2) [55,](#page-13-5) [39\]](#page-12-6). Another parallel line of work proposed an unsupervised domain-adaption approach [\[33,](#page-12-4) [40\]](#page-12-5) for training dense retrievers by generating synthetic queries on a target domain. Lastly, ColBERT [\[30\]](#page-12-7) (Contextualized late interaction over BERT) computes multiple contextualized embeddings on a token level for queries and documents and uses an maximum-similarity function for retrieving relevant documents.

Re-ranking-based Neural re-ranking approaches use the output of a first-stage retrieval system, often BM25, and re-ranks the documents to create a better comparison of the retrieved documents. Significant improvement in performance was achieved with the cross-attention mechanism of BERT [\[43\]](#page-12-3). However, at a disadvantage of a high computational overhead [\[51\]](#page-13-6).

# 3 The BEIR Benchmark

BEIR aims to provide a one-stop zero-shot evaluation benchmark for all diverse retrieval tasks. To construct a comprehensive evaluation benchmark, the selection methodology is crucial to collect tasks and datasets with desired properties. For BEIR, the methodology is motivated by the following three factors: (*i*) Diverse tasks: Information retrieval is a versatile task and the lengths of queries and indexed documents can differ between tasks. Sometimes, queries are short, like a keyword, while in other cases, they can be long like a news article. Similarly, indexed documents can sometimes be long, and for other tasks, short like a tweet. (*ii*) Diverse domains: Retrieval systems should be evaluated in various types of domains. From broad ones like News or Wikipedia, to highly specialized ones such as scientific publications in one particular field. Hence, we include domains which provide a representation of real-world problems and are diverse ranging from generic to specialized. (*iii*) Task difficulties: Our benchmark is challenging and the *difficulty* of a task included has to be sufficient. If a task is easily solved by any algorithm, it will not be useful to compare various models used for evaluation. We evaluated several tasks based on existing literature and selected popular tasks which we believe are recently developed, challenging and are not yet fully solved with existing approaches. (*iv*) Diverse annotation strategies: Creating retrieval datasets are inherently complex and are subject to *annotation biases* (see Section [6](#page-8-0) for details), which hinders a fair comparison of approaches. To reduce the impact of such biases, we selected datasets which have been created in many different ways: Some where annotated by crowd-workers, others by experts, and others are based on the feedback from large online communities.

In total, we include 18 English zero-shot evaluation datasets from 9 heterogeneous retrieval tasks. As the majority of the evaluated approaches are trained on the MS MARCO [\[42\]](#page-12-1) dataset, we also report performances on this dataset, but don't include the outcome in our zero-shot comparison. We would like to refer the reader to Appendix C where we motivate each one of the 9 retrieval tasks and 18

<span id="page-3-3"></span><span id="page-3-0"></span>

| Split (→)                   |             |                     |   |                 | Train   | Dev   |               | Test       |            |        | Avg. Word Lengths |
|-----------------------------|-------------|---------------------|---|-----------------|---------|-------|---------------|------------|------------|--------|-------------------|
| Task (↓)                    | Domain (↓)  | Dataset (↓)         |   | Title Relevancy | #Pairs  |       | #Query #Query | #Corpus    | Avg. D / Q | Query  | Document          |
| Passage-Retrieval           | Misc.       | MS MARCO [42]       | ✗ | Binary          | 532,761 | —-    | 6,980         | 8,841,823  | 1.1        | 5.96   | 55.98             |
| Bio-Medical                 | Bio-Medical | TREC-COVID [63]     | ✓ | 3-level         | —-      | —-    | 50            | 171,332    | 493.5      | 10.60  | 160.77            |
| Information                 | Bio-Medical | NFCorpus [7]        | ✓ | 3-level         | 110,575 | 324   | 323           | 3,633      | 38.2       | 3.30   | 232.26            |
| Retrieval (IR)              | Bio-Medical | BioASQ [59]         | ✓ | Binary          | 32,916  | —-    | 500           | 14,914,602 | 4.7        | 8.05   | 202.61            |
| Question                    | Wikipedia   | NQ [32]             | ✓ | Binary          | 132,803 | —-    | 3,452         | 2,681,468  | 1.2        | 9.16   | 78.88             |
| Answering                   | Wikipedia   | HotpotQA [74]       | ✓ | Binary          | 170,000 | 5,447 | 7,405         | 5,233,329  | 2.0        | 17.61  | 46.30             |
| (QA)                        | Finance     | FiQA-2018 [41]      | ✗ | Binary          | 14,166  | 500   | 648           | 57,638     | 2.6        | 10.77  | 132.32            |
| Tweet-Retrieval             | Twitter     | Signal-1M (RT) [57] | ✗ | 3-level         | —-      | —-    | 97            | 2,866,316  | 19.6       | 9.30   | 13.93             |
| News                        | News        | TREC-NEWS [56]      | ✓ | 5-level         | —-      | —-    | 57            | 594,977    | 19.6       | 11.14  | 634.79            |
| Retrieval                   | News        | Robust04 [62]       | ✗ | 3-level         | —-      | —-    | 249           | 528,155    | 69.9       | 15.27  | 466.40            |
| Argument                    | Misc.       | ArguAna [65]        | ✓ | Binary          | —-      | —-    | 1,406         | 8,674      | 1.0        | 192.98 | 166.80            |
| Retrieval                   | Misc.       | Touché-2020 [6]     | ✓ | 3-level         | —-      | —-    | 49            | 382,545    | 19.0       | 6.55   | 292.37            |
| Duplicate-Question StackEx. | Quora       | CQADupStack [23]    | ✓ | Binary          | —-      | —-    | 13,145        | 457,199    | 1.4        | 8.59   | 129.09            |
| Retrieval                   |             | Quora               | ✗ | Binary          | —-      | 5,000 | 10,000        | 522,931    | 1.6        | 9.53   | 11.44             |
| Entity-Retrieval            | Wikipedia   | DBPedia [19]        | ✓ | 3-level         | —-      | 67    | 400           | 4,635,922  | 38.2       | 5.39   | 49.68             |
| Citation-Prediction         | Scientific  | SCIDOCS [9]         | ✓ | Binary          | —-      | —-    | 1,000         | 25,657     | 4.9        | 9.38   | 176.19            |
| Fact Checking               | Wikipedia   | FEVER [58]          | ✓ | Binary          | 140,085 | 6,666 | 6,666         | 5,416,568  | 1.2        | 8.13   | 84.76             |
|                             | Wikipedia   | Climate-FEVER [13]  | ✓ | Binary          | —-      | —-    | 1,535         | 5,416,593  | 3.0        | 20.13  | 84.76             |
|                             | Scientific  | SciFact [66]        | ✓ | Binary          | 920     | —-    | 300           | 5,183      | 1.1        | 12.37  | 213.63            |

Table 1: Statistics of datasets in BEIR benchmark. Few datasets contain documents without titles. Relevancy indicates the query-document relation: binary (relevant, non-relevant) or graded into sub-levels. Avg. D/Q indicates the average relevant documents per query.

datasets in depth. Examples for each dataset are listed in Table 8. We additionally provide dataset licenses in Appendix D, and links to the datasets in Table 5.

Table [1](#page-3-0) summarizes the statistics of the datasets provided in BEIR. A majority of datasets contain binary relevancy judgements, i.e. relevant or non-relevant, and a few contain fine-grained relevancy judgements. Some datasets contain few relevant documents for a query (< 2), while other datasets like TREC-COVID [\[63\]](#page-14-1) can contain up to even 500 relevant documents for a query. Only 8 out of 19 datasets (including MS MARCO) have training data denoting the practical importance for zero-shot retrieval benchmarking. All datasets except ArguAna [\[65\]](#page-14-4) have short queries (either a single sentence or 2-3 keywords). Figure [1](#page-1-0) shows an overview of the tasks and datasets in the BEIR benchmark.

Information Retrieval (IR) is ubiquitous, there are lots of datasets available within each task and further even more tasks with retrieval. However, it is not feasible to include all datasets within the benchmark for evaluation. We tried to cover a balanced mixture of a wide range of tasks and datasets and paid importance not to overweight a specific task like question-answering. Future datasets can easily be integrated in BEIR, and existing models can be evaluated on any new dataset quickly. The BEIR website will host an actively maintained leaderboard[2](#page-3-1) with all datasets and models.

#### 3.1 Dataset and Diversity Analysis

The datasets present in BEIR are selected from diverse domains ranging from Wikipedia, scientific publications, Twitter, news, to online user communities, and many more. To measure the diversity in domains, we compute the domain overlap between the pairwise datasets using a pairwise weighted Jaccard similarity [\[24\]](#page-11-6) score on unigram word overlap between all dataset pairs. For more details on the theoretical formulation of the similarity score, please refer to Appendix E. Figure [2](#page-4-0) shows a heatmap denoting the pairwise weighted jaccard scores and the clustered force-directed placement diagram. Nodes (or datasets) close in this graph have a high word overlap, while nodes far away in the graph have a low overlap. From Figure [2,](#page-4-0) we observe a rather low weighted Jaccard word overlap across different domains, indicating that BEIR is a challenging benchmark where approaches must generalize well to diverse out-of-distribution domains.

# 3.2 BEIR Software and Framework

The BEIR software[3](#page-3-2) provides an is an easy to use Python framework (pip install beir) for model evaluation. It contains extensive wrappers to replicate experiments and evaluate models from wellknown repositories including Sentence-Transformers [\[51\]](#page-13-6), Transformers [\[70\]](#page-14-6), Anserini [\[72\]](#page-15-4), DPR [\[29\]](#page-12-2), Elasticsearch, ColBERT [\[30\]](#page-12-7), and Universal Sentence Encoder [\[73\]](#page-15-5). This makes the software useful for both academia and industry. The software also provides you with all IR-based metrics from Precision, Recall, MAP (Mean Average Precision), MRR (Mean Reciprocal Rate) to nDCG

<span id="page-3-1"></span><sup>2</sup>BEIR Leaderboard: <https://tinyurl.com/beir-leaderboard>

<span id="page-3-2"></span><sup>3</sup>BEIR Code & documentation: <https://github.com/UKPLab/beir>

<span id="page-4-1"></span><span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)

**Caption:** Figure 2 presents a heatmap of pairwise weighted Jaccard similarity scores among datasets in the BEIR benchmark, indicating low overlap across domains. The 2D representation highlights the distinctiveness of datasets, emphasizing the benchmark's challenge in evaluating retrieval models across diverse tasks.

**Caption:** Figure 2 presents a heatmap of pairwise weighted Jaccard similarity scores among datasets in the BEIR benchmark, indicating low overlap across domains. The 2D representation highlights the distinctiveness of datasets, emphasizing the benchmark's challenge in evaluating retrieval models across diverse tasks.

Figure 2: Domain overlap across each pairwise dataset in the BEIR benchmark. Heatmap (left) shows the pairwise weighted jaccard similarity scores between BEIR datasets. 2D representation (right) using a forcedirected placement algorithm with NetworkX [\[18\]](#page-11-7). We color and mark datasets differently for different domains.

(Normalised Cumulative Discount Gain) for any top-k hits. One can use the BEIR benchmark for evaluating existing models on new retrieval datasets and for evaluating new models on the included datasets.

Datasets are often scattered online and are provided in various file-formats, making the evaluation of models on various datasets difficult. BEIR introduces a standard format (corpus, queries and qrels) and converts existing datasets in this easy universal data format, allowing to evaluate faster on an increasing number of datasets.

#### 3.3 Evaluation Metric

Depending upon the nature and requirements of real-world applications, retrieval tasks can be either be precision or recall focused. To obtain comparable results across models and datasets in BEIR, we argue that it is important to leverage a single evaluation metric that can be computed comparably across all tasks. Decision support metrics such as Precision and Recall which are both rank unaware are not suitable. Binary rank-aware metrics such as MRR (Mean Reciprocal Rate) and MAP (Mean Average Precision) fail to evaluate tasks with graded relevance judgements. We find that Normalised Cumulative Discount Gain (nDCG@k) provides a good balance suitable for both tasks involving binary and graded relevance judgements. We refer the reader to Wang et al. [\[69\]](#page-14-7) for understanding the theoretical advantages of the metric. For our experiments, we utilize the Python interface of the official TREC evaluation tool [\[61\]](#page-14-8) and compute nDCG@10 for all datasets.

## 4 Experimental Setup

We use BEIR to compare diverse, recent, state-of-the-art retrieval architectures with a focus on transformer-based neural approaches. We evaluate on publicly available pre-trained checkpoints, which we provide in Table 6. Due to the length limitations of transformer-based networks, we use only the first 512 word pieces within all documents in our experiments across all neural architectures.

We group the models based on their architecture: (*i*) lexical, (*ii*) sparse, (*iii*) dense, (*iv*) late-interaction, and (*v*) re-ranking. Besides the included models, the BEIR benchmark is model agnostic and in future different model configurations can be easily incorporated within the benchmark.

(*i*) Lexical Retrieval: (*a*) BM25 [\[53\]](#page-13-0) is a commonly-used bag-of-words retrieval function based on token-matching between two high-dimensional sparse vectors with TF-IDF token weights. We use Anserini [\[34\]](#page-12-9) with the default Lucene parameters (k=0.9 and b=0.4). We index the title (if available) and passage as separate fields for documents. In our leaderboard, we also tested Elasticsearch BM25 and Anserini + RM3 expansion, but found Anserini BM25 to perform the best.

<span id="page-5-0"></span>(*ii*) Sparse Retrieval: (*a*) DeepCT [\[11\]](#page-10-3) uses a bert-base-uncased model trained on MS MARCO to learn the term weight frequencies (tf). It generates a pseudo-document with keywords multiplied with the learnt term-frequencies. We use the original setup of Dai and Callan [\[11\]](#page-10-3) in combination with BM25 with default Anserini parameters which we empirically found to perform better over the tuned MS MARCO parameters. (*b*) SPARTA [\[78\]](#page-15-2) computes similarity scores between the non-contextualized query embeddings from BERT with the contextualized document embeddings. These scores can be pre-computed for a given document, which results in a 30k dimensional sparse vector. As the original implementation is not publicly available, we re-implemented the approach. We fine-tune a DistilBERT [\[54\]](#page-13-9) model on the MS MARCO dataset and use sparse-vectors with 2,000 non-zero entries. (*c*) DocT5query [\[44\]](#page-13-10) is a popular document expansion technique using a T5 (base) [\[50\]](#page-13-11) model trained on MS MARCO to generate synthetic queries and append them to the original document for lexical search. We replicate the setup of Nogueira and Lin [\[44\]](#page-13-10) and generate 40 queries for each document and use BM25 with default Anserini parameters.

(*iii*) Dense Retrieval: (*a*) DPR [\[29\]](#page-12-2) is a two-tower bi-encoder trained with a single BM25 hard negative and in-batch negatives. We found the open-sourced Multi model to perform better over the single NQ model in our setting. The Multi-DPR model is a bert-base-uncased model trained on four QA datasets (including titles): NQ [\[32\]](#page-12-0), TriviaQA [\[28\]](#page-11-8), WebQuestions [\[4\]](#page-10-7) and CuratedTREC [\[3\]](#page-10-8). (*b*) ANCE [\[71\]](#page-15-6) is a bi-encoder constructing hard negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which in parallel updates to select hard negative training instances during fine-tuning of the model. We use the publicly available RoBERTa [\[38\]](#page-12-10) model trained on MS MARCO [\[42\]](#page-12-1) for 600K steps for our experiments. (*c*) TAS-B [\[21\]](#page-11-9) is a bi-encoder trained with Balanced Topic Aware Sampling using dual supervision from a cross-encoder and a ColBERT model. The model was trained with a combination of both a pairwise Margin-MSE [\[22\]](#page-11-10) loss and an in-batch negative loss function. We use the publicly available DistilBERT [\[54\]](#page-13-9) model for our experiments. (*d*) GenQ: is an unsupervised domain-adaption approach for dense retrieval models by training on synthetically generated data. First, we fine-tune a T5 (base) [\[50\]](#page-13-11) model on MS MARCO for 2 epochs. Then, for a target dataset we generate 5 queries for each document using a combination of top-k and nucleus-sampling (top-k: 25; top-p: 0.95). Due to resource constraints, we cap the maximum number of target documents in each dataset to 100K. For retrieval, we continue to fine-tune the TAS-B model using in-batch negatives on the synthetic queries and document pair data. Note, GenQ creates an independent model for each task.

(*iv*) Late-Interaction: (*a*) ColBERT [\[30\]](#page-12-7) encodes and represents the query and passage into a bag of multiple contextualized token embeddings. The late-interactions are aggregated with sum of the max-pooling query term and a dot-product across all passage terms. We use the ColBERT model as a dense-retriever (end-to-end retrieval as defined [\[30\]](#page-12-7)): first top-k candidates are retrieved using ANN with faiss [\[27\]](#page-11-11) (faiss depth = 100) and ColBERT re-ranks by computing the late aggregated interactions. We train a bert-base-uncased model, with maximum sequence length of 300 on the MS MARCO dataset for 300K steps.

(*v*) Re-ranking model: (*a*) BM25 + CE [\[68\]](#page-14-9) reranks the top-100 retrieved hits from a first-stage BM25 (Anserini) model. We evaluated 14 different cross-attentional re-ranking models that are publicly available on the HuggingFace model hub and found that a 6-layer, 384-h MiniLM [\[68\]](#page-14-9) cross-encoder model offers the best performance on MS MARCO. The model was trained on MS MARCO using a knowledge distillation setup with an ensemble of three teacher models: BERT-base, BERT-large, and ALBERT-large models following the setup in Hofstätter et al. [\[22\]](#page-11-10).

Training Setup: The models included for zero-shot evaluation were originally trained differently. DocT5query and DeepCT were trained for document expansion and term re-weighting. Cross encoder (MiniLM) and SPARTA were both trained with ranking data. All dense retrieval models (DPR, ANCE, and TAS-B) and ColBERT [\[30\]](#page-12-7) were trained with a mixture: ranking data and random in-batch negatives. Another vital difference lies in hard negatives, few models are trained on better optimized hard negatives whereas others using simpler hard negatives, which may suggest an unfair comparison. DPR was trained using the mined BM25 hard negatives, ColBERT with the original MS MARCO [\[42\]](#page-12-1) provided hard negatives, ANCE with mined approximate hard negatives, whereas TAS-B used a cross-model distillation from a cross-encoder and a ColBERT model together with BM25 hard negatives.

<span id="page-6-0"></span>

| Model (→)                         | Lexical                 | Sparse                  |                         |                         |                          |                         | Dense                   | Late-Interaction        | Re-ranking              |                         |
|-----------------------------------|-------------------------|-------------------------|-------------------------|-------------------------|--------------------------|-------------------------|-------------------------|-------------------------|-------------------------|-------------------------|
| Dataset (↓)                       | BM25                    | DeepCT                  | SPARTA                  | docT5query              | DPR                      | ANCE                    | TAS-B                   | GenQ                    | ColBERT                 | BM25+CE                 |
| MS MARCO                          | 0.228                   | 0.296‡                  | 0.351‡                  | 0.338‡                  | 0.177                    | 0.388‡                  | 0.408‡                  | 0.408‡                  | 0.425‡                  | 0.413‡                  |
| TREC-COVID<br>BioASQ<br>NFCorpus  | 0.656<br>0.465<br>0.325 | 0.406<br>0.407<br>0.283 | 0.538<br>0.351<br>0.301 | 0.713<br>0.431<br>0.328 | 0.332<br>0.127<br>0.189  | 0.654<br>0.306<br>0.237 | 0.481<br>0.383<br>0.319 | 0.619<br>0.398<br>0.319 | 0.677<br>0.474<br>0.305 | 0.757<br>0.523<br>0.350 |
| NQ<br>HotpotQA<br>FiQA-2018       | 0.329<br>0.603<br>0.236 | 0.188<br>0.503<br>0.191 | 0.398<br>0.492<br>0.198 | 0.399<br>0.580<br>0.291 | 0.474‡<br>0.391<br>0.112 | 0.446<br>0.456<br>0.295 | 0.463<br>0.584<br>0.300 | 0.358<br>0.534<br>0.308 | 0.524<br>0.593<br>0.317 | 0.533<br>0.707<br>0.347 |
| Signal-1M (RT)                    | 0.330                   | 0.269                   | 0.252                   | 0.307                   | 0.155                    | 0.249                   | 0.289                   | 0.281                   | 0.274                   | 0.338                   |
| TREC-NEWS<br>Robust04             | 0.398<br>0.408          | 0.220<br>0.287          | 0.258<br>0.276          | 0.420<br>0.437          | 0.161<br>0.252           | 0.382<br>0.392          | 0.377<br>0.427          | 0.396<br>0.362          | 0.393<br>0.391          | 0.431<br>0.475          |
| ArguAna<br>Touché-2020            | 0.315<br>0.367          | 0.309<br>0.156          | 0.279<br>0.175          | 0.349<br>0.347          | 0.175<br>0.131           | 0.415<br>0.240          | 0.429<br>0.162          | 0.493<br>0.182          | 0.233<br>0.202          | 0.311<br>0.271          |
| CQADupStack<br>Quora              | 0.299<br>0.789          | 0.268<br>0.691          | 0.257<br>0.630          | 0.325<br>0.802          | 0.153<br>0.248           | 0.296<br>0.852          | 0.314<br>0.835          | 0.347<br>0.830          | 0.350<br>0.854          | 0.370<br>0.825          |
| DBPedia                           | 0.313                   | 0.177                   | 0.314                   | 0.331                   | 0.263                    | 0.281                   | 0.384                   | 0.328                   | 0.392                   | 0.409                   |
| SCIDOCS                           | 0.158                   | 0.124                   | 0.126                   | 0.162                   | 0.077                    | 0.122                   | 0.149                   | 0.143                   | 0.145                   | 0.166                   |
| FEVER<br>Climate-FEVER<br>SciFact | 0.753<br>0.213<br>0.665 | 0.353<br>0.066<br>0.630 | 0.596<br>0.082<br>0.582 | 0.714<br>0.201<br>0.675 | 0.562<br>0.148<br>0.318  | 0.669<br>0.198<br>0.507 | 0.700<br>0.228<br>0.643 | 0.669<br>0.175<br>0.644 | 0.771<br>0.184<br>0.671 | 0.819<br>0.253<br>0.688 |
| Avg. Performance vs. BM25         |                         | - 27.9%                 | - 20.3%                 | + 1.6%                  | - 47.7%                  | - 7.4%                  | - 2.8%                  | - 3.6%                  | + 2.5%                  | + 11%                   |

Table 2: In-domain and zero-shot performances on BEIR benchmark. All scores denote nDCG@10. The best score on a given dataset is marked in bold, and the second best is underlined. Corresponding Recall@100 performances can be found in Table 9. ‡ indicates the in-domain performances.

## 5 Results and Analysis

In this section, we evaluate and analyze how retrieval models perform on the BEIR benchmark. Table [2](#page-6-0) reports the results of all evaluated systems on the selected benchmark datasets. As a baseline, we compare our retrieval systems against BM25. Figure [3](#page-7-0) shows, on how many datasets a respective model is able to perform better or worse than BM25.

1. In-domain performance is not a good indicator for out-of-domain generalization. We observe BM25 heavily underperforms neural approaches by 7-18 points on in-domain MS MARCO. However, BEIR reveals it to be a strong baseline for generalization and generally outperforming many other, more complex approaches. This stresses the point, that retrieval methods must be evaluated on a broad range of datasets.

2. Term-weighting fails, document expansion captures out-of-domain keyword vocabulary. DeepCT and SPARTA both use a transformer network to learn term weighting. While both methods perform well in-domain on MS MARCO, they completely fail to generalize well by under performing BM25 on nearly all datasets. In contrast, document expansion based docT5query is able to add new relevant keywords to a document and performs strong on the BEIR datasets. It outperforms BM25 on 11/18 datasets while providing a competitive performance on the remaining datasets.

3. Dense retrieval models with issues for out-of-distribution data. Dense retrieval models (esp. ANCE and TAS-B), that map queries and documents independently to vector spaces, perform strongly on certain datasets, while on many other datasets perform significantly worse than BM25. For example, dense retrievers are observed to underperform on datasets with a large domain shift compared from what they have been trained on, like in BioASQ, or task-shifts like in Touché-2020. DPR, the only non-MSMARCO trained dataset overall performs the worst in generalization on the benchmark.

4. Re-ranking and Late-Interaction models generalize well to out-of-distribution data. The cross-attentional re-ranking model (BM25+CE) performs the best and is able to outperform BM25 on almost all (16/18) datasets. It only fails on ArguAna and Touché-2020, two retrieval tasks that are extremely different to the MS MARCO training dataset. The late-interaction model ColBERT computes token embeddings independently for the query and document, and scores (query, document) pairs by a cross-attentional like MaxSim operation. It performs a bit weaker than the cross-attentional re-ranking model, but is still able to outperform BM25 on 9/18 datasets. It appears that cross-attention and cross-attentional like operations are important for a good out-of-distribution generalization.

<span id="page-7-1"></span><span id="page-7-0"></span>![](_page_7_Figure_0.jpeg)

**Caption:** Figure 3 compares zero-shot neural retrieval performances against BM25 across various datasets. Notably, re-ranking models like BM25+CE and document expansion techniques such as docT5query outperform BM25 on over half of the evaluated datasets, demonstrating their effectiveness in diverse retrieval scenarios.

**Caption:** Figure 3 compares zero-shot neural retrieval performances against BM25 across various datasets. Notably, re-ranking models like BM25+CE and document expansion techniques such as docT5query outperform BM25 on over half of the evaluated datasets, demonstrating their effectiveness in diverse retrieval scenarios.

![](_page_7_Figure_1.jpeg)

**Caption:** Figure 4 displays distribution plots of top-10 retrieved document lengths for TAS-B and ANCE models. TAS-B shows a preference for shorter documents, contrasting with ANCE's retrieval of longer documents, highlighting the impact of model architecture on document length preferences in retrieval tasks.

**Caption:** Figure 4 displays distribution plots of top-10 retrieved document lengths for TAS-B and ANCE models. TAS-B shows a preference for shorter documents, contrasting with ANCE's retrieval of longer documents, highlighting the impact of model architecture on document length preferences in retrieval tasks.

Figure 3: Comparison of zero-shot neural retrieval performances with BM25. Re-ranking based models, i.e., BM25+CE and sparse model: docT5query outperform BM25 on more than half the BEIR evaluation datasets.

Figure 4: Distribution plots [\[20\]](#page-11-12) for top-10 retrieved document lengths (in words) using TAS-B (blue, top) or ANCE (orange, bottom). TAS-B has a preference towards shorter documents in BEIR.

5. Strong training losses for dense retrieval leads to better out-of-distribution performances. TAS-B provides the best zero-shot generalization performance among its dense counterparts. It outperforms ANCE on 14/18 and DPR on 17/18 datasets respectively. We speculate that the reason lies in a strong training setup in combination of both in-domain batch negatives and Margin-MSE losses for the TAS-B model. This training loss function (with strong ensemble teachers in a Knowledge Distillation setup) shows strong generalization performances.

6. TAS-B model prefers to retrieve documents with shorter lengths. TAS-B underperforms ANCE on two datasets: TREC-COVID by 17.3 points and Touché-2020 by 7.8 points. We observed that these models retrieve documents with vastly different lengths as shown in Figure [4.](#page-7-0) On TREC-COVID, TAS-B retrieves documents with a median length of mere 10 words versus ANCE with 160 words. Similarly on Touché-2020, 14 words vs. 89 words with TAS-B and ANCE respectively. As discussed in Appendix G, this preference for shorter or longer documents is due to the used loss function.

7. Does domain adaptation help improve generalization of dense-retrievers? We evaluated GenQ, which further fine-tunes the TAS-B model on synthetic query data. It outperforms the TAS-B model on specialized domains like scientific publications, finance or StackExchange. On broader and more generic domains, like Wikipedia, it performs weaker than the original TAS-B model.

#### 5.1 Efficiency: Retrieval Latency and Index Sizes

Models need to potentially compare a single query against millions of documents at inference, hence, a high computational speed for retrieving results in real-time is desired. Besides speed, index sizes are vital and are often stored entirely in memory. We randomly sample 1 million documents from DBPedia [\[19\]](#page-11-4) and evaluate latency. For dense models, we use exact search, while for ColBERT we follow the original setup [\[30\]](#page-12-7) and use approximate nearest neighbor search. Performances on CPU were measured with an 8 core Intel Xeon Platinum 8168 CPU @ 2.70GHz and on GPU using a single Nvidia Tesla V100, CUDA 11.0.

Tradeoff between performance and retrieval latency The best out-of-distribution generalization performances by re-ranking top-100 BM25 documents and with late-interaction models come at the cost of high latency (> 350 ms), being slowest at inference. In contrast, dense retrievers are 20-30x faster (< 20ms) compared to the re-ranking models and follow a low-latency pattern. On CPU, the sparse models dominate in terms of speed (20-25ms).

Tradeoff between performance and index sizes Lexical, re-ranking and dense methods have the smallest index sizes (< 3GB) to store 1M documents from DBPedia. SPARTA requires the second largest index to store a 30k dim sparse vector while ColBERT requires the largest index as it stores multiple 128 dim

|      | DBPedia [19] (1 Million) |      | Retrieval Latency | Index  |       |  |
|------|--------------------------|------|-------------------|--------|-------|--|
| Rank | Model                    | Dim. | GPU               | CPU    | Size  |  |
| (1)  | BM25+CE                  | –    | 450ms             | 6100ms | 0.4GB |  |
| (2)  | ColBERT                  | 128  | 350ms             | –      | 20GB  |  |
| (3)  | docT5query               | –    | –                 | 30ms   | 0.4GB |  |
| (4)  | BM25                     | –    | –                 | 20ms   | 0.4GB |  |
| (5)  | TAS-B                    | 768  | 14ms              | 125ms  | 3GB   |  |
| (6)  | GenQ                     | 768  | 14ms              | 125ms  | 3GB   |  |
| (7)  | ANCE                     | 768  | 20ms              | 275ms  | 3GB   |  |
| (8)  | SPARTA                   | 2000 | –                 | 20ms   | 12GB  |  |
| (9)  | DeepCT                   | –    | –                 | 25ms   | 0.4GB |  |
| (10) | DPR                      | 768  | 19ms              | 230ms  | 3GB   |  |

Table 3: Estimated average retrieval latency and index sizes for a single query in DBPedia [\[19\]](#page-11-4). Ranked from best to worst on zero-shot BEIR. Lower the latency or memory is desired.

dense vectors for a single document. Index sizes are especially relevant when document sizes scale higher: ColBERT requires ~900GB to store the BioASQ (~15M documents) index, whereas BM25 only requires 18GB.

<span id="page-8-2"></span><span id="page-8-1"></span>

| Model (→)                                                                  | BM25<br>DeepCT | SPARTA | docT5query | DPR   | ANCE  | TAS-B | ColBERT | BM25+CE |  |  |
|----------------------------------------------------------------------------|----------------|--------|------------|-------|-------|-------|---------|---------|--|--|
| Hole@10 (in %)                                                             | 6.4%<br>19.4%  | 12.4%  | 2.8%       | 30.6% | 14.4% | 31.8% | 12.4%   | 1.6%    |  |  |
| nDCG@10 performances before and after manual annotation on TREC-COVID [63] |                |        |            |       |       |       |         |         |  |  |
| Original (w/ holes)                                                        | 0.656<br>0.406 | 0.538  | 0.713      | 0.332 | 0.654 | 0.481 | 0.677   | 0.757   |  |  |
| Annotated (w/o holes)                                                      | 0.668<br>0.472 | 0.624  | 0.714      | 0.445 | 0.735 | 0.555 | 0.735   | 0.760   |  |  |

Table 4: Hole@10 analysis on TREC-COVID. Annotated scores show improvement in performances after removing holes@10 (documents in top-10 hits unseen by annotators) across each model.

## <span id="page-8-0"></span>6 Impact of Annotation Selection Bias

Creating a perfectly unbiased evaluation dataset for retrieval is inherently complex and is subject to multiple biases induced by the: (*i*) annotation guidelines, (*ii*) annotation setup, and by the (*iii*) human annotators. Further, it is impossible to manually annotate the relevance for all (query, document)-pairs. Instead, existing retrieval methods are used to get a pool of candidate documents which are then marked for their relevance. All other unseen documents are assumed to be irrelevant. This is a source for *selection bias* [\[36\]](#page-12-11): A new retrieval system might retrieve vastly different results than the system used for the annotation. These hits are automatically assumed to be irrelevant.

Many BEIR datasets are found to be subject to a lexical bias, i.e. a lexical based retrieval system like TF-IDF or BM25 has been used to retrieve the candidates for annotation. For example, in BioASQ, candidates have been retrieved for annotation via term-matching with boosting tags [\[59\]](#page-14-2). Creation of Signal-1M (RT) involved retrieving tweets for a query with 7 out of these 8 techniques relying upon lexical term-matching signals [\[57\]](#page-13-7). Such a lexical bias disfavours approaches that don't rely on lexical matching, like dense retrieval methods, as retrieved hits without lexical overlap are automatically assumed to be irrelevant, even though the hits might be relevant for a query.

In order to study the impact of this particular type of bias, we conducted a study on the recent TREC-COVID dataset. TREC-COVID used a pooling method [\[35,](#page-12-12) [37\]](#page-12-13) to reduce the impact of the aforementioned bias: The annotation set was constructed by using the search results from the various systems participating in the challenge. Table [4](#page-8-1) shows the Hole@10 rate [\[71\]](#page-15-6) for the tested systems, i.e., how many top-10 hits is each system retrieving that have not been seen by annotators.

The results reveal large differences between approaches: Lexical approaches like BM25 and docT5query have a rather low Hole@10 value of 6.4% and 2.8%, indicating that the annotation pool contained the top-hits from lexical retrieval systems. In contrast, dense retrieval systems like ANCE and TAS-B have a much higher Hole@10 of 14.4% and 31.8%, indicating that a large fraction of hits found by these systems have not been judged by annotators. Next, we manually added for all systems, the missing annotation (or holes) following the original annotation guidelines. During annotation, we were unaware of the system who retrieved the missing annotation to avoid a preference bias. In total, we annotated 980 query-document pairs in TREC-COVID. We then re-computed nDCG@10 for all systems with this additional annotations.

As shown in Table [4,](#page-8-1) we observe that lexical approaches improves only slightly, e.g. for docT5query just from 0.713 to 0.714 after adding the missing relevance judgements. In contrast, for the dense retrieval system ANCE, the performance improves from 0.654 (slightly below BM25) to 0.735, which is 6.7 points above the BM25 performance. Similar improvements are noticed in ColBERT (5.8 points). Even though many systems contributed to the TREC-COVID annotation pool, the annotation pool is still biased towards lexical approaches.

## 7 Conclusions and Future Work

In this work, we presented BEIR: a heterogeneous benchmark for information retrieval. We provided a broader selection of target tasks ranging from narrow expert domains to open domain datasets. We included nine different retrieval tasks spanning 18 diverse datasets.

By open-sourcing BEIR, with a standardized data format and easy-to-adapt code examples for many different retrieval strategies, we take an important steps towards a unified benchmark to evaluate the zero-shot capabilities of retrieval systems. It hopefully steers innovation towards more robust retrieval systems and to new insights which retrieval architectures perform well across tasks and domains.

We studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches

<span id="page-9-0"></span>that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets. Cross-attentional re-ranking, late-interaction ColBERT, and the document expansion technique docT5query performed overall well across the evaluated tasks.

Our study of the annotation selection bias highlights the challenge of evaluating new models on existing datasets: Even though TREC-COVID is based on the predictions from many systems, contributed by a diverse set of teams, we found largely different *Hole@10* rates for the tested systems, negatively affecting non-lexical approaches. Better datasets that use diverse pooling strategies are needed for a fair evaluation of retrieval approaches. By integrating a large number of diverse retrieval systems into BEIR, creating such diverse pools becomes significantly simplified.

# 8 Limitations of the BEIR Benchmark

Even though we cover a wide range of tasks and domains in BEIR, no benchmark is perfect and has its limitations. Making those explicit is a critical point in understanding the results on the benchmark and, for future work, to propose even better benchmarks.

1. Multilingual Tasks: Although we aim for a diverse retrieval evaluation benchmark, due to the limited availability of multilingual retrieval datasets, all datasets covered in the BEIR benchmark are currently English. It is worthwhile to add more multilingual datasets [\[2,](#page-10-9) [76\]](#page-15-7) (in consideration of the selection criteria) as a next step for the benchmark. Future work could include multi- and cross-lingual tasks and models.

2. Long Document Retrieval: Most of our tasks have average document lengths up-to a few hundred words roughly equivalent to a few paragraphs. Including tasks that require the retrieval of longer documents would be highly relevant. However, as transformer-based approaches often have a length limit of 512 word pieces, a fundamental different setup would be required to compare approaches.

3. Multi-factor Search: Until now, we focused on pure textual search in BEIR. In many real-world applications, further signals are used to estimate the relevancy of documents, such as PageRank [\[46\]](#page-13-12), recency [\[14\]](#page-11-13), authority score [\[31\]](#page-12-14) or user-interactions such as click-through rates [\[49\]](#page-13-13). The integration of such signals in the tested approaches is often not straight-forward and is an interesting direction for research.

4. Multi-field Retrieval: Retrieval can often be performed over multiple fields. For example, for scientific publication we have the title, the abstract, the document body, the authors list, and the journal name. So far we focused only on datasets that have one or two fields.

5. Task-specific Models: In our benchmark, we focus on evaluating models that are able to generalize well for a broad range of retrieval tasks. Naturally in real-world, for some few tasks or domains, specialized models are available which can easily outperform generic models as they focus and perform well on a single task, lets say on question-answering. Such task-specific models do not necessarily need to generalize across all diverse tasks.

# 9 Acknowledgements

This work has been supported by the German Research Foundation (DFG) as part of the QASciInf project (grant GU 798/18-3), and the UKP SQuARE project (grant GU 798/29-1). The work has been funded by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE. We would like to thank Kexin Wang, Tim Baumgärtner, Leonardo Riberio, Luke Bates, Jan Buchmann for their helpful feedback and participation in the weekly research meetings. Additionally, we would like to thank Chenyan Xiong, Christopher Potts and Sean Macavenay for their constructive feedback and suggestions on Twitter.

## References

- [1] Amin Ahmad, Noah Constant, Yinfei Yang, and Daniel Cer. 2019. [ReQA: An Evaluation for](https://doi.org/10.18653/v1/D19-5819) [End-to-End Answer Retrieval Models.](https://doi.org/10.18653/v1/D19-5819) In *Proceedings of the 2nd Workshop on Machine Reading for Question Answering*, pages 137–146, Hong Kong, China. Association for Computational Linguistics. 17
- <span id="page-10-9"></span>[2] Akari Asai, Jungo Kasai, Jonathan Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. 2021. [XOR QA: Cross-lingual Open-Retrieval Question Answering.](https://doi.org/10.18653/v1/2021.naacl-main.46) In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 547–564, Online. Association for Computational Linguistics. [10](#page-9-0)
- <span id="page-10-8"></span>[3] Petr Baudiš and Jan Šedivy. 2015. ` [Modeling of the question answering task in the yodaqa](https://dl.acm.org/doi/10.1007/978-3-319-24027-5_20) [system.](https://dl.acm.org/doi/10.1007/978-3-319-24027-5_20) In *International Conference of the Cross-Language Evaluation Forum for European Languages*, pages 222–228. Springer. [6](#page-5-0)
- <span id="page-10-7"></span>[4] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. [Semantic Parsing on](https://www.aclweb.org/anthology/D13-1160) [Freebase from Question-Answer Pairs.](https://www.aclweb.org/anthology/D13-1160) In *Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing*, pages 1533–1544, Seattle, Washington, USA. Association for Computational Linguistics. [6](#page-5-0)
- <span id="page-10-1"></span>[5] Adam Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu Mittal. 2000. [Bridging](http://www.cs.cmu.edu/~aberger/pdf/chasm.pdf) [the lexical chasm: statistical approaches to answer-finding.](http://www.cs.cmu.edu/~aberger/pdf/chasm.pdf) In *Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval*, pages 192–199. [1,](#page-0-0) [3](#page-2-1)
- <span id="page-10-5"></span>[6] Alexander Bondarenko, Maik Fröbe, Meriem Beloucif, Lukas Gienapp, Yamen Ajjour, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, Martin Potthast, and Matthias Hagen. 2020. [Overview of Touché 2020: Argument Retrieval.](http://ceur-ws.org/Vol-2696/) In *Working Notes Papers of the CLEF 2020 Evaluation Labs*, volume 2696 of *CEUR Workshop Proceedings*. [4,](#page-3-3) 18, 22
- <span id="page-10-4"></span>[7] Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. 2016. [A full-text learning to](http://www.cl.uni-heidelberg.de/~riezler/publications/papers/ECIR2016.pdf) [rank dataset for medical information retrieval.](http://www.cl.uni-heidelberg.de/~riezler/publications/papers/ECIR2016.pdf) In *Proceedings of the 38th European Conference on Information Retrieval (ECIR 2016)*, pages 716–722. [4,](#page-3-3) 17
- <span id="page-10-0"></span>[8] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. [Reading Wikipedia to](https://doi.org/10.18653/v1/P17-1171) [Answer Open-Domain Questions.](https://doi.org/10.18653/v1/P17-1171) In *Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 1870–1879, Vancouver, Canada. Association for Computational Linguistics. [1,](#page-0-0) 17
- <span id="page-10-6"></span>[9] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. 2020. [SPECTER:](https://doi.org/10.18653/v1/2020.acl-main.207) [Document-level Representation Learning using Citation-informed Transformers.](https://doi.org/10.18653/v1/2020.acl-main.207) In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 2270–2282, Online. Association for Computational Linguistics. [4,](#page-3-3) 19
- [10] Davind Corney, Dyaa Albakour, Miguel Martinez, and Samir Moussa. 2016. [What do a Million](http://ceur-ws.org/Vol-1568/paper8.pdf) [News Articles Look like?](http://ceur-ws.org/Vol-1568/paper8.pdf) In *Proceedings of the First International Workshop on Recent Trends in News Information Retrieval co-located with 38th European Conference on Information Retrieval (ECIR 2016)*, pages 42–47. 18
- <span id="page-10-3"></span>[11] Zhuyun Dai and Jamie Callan. 2020. [Context-Aware Term Weighting For First Stage Passage](https://doi.org/10.1145/3397271.3401204) [Retrieval.](https://doi.org/10.1145/3397271.3401204) In *Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval*, SIGIR '20, page 1533–1536, New York, NY, USA. Association for Computing Machinery. [3,](#page-2-1) [6](#page-5-0)
- <span id="page-10-2"></span>[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. [BERT: Pre](https://doi.org/10.18653/v1/N19-1423)[training of Deep Bidirectional Transformers for Language Understanding.](https://doi.org/10.18653/v1/N19-1423) In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 4171– 4186, Minneapolis, Minnesota. Association for Computational Linguistics. [1,](#page-0-0) [3](#page-2-1)
- <span id="page-11-5"></span>[13] Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. 2020. [Climate-FEVER: A Dataset for Verification of Real-World Climate Claims.](https://www.climatechange.ai/papers/neurips2020/67) In *Tackling Climate Change with Machine Learning workshop at NeurIPS 2020*. [4,](#page-3-3) 19
- <span id="page-11-13"></span>[14] Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and Hongyuan Zha. 2010. [Time is of the Essence: Improving Recency Ranking Using](https://doi.org/10.1145/1772690.1772725) [Twitter Data.](https://doi.org/10.1145/1772690.1772725) In *Proceedings of the 19th International Conference on World Wide Web*, WWW '10, page 331–340, New York, NY, USA. Association for Computing Machinery. [10](#page-9-0)
- <span id="page-11-2"></span>[15] Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, and Jamie Callan. 2021. [Complement Lexical Retrieval Model with Semantic Residual Embeddings.](https://link.springer.com/chapter/10.1007/978-3-030-72113-8_10) In *Advances in Information Retrieval*, pages 146–160, Cham. Springer International Publishing. [3,](#page-2-1) 17
- <span id="page-11-1"></span>[16] Daniel Gillick, Alessandro Presta, and Gaurav Singh Tomar. 2018. [End-to-End Retrieval in](https://arxiv.org/abs/1811.08008) [Continuous Space.](https://arxiv.org/abs/1811.08008) *arXiv preprint arXiv:1811.08008*. [3](#page-2-1)
- <span id="page-11-0"></span>[17] Mandy Guo, Yinfei Yang, Daniel Cer, Qinlan Shen, and Noah Constant. 2020. [Multi-](https://arxiv.org/abs/2005.02507)[ReQA: A Cross-Domain Evaluation for Retrieval Question Answering Models.](https://arxiv.org/abs/2005.02507) *arXiv preprint arXiv:2005.02507*. [2,](#page-1-1) [3](#page-2-1)
- <span id="page-11-7"></span>[18] Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. 2008. [Exploring Network Structure, Dy](https://www.osti.gov/biblio/960616)[namics, and Function using NetworkX.](https://www.osti.gov/biblio/960616) In *Proceedings of the 7th Python in Science Conference*, pages 11 – 15, Pasadena, CA USA. [5](#page-4-1)
- <span id="page-11-4"></span>[19] Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. 2017. [DBpedia-Entity V2: A Test Collection for Entity](https://doi.org/10.1145/3077136.3080751) [Search.](https://doi.org/10.1145/3077136.3080751) In *Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval*, SIGIR '17, pages 1265–1268. ACM. [4,](#page-3-3) [8,](#page-7-1) 19
- <span id="page-11-12"></span>[20] Jerry L. Hintze and Ray D. Nelson. 1998. [Violin Plots: A Box Plot-Density Trace Synergism.](http://www.jstor.org/stable/2685478) *The American Statistician*, 52(2):181–184. [8,](#page-7-1) 24
- <span id="page-11-9"></span>[21] Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. [Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling.](https://arxiv.org/abs/2104.06967) In *Proc. of SIGIR*. [6](#page-5-0)
- <span id="page-11-10"></span>[22] Sebastian Hofstätter, Sophia Althammer, Michael Schröder, Mete Sertkan, and Allan Hanbury. 2021. [Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge](https://arxiv.org/abs/2010.02666) [Distillation.](https://arxiv.org/abs/2010.02666) *arXiv preprint arXiv:2010.02666*. [6,](#page-5-0) 20
- <span id="page-11-3"></span>[23] Doris Hoogeveen, Karin M Verspoor, and Timothy Baldwin. 2015. [CQADupStack: A bench](https://people.eng.unimelb.edu.au/tbaldwin/pubs/adcs2015.pdf)[mark data set for community question-answering research.](https://people.eng.unimelb.edu.au/tbaldwin/pubs/adcs2015.pdf) In *Proceedings of the 20th Australasian document computing symposium*, pages 1–8. [4,](#page-3-3) 18
- <span id="page-11-6"></span>[24] Sergey Ioffe. 2010. [Improved consistent sampling, weighted minhash and l1 sketching.](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36928.pdf) In *2010 IEEE International Conference on Data Mining*, pages 246–255. IEEE. [4,](#page-3-3) 20
- [25] Ming Ji, Yizhou Sun, Marina Danilevsky, Jiawei Han, and Jing Gao. 2010. [Graph Regularized](https://link.springer.com/chapter/10.1007/978-3-642-15880-3_42) [Transductive Classification on Heterogeneous Information Networks.](https://link.springer.com/chapter/10.1007/978-3-642-15880-3_42) In *Machine Learning and Knowledge Discovery in Databases*, pages 570–586, Berlin, Heidelberg. Springer Berlin Heidelberg. 19
- [26] Jing Jiang and ChengXiang Zhai. 2007. [An empirical study of tokenization strategies for](https://link.springer.com/article/10.1007/s10791-007-9027-7) [biomedical information retrieval.](https://link.springer.com/article/10.1007/s10791-007-9027-7) *Information Retrieval*, 10(4-5):341–363. 17
- <span id="page-11-11"></span>[27] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. [Billion-scale similarity search with](https://arxiv.org/abs/1702.08734) [GPUs.](https://arxiv.org/abs/1702.08734) *arXiv preprint arXiv:1702.08734*. [6](#page-5-0)
- <span id="page-11-8"></span>[28] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. [TriviaQA: A Large](https://doi.org/10.18653/v1/P17-1147) [Scale Distantly Supervised Challenge Dataset for Reading Comprehension.](https://doi.org/10.18653/v1/P17-1147) In *Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 1601–1611, Vancouver, Canada. Association for Computational Linguistics. [6](#page-5-0)
- <span id="page-12-2"></span>[29] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. [Dense Passage Retrieval for Open-Domain Question](https://doi.org/10.18653/v1/2020.emnlp-main.550) [Answering.](https://doi.org/10.18653/v1/2020.emnlp-main.550) In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 6769–6781, Online. Association for Computational Linguistics. [1,](#page-0-0) [3,](#page-2-1) [4,](#page-3-3) [6](#page-5-0)
- <span id="page-12-7"></span>[30] Omar Khattab and Matei Zaharia. 2020. [ColBERT: Efficient and Effective Passage Search via](https://doi.org/10.1145/3397271.3401075) [Contextualized Late Interaction over BERT.](https://doi.org/10.1145/3397271.3401075) In *Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval*, SIGIR '20, page 39–48, New York, NY, USA. Association for Computing Machinery. [3,](#page-2-1) [4,](#page-3-3) [6,](#page-5-0) [8](#page-7-1)
- <span id="page-12-14"></span>[31] Jon M. Kleinberg. 1999. [Authoritative Sources in a Hyperlinked Environment.](https://doi.org/10.1145/324133.324140) *J. ACM*, 46(5):604–632. [10](#page-9-0)
- <span id="page-12-0"></span>[32] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. [Natural Questions: a Benchmark for Question Answering Research.](https://aclanthology.org/Q19-1026/) *Transactions of the Association of Computational Linguistics*. [1,](#page-0-0) [4,](#page-3-3) [6,](#page-5-0) 17
- <span id="page-12-4"></span>[33] Davis Liang, Peng Xu, Siamak Shakeri, Cicero Nogueira dos Santos, Ramesh Nallapati, Zhiheng Huang, and Bing Xiang. 2020. [Embedding-based Zero-shot Retrieval through Query Generation.](https://arxiv.org/abs/2009.10270) *arXiv preprint arXiv:2009.10270*. [3](#page-2-1)
- <span id="page-12-9"></span>[34] Jimmy Lin, Matt Crane, Andrew Trotman, Jamie Callan, Ishan Chattopadhyaya, John Foley, Grant Ingersoll, Craig Macdonald, and Sebastiano Vigna. 2016. [Toward reproducible baselines:](https://link.springer.com/chapter/10.1007/978-3-319-30671-1_30) [The open-source IR reproducibility challenge.](https://link.springer.com/chapter/10.1007/978-3-319-30671-1_30) In *European Conference on Information Retrieval*, pages 408–420. Springer. [5](#page-4-1)
- <span id="page-12-12"></span>[35] Aldo Lipani. 2016. [Fairness in Information Retrieval.](https://doi.org/10.1145/2911451.2911473) In *Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval*, SIGIR '16, page 1171, New York, NY, USA. Association for Computing Machinery. [9](#page-8-2)
- <span id="page-12-11"></span>[36] Aldo Lipani. 2019. [On Biases in Information Retrieval Models and Evaluation.](https://doi.org/10.1145/3308774.3308804) *SIGIR Forum*, 52(2):172–173. [9](#page-8-2)
- <span id="page-12-13"></span>[37] Aldo Lipani, Mihai Lupu, and Allan Hanbury. 2016. [The Curious Incidence of Bias Corrections](https://publik.tuwien.ac.at/files/publik_253799.pdf) [in the Pool.](https://publik.tuwien.ac.at/files/publik_253799.pdf) In *European Conference on Information Retrieval*, pages 267–279. Springer. [9](#page-8-2)
- <span id="page-12-10"></span>[38] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. [RoBERTa: A Robustly Optimized BERT](https://arxiv.org/abs/1907.11692) [Pretraining Approach.](https://arxiv.org/abs/1907.11692) *arXiv preprint arXiv:1907.11692*. [6](#page-5-0)
- <span id="page-12-6"></span>[39] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. [Sparse, Dense, and](https://arxiv.org/abs/2005.00181) [Attentional Representations for Text Retrieval.](https://arxiv.org/abs/2005.00181) *Transactions of the Association for Computational Linguistics*, 9:329–345. [3](#page-2-1)
- <span id="page-12-5"></span>[40] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. [Zero-shot Neural](https://aclanthology.org/2021.eacl-main.92) [Passage Retrieval via Domain-targeted Synthetic Question Generation.](https://aclanthology.org/2021.eacl-main.92) In *Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume*, pages 1075–1088, Online. Association for Computational Linguistics. [3](#page-2-1)
- <span id="page-12-8"></span>[41] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018. [WWW'18 Open Challenge: Financial Opinion Mining](https://doi.org/10.1145/3184558.3192301) [and Question Answering.](https://doi.org/10.1145/3184558.3192301) In *Companion Proceedings of the The Web Conference 2018*, WWW '18, page 1941–1942, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee. [4,](#page-3-3) 18
- <span id="page-12-1"></span>[42] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. [MS MARCO: A Human Generated MAchine Reading COmprehension Dataset.](https://www.microsoft.com/en-us/research/publication/ms-marco-human-generated-machine-reading-comprehension-dataset/) [1,](#page-0-0) [3,](#page-2-1) [4,](#page-3-3) [6,](#page-5-0) 17
- <span id="page-12-3"></span>[43] Rodrigo Nogueira and Kyunghyun Cho. 2020. [Passage Re-ranking with BERT.](http://arxiv.org/abs/1901.04085) *arXiv preprint arXiv:1901.04085*. [1,](#page-0-0) [3,](#page-2-1) 17
- <span id="page-13-10"></span>[44] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. [From doc2query to docTTTTTquery.](https://cs.uwaterloo.ca/~jimmylin/publications/Nogueira_Lin_2019_docTTTTTquery-v2.pdf) *Online preprint*. [6](#page-5-0)
- <span id="page-13-4"></span>[45] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. [Document Expansion by](https://arxiv.org/abs/1904.08375) [Query Prediction.](https://arxiv.org/abs/1904.08375) *arXiv preprint arXiv:1904.08375*. [3](#page-2-1)
- <span id="page-13-12"></span>[46] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. [The PageRank](http://ilpubs.stanford.edu:8090/422/) [Citation Ranking: Bringing Order to the Web.](http://ilpubs.stanford.edu:8090/422/) Technical Report 1999-66, Stanford InfoLab. Previous number = SIDL-WP-1999-0120. [10](#page-9-0)
- <span id="page-13-2"></span>[47] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. [KILT: a Benchmark for Knowledge Intensive Lan](https://doi.org/10.18653/v1/2021.naacl-main.200)[guage Tasks.](https://doi.org/10.18653/v1/2021.naacl-main.200) In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 2523–2544, Online. Association for Computational Linguistics. [2](#page-1-1)
- <span id="page-13-1"></span>[48] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. [RocketQA: An Optimized Training Approach to Dense Passage](https://doi.org/10.18653/v1/2021.naacl-main.466) [Retrieval for Open-Domain Question Answering.](https://doi.org/10.18653/v1/2021.naacl-main.466) In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 5835–5847, Online. Association for Computational Linguistics. [1,](#page-0-0) 17
- <span id="page-13-13"></span>[49] Filip Radlinski, Madhu Kurup, and Thorsten Joachims. 2008. [How Does Clickthrough Data](https://doi.org/10.1145/1458082.1458092) [Reflect Retrieval Quality?](https://doi.org/10.1145/1458082.1458092) In *Proceedings of the 17th ACM Conference on Information and Knowledge Management*, CIKM '08, page 43–52, New York, NY, USA. Association for Computing Machinery. [10](#page-9-0)
- <span id="page-13-11"></span>[50] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. [Exploring the Limits of Transfer Learning with a](http://jmlr.org/papers/v21/20-074.html) [Unified Text-to-Text Transformer.](http://jmlr.org/papers/v21/20-074.html) *Journal of Machine Learning Research*, 21(140):1–67. [6](#page-5-0)
- <span id="page-13-6"></span>[51] Nils Reimers and Iryna Gurevych. 2019. [Sentence-BERT: Sentence Embeddings using Siamese](https://doi.org/10.18653/v1/D19-1410) [BERT-Networks.](https://doi.org/10.18653/v1/D19-1410) In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 3982–3992, Hong Kong, China. Association for Computational Linguistics. [3,](#page-2-1) [4](#page-3-3)
- <span id="page-13-3"></span>[52] Nils Reimers and Iryna Gurevych. 2021. [The Curse of Dense Low-Dimensional Information](https://doi.org/10.18653/v1/2021.acl-short.77) [Retrieval for Large Index Sizes.](https://doi.org/10.18653/v1/2021.acl-short.77) In *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)*, pages 605–611, Online. Association for Computational Linguistics. [2](#page-1-1)
- <span id="page-13-0"></span>[53] Stephen Robertson and Hugo Zaragoza. 2009. [The Probabilistic Relevance Framework: BM25](https://doi.org/10.1561/1500000019) [and Beyond.](https://doi.org/10.1561/1500000019) *Foundations and Trends in Information Retrieval*, 3(4):333–389. [1,](#page-0-0) [3,](#page-2-1) [5](#page-4-1)
- <span id="page-13-9"></span>[54] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. [DistilBERT, a](https://arxiv.org/abs/1910.01108) [distilled version of BERT: smaller, faster, cheaper and lighter.](https://arxiv.org/abs/1910.01108) In *5th Workshop on Energy Efficient Machine Learning and Cognitive Computing at NeurIPS 2019*. [6](#page-5-0)
- <span id="page-13-5"></span>[55] Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. [Real-Time Open-Domain Question Answering with Dense-Sparse Phrase](https://doi.org/10.18653/v1/P19-1436) [Index.](https://doi.org/10.18653/v1/P19-1436) In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pages 4430–4441, Florence, Italy. Association for Computational Linguistics. [3](#page-2-1)
- <span id="page-13-8"></span>[56] Ian Soboroff, Shudong Huang, and Donna Harman. 2019. [TREC 2019 News Track Overview.](https://trec.nist.gov/pubs/trec28/papers/OVERVIEW.N.pdf) In *TREC*. [4,](#page-3-3) 18
- <span id="page-13-7"></span>[57] Axel Suarez, Dyaa Albakour, David Corney, Miguel Martinez, and Jose Esquivel. 2018. [A Data](https://link.springer.com/chapter/10.1007/978-3-319-76941-7_76) [Collection for Evaluating the Retrieval of Related Tweets to News Articles.](https://link.springer.com/chapter/10.1007/978-3-319-76941-7_76) In *40th European Conference on Information Retrieval Research (ECIR 2018), Grenoble, France, March, 2018.*, pages 780–786. [4,](#page-3-3) [9,](#page-8-2) 18
- <span id="page-14-0"></span>[58] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. [FEVER:](https://doi.org/10.18653/v1/N18-1074) [a Large-scale Dataset for Fact Extraction and VERification.](https://doi.org/10.18653/v1/N18-1074) In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 809–819, New Orleans, Louisiana. Association for Computational Linguistics. [1,](#page-0-0) [4,](#page-3-3) 19
- <span id="page-14-2"></span>[59] George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et al. 2015. [An overview of the BIOASQ large-scale biomedical semantic](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0564-6) [indexing and question answering competition.](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0564-6) *BMC bioinformatics*, 16(1):138. [4,](#page-3-3) [9,](#page-8-2) 17
- [60] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019. [Representation Learning with](https://arxiv.org/abs/1807.03748) [Contrastive Predictive Coding.](https://arxiv.org/abs/1807.03748) *arXiv preprint arXiv:1807.03748*. 20
- <span id="page-14-8"></span>[61] Christophe Van Gysel and Maarten de Rijke. 2018. [Pytrec\\_eval: An Extremely Fast Python](https://arxiv.org/abs/1805.01597) [Interface to trec\\_eval.](https://arxiv.org/abs/1805.01597) In *SIGIR*. ACM. [5](#page-4-1)
- <span id="page-14-3"></span>[62] Ellen Voorhees. 2005. [Overview of the TREC 2004 Robust Retrieval Track.](https://doi.org/https://doi.org/10.6028/NIST.SP.500-261) Special Publication (NIST SP), National Institute of Standards and Technology, Gaithersburg, MD. [4,](#page-3-3) 18
- <span id="page-14-1"></span>[63] Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R. Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021. [TREC-COVID: Constructing a](https://doi.org/10.1145/3451964.3451965) [Pandemic Information Retrieval Test Collection.](https://doi.org/10.1145/3451964.3451965) *SIGIR Forum*, 54(1). [2,](#page-1-1) [4,](#page-3-3) [9,](#page-8-2) 17
- [64] Henning Wachsmuth, Martin Potthast, Khalid Al-Khatib, Yamen Ajjour, Jana Puschmann, Jiani Qu, Jonas Dorsch, Viorel Morari, Janek Bevendorff, and Benno Stein. 2017. [Building an](https://www.aclweb.org/anthology/W17-5106) [Argument Search Engine for the Web.](https://www.aclweb.org/anthology/W17-5106) In *4th Workshop on Argument Mining (ArgMining 2017) at EMNLP*, pages 49–59. Association for Computational Linguistics. 18
- <span id="page-14-4"></span>[65] Henning Wachsmuth, Shahbaz Syed, and Benno Stein. 2018. [Retrieval of the Best Coun](http://aclweb.org/anthology/P18-1023)[terargument without Prior Topic Knowledge.](http://aclweb.org/anthology/P18-1023) In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 241–251. Association for Computational Linguistics. [4,](#page-3-3) 18
- <span id="page-14-5"></span>[66] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. [Fact or Fiction: Verifying Scientific Claims.](https://doi.org/10.18653/v1/2020.emnlp-main.609) In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 7534–7550, Online. Association for Computational Linguistics. [4,](#page-3-3) 19
- [67] Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Doug Burdick, Darrin Eide, Kathryn Funk, Yannis Katsis, Rodney Michael Kinney, Yunyao Li, Ziyang Liu, William Merrill, Paul Mooney, Dewey A. Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex D. Wade, Kuansan Wang, Nancy Xin Ru Wang, Christopher Wilhelm, Boya Xie, Douglas M. Raymond, Daniel S. Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020. [CORD-19: The COVID-19 Open Research Dataset.](https://aclanthology.org/2020.nlpcovid19-acl.1) In *Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020*, Online. Association for Computational Linguistics. 17
- <span id="page-14-9"></span>[68] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. [MiniLM:](https://proceedings.neurips.cc/paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) [Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers.](https://proceedings.neurips.cc/paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) In *Advances in Neural Information Processing Systems*, volume 33, pages 5776–5788. Curran Associates, Inc. [6](#page-5-0)
- <span id="page-14-7"></span>[69] Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, and Tie-Yan Liu. 2013. [A theoretical](http://proceedings.mlr.press/v30/Wang13.pdf) [analysis of NDCG ranking measures.](http://proceedings.mlr.press/v30/Wang13.pdf) In *Proceedings of the 26th annual conference on learning theory (COLT 2013)*, volume 8, page 6. [5](#page-4-1)
- <span id="page-14-6"></span>[70] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. [Transformers:](https://www.aclweb.org/anthology/2020.emnlp-demos.6) [State-of-the-Art Natural Language Processing.](https://www.aclweb.org/anthology/2020.emnlp-demos.6) In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pages 38–45, Online. Association for Computational Linguistics. [4](#page-3-3)
- <span id="page-15-6"></span>[71] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. [Approximate Nearest Neighbor Negative Contrastive](https://openreview.net/forum?id=zeFrfgyZln) [Learning for Dense Text Retrieval.](https://openreview.net/forum?id=zeFrfgyZln) In *International Conference on Learning Representations*. [6,](#page-5-0) [9](#page-8-2)
- <span id="page-15-4"></span>[72] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. [Anserini: Enabling the Use of Lucene for Infor](https://doi.org/10.1145/3077136.3080721)[mation Retrieval Research.](https://doi.org/10.1145/3077136.3080721) In *Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval*, SIGIR '17, page 1253–1256, New York, NY, USA. Association for Computing Machinery. [4](#page-3-3)
- <span id="page-15-5"></span>[73] Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-hsuan Sung, Brian Strope, and Ray Kurzweil. 2020. [Multilingual Universal Sentence Encoder for Semantic Retrieval.](https://doi.org/10.18653/v1/2020.acl-demos.12) In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations*, pages 87–94, Online. Association for Computational Linguistics. [4](#page-3-3)
- <span id="page-15-3"></span>[74] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. [HotpotQA: A Dataset for Diverse, Explainable Multi-hop](https://doi.org/10.18653/v1/D18-1259) [Question Answering.](https://doi.org/10.18653/v1/D18-1259) In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 2369–2380, Brussels, Belgium. Association for Computational Linguistics. [4,](#page-3-3) 18
- <span id="page-15-1"></span>[75] Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. [Pretrained Transformers for Text](https://doi.org/10.1145/3437963.3441667) [Ranking: BERT and Beyond.](https://doi.org/10.1145/3437963.3441667) In *Proceedings of the 14th ACM International Conference on Web Search and Data Mining*, WSDM '21, page 1154–1156, New York, NY, USA. Association for Computing Machinery. [1,](#page-0-0) [3](#page-2-1)
- <span id="page-15-7"></span>[76] Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021. [Mr. TyDi: A Multi-lingual](https://arxiv.org/abs/2108.08787) [Benchmark for Dense Retrieval.](https://arxiv.org/abs/2108.08787) *arXiv preprint arXiv:2108.08787*. [10](#page-9-0)
- <span id="page-15-0"></span>[77] Yun Zhang, David Lo, Xin Xia, and Jian-Ling Sun. 2015. [Multi-factor duplicate question](https://link.springer.com/content/pdf/10.1007/s11390-015-1576-4.pdf) [detection in stack overflow.](https://link.springer.com/content/pdf/10.1007/s11390-015-1576-4.pdf) *Journal of Computer Science and Technology*, 30(5):981–997. [1](#page-0-0)
- <span id="page-15-2"></span>[78] Tiancheng Zhao, Xiaopeng Lu, and Kyusong Lee. 2021. [SPARTA: Efficient Open-Domain](https://doi.org/10.18653/v1/2021.naacl-main.47) [Question Answering via Sparse Transformer Matching Retrieval.](https://doi.org/10.18653/v1/2021.naacl-main.47) In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 565–575, Online. Association for Computational Linguistics. [3,](#page-2-1) [6](#page-5-0)